## Confusores, artefactos y otras amenazas a la validez

Si consideramos el tema de la validez en términos generales, las dos grandes preocupaciones que tenemos son los *confusores* y los *artefactos*. Estos dos términos se definen de la siguiente manera:

- **Confusor**: Un confusor es una variable adicional, a menudo no medida, que resulta estar relacionada tanto con las variables predictoras como con los resultados. La existencia de confusores amenaza la validez interna del estudio, porque no podés saber si la variable predictora causa el resultado, o si lo causa la variable confusora, etc.

- **Artefacto**: Se dice que un resultado es “artefactual” si solo se sostiene en la situación especial que se usó para hacer el estudio. La posibilidad de que tu resultado sea un artefacto representa una amenaza a la validez externa, porque plantea la posibilidad de que no puedas generalizar tus resultados a la población real que te interesa.

Como regla general, los confusores son una preocupación mayor en los estudios no experimentales, precisamente porque no son experimentos propiamente dichos: por definición, estás dejando muchas cosas sin controlar, así que hay mucho margen para que los confusores se metan en tu estudio. La investigación experimental tiende a ser mucho menos vulnerable a los confusores: cuanto más control tenés sobre lo que ocurre durante el estudio, más podés evitar que aparezcan confusores.

Sin embargo, siempre hay compensaciones, y cuando empezamos a pensar en artefactos en lugar de confusores, la situación se invierte por completo. En general, los resultados artefactuales tienden a ser una preocupación mayor en los estudios experimentales que en los no experimentales. Para ver esto, ayuda entender que muchas veces los estudios son no experimentales precisamente porque la persona investigadora quiere examinar el comportamiento humano en un contexto más naturalista. Al trabajar en un contexto más cercano al mundo real, perdés control experimental (lo que te hace vulnerable a confusores), pero como tendés a estudiar como la psicología humana “en estado salvaje”, reducís las probabilidades de obtener un resultado artefactual. O, dicho de otra manera, cuando sacás la psicología del entorno natural y la llevás al laboratorio (lo que normalmente tenemos que hacer para ganar control experimental), siempre corrés el riesgo de terminar estudiando algo diferente de lo que realmente querías estudiar: lo cual es, más o menos, la definición de un artefacto.

Advertencia: lo anterior es solo una guía general. Es totalmente posible que haya confusores en un experimento, y también obtener resultados artefactuales en estudios no experimentales. Esto puede pasar por muchas razones, entre ellas, errores de la persona investigadora. En la práctica, es realmente difícil prever todo por adelantado, y hasta les investigadores más competentes cometen errores. Pero otras veces es inevitable, simplemente porque la persona investigadora tiene ética (por ejemplo, ver “abandono diferencial”).

Bien. En cierto sentido, casi cualquier amenaza a la validez puede caracterizarse como un confusor o un artefacto: son conceptos bastante vagos. Así que veamos algunos de los ejemplos más comunes...

### Efectos de historia

Los **efectos de historia** se refieren a la posibilidad de que ocurran eventos específicos durante el desarrollo del estudio que puedan influir en los resultados. Por ejemplo, algo podría pasar entre una medición pretest y una postest. O entre les participantes 23 y 24. Alternativamente, puede ser que estés revisando un estudio antiguo, que era perfectamente válido en su momento, pero el mundo ha cambiado lo suficiente desde entonces como para que sus conclusiones ya no sean confiables. Algunos ejemplos de lo que contarían como efectos de historia:

- Estás interesado en cómo la gente piensa sobre el riesgo y la incertidumbre. Comenzás a recolectar datos en diciembre de 2010. Pero encontrar participantes y juntar datos lleva tiempo, así que seguís trabajando en eso durante febrero de 2011. Lamentablemente para vos (y aún más lamentablemente para otres), las inundaciones en Queensland ocurrieron en enero de 2011, causando miles de millones de dólares en daños...

res en daños y matando a muchas personas. No sorprende que las personas evaluadas en febrero de 2011 expresen creencias bastante distintas sobre cómo manejar el riesgo que aquellas evaluadas en diciembre de 2010. ¿Cuál (si alguna) de estas refleja las creencias “verdaderas” de les participantes? Creo que la respuesta probablemente sea ambas: las inundaciones en Queensland cambiaron genuinamente las creencias del público australiano, aunque tal vez solo temporalmente. Lo importante acá es que la “historia” de las personas evaluadas en febrero es bastante distinta de la de quienes fueron evaluades en diciembre.

- Estás evaluando los efectos psicológicos de una nueva droga contra la ansiedad. Entonces lo que hacés es medir la ansiedad antes de administrar la droga (por ejemplo, mediante autoinforme y medidas fisiológicas, digamos), luego administrás la droga, y después volvés a tomar las mismas medidas. Sin embargo, en el medio, como tu laboratorio está en Los Ángeles, ocurre un terremoto, lo cual incrementa la ansiedad de les participantes.

### Efectos de maduración

Al igual que los efectos de historia, los **efectos de maduración** están fundamentalmente relacionados con el cambio a lo largo del tiempo. Sin embargo, los efectos de maduración no son una respuesta a eventos específicos. Más bien, tienen que ver con cómo cambian las personas por sí solas con el paso del tiempo: envejecemos, nos cansamos, nos aburrimos, etc. Algunos ejemplos de efectos de maduración:

- Cuando hacés investigación en psicología del desarrollo, necesitás tener en cuenta que les niñes crecen bastante rápido. Supongamos que querés averiguar si cierto truco educativo ayuda a aumentar el vocabulario en niñes de 3 años. Algo que necesitás considerar es que el tamaño del vocabulario en niñes de esa edad está creciendo a un ritmo increíble (varias palabras por día), por sí solo. Si diseñás tu estudio sin tener en cuenta este efecto de maduración, entonces no vas a poder saber si tu truco educativo realmente funciona.

- Cuando realizás un experimento muy largo en el laboratorio (digamos, algo que dura 3 horas), es muy probable que las personas empiecen a aburrirse y a cansarse, y que ese efecto de maduración va a causar una disminución en el rendimiento, independientemente de cualquier otra cosa que esté pasando en el experimento.

### Efectos de pruebas repetidas

Un tipo importante de efecto de historia es el efecto de las **pruebas repetidas**. Supongamos que quiero tomar dos mediciones de algún constructo psicológico (por ejemplo, ansiedad). Algo que podría preocuparme es si la primera medición afecta la segunda. En otras palabras, este es un efecto de historia en el que el “evento” que influye sobre la segunda medición es ¡la primera medición en sí misma! Esto no es nada raro. Algunos ejemplos de esto incluyen:

- *Aprendizaje y práctica*: por ejemplo, la “inteligencia” en el tiempo 2 podría parecer más alta que en el tiempo 1 porque les participantes aprendieron las reglas generales para resolver preguntas “tipo test de inteligencia” durante la primera sesión.

- *Familiaridad con la situación de evaluación*: por ejemplo, si las personas están nerviosas en el tiempo 1, eso podría hacer que su rendimiento baje; después de atravesar la primera sesión, podrían calmarse...

precisamente porque ya vieron cómo es la situación de evaluación.

- *Cambios auxiliares causados por la evaluación*: por ejemplo, si un cuestionario para medir el estado de ánimo es aburrido, entonces el estado de ánimo medido en el tiempo 2 probablemente sea “aburrido”, justamente por haber pasado por una medición aburrida en el tiempo 1.

### Sesgo de selección

El **sesgo de selección** es un término bastante amplio. Supongamos que estás llevando a cabo un experimento con dos grupos de participantes, donde cada grupo recibe un “tratamiento” diferente, y querés ver si esos tratamientos generan distintos resultados. Sin embargo, supongamos que, a pesar de tus mejores esfuerzos, terminás con un desequilibrio de género entre los grupos (por ejemplo, el grupo A tiene 80% mujeres y el grupo B tiene 50%). Puede sonar como algo que nunca pasaría, pero creeme, pasa. Este es un ejemplo de sesgo de selección, en el cual las personas que “fueron asignadas” a los dos grupos tienen características distintas. Si resulta que alguna de esas características es relevante (por ejemplo, que tu tratamiento funciona mejor para mujeres que para varones), entonces estás en serios problemas.

### Abandono diferencial

Un peligro bastante sutil al que hay que prestarle atención se llama **abandono diferencial**, que es un tipo de sesgo de selección causado por el propio estudio. Supongamos que, por primera vez en la historia de la psicología, logro encontrar una muestra perfectamente balanceada y representativa de personas. Empiezo a correr “el experimento increíblemente largo y tedioso de Dan” sobre mi muestra perfecta, pero como mi estudio es increíblemente largo y tedioso, mucha gente empieza a abandonar. No puedo evitarlo: como vamos a discutir más adelante en el capítulo sobre ética en la investigación, les participantes tienen absolutamente el derecho de dejar cualquier experimento, en cualquier momento, por la razón que se les ocurra, y como investigadores tenemos la obligación moral (y profesional) de recordarles que tienen ese derecho.

Entonces, supongamos que “el experimento increíblemente largo y tedioso de Dan” tiene una tasa de abandono muy alta. ¿Qué probabilidades creés que hay de que ese a...

bandono sea aleatorio? Respuesta: cero. Casi con certeza, las personas que permanecen son más concienzudas, más tolerantes al aburrimiento, etc., que aquellas que se van. En la medida en que (por ejemplo) la concienzudez sea relevante para el fenómeno psicológico que me interesa, este abandono puede disminuir la validez de mis resultados.

Cuando se piensa en los efectos del abandono diferencial, a veces resulta útil distinguir entre dos tipos diferentes. El primero es el **abandono homogéneo**, en el que el efecto del abandono es el mismo para todos los grupos, tratamientos o condiciones. En el ejemplo que di más arriba, el abandono diferencial sería homogéneo si (y solo si) les participantes que se aburren fácilmente están abandonando todas las condiciones del experimento más o menos al mismo ritmo. En general, el principal efecto del abandono homogéneo es que puede volver tu muestra no representativa. En ese caso, la principal preocupación es que la generalización de los resultados disminuye: en otras palabras perdés validez externa.

El segundo tipo de abandono diferencial es el **abandono heterogéneo**, en el que el efecto del abandono es distinto para diferentes grupos. Este es un problema mucho más grave: no solo tenés que preocuparte por tu validez externa, sino también por tu validez interna. Para ver por qué es así, consideremos un estudio muy tonto en el que quiero ver si insultar a las personas hace que actúen de manera más obediente. No sé por qué alguien querría estudiar eso, pero supongamos que a mí me importa muchísimo. Entonces, diseño mi experimento con dos condiciones. En la condición de “tratamiento”, la persona experimentadora insulta a la persona participante y luego le da un cuestionario diseñado para medir obediencia. En la condición de “control”, la persona experimentadora conversa un poco sin decir nada importante y luego le da el mismo cuestionario.

Dejando de lado los dudosos méritos científicos y éticos de tal estudio, pensemos qué podría salir mal acá. Como regla general, cuando alguien me insulta en la cara, tiendo a volverme mucho menos cooperativo. Así que hay una buena probabilidad de que muchas más personas abandonen la condición de tratamiento que la condición de control. Y ese abandono no va a ser aleatorio. Las personas más propensas a abandonar probablemente sean aquellas a las que no les importa tanto la importancia de quedarse obedientemente en el experimento. Como las personas más testarudas y desobedientes se fueron del grupo de tratamiento pero no del grupo de control, hemos introducido un confusor: las personas que efectivamente completaron el cuestionario en el grupo de tratamiento ya eran *más* propensas a ser obedientes y cumplidoras que las del grupo de control. En resumen, en este estudio insultar a la gente no la vuelve más obediente: ¡hace que les más desobedientes se vayan del experimento! La validez interna de este experimento queda completamente arruinada.

### Sesgo por no respuesta

El **sesgo por no respuesta** está estrechamente relacionado con el sesgo de selección y con el abandono diferencial. La versión más simple del problema va así: enviás una encuesta por correo a 1000 personas, y solo 300 responden. Las 300 personas que respondieron casi con seguridad no son una submuestra aleatoria. Las personas que responden encuestas son sistemáticamente diferentes de las que no lo hacen. Esto genera un problema cuando intentás generalizar a partir de esas 300 personas que sí respondieron hacia toda la población, ya que ahora tenés una muestra muy poco aleatoria.

Sin embargo, el problema del sesgo por no respuesta es más general que eso. Entre las (digamos) 300 personas que respondieron la encuesta, podrías encontrar que no todes contestan todas las preguntas. Si (por ejemplo) 80 personas eligieron no responder una de tus preguntas, ¿eso introduce problemas? Como siempre, la respuesta es: tal vez. Si la pregunta que no se respondió estaba en la última página del cuestionario, y esas 80 encuestas fueron devueltas sin la última página, hay una buena probabilidad de que esos datos faltantes no sean un gran problema: probablemente las páginas simplemente se desprendieron. Sin embargo, si la pregunta que 80 personas no respondieron era la más confrontativa o invasiva del cuestionario, entonces casi con certeza tenés un problema. En esencia, lo que estás enfrentando acá es lo que se llama el problema de **datos faltantes**. Si los datos que faltan se “perdieron” de manera aleatoria, entonces no es un gran problema. Si faltan de forma sistemática, entonces sí puede ser un gran problema.

### Regresión a la media

La **regresión a la media** es una curiosa variación del sesgo de selección. Se refiere a cualquier situación en la que seleccionás datos basándote en un valor extremo de alguna medición. Debido a la variación natural de la medición, eso casi con certeza significa que, cuando tomes una segunda medición, ese nuevo valor será menos extremo que el primero, simplemente por azar.

Acá va un ejemplo. Supongamos que me interesa saber si una educación en psicología tiene un efecto negativo en les estudiantes muy inteligentes. Para esto, busco a les 20 estudiantes de psicología con los mejores...

calificaciones del secundario y analizo qué tan bien les va en la universidad. Resulta que les va mucho mejor que el promedio, pero no están encabezando la clase en la universidad, aunque sí lo hacían en el secundario. ¿Qué está pasando? El primer pensamiento natural es que esto debe significar que las clases de psicología están teniendo un efecto negativo sobre eses estudiantes. Sin embargo, aunque esa podría ser la explicación, lo más probable es que lo que estás viendo sea un ejemplo de “regresión a la media”.

Para entender cómo funciona, tomémonos un momento para pensar qué se necesita para obtener la mejor nota en una clase, ya sea en el secundario o en la universidad. Cuando tenés una clase grande, va a haber *muchas* personas muy inteligentes. Para sacar la mejor nota tenés que ser muy inteligente, trabajar muy duro, y tener un poco de suerte. El examen tiene que hacer exactamente las preguntas correctas para tus habilidades particulares, y no tenés que cometer errores tontos (que todes cometemos alguna vez) al responderlas. Y ese es el punto: la inteligencia y el esfuerzo son cualidades que se transfieren de una clase a otra. La suerte, no. Les que tuvieron suerte en el secundario no van a ser les mismes que tienen suerte en la universidad. Esa es, justamente, la definición de “suerte”. Como consecuencia, cuando seleccionás personas con valores muy extremos en una medición (les 20 mejores estudiantes), estás seleccionando por esfuerzo, habilidad y suerte. Pero como la suerte no se transfiere a la segunda medición (solo la habilidad y el esfuerzo sí lo hacen), es de esperar que todes eses estudiantes bajen un poco cuando los medís por segunda vez (en la universidad). Así que sus calificaciones caen un poco, se acercan de nuevo al resto. Eso es la regresión a la media.

La regresión a la media es sorprendentemente común. Por ejemplo, si dos personas muy altas tienen hijes, sus hijes tenderán a ser más altos que el promedio, pero no tanto como sus padres. Lo inverso ocurre con padres muy bajos: dos padres muy bajos tenderán a tener hijes bajes, pero aun así eses hijes tenderán a ser más altos que sus padres. También puede ser extremadamente sutil. Por ejemplo, se han hecho estudios que sugerían que las personas aprenden mejor del feedback negativo que del positivo. Sin embargo, la forma en que se trató de demostrar esto fue dando refuerzo positivo cuando les participantes lo hacían bien, y refuerzo negativo cuando lo hacían mal. Y lo que se observa es que después del refuerzo positivo, el desempeño tendía a empeorar; pero después del refuerzo negativo, tendía a mejorar. ¡Pero! Fijate que acá hay un sesgo de selección: cuando las personas lo hacen muy bien, estás seleccionando valores “altos”, y por lo tanto deberías *esperar* (por la regresión a la media) que el rendimiento en la siguiente prueba sea peor, independientemente de si se da refuerzo o no. De manera similar, después de una mala prueba, la gente tenderá a mejorar por sí sola. La aparente superioridad del feedback negativo es un artefacto causado por la regresión a la media [@Kahneman1973].

### Sesgo del experimentador

El **sesgo del experimentador** puede presentarse de múltiples formas. La idea básica es que la persona experimentadora, a pesar de tener las mejores intenciones, puede terminar influyendo accidentalmente en los resultados del experimento al comunicar sutilmente la “respuesta correcta” o el “comportamiento deseado” a les participantes. Típicamente, esto ocurre porque la persona experimentadora posee un conocimiento especial que la persona participante no tiene —ya sea la respuesta correcta a las preguntas que se hacen, o el patrón de desempeño esperado para la condición en la que se encuentra la persona participante, etc.

El ejemplo clásico de esto es el estudio de caso de “Hans el listo”, que data de 1907 [@Pfungst1911; @Hothersall2004]. Hans el listo era un caballo que aparentemente podía leer, contar y realizar otras hazañas de inteligencia típicamente humanas. Después de que Hans se hizo famoso, les psicólogues comenzaron a examinar más de cerca su comportamiento. Resultó que —como era de esperarse— Hans no sabía hacer cuentas. En realidad, Hans estaba respondiendo a les observadores humanes a su alrededor. Como elles sí sabían contar, el caballo había aprendido a cambiar su comportamiento cuando les humanes cambiaban el suyo.

La solución general al problema del sesgo del experimentador es realizar estudios doble ciego, en los que ni la persona experimentadora ni la persona participante saben en qué condición se encuentra la persona participante, ni cuál es el comportamiento deseado. Esta es una muy buena solución al problema, pero es importante reconocer que no es del todo ideal y que es difícil implementarla perfectamente.

Por ejemplo, la manera obvia en que podría intentar construir un estudio doble ciego es pedirle a une de mis estudiantes de doctorado (alguien que no sepa nada sobre el experimento) que lleve a cabo el estudio. Eso parecería suficiente. La única persona (yo) que conoce todos los detalles (por ejemplo, las respuestas correctas a las preguntas, la asignación de participantes a condiciones) no interactúa con les participantes, y la persona que sí habla con elles (el/la estudiante de doctorado) no sabe nada.

Salvo que, esa última parte es muy poco probable que sea cierta. Para que el/la estudiante de doctorado pueda llevar a cabo el estudio de manera efectiva, tiene que haber recibido instrucciones mías, la persona investigadora. Y, como ocurre, ese/a estudiante también me conoce, y sabe un poco sobre mis creencias generales acerca de las personas y la psicología (por ejemplo, tiendo a pensar que les seres humanos son mucho más inteligentes de lo que les psicólogues les reconocen). Como resultado de todo esto, es casi imposible que la persona experimentadora evite conocer, aunque sea un poco, mis expectativas. Y hasta un poco de conocimiento puede tener efecto: supongamos que la persona experimentadora transmite accidentalmente el hecho de que se espera que les participantes lo hagan bien en esta tarea. Bueno, hay algo que se llama el “efecto Pigmalión”: si esperás grandes cosas de alguien, esa persona tenderá a estar a la altura; pero si esperás que fracase, probablemente también lo haga. En otras palabras, las expectativas se vuelven profecías autocumplidas.


### Efectos de demanda y reactividad

Cuando hablamos del sesgo del experimentador, la preocupación es que el conocimiento o los deseos de la persona experimentadora respecto al experimento se comuniquen a les participantes, y que eso afecte su comportamiento [@Rosenthal1966]. Sin embargo, incluso si lográs evitar que eso ocurra, es casi imposible evitar que les participantes sepan que están participando en un estudio psicológico. Y el mero hecho de saber que alguien te está observando o estudiando puede tener un efecto bastante fuerte sobre el comportamiento. A esto se lo denomina en general como **reactividad** o **efectos de demanda**. La idea básica se captura en el efecto Hawthorne: las personas modifican su rendimiento debido a la atención que el estudio pone sobre elles. El efecto toma su nombre de la fábrica “Hawthorne Works” en las afueras de Chicago [@Adair1984]. Un estudio hecho en la década de 1920 que examinaba los efectos de la iluminación sobre la productividad de les trabajadores en la fábrica resultó mostrar un efecto del hecho de que les trabajadores sabían que estaban siendo estudiades, más que de la iluminación.

Para ser un poco más específicos sobre algunas de las formas en que el solo hecho de estar en un estudio puede cambiar cómo actúan las personas, ayuda pensar como un/a psicólogue social y observar algunos de los *roles* que las personas podrían adoptar durante un experimento, pero que tal vez no adoptarían si los mismos eventos ocurrieran en el mundo real:

- La *persona participante colaboradora* intenta ser demasiado útil para la persona investigadora: busca entender la hipótesis y confirmarla.

- La *persona participante negativa* hace exactamente lo opuesto a la colaboradora: busca sabotear el estudio o refutar la hipótesis de alguna manera.

- La *persona participante fiel* es antinaturalmente obediente: busca seguir las instrucciones a la perfección, sin importar lo que hubiera hecho en una situación más realista.

- La *persona participante aprensiva* se pone nerviosa por estar siendo evaluada o estudiada, tanto que su comportamiento se vuelve altamente antinatural o excesivamente deseable socialmente.


### Efectos placebo

El **efecto placebo** es un tipo específico de efecto de demanda que nos preocupa bastante. Se refiere a la situación en la que el simple hecho de recibir un tratamiento provoca una mejora en los resultados. El ejemplo clásico proviene de los ensayos clínicos: si le das a una persona un fármaco completamente inerte desde el punto de vista químico y le decís que es una cura para una enfermedad, esa persona tiende a mejorar más rápido que alguien que no recibe ningún tratamiento. En otras palabras, es la *creencia* de que están recibiendo tratamiento lo que causa la mejora, no el medicamento en sí.


### Efectos de situación, medición y subpoblación

En algunos aspectos, estos términos funcionan como una categoría general para “todas las demás amenazas a la validez externa”. Se refieren al hecho de que la elección de la subpoblación de la que obtenés a tus participantes, la ubicación, el momento y la manera en que llevás a cabo tu estudio (incluyendo quién recolecta los datos), y las herramientas que usás para hacer tus mediciones podrían estar influyendo en los resultados. En concreto, la preocupación es que estas cosas puedan estar influyendo en los resultados de una manera tal que impida generalizar a una gama más amplia de personas, contextos y métodos de medición.


### Fraude, engaño y autoengaño

> *Es difícil lograr que alguien entienda algo, cuando su salario depende de que no lo entienda.*  
> — Upton Sinclair

Una última cosa que siento que debería mencionar. Mientras leía lo que suelen decir los libros de texto sobre cómo evaluar la validez de un estudio, no pude evitar notar que parecen asumir que la persona investigadora es honesta. Me resulta gracioso. Aunque la gran mayoría de les científiques son honestes —al menos en mi experiencia—, algunes no lo son. No solo eso: como mencioné antes, les científiques no son inmunes al sesgo de creencias —es fácil que une investigadore termine engañándose a sí misme para creer algo falso, y eso puede llevarle a hacer investigaciones sutilmente defectuosas, y luego a ocultar esas fallas al redactar el informe. Así que necesitás considerar no solo la (probablemente poco común) posibilidad de fraude deliberado, sino también la (probablemente bastante común) posibilidad de que la investigación esté sesgada de manera no intencional. Abrí algunos libros de texto estándar y no encontré mucho sobre este problema, así que acá va mi propio intento de listar algunas formas en que pueden surgir estas cuestiones:

- **Fabricación de datos**. A veces, las personas simplemente inventan los datos. Ocasionalmente esto se hace con “buenas” intenciones. Por ejemplo, la persona investigadora cree que los datos fabricados reflejan la verdad, y pueden ser versiones “ligeramente corregidas” de datos reales. En otras ocasiones, el fraude es deliberado y malicioso. Algunos ejemplos de alto perfil en los que se ha alegado o demostrado fabricación de datos incluyen a Cyril Burt (un psicólogo que se cree que falsificó parte de sus datos), Andrew Wakefield (acusado de falsificar sus datos que relacionaban la vacuna triple viral con el autismo) y Hwang Woo-suk (quien falsificó muchos de sus datos sobre investigación con células madre).

- **Engaños (hoaxes)**. Los engaños comparten muchas similitudes con la fabricación de datos, pero se diferencian en el propósito. Un engaño suele ser una broma, y muchos están diseñados para que eventualmente se descubran. A menudo, el objetivo de un engaño es desacreditar a alguien o a un campo entero. Ha habido bastantes engaños científicos conocidos a lo largo de los años (por ejemplo, el hombre de Piltdown), algunos de los cuales fueron intentos deliberados de desacreditar ciertos campos de investigación (por ejemplo, el caso Sokal).

- **Falsificación o tergiversación de los datos**. Aunque el fraude es lo que se lleva los titulares, en mi experiencia es mucho más común ver datos tergiversados. Cuando digo esto, no me refiero a los medios de comunicación interpretando mal los resultados (que lo hacen casi siempre). Me refiero a que muchas veces los datos no dicen realmente lo que les investigadores creen que dicen. Mi impresión es que, casi siempre, esto no se debe a una deshonestidad deliberada, sino a una falta de sofisticación en los análisis de datos. Por ejemplo, pensá en el caso de la paradoja de Simpson que mencioné al comienzo de estas notas. Es muy común ver a personas presentar datos “agregados” de algún tipo; y a veces, cuando profundizás y conseguís los datos crudos por tu cuenta, encontrás que los datos agregados cuentan una historia diferente que los desagregados. Alternativamente, podrías descubrir que algún aspecto de los datos está siendo ocultado porque cuenta una historia incómoda (por ejemplo, la persona investigadora podría optar por no mencionar una determinada variable). Hay muchas variantes de esto; muchas de las cuales son muy difíciles de detectar.

- **Diseño “defectuoso” del estudio**. Bien, esto es más sutil. Básicamente, el problema acá es que une investigadore diseña un estudio que tiene fallas incorporadas, y esas fallas nunca se informan en el artículo. Los datos que se reportan son completamente reales, y están analizados correctamente, pero provienen de un estudio que en realidad está muy mal planteado. La persona investigadora realmente quiere encontrar un efecto en particular, así que el estudio se arma de manera tal que resulte “fácil” (aunque artefactual) observar ese efecto. Una manera muy astuta de hacer esto —por si tenés ganas de incursionar en un poco de fraude— es diseñar un experimento en el que sea obvio para les participantes qué es lo que “se supone” que deben hacer, y luego dejar que la reactividad haga su magia. Si querés, podés agregarle todos los adornos de un experimento doble ciego, etc. No va a hacer ninguna diferencia, ya que los propios materiales del estudio les están diciendo sutilmente a les participantes lo que se espera que hagan. Cuando redactás los resultados, el fraude no va a ser obvio para la persona lectora: lo que es evidente para la persona participante en el contexto experimental no siempre es evidente para quien lee el artículo. Claro, la manera en que describí esto suena como si *siempre* fuera fraude: probablemente haya casos donde esto se haga de forma deliberada, pero en mi experiencia la preocupación más grande ha sido con los errores no intencionales de diseño. La persona investigadora *cree*... y así, el estudio termina teniendo una falla estructural, y esa falla mágicamente desaparece cuando se escribe el artículo para su publicación.


- **Minería de datos e hipótesis post hoc**. Otra forma en la que les autores de un estudio pueden, más o menos, mentir sobre lo que encontraron es mediante lo que se llama “minería de datos”. Como vamos a discutir más adelante en la clase, si analizás los datos de muchas formas diferentes, eventualmente vas a encontrar algo que “parece” un efecto real, pero que no lo es. Eso es lo que se llama “minería de datos”. Antes era bastante raro, porque analizar datos solía llevar semanas, pero ahora que todes tienen software estadístico muy potente en sus computadoras, se ha vuelto bastante común. La minería de datos en sí no es “incorrecta”, pero cuanto más la hacés, más grande es el riesgo que estás corriendo. Lo que *sí* está mal (y, sospecho, es común) es hacer minería de datos **sin reconocerlo**. Es decir, la persona investigadora prueba todos los análisis posibles conocidos por la humanidad, encuentra uno que “funciona”, y luego finge que ese era el único análisis que pensaba hacer desde el principio. Peor aún, muchas veces “inventa” una hipótesis después de mirar los datos, para cubrir la minería de datos. Para ser claros: no está mal cambiar tus creencias después de mirar los datos, y reanalizar los datos con tus nuevas hipótesis *post hoc*. Lo que sí está mal (y, sospecho, es común) es no reconocer que hiciste eso. Si lo reconocés, otres investigadores pueden tenerlo en cuenta. Si no lo hacés, no pueden. Y eso vuelve tu comportamiento engañoso. ¡Mal!

## Resumen

Este capítulo no pretende ofrecer una discusión exhaustiva sobre los métodos de investigación en psicología: para hacerle justicia al tema se necesitaría otro volumen igual de largo que este. Sin embargo, en la vida real, las estadísticas y el diseño de estudios están fuertemente entrelazados, así que es muy útil repasar algunos temas clave. En este capítulo, discutí brevemente los siguientes temas:

- **Introducción a la medición psicológica**: ¿Qué significa operacionalizar un constructo teórico? ¿Qué significa tener variables y tomar mediciones?

- **Escalas de medición y tipos de variables**: Recordá que hay *dos* distinciones diferentes acá: está la diferencia entre datos discretos y continuos, y está la diferencia entre los cuatro tipos de escala (nominal, ordinal, de intervalo y de razón).

- **Confiabilidad de una medición**: Si mido dos veces lo “mismo”, ¿debería esperar ver el mismo resultado? Solo si mi medición es confiable. Pero, ¿qué significa hablar de hacer lo “mismo”? Bueno, por eso tenemos distintos tipos de confiabilidad. Acordate cuáles son.

- **Terminología: predictoras y resultados**: ¿Qué rol juegan las variables en un análisis? ¿Te acordás la diferencia entre variables predictoras y variables resultado? ¿Entre dependientes e independientes? Etc.

- **Diseños de investigación experimentales y no experimentales**: ¿Qué es lo que hace que un experimento sea un experimento? ¿Una bata blanca de laboratorio? ¿O tiene algo que ver con el control que ejerce la persona investigadora sobre las variables?

- **Validez y sus amenazas**: ¿Tu estudio mide lo que querés que mida? ¿Cómo podrían salir mal las cosas? ¿Y soy yo, o esa fue una lista larguísima de posibles maneras en que las cosas pueden salir mal?

Todo esto debería dejar en claro que el diseño del estudio es una parte crítica de la metodología de investigación. Armé este capítulo a partir del clásico librito de @Campbell1963, pero por supuesto existen muchos otros libros de texto sobre diseño de investigación...
