[
  {
    "objectID": "01-Science_Data.html#sobre-la-psicología-de-la-estadística",
    "href": "01-Science_Data.html#sobre-la-psicología-de-la-estadística",
    "title": "1  ¿Por qué estadística?",
    "section": "",
    "text": "¿Por qué hacen estadística? ¿Por qué no usar solo el sentido común?\n\n\n\n\n1.1.1 La maldición del sesgo de creencia\nLa mayoría de las personas son bastante inteligentes. Claramente más que otras especies con las que compartimos el planeta (aunque muchos no estarían de acuerdo). Nuestras mentes son asombrosas, capaces de hazañas increíbles de pensamiento y razonamiento. Pero eso no nos hace perfectos. Una de las muchas cosas que la psicología ha demostrado a lo largo del tiempo es que nos cuesta mucho mantenernos neutrales, evaluar la evidencia de manera imparcial y sin dejarnos llevar por nuestras creencias previas. Un buen ejemplo de esto es el sesgo de creencia en el razonamiento lógico: si le pedís a alguien que juzgue si un argumento es lógicamente válido (es decir, si la conclusión se sigue de las premisas), solemos dejarnos influenciar por qué tan creíble parece la conclusión, incluso cuando no deberíamos. Por ejemplo, este argumento es válido y tiene una conclusión creíble:\n\nNingún cigarrillo es barato (Premisa 1)\nAlgunas cosas adictivas son baratas (Premisa 2)\nPor lo tanto, algunas cosas adictivas no son cigarrillos (Conclusión)\n\nY este también es válido, pero la conclusión suena menos creíble:\n\nNinguna cosa adictiva es barata (Premisa 1)\nAlgunos cigarrillos son baratos (Premisa 2)\nPor lo tanto, algunos cigarrillos no son adictivos (Conclusión)\n\nLa estructura lógica del argumento #2 es idéntica a la del #1, y ambos son válidos. Sin embargo, en el segundo caso, hay buenas razones para pensar que la premisa 1 es falsa, así que probablemente también lo sea la conclusión. Pero eso no importa: un argumento es deductivamente válido si la conclusión se sigue lógicamente de las premisas. No importa si las premisas o la conclusión son verdaderas. Ahora veamos un argumento inválido con una conclusión creíble:\n\nNinguna cosa adictiva es barata (Premisa 1)\nAlgunos cigarrillos son baratos (Premisa 2)\nPor lo tanto, algunas cosas adictivas no son cigarrillos (Conclusión)\n\nY uno inválido con una conclusión poco creíble:\n\nNingún cigarrillo es barato (Premisa 1)\nAlgunas cosas adictivas son baratas (Premisa 2)\nPor lo tanto, algunos cigarrillos no son adictivos (Conclusión)\n\nAhora bien, supongamos que las personas realmente pueden dejar de lado sus sesgos previos sobre qué es verdadero y qué no, y evaluar un argumento únicamente por su lógica. Esperaríamos que el 100% diga que los argumentos válidos son válidos, y que el 0% diga que los inválidos lo son. Si hiciéramos un experimento sobre esto, esperaríamos ver datos así:\n\n\n\n\nla conclusión parece verdadera\nla conclusión parece falsa\n\n\n\n\nel argumento es válido\n100% dice “válido”\n100% dice “válido”\n\n\nel argumento es inválido\n0% dice “válido”\n0% dice “válido”\n\n\n\nSi los datos psicológicos se vieran así (o incluso se acercaran bastante), podríamos sentirnos seguros confiando en nuestras corazonadas. O sea, estaría perfecto dejar que los científicos evaluaran los datos según su sentido común, sin molestarse con todo este asunto turbio de la estadística. Pero ustedes ya han cursado psicología, y a esta altura seguro sospechan hacia dónde va esto.\nEn un estudio clásico, Evans, Barston, y Pollard (1983) hicieron un experimento sobre esto mismo. Y lo que encontraron fue que, cuando las creencias previas (o sea, los sesgos) coincidían con la estructura lógica del argumento, todo iba bastante bien:\n\n\n\n\nla conclusión parece verdadera\nla conclusión parece falsa\n\n\n\n\nel argumento es válido\n92% dice “válido”\n–\n\n\nel argumento es inválido\n–\n8% dice “válido”\n\n\n\nNo es perfecto, pero está bastante bien. Ahora mirá lo que pasa cuando la intuición sobre la verdad de la conclusión entra en conflicto con la lógica del argumento:\n\n\n\n\nla conclusión parece verdadera\nla conclusión parece falsa\n\n\n\n\nel argumento es válido\n92% dice “válido”\n46% dice “válido”\n\n\nel argumento es inválido\n92% dice “válido”\n8% dice “válido”\n\n\n\nAy. Eso ya no está tan bien. Al parecer, cuando a la gente se le presenta un argumento fuerte que contradice sus creencias previas, le cuesta bastante incluso reconocer que es un buen argumento (solo el 46% lo logró). Y peor todavía: cuando se les da un argumento débil que coincide con sus creencias, casi nadie nota que es débil (¡el 92% se equivocó!).\nAhora bien, si lo pensás, tampoco es que estos datos sean una catástrofe total. En promedio, a la gente le va un poco mejor que al azar al tratar de compensar sus sesgos previos: más o menos el 60% de los juicios fueron correctos (el azar daría 50%). Aun así, si fueras un profesional encargado de evaluar evidencia, y alguien te ofreciera una herramienta mágica que mejora tus chances de decidir bien del 60% al 95%, ¿no la aceptarías? Obvio que sí. Por suerte, esa herramienta existe. No es magia, es estadística. Y esa es la razón #1 por la que los científicos aman la estadística: es demasiado fácil creer lo que una ya cree; así que si queremos “creer en los datos” en lugar de nuestras intuiciones, necesitamos una ayudita para mantener a raya nuestros sesgos. Eso es lo que hace la estadística: nos ayuda a ser honestos.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>¿Por qué estadística?</span>"
    ]
  },
  {
    "objectID": "01-Science_Data.html#la-advertencia-de-la-paradoja-de-simpson",
    "href": "01-Science_Data.html#la-advertencia-de-la-paradoja-de-simpson",
    "title": "1  ¿Por qué estadística?",
    "section": "1.2 La advertencia de la paradoja de Simpson",
    "text": "1.2 La advertencia de la paradoja de Simpson\nLa siguiente es una historia real (creo…). En 1973, la Universidad de California en Berkeley estaba preocupada por los ingresos a sus programas de posgrado. En particular, el problema era la distribución por género de los ingresos, que se veía así:\n\n\n\n\nNúmero de postulantes\nPercentaje admitido\n\n\nHombres\n8442\n44%\n\n\nMujeres\n4321\n35%\n\n\n\nY tenían miedo de que los demandaran. Con casi 13.000 postulantes, una diferencia del 9% entre hombres y mujeres es demasiado grande como para ser una simple coincidencia. Bastante convincente, ¿no? Y si te dijera que estos datos en realidad reflejan un leve sesgo a favor de las mujeres (¡más o menos!), probablemente pensarías que estoy loca o que soy sexista.\n\n\n\n\n\n\nExtra\n\n\n\nVersiones anteriores de estas notas sugerían incorrectamente que Berkeley fue demandada —aparentemente eso no es cierto. Hay un buen comentario al respecto acá: https://www.refsmmat.com/posts/2016-05-08-simpsons-paradox-berkeley.html. ¡Gracias a Wilfried Van Hirtum por señalarlo!\n\n\nCuando la gente empezó a mirar los datos con más detalle (Bickel, Hammel, y O’Connell 1975), la historia resultó ser bastante diferente. En particular, cuando se analizaron los ingresos por departamento, se encontró que la mayoría de los departamentos en realidad tenían una tasa de admisión más alta para mujeres que para hombres. La siguiente tabla muestra los datos de los seis departamentos más grandes (los nombres fueron removidos por privacidad):\n\n\n\n\n\n\n\n\n\n\nDepartamento\nPostulantes (H)\n% admitidos\nPostulantes (M)\n% admitidas\n\n\nA\n825\n62%\n108\n82%\n\n\nB\n560\n63%\n25\n68%\n\n\nC\n325\n37%\n593\n34%\n\n\nD\n417\n33%\n375\n35%\n\n\nE\n191\n28%\n393\n24%\n\n\nF\n272\n6%\n341\n7%\n\n\n\nIncreíblemente, la mayoría de los departamentos admitían más mujeres (en proporción) que hombres. Sin embargo, la tasa de admisión general en toda la universidad era más baja para las mujeres. ¿Cómo puede ser? ¿Cómo pueden ser verdaderas las dos cosas al mismo tiempo?\nEsto es lo que pasa. Primero, notá que los departamentos no son todos iguales en cuanto a su porcentaje de admisión: algunos (como ingeniería o química) tienden a admitir a una gran parte de quienes se postulan; otros (como literatura inglesa) tienden a rechazar a la mayoría, incluso si tienen buen perfil. Así que, entre los seis departamentos de arriba, el departamento A es el más generoso, seguido por B, C, D, E y F, en ese orden. A continuación, fijate que los hombres y las mujeres tendían a postularse a departamentos diferentes. Si ordenamos los departamentos según el número total de postulaciones masculinas, obtenemos A&gt;B&gt;D&gt;C&gt;F&gt;E (los “departamentos fáciles” están en negrita). En general, los hombres tendieron a postularse a los departamentos con tasas de admisión altas. Ahora compara esto con la distribución de las postulaciones femeninas. Clasificar los departamentos según el número total de postulaciones femeninas produce un orden bastante diferente: C&gt;E&gt;D&gt;F&gt;A&gt;B. En otras palabras, estos datos parecen sugerir que las mujeres tendieron a postularse a departamentos “más difíciles”.\nY si miramos Figura 1.1, vemos que esta tendencia es sistemática, y bastante notable. Este fenómeno se conoce como la paradoja de Simpson. No es común, pero ocurre en la vida real, y suele sorprender muchísimo cuando uno se la encuentra por primera vez. Mucha gente incluso se niega a creer que sea real. Pero lo es. Y aunque hay muchas lecciones estadísticas sutiles escondidas acá, quiero usar este ejemplo para señalar un punto más importante: hacer investigación es difícil, y está lleno de trampas sutiles y contraintuitivas esperando a quien no esté atento. Esa es la razón #2 por la que los científiquos aman la estadística, y por la que enseñamos métodos de investigación. Porque la ciencia es difícil, y la verdad a veces está escondida en los rincones más enredados de los datos.\n\nknitr::include_graphics(\"imgs/figures/1Simpson.png\")\n\n\n\n\n\n\n\nFigura 1.1: Datos de admisión a la universidad de Berkeley en 1973. Esta figura muestra la tasa de admisión de los 85 departamentos que recibieron al menos una postulante mujer, en función del porcentaje de postulantes que eran mujeres. El gráfico es una recreación de la Figura 1 de Bickel et al. (1975). Los círculos representan departamentos con más de 40 postulantes; el área del círculo es proporcional al número total de postulantes. Las cruces representan departamentos con menos de 40 postulantes.\n\n\n\n\n\nAntes de dejar este tema por completo, quiero señalar algo realmente importante que muchas veces se pasa por alto en los cursos de métodos de investigación: la estadística solo resuelve parte del problema. Recordemos que todo esto empezó con la preocupación de que el proceso de admisión de Berkeley pudiera estar sesgado en contra de las mujeres. Cuando miramos los datos “agregados”, parecía que la universidad estaba discriminando. Pero cuando desagregamos y analizamos el comportamiento de los departamentos individuales, resultó que, en todo caso, había un sesgo a favor de las mujeres. El sesgo de género en las cifras totales se debía a que las mujeres tendían a postularse a los departamentos más exigentes. Desde una perspectiva legal, eso probablemente exime a la universidad. Las decisiones de admisión se toman a nivel de cada departamento (y hay buenas razones para que sea así), y a ese nivel las decisiones fueron más o menos imparciales (el leve sesgo a favor de las mujeres era pequeño y no sistemático). Como la universidad no puede decidir a qué departamento se postula cada quien, y la decisión se toma dentro del departamento, no se le puede responsabilizar por los sesgos que esas elecciones puedan producir.\nEsa era la base de mis comentarios un poco superficiales más arriba, pero no es toda la historia, ¿no? Si miramos esto desde una perspectiva más sociológica o psicológica, podríamos preguntarnos por qué hay diferencias tan marcadas en las postulaciones. ¿Por qué hay más hombres postulando a ingeniería y más mujeres a literatura? ¿Y por qué los departamentos con más postulaciones femeninas tienden a tener tasas de admisión más bajas que aquellos con más postulaciones masculinas? ¿No podría eso reflejar también un sesgo de género, aunque cada departamento individual no lo tenga? Supongamos, hipotéticamente, que los hombres prefieren postularse a ciencias duras y las mujeres a humanidades. Y supongamos que las tasas bajas de admisión en humanidades se deben a que el gobierno no quiere financiarlas (por ejemplo, los cupos para doctorados suelen estar atados a proyectos con financiación estatal). ¿Eso es un sesgo de género? ¿O simplemente una visión poco iluminada sobre el valor de las humanidades? ¿Y si alguien con poder político recortó fondos para humanidades porque piensa que “son cosas de chicas, sin utilidad”? Eso ya suena claramente sesgado por género. Nada de esto lo resuelve la estadística. Pero sí importa para el proyecto de investigación. Si te interesan los efectos estructurales de sesgos de género sutiles, probablemente querrás mirar tanto los datos agregados como los desagregados. Si solo te interesa cómo se toman decisiones dentro de Berkeley, entonces solo te importan los datos desagregados.\nEn resumen, hay muchas preguntas fundamentales que no se responden con estadística, pero que cambian por completo cómo analizás e interpretás los datos. Y por eso deberías pensar en la estadística como una herramienta para ayudarte a entender tus datos, ni más ni menos. Es una herramienta poderosa, pero no reemplaza la reflexión cuidadosa.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>¿Por qué estadística?</span>"
    ]
  },
  {
    "objectID": "01-Science_Data.html#estadística-en-psicología",
    "href": "01-Science_Data.html#estadística-en-psicología",
    "title": "1  ¿Por qué estadística?",
    "section": "1.3 Estadística en psicología",
    "text": "1.3 Estadística en psicología\nEspero que la discusión anterior haya ayudado a entender por qué la ciencia en general le da tanta importancia a la estadística. Pero seguro todavía tenés muchas preguntas sobre qué rol juega la estadística en psicología, y en especial por qué las materias de psicología tienen tantas clases dedicadas a esto. Así que acá va mi intento de responder algunas…\n\n¿Por qué hay tanta estadística en psicología?\n\nSiendo sinceros, hay varias razones, y algunas son mejores que otras. La más importante es que la psicología es una ciencia estadística. ¿Qué quiero decir con eso? Que lo que estudiamos son personas. Personas reales, complicadas, caóticas, contradictorias. La física estudia “cosas” como electrones, y aunque la física tiene sus complejidades, los electrones no tienen mente propia. No tienen opiniones, no difieren entre sí de maneras extrañas y arbitrarias, no se aburren a mitad de un experimento, y no se enojan con el experimentador para luego tratar deliberadamente de sabotear los datos (no es que yo alguna vez haya hecho eso…). A un nivel fundamental, la psicología es más difícil que la física.\nBásicamente, te enseñamos estadística porque, como psicólogo, necesitás ser mejor en esto que un físico. En física a veces se dice: “si tu experimento necesita estadística, tendrías que haber hecho un mejor experimento”. Pero ellos pueden decir eso porque sus objetos de estudio son ridículamente simples comparados con el lío que enfrentamos en las ciencias sociales. No es solo psicología: casi todas las ciencias sociales dependen mucho de la estadística. No porque seamos experimentadores mediocres, sino porque elegimos problemas más difíciles. Te enseñamos estadística porque realmente la vas a necesitar.\n\n¿No puede encargarse otra persona de la estadística?\n\nHasta cierto punto, sí, pero no completamente. No hace falta que te conviertas en un estadístico profesional para hacer psicología, pero sí necesitas alcanzar un cierto nivel de competencia estadística. Para mí, hay tres razones por las que todo investigador en psicología debería saber estadística básica:\n\nLa razón fundamental: la estadística está completamente ligada al diseño de investigación. Si querés diseñar buenos estudios, tenés que entender al menos lo básico.\nEn segundo lugar, si querés ser bueno en la parte psicológica de la investigación, necesitás poder entender la literatura psicológica, ¿no? Pero casi todos los artículos en esa literatura presentan resultados de análisis estadísticos. Así que, si de verdad querés entender la psicología, tenés que poder entender qué hicieron los demás con sus datos. Y eso significa saber al menos algo de estadística.\nSer dependiente de otros para hacer análisis estadísticos tiene un gran problema práctico: es caro. En la mayoría de los casos no vas a tener presupuesto para contratar un estadístico. Así que, por economía, tenés que poder arreglártelas por tu cuenta.\n\nY ojo, esto no aplica solo a investigadores. Si querés ejercer como psicólogo y mantenerte al día, te conviene poder leer la literatura científica, que está llena de estadística.\n\nNo me interesan ni los trabajos, ni la investigación, ni lo clínico. ¿Igual necesito estadística?\n\nAhora ya me estás tomando el pelo. Pero igual te debería importar. La estadística te debería importar como le debería importar a todo el mundo: vivimos en el siglo XXI, y los datos están por todos lados. Honestamente, con el mundo como está hoy, saber un poco de estadística es casi una herramienta de supervivencia.\nY eso nos lleva al próximo tema…",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>¿Por qué estadística?</span>"
    ]
  },
  {
    "objectID": "01-Science_Data.html#estadística-en-la-vida-cotidiana",
    "href": "01-Science_Data.html#estadística-en-la-vida-cotidiana",
    "title": "1  ¿Por qué estadística?",
    "section": "1.4 Estadística en la vida cotidiana",
    "text": "1.4 Estadística en la vida cotidiana\n\n“Nos estamos ahogando en información,\npero nos morimos de hambre por conocimiento”\n\n– Varios autores, original probablemente de John Naisbitt\n\nCuando empecé a escribir estas notas, tomé los 20 artículos más recientes del sitio de noticias de ABC. De esos 20, resultó que 8 trataban sobre algo que yo consideraría un tema estadístico; 6 de esos 8 tenían algún error. El error más común, por si te da curiosidad, era no reportar datos de línea de base (por ejemplo, el artículo dice que el 5% de las personas en la situación X tienen cierta característica Y, pero no dice cuán común es esa característica en la población general). El punto no es que los periodistas sean malísimos en estadística (aunque casi siempre lo son), sino que tener un conocimiento básico de estadística te ayuda mucho para darte cuenta cuando alguien se equivoca —o directamente te está mintiendo. Quizás, una de las mayores consecuencias de aprender estadística es que te vas a enojar con el diario o con internet mucho más seguido :).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>¿Por qué estadística?</span>"
    ]
  },
  {
    "objectID": "01-Science_Data.html#hay-más-en-los-métodos-de-investigación-que-la-estadística",
    "href": "01-Science_Data.html#hay-más-en-los-métodos-de-investigación-que-la-estadística",
    "title": "1  ¿Por qué estadística?",
    "section": "1.5 Hay más en los métodos de investigación que la estadística",
    "text": "1.5 Hay más en los métodos de investigación que la estadística\nHasta ahora, la mayoría de lo que dije fue sobre estadística, así que no te culpo si pensás que es lo único que me importa en la vida. Para ser justes, no estarías tan equivocade, pero la metodología de investigación es un concepto más amplio. Por eso, los cursos de métodos suelen cubrir un montón de temas más ligados a los aspectos prácticos del diseño de investigaciones, especialmente cuando trabajás con personas. Sin embargo, el 99% de los miedos estudiantiles tienen que ver con la parte estadística del curso, así que me enfoqué en eso, y espero haberte convencido de que la estadística importa, y más importante aún, que no hay que tenerle miedo. Dicho eso, es bastante típico que los cursos introductorios de métodos estén muy cargados de estadística. Y no es (generalmente) porque les docentes sean gente malvada. Al contrario. Los cursos intro se enfocan tanto en estadística porque la vas a necesitar antes que el resto del entrenamiento metodológico. ¿Por qué? Porque en casi todas tus otras materias, los trabajos prácticos van a requerir análisis estadístico, mucho más que cualquier otra herramienta de metodología. No es común que te pidan diseñar un estudio desde cero (en cuyo caso sí necesitarías saber mucho sobre diseño de investigación), pero sí es común que te pidan analizar e interpretar datos recolectados por otra persona (en cuyo caso necesitás estadística). En ese sentido, si pensás en lo que más te va a servir para rendir bien en tus materias, la estadística es lo más urgente.\nPero “urgente” no es lo mismo que “importante”: las dos cosas lo son. De verdad quiero insistir en que el diseño de investigación es tan importante como el análisis de datos, y este libro le dedica bastante espacio. Aun así, mientras que la estadística tiene un carácter más universal y te da un conjunto de herramientas útiles para casi cualquier investigación en psicología, el diseño de investigación no es tan generalizable. Hay principios básicos que todes deberían tener en cuenta, pero muchos aspectos del diseño son muy específicos del área en la que trabajás. Y como lo que más importa está en los detalles, esos detalles casi nunca aparecen en un curso introductorio de estadística y métodos."
  },
  {
    "objectID": "01-Science_Data.html#una-breve-introducción-al-diseño-de-investigación",
    "href": "01-Science_Data.html#una-breve-introducción-al-diseño-de-investigación",
    "title": "1  ¿Por qué estadística?",
    "section": "1.6 Una breve introducción al diseño de investigación",
    "text": "1.6 Una breve introducción al diseño de investigación\nEn este capítulo vamos a empezar a pensar en las ideas básicas que intervienen al diseñar un estudio, recolectar datos, verificar si la recolección funciona, etc. No te va a dar suficiente info como para que puedas diseñar tus propios estudios, pero sí te va a dar herramientas básicas para evaluar los estudios de otros. Sin embargo, como este libro está más enfocado en el análisis de datos que en la recolección, solo voy a dar un panorama general.\nEste capítulo es “especial” por dos razones. Primero, es mucho más específico de la psicología que los capítulos que vienen después. Segundo, se enfoca mucho más en el problema científico del diseño metodológico, y mucho menos en el problema estadístico del análisis de datos. De todos modos, los dos problemas están relacionados entre sí, así que es común que los libros de estadística los traten con cierto detalle.\nEste capítulo se basa bastante en Campbell y Stanley (1963) para el diseño de estudios, y en Stevens (1946) para la discusión sobre escalas de medición.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>¿Por qué estadística?</span>"
    ]
  },
  {
    "objectID": "01-Science_Data.html#introducción-a-la-medición-psicológica",
    "href": "01-Science_Data.html#introducción-a-la-medición-psicológica",
    "title": "1  ¿Por qué estadística?",
    "section": "1.7 Introducción a la medición psicológica",
    "text": "1.7 Introducción a la medición psicológica\nLo primero que hay que entender es que la recolección de datos puede pensarse como una forma de medición. Es decir, lo que estamos tratando de hacer acá es medir algo sobre el comportamiento o la mente humana. ¿Qué quiero decir con “medición”?\n\n1.7.1 Algunas ideas sobre la medición psicológica\nLa medición es un concepto sutil, pero básicamente se trata de encontrar una forma de asignar números, etiquetas u otras descripciones bien definidas a “cosas”. Así que, cualquiera de los siguientes ejemplos cuenta como una medición psicológica:\n\nMi edad es 33 años.\nNo me gustan las anchoas.\nMi sexo cromosómico es masculino.\nMi género autoidentificado es masculino.\n\nEn esta lista, lo que está en negrita es “lo que se quiere medir”, y lo que está en cursiva es “la medida en sí”. Podemos ir un poco más allá y pensar en el conjunto de posibles mediciones que podrían haber surgido en cada caso:\n\nMi edad (en años) podría haber sido 0, 1, 2, 3… etc. El límite superior de lo que podría ser mi edad es un poco impreciso, pero en la práctica podrías decir tranquilamente que la edad máxima posible es 150, ya que ningún ser humano ha vivido tanto.\nAl preguntarme si me gustan las anchoas, podría haber respondido sí, no, no tengo opinión, o a veces.\nMi sexo cromosómico casi seguro será masculino (XY) o femenino (XX), pero hay otras posibilidades, como el síndrome de Klinefelter (XXY), que es más similar al masculino. Y seguro hay más posibilidades.\nMi género autoidentificado también podría ser masculino o femenino, pero no tiene por qué coincidir con el sexo cromosómico. También podría identificarme con ninguno, o declararme explícitamente transgénero.\n\nComo ves, para algunas cosas (como la edad) es bastante claro cuál es el conjunto de valores posibles, pero para otras se vuelve más complicado. Incluso con la edad, hay sutilezas. En el ejemplo anterior asumí que medir en años estaba bien. Pero si sos psicólogo del desarrollo, eso es muy burdo: se suele usar años y meses (por ejemplo, “2;11” para 2 años y 11 meses). Si trabajás con recién nacidos, tal vez necesites medir días desde el nacimiento, o incluso horas.\nY si lo pensás más, te das cuenta de que el concepto de “edad” no es tan preciso. En general, cuando decimos “edad” nos referimos al tiempo desde el nacimiento. Pero eso no siempre es lo ideal. Supongamos que te interesa estudiar el control ocular en recién nacidos. En ese caso, podrías cuestionarte si “nacimiento” es el punto de referencia adecuado. Si María nace 3 semanas antes de término y Juana una semana después, ¿tienen la “misma edad” si las observamos a las 2 horas de nacidas? Socialmente, sí. Pero desde una perspectiva biológica, no. Tal vez convenga definir dos conceptos distintos: tiempo desde la concepción, y tiempo desde el nacimiento. Para adultos no hace diferencia, pero para recién nacidos sí.\nMás allá de eso, está la cuestión metodológica: ¿qué “método de medición” vas a usar para averiguar la edad? Como antes, hay muchas posibilidades.\n\nPodés simplemente preguntar: “¿cuántos años tenés?” El autorreporte es rápido, barato y sencillo, pero solo sirve con personas lo suficientemente grandes como para entender la pregunta, y alguna gente miente sobre su edad.\nPodrías preguntarle a una persona referente (por ejemplo, a la madre o al padre): “¿Qué edad tiene tu hijo?” Este método es rápido, y cuando se trata de niños no suele ser complicado, ya que casi siempre hay algún adulto presente. No funciona tan bien si querés saber la “edad desde la concepción”, porque muchas veces los padres no saben con certeza cuándo ocurrió. Para eso, quizás necesites otra autoridad (por ejemplo, un obstetra).\nPodés consultar registros oficiales, como partidas de nacimiento. Es un proceso tedioso y lleva tiempo, pero puede ser útil (por ejemplo, si la persona ya falleció).\n\n\n\n1.7.2 Operacionalización: definir tu medición\nTodas las ideas discutidas en la sección anterior se relacionan con el concepto de operacionalización. Para ser un poco más precisos, es el proceso de tomar un concepto significativo pero vago, y convertirlo en una medición precisa. La operacionalización puede incluir varias cosas diferentes:\n\nSer preciso sobre qué intentás medir. Por ejemplo, ¿“edad” significa tiempo desde el nacimiento o desde la concepción en el contexto de tu investigación?\nDefinir el método que vas a usar. ¿Vas a usar autorreporte, le vas a preguntar a alguien más, o vas a consultar registros? Y si usás autorreporte, ¿cómo vas a formular la pregunta?\nDefinir el conjunto de valores permitidos: Tené en cuenta que estos valores no siempre tienen que ser numéricos, aunque muchas veces lo son. Al medir la edad, los valores son numéricos, pero aún así necesitamos pensar bien qué números vamos a permitir. ¿Queremos la edad en años, en años y meses, en días, en horas? Y así. Para otros tipos de mediciones (por ejemplo, el género), los valores no son numéricos. Pero, como antes, tenemos que pensar qué valores vamos a permitir. Si les pedimos a las personas que autoinformen su género, ¿qué opciones les vamos a ofrecer? ¿Alcanza con permitir solo “masculino” o “femenino”? ¿Necesitamos una opción de “otro”? ¿O sería mejor no dar opciones cerradas y dejar que respondan con sus propias palabras? Y si abrimos el conjunto de valores posibles para incluir todas las respuestas verbales, ¿cómo vamos a interpretar sus respuestas?\n\nLa operacionalización no es una tarea sencilla, y no existe una única manera “correcta” de llevarla a cabo. La forma en que elijas operacionalizar un concepto informal como “edad” o “género” en una medición formal depende del uso que vayas a darle a esa medición. A menudo, vas a encontrar que la comunidad científica que trabaja en tu área ya tiene ideas bastante consolidadas sobre cómo hacerlo. En otras palabras, la operacionalización debe pensarse caso por caso. Dicho esto, aunque hay muchos aspectos que dependen del proyecto de investigación particular, también hay elementos que son bastante generales.\nAntes de continuar, quiero detenerme un momento para aclarar algunos términos y, de paso, introducir uno más. A continuación, presento cuatro conceptos estrechamente relacionados entre sí:\n\nUn constructo teórico: es lo que querés medir, como “edad”, “género” u “opinión”. No se puede observar directamente, y suele ser algo vago.\nUna medida: es el método o herramienta que usás para hacer tus observaciones. Puede ser una pregunta en una encuesta, una observación conductual o una resonancia.\nUna operacionalización: es la conexión lógica entre el constructo y la medida, o el proceso por el cual derivás una medida a partir del constructo teórico.\nUna variable: Finalmente, un término nuevo: una variable es lo que obtenemos cuando aplicamos una medida a algo del mundo real. Es decir, las variables son los datos concretos que terminamos registrando en nuestros conjuntos de datos.\n\nEn la práctica, incluso los científicos tienden a mezclar estos términos, pero es muy útil tratar de comprender en qué se distinguen.\n\n\n1.7.3 Escalas de medición\nComo vimos en la sección anterior, el resultado de una medición psicológica se llama variable. Pero no todas las variables son del mismo tipo cualitativo, y es muy útil entender qué tipos existen. Un concepto muy útil para distinguir entre tipos de variables es el de escalas de medición.\n\n\n1.7.4 Escala nominal\nUna variable de escala nominal (también llamada categórica) es aquella en la que no hay una relación particular entre las distintas opciones: para este tipo de variables no tiene sentido decir que una es “mayor” o “mejor” que otra, y mucho menos calcular un promedio. El ejemplo clásico es el “color de ojos”. Pueden ser azules, verdes o marrones, entre otras opciones, pero ninguna es mejor que otra. Por eso sería raro hablar de un “color de ojos promedio”. Lo mismo pasa con el género: ser hombre no es mejor ni peor que ser mujer, y no tiene sentido hablar de un “género promedio”. En resumen, las variables nominales son aquellas en las que lo único que podés decir es que las categorías son diferentes. Y nada más.\nVeamos esto más de cerca. Supongamos que estoy investigando cómo viaja la gente al trabajo. Una variable que tendría que medir es el medio de transporte. Esta variable “medio de transporte” podría incluir varios valores, incluyendo: “tren”, “ómnibus”, “auto”, “bicicleta”, etc. Supongamos por ahora que esas cuatro son las únicas opciones, y que al preguntar a 100 personas cómo llegaron hoy al trabajo, obtengo esto:\n\n\n\nMedio de transporte\nCantidad de personas\n\n\n\n\n(1) Tren\n12\n\n\n(2) Ómnibus\n30\n\n\n(3) Auto\n48\n\n\n(4) Bicicleta\n10\n\n\n\nEntonces, ¿cuál es el medio de transporte promedio? Obviamente, no lo hay. Es una pregunta sin sentido. Podés decir que el auto es el medio más popular y que el tren es el menos popular, pero nada más. Fijate además que el orden en que puse las opciones no importa mucho. Podría haber presentado los datos así, y nada cambia realmente:\n\n\n\nMedio de transporte\nCantidad de personas\n\n\n\n\n(3) Auto\n48\n\n\n(1) Tren\n12\n\n\n(4) Bicicleta\n10\n\n\n(2) Ómnibus\n30\n\n\n\n\n\n1.7.5 Escala ordinal\nLas variables de escala ordinal tienen un poco más de estructura que las variables de escala nominal, pero no mucho más. Una variable ordinal es aquella en la que hay una forma natural y significativa de ordenar las distintas opciones, pero no se puede hacer mucho más. El ejemplo habitual es la “posición de llegada en una carrera”. Podés decir que la persona que llegó primera fue más rápida que la que llegó segunda, pero no sabés cuánto más rápida. Como consecuencia, sabemos que 1.º \\(&gt;\\) 2.º y que 2.º \\(&gt;\\) 3.º, pero la diferencia entre 1.º y 2.º podría ser mucho mayor que la diferencia entre 2.º y 3.º.\nAcá hay un ejemplo más interesante desde el punto de vista psicológico. Supongamos que me interesa conocer las actitudes de las personas frente al cambio climático, y les pido que elijan una de estas cuatro afirmaciones, la que más se acerque a sus creencias:\n\n\nLas temperaturas están subiendo debido a la actividad humana\nLas temperaturas están subiendo, pero no sabemos por qué\nLas temperaturas están subiendo, pero no por causa humana\nLas temperaturas no están subiendo\n\n\nFijate que estas cuatro afirmaciones tienen un orden natural, en cuanto al “grado en que coinciden con el consenso científico actual”. La afirmación 1 coincide completamente, la 2 coincide de forma razonable, la 3 no coincide mucho, y la 4 está en fuerte desacuerdo con la ciencia. Así que, en relación con lo que me interesa (el grado en que la gente avala la ciencia), puedo ordenarlas como 1 \\(&gt;\\) 2 \\(&gt;\\) 3 $&gt;$4. Por eso, sería muy raro listar las opciones así…\n\n\nLas temperaturas están subiendo, pero no por causa humana\nLas temperaturas están subiendo debido a la actividad humana\nLas temperaturas no están subiendo\nLas temperaturas están subiendo, pero no sabemos por qué\n\n\n…porque parece violar la “estructura” natural de la pregunta.\nAhora supongamos que les hago esta pregunta a 100 personas y obtengo las siguientes respuestas:\n\n\n\n\n\n\n\n\nPersonas\n\n\n\n\n(1) Las temperaturas están subiendo debido a la actividad humana\n51\n\n\n(2) Las temperaturas están subiendo, pero no sabemos por qué\n20\n\n\n(3) Las temperaturas están subiendo, pero no por causa humana\n10\n\n\n(4) Las temperaturas no están subiendo\n19\n\n\n\nAl analizar estos datos, parece bastante razonable agrupar las respuestas (1), (2) y (3), y decir que 81 de 100 personas están dispuestas a al menos en parte avalar la ciencia. Y también es razonable agrupar (2), (3) y (4), y decir que 49 de 100 expresan al menos cierto desacuerdo con el consenso científico. Pero sería completamente extraño agrupar (1), (2) y (4) y decir que 90 de 100 personas dijeron… ¿qué? No hay nada sensato que permita agrupar esas respuestas.\nDicho eso, aunque sí podemos usar el orden natural de estas opciones para construir agrupamientos sensatos, lo que no podemos hacer es promediarlas. Por ejemplo, en este caso, la respuesta “promedio” sería 1.97. Si podés explicarme qué significa eso, me encantaría saberlo. ¡Porque a mí me suena a puro disparate!\n\n\n1.7.6 Escala de intervalo\nA diferencia de las variables de escala nominal y ordinal, las variables de escala de intervalo y de razón son aquellas en las que el valor numérico tiene un significado genuino. En el caso de las variables de escala de intervalo, las diferencias entre los números son interpretables, pero la variable no tiene un valor cero “natural”. Un buen ejemplo de una variable de escala de intervalo es la temperatura medida en grados Celsius. Por ejemplo, si ayer hizo 15 °C y hoy hace 18 °C, la diferencia de 3 °C entre ambos días tiene un significado real. Además, esa diferencia de 3 °C es exactamente la misma que la diferencia entre 7 °C y 10 °C. En resumen, la suma y la resta tienen sentido para variables de escala de intervalo.\nSin embargo, notá que 0 °C no significa “ausencia total de temperatura”: en realidad significa “la temperatura a la que se congela el agua”, lo cual es bastante arbitrario. Como consecuencia, resulta inútil intentar multiplicar o dividir temperaturas. Está mal decir que 20 °C es el doble de caliente que 10 °C, así como también es raro y carece de sentido decir que 20 °C es “menos dos veces más caliente” que –10 °C.\nVeamos otro ejemplo más relacionado con la psicología. Supongamos que me interesa estudiar cómo han cambiado las actitudes de los estudiantes de primer año de universidad a lo largo del tiempo. Obviamente, voy a querer registrar el año en que cada estudiante ingresó. Ese año es una variable de escala de intervalo. Un estudiante que ingresó en 2003 lo hizo cinco años antes que uno que ingresó en 2008. Sin embargo, sería completamente absurdo dividir 2008 entre 2003 y decir que la segunda persona ingresó “1,0024 veces más tarde” que la primera. Eso no tiene ningún sentido.\n\n\n\n1.7.7 Escala de razón\nEl cuarto y último tipo de variable que vamos a considerar es la variable de escala de razón, en la cual el cero realmente significa cero, y está bien multiplicar y dividir. Un buen ejemplo psicológico de una variable de escala de razón es el tiempo de respuesta. En muchas tareas es común registrar cuánto tiempo le lleva a alguien resolver un problema o responder una pregunta, ya que eso puede ser un indicador de cuán difícil es la tarea. Supongamos que Alan tarda 2,3 segundos en responder una pregunta, mientras que Ben tarda 3,1 segundos. Al igual que con una variable de escala de intervalo, la suma y la resta tienen sentido: Ben realmente tardó (3{,}1 - 2{,}3 = 0{,}8) segundos más que Alan. Sin embargo, también tiene sentido multiplicar y dividir: Ben tardó (3{,}1 / 2{,}3 = 1{,}35) veces más que Alan en responder. Y esto tiene sentido porque, para una variable de escala de razón como el tiempo de respuesta, “cero segundos” realmente significa “ningún tiempo en absoluto”.\n\n\n\n1.7.8 Variables continuas versus discretas\nHay otro tipo de distinción que tenés que tener en cuenta respecto a los tipos de variables con los que podés encontrarte. Es la distinción entre variables continuas y variables discretas. La diferencia es la siguiente:\n\nUna variable continua es aquella en la que, entre cualquier par de valores que se te ocurra, siempre es lógicamente posible que exista otro valor intermedio.\nUna variable discreta es, en efecto, una variable que no es continua. En una variable discreta, a veces no hay nada en el medio.\n\nEstas definiciones pueden parecer un poco abstractas, pero son bastante simples una vez que ves algunos ejemplos. Por ejemplo, el tiempo de respuesta es una variable continua. Si Alan tarda 3,1 segundos y Ben tarda 2,3 segundos en responder una pregunta, entonces es posible que Cameron haya tardado 3,0 segundos, es decir, un valor intermedio. Y claro, también sería posible que David tardara 3,031 segundos, lo que lo pondría entre Cameron y Alan. Aunque en la práctica tal vez no podamos medir el tiempo de respuesta con tanta precisión, en principio sí se puede. Como siempre es posible encontrar un nuevo valor entre cualquier par de valores, decimos que el tiempo de respuesta es una variable continua.\nLas variables discretas aparecen cuando esta regla se rompe. Por ejemplo, las variables de escala nominal son siempre discretas: no hay un medio de transporte que esté “entre” el tren y la bicicleta, al menos no de la manera estrictamente matemática en la que 2,3 está entre 2 y 3. Así que el tipo de transporte es una variable discreta. Del mismo modo, las variables ordinales también son siempre discretas: aunque el “2.º lugar” esté entre el “1.º” y el “3.º”, no hay nada que pueda caer entre el 1.º y el 2.º lugar. Las variables de escala de intervalo y de razón pueden ser continuas o discretas. Como vimos antes, el tiempo de respuesta (una variable de razón) es continua. La temperatura en grados Celsius (una variable de intervalo) también es continua. Pero el año en que alguien ingresó a la universidad (una variable de intervalo) es discreta: no hay ningún año entre 2002 y 2003. El número de respuestas correctas en un test de verdadero/falso (una variable de razón) también es discreto: como una pregunta de verdadero/falso no permite estar “parcialmente correcto”, no hay nada entre 5/10 y 6/10.\nLa siguiente tabla resume la relación entre las escalas de medición y la distinción continuo/discreto. Las celdas con una “x” marcan combinaciones posibles. Estoy tratando de dejar bien en claro este punto porque: (a) algunos manuales no lo explican bien, y (b) muy seguido la gente dice cosas como “variable discreta” cuando en realidad quiere decir “variable de escala nominal”. Es una confusión bastante desafortunada.\n\n\n\n\ncontinua\ndiscreta\n\n\n\n\nnominal\n\nx\n\n\nordinal\n\nx\n\n\nintervalo\nx\nx\n\n\nrazón\nx\nx\n\n\n\nRelación entre las escalas de medición y la distinción continuo/discreto. Las celdas con “x” marcan combinaciones posibles.\n\n\n1.7.9 Algunas complejidades\nBueno, sé que esto te va a sorprender, pero… el mundo real es mucho más desordenado de lo que sugiere esta pequeña clasificación. Muy pocas variables en la vida real encajan perfectamente en estas categorías ordenadas, así que hay que tener cuidado de no tratar las escalas de medición como si fueran reglas rígidas. No funcionan así: son guías pensadas para ayudarte a reflexionar sobre en qué situaciones conviene tratar a las variables de manera diferente. Nada más.\nAsí que tomemos un ejemplo clásico, quizás el ejemplo clásico, de una herramienta de medición psicológica: la escala de Likert. La humilde escala de Likert es el pan de cada día en el diseño de encuestas. Vos mismo habrás completado cientos, tal vez miles, y probablemente ya usaste una. Supongamos que tenés una pregunta en una encuesta que se ve así:\n\n¿Cuál de las siguientes opciones describe mejor tu opinión sobre la afirmación: “todos los piratas son increíblemente geniales”?\n\n\n\nTotalmente en desacuerdo\n\n\n\nEn desacuerdo\n\n\n\nNi de acuerdo ni en desacuerdo\n\n\n\nDe acuerdo\n\n\n\nTotalmente de acuerdo\n\n\nY luego se le presentan al participante las siguientes opciones:\n\n\nTotalmente en desacuerdo\n\nEn desacuerdo\n\nNi de acuerdo ni en desacuerdo\n\nDe acuerdo\n\nTotalmente de acuerdo\n\n\nEste conjunto de ítems es un ejemplo de una escala de Likert de 5 puntos: se le pide a la gente que elija entre una de varias (en este caso 5) posibilidades claramente ordenadas, generalmente con una descripción verbal en cada caso. Sin embargo, no es necesario que todos los ítems estén explícitamente etiquetados. Este también es un ejemplo perfectamente válido de una escala de Likert de 5 puntos:\n\n\nTotalmente en desacuerdo\n\n\n\n\nTotalmente de acuerdo\n\n\nLas escalas de Likert son herramientas muy útiles, aunque algo limitadas. La pregunta es: ¿qué tipo de variable son? Obviamente son discretas, ya que no se puede responder “2,5”. Claramente no son de escala nominal, porque los ítems están ordenados; y tampoco son de escala de razón, porque no hay un cero natural.\nPero entonces, ¿son de escala ordinal o de intervalo? Un argumento sostiene que no podemos realmente demostrar que la diferencia entre “totalmente de acuerdo” y “de acuerdo” sea del mismo tamaño que la diferencia entre “de acuerdo” y “ni de acuerdo ni en desacuerdo”. De hecho, en la vida cotidiana es bastante obvio que no son iguales. Esto sugiere que deberíamos tratar las escalas de Likert como variables ordinales. Por otro lado, en la práctica, la mayoría de los participantes parece tomarse bastante en serio eso de “en una escala del 1 al 5”, y tienden a comportarse como si las diferencias entre las cinco opciones de respuesta fueran más o menos similares entre sí. Como consecuencia, muchos investigadores tratan los datos de escalas de Likert como si fueran de escala de intervalo. No lo son realmente, pero en la práctica se parecen lo suficiente como para que usualmente pensemos en ellas como de cuasi-escala de intervalo.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>¿Por qué estadística?</span>"
    ]
  },
  {
    "objectID": "01-Science_Data.html#evaluación-de-la-confiabilidad-de-una-medición",
    "href": "01-Science_Data.html#evaluación-de-la-confiabilidad-de-una-medición",
    "title": "1  ¿Por qué estadística?",
    "section": "1.8 Evaluación de la confiabilidad de una medición",
    "text": "1.8 Evaluación de la confiabilidad de una medición\nA esta altura ya reflexionamos un poco sobre cómo operacionalizar un constructo teórico y, a partir de eso, construir una medida psicológica; y vimos que, al aplicar esas medidas, terminamos con variables, que pueden ser de muchos tipos distintos. En este punto, deberíamos empezar a discutir la pregunta obvia: ¿la medición es buena? Vamos a abordarlo en términos de dos ideas relacionadas: confiabilidad y validez. En términos simples, la confiabilidad de una medida te dice cuán precisamente estás midiendo algo, mientras que la validez te dice cuán exacta es esa medida.\nLa confiabilidad es en realidad un concepto muy simple: se refiere a la repetibilidad o consistencia de tu medición. Medir mi peso con una balanza de baño es muy confiable: si me subo y bajo varias veces, va a seguir marcando lo mismo. Medir mi inteligencia preguntándole a mi mamá no es nada confiable: algunos días me dice que soy medio lenta, y otros días que soy una completa idiota. Fijate que este concepto de confiabilidad es distinto de la cuestión de si la medición es correcta (la exactitud de una medición tiene que ver con su validez, no con su confiabilidad). Si estoy sosteniendo una bolsa de papas al subirme y bajarme de la balanza del baño, la medición seguirá siendo confiable: siempre me va a dar el mismo número. Sin embargo, esta respuesta altamente confiable no se corresponde en absoluto con mi peso real, por lo tanto está equivocada. En términos técnicos, esta es una medición confiable pero inválida. De forma similar, aunque la estimación que hace mi mamá de mi inteligencia sea poco confiable, podría tener razón. Tal vez simplemente no soy muy brillante, y aunque su estimación fluctúe bastante de un día a otro, en esencia sea correcta. Eso sería una medida inválida pero confiable. Claro que, hasta cierto punto, si las estimaciones de mi mamá son demasiado inconsistentes, va a ser muy difícil saber cuál de todas sus opiniones sobre mi inteligencia es la verdadera. En ese sentido, una medición muy poco confiable tiende a terminar siendo inválida en la práctica; tanto así que muchos dirían que la confiabilidad es necesaria (aunque no suficiente) para garantizar la validez.\nBien, ahora que tenemos clara la distinción entre confiabilidad y validez, pensemos en las distintas maneras en que podemos medir la confiabilidad:\n\nConfiabilidad test-retest. Se refiere a la consistencia en el tiempo: si repetimos la medición más adelante, ¿obtenemos la misma respuesta?\nConfiabilidad entre evaluadores (inter-rater reliability). Se refiere a la consistencia entre personas: si otra persona repite la medición (por ejemplo, si otra persona evalúa mi inteligencia), ¿obtiene la misma respuesta?\nConfiabilidad de formas paralelas. Se refiere a la consistencia entre mediciones teóricamente equivalentes: si uso otra balanza para pesarme, ¿marca lo mismo?\nConfiabilidad de consistencia interna. Si una medición está compuesta por muchas partes que cumplen funciones similares (por ejemplo, una escala de personalidad que suma varias preguntas), ¿tienden esas partes a dar resultados similares?\n\nNo todas las mediciones necesitan cumplir todas las formas de confiabilidad. Por ejemplo, la evaluación educativa puede pensarse como una forma de medición. En una de las materias que enseño, Ciencia Cognitiva Computacional, la estructura de evaluación incluye un componente de investigación y un examen (entre otras cosas). El examen está diseñado para medir algo diferente al componente de investigación, así que la evaluación total tiene una baja consistencia interna. Sin embargo, dentro del examen hay varias preguntas que están pensadas para medir más o menos lo mismo, y esas tienden a producir resultados similares; por lo tanto, el examen por sí solo tiene una consistencia interna bastante alta. Que es como debería ser. Solo deberías exigir confiabilidad cuando realmente querés medir la misma cosa.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>¿Por qué estadística?</span>"
    ]
  },
  {
    "objectID": "01-Science_Data.html#el-rol-de-las-variables-predictoras-y-dependientes",
    "href": "01-Science_Data.html#el-rol-de-las-variables-predictoras-y-dependientes",
    "title": "1  ¿Por qué estadística?",
    "section": "1.9 El rol de las variables: predictoras y dependientes",
    "text": "1.9 El rol de las variables: predictoras y dependientes\nBien, me queda una última pieza de terminología que necesito explicarte antes de dejar atrás el tema de las variables. Normalmente, cuando hacemos una investigación terminamos con muchas variables distintas. Luego, cuando analizamos nuestros datos, usualmente tratamos de explicar algunas de esas variables en función de otras. Es importante mantener bien separadas las dos funciones: la “que explica” y la “que es explicada”. Así que dejemos esto claro desde ahora.\nPrimero, conviene ir acostumbrándonos a la idea de usar símbolos matemáticos para describir variables, porque lo vamos a hacer muchas veces. Vamos a denotar la variable “que queremos explicar” como \\(Y\\), y las variables “que la explican” como \\(X_1\\), \\(X_2\\), etc.\nAhora bien, cuando hacemos un análisis, usamos nombres diferentes para \\(X\\) e \\(Y\\), ya que cumplen distintos roles. Los nombres clásicos para estos roles son variable independiente (VI) y variable dependiente (VD). La VI es la que usamos para explicar (es decir, \\(X\\)), y la VD es la que queremos explicar (es decir, \\(Y\\)). La lógica detrás de estos nombres es algo así: si hay una relación entre \\(X\\) e \\(Y\\), entonces podemos decir que \\(Y\\) depende de \\(X\\), y si el estudio está bien diseñado, entonces \\(X\\) no depende de nada más. Sin embargo, personalmente encuentro estos nombres horribles: son difíciles de recordar y muy engañosos, porque (a) la VI nunca es realmente “independiente de todo lo demás”, y (b) si no hay relación, entonces la VD no depende de la VI. Y de hecho, como no soy la única que piensa que “VI” y “VD” son nombres espantosos, hay varias alternativas que me parecen más acertadas.\nPor ejemplo, en un experimento, la VI se refiere a la manipulación, y la VD a la medición. Así que podríamos usar variable manipulada (para la independiente) y variable medida (para la dependiente).\n\nTerminología utilizada para distinguir los distintos roles que puede cumplir una variable en el análisis de datos.\n\n\nRol de la variable\nNombre clásico\nNombre moderno\n\n\n\n\n“la que se explica”\nvariable dependiente (VD)\nMedición\n\n\n“la que explica”\nvariable independiente (VI)\nManipulación\n\n\n\nTambién podríamos usar predictoras y resultados. La idea es que tratamos de usar \\(X\\) (las predictoras) para hacer conjeturas sobre \\(Y\\) (los resultados). Esto se resume en la siguiente tabla:\n\nTerminología utilizada para distinguir los distintos roles que puede cumplir una variable en el análisis de datos.\n\n\nRol de la variable\nNombre clásico\nNombre moderno\n\n\n\n\n“la que se explica”\nvariable dependiente (VD)\nresultado\n\n\n“la que explica”\nvariable independiente (VI)\npredictora",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>¿Por qué estadística?</span>"
    ]
  },
  {
    "objectID": "01-Science_Data.html#investigación-experimental-y-no-experimental",
    "href": "01-Science_Data.html#investigación-experimental-y-no-experimental",
    "title": "1  ¿Por qué estadística?",
    "section": "1.10 Investigación experimental y no experimental",
    "text": "1.10 Investigación experimental y no experimental\nUna de las grandes distinciones que conviene tener presente es la diferencia entre “investigación experimental” e “investigación no experimental”. Cuando hacemos esta distinción, en realidad estamos hablando del grado de control que tiene la persona investigadora sobre los participantes y los eventos del estudio.\n\n1.10.1 Investigación experimental\nLa característica clave de la investigación experimental es que la persona investigadora controla todos los aspectos del estudio, especialmente lo que los participantes experimentan durante el mismo. En particular, manipula o varía algo (las variables independientes), y luego deja que la variable dependiente varíe de forma natural. La idea es variar deliberadamente algo en el mundo (la VI) para ver si eso tiene algún efecto causal sobre los resultados. Además, para asegurarse de que no haya otra cosa, aparte de la variable manipulada, causando los resultados, todo lo demás se mantiene constante o se “balancea” de alguna manera para que no afecte el experimento. En la práctica, es casi imposible pensar en todas las demás cosas que podrían influir en los resultados de un experimento, y mucho menos mantenerlas constantes. La solución estándar para esto es la aleatorización: es decir, asignar al azar a las personas a distintos grupos, y luego darle a cada grupo un tratamiento distinto (es decir, asignarles distintos valores de las variables predictoras). Hablaremos más de la aleatorización más adelante en el curso, pero por ahora alcanza con decir que lo que hace la aleatorización es minimizar (aunque no eliminar) las probabilidades de que haya diferencias sistemáticas entre los grupos.\nVeamos un ejemplo muy simple, completamente irreal y groseramente antiético. Supongamos que querés averiguar si fumar causa cáncer de pulmón. Una forma de hacerlo sería encontrar personas que fuman y personas que no fuman, y ver si les fumadores tienen una tasa más alta de cáncer de pulmón. Esto no es un experimento propiamente dicho, porque la persona investigadora no tiene mucho control sobre quién fuma y quién no. Y eso importa: por ejemplo, podría ser que quienes eligen fumar también tiendan a tener dietas poco saludables, o que trabajen en minas de asbesto, o lo que sea. El punto es que los grupos (fumadores y no fumadores) difieren en muchas cosas, no solo en fumar. Así que podría ser que la mayor incidencia de cáncer de pulmón entre fumadores se deba a otra cosa, y no al tabaquismo en sí. En términos técnicos, esas otras cosas (como la dieta) se llaman “confusores” (o confounds, en inglés), y hablaremos de eso en un momento.\nMientras tanto, veamos cómo sería un experimento propiamente dicho. Recordá que nuestra preocupación era que fumadores y no fumadores podrían diferir en muchas cosas. La solución —siempre y cuando no tengas ética— es controlar quién fuma y quién no. Específicamente, si dividimos aleatoriamente a los participantes en dos grupos, y forzamos a la mitad a convertirse en fumadores, entonces es muy poco probable que los grupos difieran en otra cosa aparte del hecho de que la mitad fuma. De ese modo, si el grupo que fuma desarrolla cáncer a una tasa más alta que el grupo que no fuma, entonces podemos tener bastante confianza en que (a) fumar sí causa cáncer y (b) somos asesinos.\n\n\n1.10.2 Investigación no experimental\nLa investigación no experimental es un término amplio que abarca “cualquier estudio en el que la persona investigadora no tiene tanto control como en un experimento”. Obviamente, tener control es algo que los científicos suelen querer, pero como ilustra el ejemplo anterior, hay muchas situaciones en las que no podés (o no deberías) intentar ejercer ese control. Ya que es profundamente antiético (y casi con certeza ilegal) forzar a alguien a fumar para ver si desarrolla cáncer, este es un buen ejemplo de una situación en la que definitivamente no deberías tratar de controlar experimentalmente. Pero también hay otras razones. Incluso dejando de lado las cuestiones éticas, nuestro “experimento del tabaco” tiene algunos problemas más. Por ejemplo, cuando sugerí que forzáramos a la mitad de los participantes a fumar, estaba hablando de empezar con una muestra de personas no fumadoras, y luego forzarlas a convertirse en fumadoras. Si bien esto suena como el sólido y perverso diseño experimental que disfrutaría un científico loco, tal vez no sea una manera muy válida de investigar el efecto en el mundo real. Supongamos, por ejemplo, que fumar solo causa cáncer de pulmón cuando las personas tienen dietas poco saludables, y que además quienes normalmente fuman tienden a tener dietas poco saludables. Sin embargo, como en nuestro experimento los “fumadores” no son fumadores “naturales” (es decir, forzamos a personas no fumadoras a empezar a fumar, sin que adoptaran todas las demás características típicas de los fumadores en la vida real), es probable que tengan mejores dietas. Como resultado, en este ejemplo tonto, puede que no desarrollen cáncer de pulmón, y entonces nuestro experimento fallaría, porque viola la estructura del mundo “natural” (el nombre técnico de esto es un resultado “artefactual”; ya hablaremos de eso más adelante).\nUna distinción útil entre dos tipos de investigación no experimental es la diferencia entre la investigación cuasi-experimental y los estudios de caso. El ejemplo que discutimos antes —en el que queremos examinar la incidencia de cáncer de pulmón entre fumadores y no fumadores, sin controlar quién fuma y quién no— es un diseño cuasi-experimental. Es decir, se parece a un experimento, pero no controlamos las variables predictoras (VIs). Igual podemos usar estadísticas para analizar los resultados, solo que tenemos que ser mucho más cuidadosos.\nLa alternativa, los estudios de caso, apuntan a brindar una descripción muy detallada de uno o unos pocos casos. En general, no se pueden usar estadísticas para analizar los resultados de estudios de caso, y usualmente es muy difícil sacar conclusiones generales sobre “la gente en general” a partir de unos pocos ejemplos aislados. Sin embargo, los estudios de caso son muy útiles en algunas situaciones. Primero, hay ocasiones en las que no tenés otra alternativa: la neuropsicología se encuentra con esto todo el tiempo. A veces no se puede encontrar muchas personas con daño cerebral en una zona específica, así que lo único que podés hacer es describir con el mayor detalle y cuidado posible los casos que sí tenés. Sin embargo, los estudios de caso también tienen ventajas genuinas: como no tenés tantas personas para estudiar, podés invertir mucho más tiempo y esfuerzo en tratar de entender los factores específicos en juego en cada caso. Y eso es algo muy valioso. Como consecuencia, los estudios de caso pueden complementar los enfoques más estadísticos que ves en los diseños experimentales y cuasi-experimentales. No vamos a hablar mucho de estudios de caso en estas clases, ¡pero siguen siendo herramientas muy valiosas!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>¿Por qué estadística?</span>"
    ]
  },
  {
    "objectID": "01-Science_Data.html#evaluar-la-validez-de-un-estudio",
    "href": "01-Science_Data.html#evaluar-la-validez-de-un-estudio",
    "title": "1  ¿Por qué estadística?",
    "section": "1.11 Evaluar la validez de un estudio",
    "text": "1.11 Evaluar la validez de un estudio\nMás que cualquier otra cosa, un científico quiere que su investigación sea “válida”. La idea conceptual detrás de la validez es muy simple: ¿podés confiar en los resultados de tu estudio? Si no, el estudio es inválido. Sin embargo, aunque es fácil de enunciar, en la práctica es mucho más difícil verificar la validez que la confiabilidad. Y para ser sinceros, no hay una noción precisa y universalmente aceptada de qué es la validez. De hecho, existen muchos tipos diferentes de validez, cada una con sus propios problemas, y no todas son relevantes para todos los estudios. Voy a hablar de cinco tipos diferentes:\n\nValidez interna\n\nValidez externa\n\nValidez de constructo\n\nValidez aparente\n\nValidez ecológica\n\nPara darte una guía rápida de lo que importa acá… (1) La validez interna y externa son las más importantes, ya que están directamente relacionadas con la pregunta fundamental de si tu estudio realmente funciona. (2) La validez de constructo pregunta si estás midiendo lo que creés que estás midiendo. (3) La validez aparente no es especialmente importante, excepto en la medida en que te preocupen las “apariencias”. (4) La validez ecológica es un caso especial de validez aparente que se refiere a un tipo de apariencia que puede importarte bastante.\n\n1.11.1 Validez interna\nLa validez interna se refiere al grado en que podés sacar conclusiones correctas sobre las relaciones causales entre variables. Se llama “interna” porque refiere a las relaciones entre cosas “dentro” del estudio. Vamos a ilustrar este concepto con un ejemplo simple. Supongamos que te interesa saber si una educación universitaria mejora la escritura. Para eso, reunís un grupo de estudiantes de primer año, les pedís que escriban un ensayo de 1000 palabras, y contás los errores ortográficos y gramaticales. Luego hacés lo mismo con estudiantes de tercer año, que obviamente tienen más educación universitaria que los de primer año. Y supongamos que los de tercer año cometen menos errores. Entonces concluís que la educación universitaria mejora las habilidades de escritura. ¿No? El gran problema de este experimento es que los estudiantes de tercer año son mayores y tienen más experiencia escribiendo. Así que no es fácil saber cuál es la relación causal real: ¿las personas mayores escriben mejor? ¿O las personas con más experiencia escribiendo? ¿O las personas con más educación? ¿Cuál de estas es la verdadera causa del mejor desempeño de los de tercer año? ¿La edad? ¿La experiencia? ¿La educación? No lo podés saber. Este es un ejemplo de una falla de validez interna, porque tu estudio no permite desentrañar bien las relaciones causales entre las variables.\n\n\n1.11.2 Validez externa\nLa validez externa se refiere a la generalización de tus resultados. Es decir, hasta qué punto esperás ver el mismo patrón de resultados en la “vida real” que el que observaste en tu estudio. Más precisamente, cualquier estudio que hagas en psicología va a involucrar un conjunto bastante específico de preguntas o tareas, va a ocurrir en un entorno específico y va a incluir participantes de un subgrupo particular. Entonces, si resulta que los resultados no se generalizan a personas o situaciones fuera de las que estudiaste, entonces tenés un problema de validez externa.\nEl ejemplo clásico de este problema es el hecho de que una gran proporción de los estudios en psicología usan estudiantes universitarios de psicología como participantes. Pero claramente, a los investigadores no les interesa solo ese grupo: les interesa la gente en general. Dado eso, un estudio que solo incluye estudiantes de psicología como participantes siempre corre el riesgo de carecer de validez externa. Es decir, si hay algo “especial” en los estudiantes de psicología que los hace diferentes de la población general en algún aspecto relevante, entonces podríamos preocuparnos por la validez externa.\nDicho esto, es absolutamente crucial darse cuenta de que un estudio que incluye únicamente estudiantes de psicología no necesariamente tiene un problema de validez externa. Lo voy a mencionar más adelante, pero como es un error muy común, lo digo ya: la validez externa se ve amenazada por la elección de población si (a) la población de la que tomaste tu muestra es muy limitada (por ejemplo, estudiantes de psicología) y (b) esa población limitada es sistemáticamente diferente de la población general en algún aspecto relevante para el fenómeno psicológico que querés estudiar. Esa parte en cursiva es la que muchos olvidan: es cierto que los estudiantes de psicología difieren de la población general en muchas cosas, así que un estudio que solo incluye estudiantes puede tener problemas de validez externa. Pero si esas diferencias no son relevantes para el fenómeno que estás estudiando, entonces no hay de qué preocuparse. Para hacerlo más concreto, acá van dos ejemplos extremos:\n\nQuerés medir “las actitudes del público general hacia la psicoterapia”, pero todes tus participantes son estudiantes de psicología. Este estudio casi con certeza tendría problemas de validez externa.\nQuerés medir la efectividad de una ilusión visual, y todos tus participantes son estudiantes de psicología. Este estudio muy probablemente no tendría problemas de validez externa.\n\nDespués de haberme centrado varios párrafos en la elección de participantes (porque es el tema que más preocupa a todos), vale la pena recordar que la validez externa es un concepto más amplio. Los siguientes también son ejemplos de cosas que pueden amenazar la validez externa, según qué tipo de estudio estés haciendo:\n\nPuede que la gente responda un “cuestionario de psicología” de un modo que no refleja cómo actuaría en la vida real.\nTu experimento de laboratorio sobre “aprendizaje humano” podría tener una estructura diferente de los problemas de aprendizaje que la gente enfrenta fuera del laboratorio.\n\n\n\n1.11.3 Validez de constructo\nLa validez de constructo es, básicamente, la pregunta de si estás midiendo lo que realmente querés medir. Una medición tiene buena validez de constructo si realmente mide el constructo teórico correcto, y mala validez de constructo si no lo hace. Para dar un ejemplo muy simple (aunque ridículo), supongamos que quiero investigar qué porcentaje de estudiantes universitarios hacen trampa en los exámenes. Y la manera en que intento medirlo es pidiéndoles a los estudiantes que hacen trampa que se paren en el aula para que pueda contarlos. Cuando hago esto en una clase de 300 estudiantes, nadie se levanta. Entonces concluyo que el porcentaje de estudiantes que hacen trampa es 0%. Claramente esto es ridículo. Pero el punto acá no es que sea un gran ejemplo metodológico, sino que ilustra qué es la validez de constructo. El problema con mi medición es que aunque intento medir “el porcentaje de personas que hacen trampa”, lo que en realidad estoy midiendo es “el porcentaje de personas lo suficientemente tontas como para admitirlo, o lo suficientemente provocadoras como para fingir que lo hacen”. Obviamente, ¡eso no es lo mismo! Así que mi estudio sale mal porque mi medición tiene una validez de constructo muy pobre.\n\n\n1.11.4 Validez aparente\nLa validez aparente simplemente se refiere a si una medición “parece” estar haciendo lo que se supone que hace, y nada más. Si diseño un test de inteligencia, y la gente lo mira y dice “no, ese test no mide inteligencia”, entonces mi medición carece de validez aparente. Así de simple. Obviamente, la validez aparente no es muy importante desde una perspectiva científica pura. Después de todo, lo que nos importa es si la medición realmente hace lo que se supone que haga, no si parece hacerlo. Como consecuencia, generalmente no nos preocupa demasiado la validez aparente. Dicho esto, el concepto tiene tres usos pragmáticos útiles:\n\nA veces, un científico con experiencia tiene la “intuición” de que una medición no va a funcionar. Aunque esas intuiciones no tienen valor como evidencia en sí mismas, suele valer la pena prestarles atención. Porque muchas veces las personas tienen conocimientos que no pueden verbalizar del todo, puede haber algo preocupante incluso si no puedes explicar bien qué. En otras palabras, si alguien en quien confiás critica la validez aparente de tu estudio, conviene tomarse el tiempo de revisar bien el diseño para ver si encontrás razones por las que podría fallar. Ahora bien, si no encontrás nada, entonces probablemente no haya motivo para preocuparse: al fin y al cabo, la validez aparente no es muy relevante.\nMuchas veces (muchísimas), personas completamente desinformadas también tienen la “intuición” de que tu investigación es mala. Y la van a criticar en internet o donde sea. Si mirás de cerca, vas a notar que muchas de esas críticas están centradas únicamente en cómo “luce” el estudio, pero no en nada más profundo. El concepto de validez aparente sirve para explicarles amablemente que necesitan fundamentar mejor sus argumentos.\nEn relación con lo anterior: si las creencias de personas sin formación científica son clave (por ejemplo, en investigación aplicada donde querés convencer a responsables de políticas), entonces tenés que preocuparte por la validez aparente. Simplemente porque —te guste o no— mucha gente va a usar la validez aparente como sustituto de la validez real. Si querés que el gobierno cambie una ley con base en datos científicos o psicológicos, no va a importar cuán buenos sean realmente tus estudios. Si carecen de validez aparente, los políticos te van a ignorar. Claro que es algo injusto que las políticas dependan más de las apariencias que en hechos, pero así funcionan las cosas.\n\n\n\n1.11.5 Validez ecológica\nLa validez ecológica es una noción diferente de validez, parecida a la validez externa, pero menos importante. La idea es que, para que un estudio sea ecológicamente válido, toda su configuración debería parecerse mucho al escenario real que se está investigando. En cierto sentido, la validez ecológica es un tipo de validez aparente —efiere principalmente a si el estudio “luce” correcto, pero con un poco más de rigor. Para que tenga validez ecológica, el estudio tiene que parecer correcto de una manera bastante específica. La idea que la sustenta es la intuición de que un estudio con validez ecológica tiene más probabilidades de tener validez externa. No es una garantía, claro. Pero lo bueno de la validez ecológica es que es mucho más fácil de evaluar que la validez externa. Un ejemplo simple serían los estudios de identificación de testigos. Muchos de estos estudios suelen hacerse en entornos universitarios, a menudo con una disposición bastante simple de rostros para observar, en lugar de una rueda de reconocimiento real. El tiempo entre ver al “criminal” y que se le pida a la persona que lo identifique en la “rueda” suele ser más corto. El “crimen” no es real, así que no hay posibilidad de que el testigo esté asustado, y no hay policías presentes, así que hay menos probabilidad de que sientan presión. Todo eso significa que el estudio definitivamente carece de validez ecológica. Tal vez (o tal vez no) también carezca de validez externa.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>¿Por qué estadística?</span>"
    ]
  },
  {
    "objectID": "01-Science_Data.html#confusores-artefactos-y-otras-amenazas-a-la-validez",
    "href": "01-Science_Data.html#confusores-artefactos-y-otras-amenazas-a-la-validez",
    "title": "1  ¿Por qué estadística?",
    "section": "1.12 Confusores, artefactos y otras amenazas a la validez",
    "text": "1.12 Confusores, artefactos y otras amenazas a la validez\nSi consideramos el tema de la validez en términos generales, las dos grandes preocupaciones que tenemos son los confusores y los artefactos. Estos dos términos se definen de la siguiente manera:\n\nConfusor: Un confusor es una variable adicional, a menudo no medida, que resulta estar relacionada tanto con las variables predictoras como con los resultados. La existencia de confusores amenaza la validez interna del estudio, porque no podés saber si la variable predictora causa el resultado, o si lo causa la variable confusora, etc.\nArtefacto: Se dice que un resultado es “artefactual” si solo se sostiene en la situación especial que se usó para hacer el estudio. La posibilidad de que tu resultado sea un artefacto representa una amenaza a la validez externa, porque plantea la posibilidad de que no puedas generalizar tus resultados a la población real que te interesa.\n\nComo regla general, los confusores son una preocupación mayor en los estudios no experimentales, precisamente porque no son experimentos propiamente dichos: por definición, estás dejando muchas cosas sin controlar, así que hay mucho margen para que los confusores se metan en tu estudio. La investigación experimental tiende a ser mucho menos vulnerable a los confusores: cuanto más control tenés sobre lo que ocurre durante el estudio, más podés evitar que aparezcan confusores.\nSin embargo, una de cal y otra de arena: cuando empezamos a pensar en artefactos en lugar de confusores, la situación se invierte por completo. En general, los resultados artefactuales tienden a ser una preocupación mayor en los estudios experimentales que en los no experimentales. Para ver esto, ayuda entender que muchas veces los estudios son no experimentales precisamente porque la persona investigadora quiere examinar el comportamiento humano en un contexto más naturalista. Al trabajar en un contexto más cercano al mundo real, perdés control experimental (lo que te hace vulnerable a confusores), pero como tendés a estudiar la psicología humana “en estado salvaje”, reducís las probabilidades de obtener un resultado artefactual. O, dicho de otra manera, cuando sacás la psicología del entorno natural y la llevás al laboratorio (lo que normalmente tenemos que hacer para ganar control experimental), siempre corrés el riesgo de terminar estudiando algo diferente de lo que realmente querías estudiar: lo cual es, más o menos, la definición de un artefacto.\nAdvertencia: lo anterior es solo una guía general. Es totalmente posible que haya confusores en un experimento, y también obtener resultados artefactuales en estudios no experimentales. Esto puede pasar por muchas razones, entre ellas, errores de la persona investigadora. En la práctica, es realmente difícil prever todo por adelantado, y hasta los investigadores más competentes cometen errores. Pero otras veces es inevitable, simplemente porque la persona investigadora tiene ética (por ejemplo, ver “abandono diferencial”).\nBien. En cierto sentido, casi cualquier amenaza a la validez puede caracterizarse como un confusor o un artefacto: son conceptos bastante vagos. Así que veamos algunos de los ejemplos más comunes…\n\n1.12.1 Efectos de historia\nLos efectos de historia se refieren a la posibilidad de que ocurran eventos específicos durante el desarrollo del estudio que puedan influir en los resultados. Por ejemplo, algo podría pasar entre una medición pretest y una postest. O entre los participantes 23 y 24. Alternativamente, puede ser que estés revisando un estudio antiguo, que era perfectamente válido en su momento, pero el mundo ha cambiado lo suficiente desde entonces como para que sus conclusiones ya no sean confiables. Algunos ejemplos de lo que contarían como efectos de historia:\n\nEstás interesado en cómo la gente piensa sobre el riesgo y la incertidumbre. Comenzás a recolectar datos en diciembre de 2010. Pero encontrar participantes y juntar datos lleva tiempo, así que seguís trabajando en eso durante febrero de 2011. Lamentablemente para vos (y aún más lamentablemente para otros), las inundaciones en Queensland ocurrieron en enero de 2011, causando miles de millones de dólares en daños y matando a muchas personas. No sorprende que las personas evaluadas en febrero de 2011 expresen creencias bastante distintas sobre cómo manejar el riesgo que aquellas evaluadas en diciembre de 2010. ¿Cuál (si alguna) de estas refleja las creencias “verdaderas” de los participantes? Creo que la respuesta probablemente sea ambas: las inundaciones en Queensland cambiaron genuinamente las creencias del público australiano, aunque tal vez solo temporalmente. Lo importante acá es que la “historia” de las personas evaluadas en febrero es bastante distinta de la de quienes fueron evaluades en diciembre.\nEstás evaluando los efectos psicológicos de una nueva droga contra la ansiedad. Entonces lo que hacés es medir la ansiedad antes de administrar la droga (por ejemplo, mediante autoinforme y medidas fisiológicas, digamos), luego administrás la droga, y después volvés a tomar las mismas medidas. Sin embargo, en el medio, como tu laboratorio está en Los Ángeles, ocurre un terremoto, lo cual incrementa la ansiedad de los participantes.\n\n\n\n1.12.2 Efectos de maduración\nAl igual que los efectos de historia, los efectos de maduración están fundamentalmente relacionados con el cambio a lo largo del tiempo. Sin embargo, los efectos de maduración no son una respuesta a eventos específicos. Más bien, tienen que ver con cómo cambian las personas por sí solas con el paso del tiempo: envejecemos, nos cansamos, nos aburrimos, etc. Algunos ejemplos de efectos de maduración:\n\nCuando hacés investigación en psicología del desarrollo, necesitás tener en cuenta que los niños crecen bastante rápido. Supongamos que querés averiguar si cierta estrategia educativa ayuda a aumentar el vocabulario en niños de 3 años. Algo que necesitás considerar es que el tamaño del vocabulario en niños de esa edad está creciendo a un ritmo increíble (varias palabras por día), por sí solo. Si diseñás tu estudio sin tener en cuenta este efecto de maduración, entonces no vas a poder saber si tu estrategia educativa realmente funciona.\nCuando realizás un experimento muy largo en el laboratorio (digamos, algo que dura 3 horas), es muy probable que las personas empiecen a aburrirse y a cansarse, y que ese efecto de maduración va a causar una disminución en el rendimiento, independientemente de cualquier otra cosa que esté pasando en el experimento.\n\n\n\n1.12.3 Efectos de pruebas repetidas\nUn tipo importante de efecto de historia es el efecto de las pruebas repetidas. Supongamos que quiero tomar dos mediciones de algún constructo psicológico (por ejemplo, ansiedad). Algo que podría preocuparme es si la primera medición afecta la segunda. En otras palabras, este es un efecto de historia en el que el “evento” que influye sobre la segunda medición es ¡la primera medición en sí misma! Esto no es nada raro. Algunos ejemplos de esto incluyen:\n\nAprendizaje y práctica: por ejemplo, la “inteligencia” en el tiempo 2 podría parecer más alta que en el tiempo 1 porque los participantes aprendieron las reglas generales para resolver preguntas “tipo test de inteligencia” durante la primera sesión.\nFamiliaridad con la situación de evaluación: por ejemplo, si las personas están nerviosas en el tiempo 1, eso podría hacer que su rendimiento baje; después de atravesar la primera sesión, podrían calmarse precisamente porque ya vieron cómo es la situación de evaluación.\nCambios auxiliares causados por la evaluación: por ejemplo, si un cuestionario para medir el estado de ánimo es aburrido, entonces el estado de ánimo medido en el tiempo 2 probablemente sea “aburrido”, justamente por haber pasado por una evaluación aburrida en el tiempo 1.\n\n\n\n1.12.4 Sesgo de selección\nEl sesgo de selección es un término bastante amplio. Supongamos que estás llevando a cabo un experimento con dos grupos de participantes, donde cada grupo recibe un “tratamiento” diferente, y querés ver si esos tratamientos generan distintos resultados. Sin embargo, supongamos que, a pesar de tus mejores esfuerzos, terminás con un desequilibrio de género entre los grupos (por ejemplo, el grupo A tiene 80% mujeres y el grupo B tiene 50%). Puede sonar como algo que nunca pasaría, pero creeme, pasa. Este es un ejemplo de sesgo de selección, en el cual las personas que fueron asignadas a los dos grupos tienen características distintas. Si resulta que alguna de esas características es relevante (por ejemplo, que tu tratamiento funciona mejor para mujeres que para varones), entonces estás en serios problemas.\n\n\n1.12.5 Abandono diferencial\nUn peligro bastante sutil al que hay que prestarle atención se llama abandono diferencial, que es un tipo de sesgo de selección causado por el propio estudio. Supongamos que, por primera vez en la historia de la psicología, logro encontrar una muestra perfectamente balanceada y representativa de personas. Empiezo a correr “el experimento increíblemente largo y tedioso de Dan” sobre mi muestra perfecta, pero como mi estudio es increíblemente largo y tedioso, mucha gente empieza a abandonar. No puedo evitarlo: como vamos a discutir más adelante en el capítulo sobre ética en la investigación, los participantes tienen absolutamente el derecho de dejar cualquier experimento, en cualquier momento, por la razón que se les ocurra, y como investigadores tenemos la obligación moral (y profesional) de recordarles que tienen ese derecho. Entonces, supongamos que “el experimento increíblemente largo y tedioso de Dan” tiene una tasa de abandono muy alta. ¿Qué probabilidades creés que hay de que ese abandono sea aleatorio? Respuesta: cero. Casi con certeza, las personas que permanecen son más concienzudas, más tolerantes al aburrimiento, etc., que aquellas que se van. En la medida en que (por ejemplo) la concienzudez sea relevante para el fenómeno psicológico que me interesa, este abandono puede disminuir la validez de mis resultados.\nCuando se piensa en los efectos del abandono diferencial, a veces resulta útil distinguir entre dos tipos diferentes. El primero es el abandono homogéneo, en el que el efecto del abandono es el mismo para todos los grupos, tratamientos o condiciones. En el ejemplo que di más arriba, el abandono diferencial sería homogéneo si (y solo si) los participantes que se aburren fácilmente están abandonando todas las condiciones del experimento más o menos al mismo ritmo. En general, el principal efecto del abandono homogéneo es que puede volver tu muestra no representativa. En ese caso, la principal preocupación es que la generalización de los resultados disminuye: en otras palabras perdés validez externa.\nEl segundo tipo de abandono diferencial es el abandono heterogéneo, en el que el efecto del abandono es distinto para diferentes grupos. Este es un problema mucho más grave: no solo tenés que preocuparte por tu validez externa, sino también por tu validez interna. Para ver por qué es así, consideremos un estudio muy tonto en el que quiero ver si insultar a las personas hace que actúen de manera más obediente. No sé por qué alguien querría estudiar eso, pero supongamos que a mí me importa muchísimo. Entonces, diseño mi experimento con dos condiciones. En la condición de “tratamiento”, la persona experimentadora insulta a la persona participante y luego le da un cuestionario diseñado para medir obediencia. En la condición de “control”, la persona experimentadora conversa un poco sin decir nada importante y luego le da el mismo cuestionario. Dejando de lado los dudosos méritos científicos y éticos de tal estudio, pensemos qué podría salir mal acá. Como regla general, cuando alguien me insulta en la cara, tiendo a volverme mucho menos cooperativo. Así que hay una buena probabilidad de que muchas más personas abandonen la condición de tratamiento que la condición de control. Y ese abandono no va a ser aleatorio. Las personas más propensas a abandonar probablemente sean aquellas a las que no les importa tanto la importancia de quedarse obedientemente en el experimento. Como las personas más testarudas y desobedientes se fueron del grupo de tratamiento pero no del grupo de control, hemos introducido un confusor: las personas que efectivamente completaron el cuestionario en el grupo de tratamiento ya eran más propensas a ser obedientes y cumplidoras que las del grupo de control. En resumen, en este estudio insultar a la gente no la vuelve más obediente: ¡hace que los más desobedientes se vayan del experimento! La validez interna de este experimento queda completamente arruinada.\n\n\n1.12.6 Sesgo por no respuesta\nEl sesgo por no respuesta está estrechamente relacionado con el sesgo de selección y con el abandono diferencial. La versión más simple del problema va así: enviás una encuesta por correo a 1000 personas, y solo 300 responden. Las 300 personas que respondieron casi con seguridad no son una submuestra aleatoria. Las personas que responden encuestas son sistemáticamente diferentes de las que no lo hacen. Esto genera un problema cuando intentás generalizar a partir de esas 300 personas que sí respondieron hacia toda la población, ya que ahora tenés una muestra muy poco aleatoria.\nSin embargo, el problema del sesgo por no respuesta es más general que eso. Entre las (digamos) 300 personas que respondieron la encuesta, podrías encontrar que no todas contestan todas las preguntas. Si (por ejemplo) 80 personas eligieron no responder una de tus preguntas, ¿eso introduce problemas? Como siempre, la respuesta es: tal vez. Si la pregunta que no se respondió estaba en la última página del cuestionario, y esas 80 encuestas fueron devueltas sin la última página, hay una buena probabilidad de que esos datos faltantes no sean un gran problema: probablemente las páginas simplemente se desprendieron. Sin embargo, si la pregunta que 80 personas no respondieron era la más confrontativa o invasiva del cuestionario, entonces casi con certeza tenés un problema. En esencia, lo que estás enfrentando acá es lo que se llama el problema de datos faltantes. Si los datos que faltan se “perdieron” de manera aleatoria, entonces no es un gran problema. Si faltan de forma sistemática, entonces sí puede ser un gran problema.\n\n\n1.12.7 Regresión a la media\nLa regresión a la media es una curiosa variación del sesgo de selección. Se refiere a cualquier situación en la que seleccionás datos basándote en un valor extremo de alguna medición. Debido a la variación natural de la medición, eso casi con certeza significa que, cuando tomes una segunda medición, ese nuevo valor será menos extremo que el primero, simplemente por azar.\nAcá va un ejemplo. Supongamos que me interesa saber si una formación en psicología tiene un efecto negativo en los estudiantes muy inteligentes. Para esto, busco a los 20 estudiantes de psicología con las mejores calificaciones del secundario y analizo qué tan bien les va en la universidad. Resulta que les va mucho mejor que el promedio, pero no están encabezando la clase en la universidad, aunque sí lo hacían en el secundario. ¿Qué está pasando? El primer pensamiento natural es que esto debe significar que las clases de psicología están teniendo un efecto negativo sobre esos estudiantes. Sin embargo, aunque esa podría ser la explicación, lo más probable es que lo que estás viendo sea un ejemplo de “regresión a la media”. Para entender cómo funciona, tomémonos un momento para pensar qué se necesita para obtener la mejor nota en una clase, ya sea en el secundario o en la universidad. Cuando tenés una clase grande, va a haber muchas personas muy inteligentes. Para sacar la mejor nota tenés que ser muy inteligente, trabajar muy duro, y tener un poco de suerte. El examen tiene que hacer exactamente las preguntas correctas para tus habilidades particulares, y no tenés que cometer errores tontos (que todos cometemos alguna vez) al responderlas. Y ese es el punto: la inteligencia y el esfuerzo son cualidades que se transfieren de una clase a otra. La suerte, no. Los que tuvieron suerte en el secundario no van a ser los mismos que tienen suerte en la universidad. Esa es, justamente, la definición de “suerte”. Como consecuencia, cuando seleccionás personas con valores muy extremos en una medición (los 20 mejores estudiantes), estás seleccionando por esfuerzo, habilidad y suerte. Pero como la suerte no se transfiere a la segunda medición (solo la habilidad y el esfuerzo sí lo hacen), es de esperar que todos esos estudiantes bajen un poco cuando los medís por segunda vez (en la universidad). Así que sus calificaciones caen un poco, se acercan de nuevo al resto. Eso es la regresión a la media.\nLa regresión a la media es sorprendentemente común. Por ejemplo, si dos personas muy altas tienen hijos, sus hijos tenderán a ser más altos que el promedio, pero no tanto como sus padres. Lo inverso ocurre con padres muy bajos: dos padres muy bajos tenderán a tener hijos bajos, pero aún así esos hijos tenderán a ser más altos que sus padres. También puede ser extremadamente sutil. Por ejemplo, se han hecho estudios que sugerían que las personas aprenden mejor del feedback negativo que del positivo. Sin embargo, la forma en que se trató de demostrar esto fue dando refuerzo positivo cuando los participantes lo hacían bien, y refuerzo negativo cuando lo hacían mal. Y lo que se observa es que después del refuerzo positivo, el desempeño tendía a empeorar; pero después del refuerzo negativo, tendía a mejorar. ¡Pero! Fijate que acá hay un sesgo de selección: cuando las personas lo hacen muy bien, estás seleccionando valores “altos”, y por lo tanto deberías esperar (por la regresión a la media) que el rendimiento en la siguiente prueba sea peor, independientemente de si se da refuerzo o no. De manera similar, después de una mala prueba, la gente tenderá a mejorar por sí sola. La aparente superioridad del feedback negativo es un artefacto causado por la regresión a la media (Kahneman y Tversky 1973).\n\n\n1.12.8 Sesgo del experimentador\nEl sesgo del experimentador puede presentarse de múltiples formas. La idea básica es que la persona experimentadora, a pesar de tener las mejores intenciones, puede terminar influyendo accidentalmente en los resultados del experimento al comunicar sutilmente la “respuesta correcta” o el “comportamiento deseado” a los participantes. Típicamente, esto ocurre porque la persona experimentadora posee un conocimiento especial que la persona participante no tiene —ya sea la respuesta correcta a las preguntas que se hacen, o el patrón de desempeño esperado para la condición en la que se encuentra la persona participante, etc. El ejemplo clásico de esto es el estudio de caso de “Hans el listo”, que data de 1907 (Pfungst 1911; Hothersall 2004). Hans el listo era un caballo que aparentemente podía leer, contar y realizar otras hazañas de inteligencia típicamente humanas. Después de que Hans se hizo famoso, los psicólogos comenzaron a examinar más de cerca su comportamiento. Resultó que —como era de esperarse— Hans no sabía hacer cuentas. En realidad, Hans estaba respondiendo a los observadores humanos a su alrededor. Como ellos sí sabían contar, el caballo había aprendido a cambiar su comportamiento cuando los humanos cambiaban el suyo.\nLa solución general al problema del sesgo del experimentador es realizar estudios doble ciego, en los que ni la persona experimentadora ni la persona participante saben en qué condición se encuentra la persona participante, ni cuál es el comportamiento deseado. Esta es una muy buena solución al problema, pero es importante reconocer que no es del todo ideal y que es difícil implementarla perfectamente.\nPor ejemplo, la manera obvia en que podría intentar construir un estudio doble ciego es pedirle a uno de mis estudiantes de doctorado (alguien que no sepa nada sobre el experimento) que lleve a cabo el estudio. Eso parecería suficiente. La única persona (yo) que conoce todos los detalles (por ejemplo, las respuestas correctas a las preguntas, la asignación de participantes a condiciones) no interactúa con los participantes, y la persona que sí habla con ellos (el estudiante de doctorado) no sabe nada.\nSalvo que, esa última parte es muy poco probable que sea cierta. Para que el estudiante de doctorado pueda llevar a cabo el estudio de manera efectiva, tiene que haber recibido instrucciones mías, la persona investigadora. Y, como ocurre, ese estudiante también me conoce, y sabe un poco sobre mis creencias generales acerca de las personas y la psicología (por ejemplo, tiendo a pensar que los seres humanos son mucho más inteligentes de lo que los psicólogos reconocen). Como resultado de todo esto, es casi imposible que la persona experimentadora evite conocer, aunque sea un poco, mis expectativas. Y hasta un poco de conocimiento puede tener efecto: supongamos que la persona experimentadora transmite accidentalmente el hecho de que se espera que los participantes lo hagan bien en esta tarea. Bueno, hay algo que se llama el “efecto Pigmalión”: si esperás grandes cosas de alguien, esa persona tenderá a estar a la altura; pero si esperás que fracase, probablemente lo haga. En otras palabras, las expectativas se vuelven profecías autocumplidas.\n\n\n1.12.9 Efectos de demanda y reactividad\nCuando hablamos del sesgo del experimentador, la preocupación es que el conocimiento o los deseos de la persona experimentadora respecto al experimento se comuniquen a los participantes, y que eso afecte su comportamiento (Rosenthal 1966). Sin embargo, incluso si lográs evitar que eso ocurra, es casi imposible evitar que los participantes sepan que están participando en un estudio psicológico. Y el mero hecho de saber que alguien te está observando o estudiando puede tener un efecto bastante fuerte sobre el comportamiento. A esto se lo denomina en general como reactividad o efectos de demanda. La idea básica se captura en el efecto Hawthorne: las personas modifican su rendimiento debido a la atención que el estudio pone sobre elles. El efecto toma su nombre de la fábrica “Hawthorne Works” en las afueras de Chicago (Adair 1984). Un estudio hecho en la década de 1920 que examinaba los efectos de la iluminación sobre la productividad de los trabajadores en la fábrica resultó mostrar un efecto del hecho de que los trabajadores sabían que estaban siendo estudiades, más que de la iluminación.\nPara ser un poco más específicos sobre algunas de las formas en que el solo hecho de estar en un estudio puede cambiar cómo actúan las personas, ayuda pensar como un psicólogo social y observar algunos de los roles que las personas podrían adoptar durante un experimento, pero que tal vez no adoptarían si los mismos eventos ocurrieran en el mundo real:\n\nLa persona participante colaboradora intenta ser demasiado útil para la persona investigadora: busca entender la hipótesis y confirmarla.\nLa persona participante negativa hace exactamente lo opuesto a la colaboradora: busca sabotear o destruir el estudio o la hipótesis de alguna manera.\nLa persona participante fiel es antinaturalmente obediente: busca seguir las instrucciones a la perfección, sin importar lo que hubiera sucedido en una situación más realista.\nLa persona participante aprensiva se pone nerviosa por estar siendo evaluada o estudiada, tanto que su comportamiento se vuelve altamente antinatural o excesivamente deseable socialmente.\n\n\n\n1.12.10 Efectos placebo\nEl efecto placebo es un tipo específico de efecto de demanda que nos preocupa bastante. Se refiere a la situación en la que el simple hecho de recibir un tratamiento provoca una mejora en los resultados. El ejemplo clásico proviene de los ensayos clínicos: si le das a una persona un fármaco completamente inerte desde el punto de vista químico y le decís que es una cura para una enfermedad, esa persona tiende a mejorar más rápido que alguien que no recibe ningún tratamiento. En otras palabras, es la creencia de que están recibiendo tratamiento lo que causa la mejora, no el medicamento en sí.\n\n\n1.12.11 Efectos de situación, medición y subpoblación\nEn algunos aspectos, estos términos funcionan como una categoría general para “todas las demás amenazas a la validez externa”. Se refieren al hecho de que la elección de la subpoblación de la que obtenés a tus participantes, la ubicación, el momento y la manera en que llevás a cabo tu estudio (incluyendo quién recolecta los datos), y las herramientas que usás para hacer tus mediciones podrían estar influyendo en los resultados. En concreto, la preocupación es que estas cosas puedan estar influyendo en los resultados de una manera tal que impida generalizar a una gama más amplia de personas, contextos y métodos de medición.\n\n\n1.12.12 Fraude, engaño y autoengaño\n\nEs difícil lograr que alguien entienda algo, cuando su salario depende de que no lo entienda.\n— Upton Sinclair\n\nUna última cosa que siento que debería mencionar. Mientras leía lo que suelen decir los libros de texto sobre cómo evaluar la validez de un estudio, no pude evitar notar que parecen asumir que la persona investigadora es honesta. Me resulta gracioso. Aunque la gran mayoría de los científicos son honestos —al menos en mi experiencia—, algunos no lo son. No solo eso: como mencioné antes, los científicos no son inmunes al sesgo de creencias —es fácil que un investigador termine engañándose a sí misme para creer algo falso, y eso puede llevarle a hacer investigaciones sutilmente defectuosas, y luego a ocultar esas fallas al redactar el informe. Así que necesitás considerar no solo la (probablemente poco común) posibilidad de fraude deliberado, sino también la (probablemente bastante común) posibilidad de que la investigación esté sesgada de manera no intencional. Abrí algunos libros de texto estándar y no encontré mucho sobre este problema, así que acá va mi propio intento de listar algunas formas en que pueden surgir estas cuestiones:\n\nFabricación de datos. A veces, las personas simplemente inventan los datos. Ocasionalmente esto se hace con “buenas” intenciones. Por ejemplo, la persona investigadora cree que los datos fabricados reflejan la verdad, y pueden ser versiones “ligeramente corregidas” de datos reales. En otras ocasiones, el fraude es deliberado y malicioso. Algunos ejemplos de alto perfil en los que se ha alegado o demostrado fabricación de datos incluyen a Cyril Burt (un psicólogo que se cree que falsificó parte de sus datos), Andrew Wakefield (acusado de falsificar sus datos que relacionaban la vacuna triple viral con el autismo) y Hwang Woo-suk (quien falsificó muchos de sus datos sobre investigación con células madre).\nEngaños (hoaxes). Los engaños comparten muchas similitudes con la fabricación de datos, pero se diferencian en el propósito. Un engaño suele ser una broma, y muchos están diseñados para que (eventualmente) se descubran. A menudo, el objetivo de un engaño es desacreditar a alguien o a un campo entero. Ha habido bastantes engaños científicos conocidos a lo largo de los años (por ejemplo, el hombre de Piltdown), algunos de los cuales fueron intentos deliberados de desacreditar ciertos campos de investigación (por ejemplo, el caso Sokal).\nFalsificación o tergiversación de los datos. Aunque el fraude es lo que se lleva los titulares, en mi experiencia es mucho más común ver datos tergiversados. Cuando digo esto, no me refiero a los medios de comunicación interpretando mal los resultados (lo cual hacen casi siempre). Me refiero a que muchas veces los datos no dicen realmente lo que les investigadores creen que dicen. Mi impresión es que, casi siempre, esto no se debe a una deshonestidad deliberada, sino a una falta de sofisticación en los análisis de datos. Por ejemplo, pensá en el caso de la paradoja de Simpson que mencioné al comienzo de estas notas. Es muy común ver a personas presentar datos “agregados” de algún tipo; y a veces, cuando profundizás y conseguís los datos crudos por tu cuenta, encontrás que los datos agregados cuentan una historia diferente que los desagregados. Alternativamente, podrías descubrir que algún aspecto de los datos está siendo ocultado porque cuenta una historia incómoda (por ejemplo, la persona investigadora podría optar por no mencionar una determinada variable). Hay muchas variantes de esto; muchas de las cuales son muy difíciles de detectar.\nDiseño “defectuoso” del estudio. Bien, esto es más sutil. Básicamente, el problema acá es que un investigador diseña un estudio que tiene fallas estructurales, y esas fallas nunca se informan en el artículo. Los datos que se reportan son completamente reales, y están analizados correctamente, pero provienen de un estudio que en realidad está muy mal planteado. La persona investigadora realmente quiere encontrar un efecto en particular, así que el estudio se arma de manera tal que resulte “fácil” (aunque artefactual) observar ese efecto. Una manera muy astuta de hacer esto —por si tenés ganas de incursionar en un poco de fraude— es diseñar un experimento en el que sea obvio para los participantes qué es lo que “se supone” que deben hacer, y luego dejar que la reactividad haga su magia. Si querés, podés agregarle todos los adornos de un experimento doble ciego, etc. No va a hacer ninguna diferencia, ya que los propios materiales del estudio les están diciendo sutilmente a los participantes lo que se espera que hagan. Cuando redactás los resultados, el fraude no va a ser obvio para la persona lectora: lo que es evidente para la persona participante en el contexto experimental no siempre es evidente para quien lee el artículo. Claro, la manera en que describí esto suena como si siempre fuera fraude: probablemente haya casos donde esto se haga de forma deliberada, pero en mi experiencia la preocupación más grande ha sido con los errores no intencionales de diseño. La persona investigadora cree… y así, el estudio termina teniendo una falla estructural, y esa falla mágicamente desaparece cuando se escribe el artículo para su publicación.\nMinería de datos e hipótesis post hoc. Otra forma en la que los autores de un estudio pueden, más o menos, mentir sobre lo que encontraron es mediante lo que se llama “minería de datos”. Como vamos a discutir más adelante en la clase, si analizás los datos de muchas formas diferentes, eventualmente vas a encontrar algo que “parece” un efecto real, pero que no lo es. Eso es lo que se llama “minería de datos”. Antes era bastante raro, porque analizar datos solía llevar semanas, pero ahora que todos tienen software estadístico muy potente en sus computadoras, se ha vuelto bastante común. La minería de datos en sí no es “incorrecta”, pero cuanto más la hacés, más grande es el riesgo que estás corriendo. Lo que sí está mal (y, sospecho, es común) es hacer minería de datos sin reconocerlo. Es decir, la persona investigadora prueba todos los análisis posibles conocidos por la humanidad, encuentra uno que “funciona”, y luego finge que ese era el único análisis que pensaba hacer desde el principio. Peor aún, muchas veces “inventa” una hipótesis después de mirar los datos, para cubrir la minería de datos. Para ser claros: no está mal cambiar tus creencias después de mirar los datos, y reanalizar los datos con tus nuevas hipótesis post hoc. Lo que sí está mal (y, sospecho, es común) es no reconocer que hiciste eso. Si lo reconocés, otros investigadores pueden tenerlo en cuenta. Si no lo hacés, no pueden. Y eso vuelve tu comportamiento engañoso. ¡Mal!\nSesgo de publicación y autocensura. Finalmente, un sesgo muy extendido es el de la no publicación de resultados negativos. Y esto es casi imposible de evitar. Las revistas científicas no publican todo lo que se les envía: tienden a preferir los artículos que muestran que “algo pasó”. Entonces, si 20 personas hacen un experimento para ver si leer Finnegans Wake causa locura en humanos, y 19 encuentran que no, ¿cuál pensás que se va a publicar? Obviamente, el único estudio que sí encontró que Finnegans Wake causa locura. Eso es un ejemplo de sesgo de publicación: como nadie publica los 19 estudios que no encontraron efecto, una persona ingenua jamás sabría que existieron. Peor aún, muchos investigadores internalizan este sesgo y terminan autocensurando sus propios trabajos. Como saben que los resultados negativos no van a ser aceptados para su publicación, ni siquiera intentan reportarlos. Como dice una amiga mía: “por cada experimento que lográs publicar, tenés diez fracasos detrás”. Y tiene razón. El problema es que, si bien algunos (quizás la mayoría) de esos estudios fallaron por razones aburridas (por ejemplo, porque cometiste algún error), otros podrían ser verdaderos resultados nulos que deberías reconocer cuando escribís el experimento que sí salió “bien”. Y distinguir entre una cosa y la otra muchas veces es difícil. Un buen punto de partida es un artículo de Ioannidis (2005) con el deprimente título “Por qué la mayoría de los hallazgos publicados son falsos”. También te recomiendo el trabajo de Kühberger, Fritz, y Scherndl (2014), que presenta evidencia estadística de que esto realmente pasa en psicología.\n\nSeguramente hay muchos más temas como este para pensar, pero con esto alcanza para empezar. Lo que realmente quiero señalar es una verdad tan obvia como incómoda: la ciencia del mundo real la hacen personas reales, y solo alguien muy ingenuo puede creer que todos son siempre honestos e imparciales. Los científicos de verdad no suelen ser tan ingenuos, pero por alguna razón el mundo prefiere fingir que lo somos, y los manuales que solemos escribir refuerzan ese estereotipo.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>¿Por qué estadística?</span>"
    ]
  },
  {
    "objectID": "01-Science_Data.html#resumen",
    "href": "01-Science_Data.html#resumen",
    "title": "1  ¿Por qué estadística?",
    "section": "1.13 Resumen",
    "text": "1.13 Resumen\nEste capítulo no pretende ofrecer una discusión exhaustiva sobre los métodos de investigación en psicología: para hacerle justicia al tema se necesitaría otro volumen igual de largo que este. Sin embargo, en la vida real, las estadísticas y el diseño de estudios están fuertemente entrelazados, así que es muy útil repasar algunos temas clave. En este capítulo, discutí brevemente los siguientes temas:\n\nIntroducción a la medición psicológica: ¿Qué significa operacionalizar un constructo teórico? ¿Qué significa tener variables y tomar mediciones?\nEscalas de medición y tipos de variables: Recordá que hay dos distinciones diferentes acá: está la diferencia entre datos discretos y continuos, y está la diferencia entre los cuatro tipos de escala (nominal, ordinal, de intervalo y de razón).\nConfiabilidad de una medición: Si mido dos veces lo “mismo”, ¿debería esperar ver el mismo resultado? Solo si mi medición es confiable. Pero, ¿qué significa hablar de hacer lo “mismo”? Bueno, por eso tenemos distintos tipos de confiabilidad. Acordate cuáles son.\nTerminología: predictoras y resultados: ¿Qué rol juegan las variables en un análisis? ¿Te acordás la diferencia entre variables predictoras y variables resultado? ¿Entre dependientes e independientes? Etc.\nDiseños de investigación experimentales y no experimentales: ¿Qué es lo que hace que un experimento sea un experimento? ¿Una bata blanca de laboratorio? ¿O tiene algo que ver con el control que ejerce la persona investigadora sobre las variables?\nValidez y sus amenazas: ¿Tu estudio mide lo que querés que mida? ¿Cómo podrían salir mal las cosas? ¿Y soy yo, o esa fue una lista larguísima de posibles maneras en que las cosas pueden salir mal?\n\nTodo esto debería dejar en claro que el diseño del estudio es una parte crítica de la metodología de investigación. Armé este capítulo a partir del clásico libro de Campbell y Stanley (1963), pero hay una gran cantidad de manuales sobre diseño de investigación. Con solo pasar unos minutos en tu buscador favorito vas a encontrar decenas.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>¿Por qué estadística?</span>"
    ]
  },
  {
    "objectID": "01-Science_Data.html#videos",
    "href": "01-Science_Data.html#videos",
    "title": "1  ¿Por qué estadística?",
    "section": "1.14 Videos",
    "text": "1.14 Videos\n\n1.14.1 Términos en Estadística (video en inglés)\n\n\n:::\n\n\n\n\nAdair, G. 1984. «The Hawthorne Effect: A Reconsideration of the Methodological Artifact». Journal of Applied Psychology 69: 334-45. https://doi.org/10.1037/0021-9010.69.2.334.\n\n\nBickel, P. J., E. A. Hammel, y J. W. O’Connell. 1975. «Sex Bias in Graduate Admissions: Data from Berkeley». Science (New York, N.Y.) 187: 398-404. https://doi.org/10.1126/science.187.4175.398.\n\n\nCampbell, D. T., y J. C. Stanley. 1963. Experimental and Quasi-Experimental Designs for Research. Boston, MA: Houghton Mifflin.\n\n\nEvans, J. St. B. T., J. L. Barston, y P. Pollard. 1983. «On the Conflict between Logic and Belief in Syllogistic Reasoning». Memory and Cognition 11: 295-306. https://doi.org/10.3758/BF03196976.\n\n\nHothersall, D. 2004. History of Psychology. McGraw-Hill.\n\n\nIoannidis, John P. A. 2005. «Why Most Published Research Findings Are False». PLoS medicine 2 (8): 697-701. https://doi.org/10.1371/journal.pmed.0020124.\n\n\nKahneman, D., y A. Tversky. 1973. «On the Psychology of Prediction». Psychological Review 80: 237-51. https://doi.org/10.1037/h0034747.\n\n\nKühberger, A, A Fritz, y T. Scherndl. 2014. «Publication Bias in Psychology: A Diagnosis Based on the Correlation between Effect Size and Sample Size». Public Library of Science One 9: 1-8.\n\n\nPfungst, O. 1911. Clever Hans (The Horse of Mr. von Osten): A Contribution to Experimental Animal and Human Psychology. Traducido por C. L. Rahn. New York: Henry Holt.\n\n\nRosenthal, R. 1966. Experimenter Effects in Behavioral Research. New York: Appleton.\n\n\nStevens, S. S. 1946. «On the Theory of Scales of Measurement». Science (New York, N.Y.) 103: 677-80. https://doi.org/10.1126/science.103.2684.677.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>¿Por qué estadística?</span>"
    ]
  },
  {
    "objectID": "02-Describing_Data.html#this-is-what-too-many-numbers-looks-like",
    "href": "02-Describing_Data.html#this-is-what-too-many-numbers-looks-like",
    "title": "2  Describing Data",
    "section": "2.1 This is what too many numbers looks like",
    "text": "2.1 This is what too many numbers looks like\nLet’s say you wanted to know how happy people are. So, you ask thousands of people on the street how happy they are. You let them pick any number they want from negative infinity to positive infinity. Then you record all the numbers. Now what?\nWell, how about you look at the numbers and see if that helps you determine anything about how happy people are. What could the numbers look like. Perhaps something like this:\n\n\n\n\n\n164\n626\n-59\n-37\n333\n-736\n-305\n593\n-17\n560\n\n\n197\n297\n-479\n-889\n-110\n412\n797\n580\n211\n-54\n\n\n497\n89\n-281\n340\n125\n-260\n-44\n-315\n407\n961\n\n\n166\n12\n939\n-22\n17\n993\n140\n-613\n388\n199\n\n\n405\n54\n-238\n-435\n-27\n-156\n303\n-328\n266\n-53\n\n\n422\n390\n357\n1105\n-576\n978\n-56\n507\n1181\n638\n\n\n413\n272\n-655\n-1022\n-59\n288\n845\n-430\n697\n-255\n\n\n-319\n-412\n201\n751\n-267\n776\n670\n445\n-190\n-308\n\n\n377\n905\n-125\n672\n141\n21\n-347\n149\n-146\n-489\n\n\n111\n-76\n-143\n167\n193\n-557\n-293\n-171\n312\n-214\n\n\n252\n502\n525\n-593\n548\n29\n317\n436\n636\n-236\n\n\n246\n-352\n194\n-167\n545\n-76\n-992\n535\n341\n-42\n\n\n481\n81\n577\n-994\n-101\n-51\n-630\n-726\n45\n372\n\n\n327\n-529\n860\n-153\n957\n1283\n137\n-687\n102\n424\n\n\n118\n-1042\n258\n283\n333\n615\n265\n-146\n1099\n138\n\n\n769\n130\n889\n563\n390\n719\n-12\n-41\n-534\n104\n\n\n-235\n-693\n-29\n-215\n523\n616\n341\n638\n163\n-35\n\n\n443\n-824\n664\n297\n-130\n601\n-333\n-176\n-944\n-1052\n\n\n-124\n-495\n-310\n373\n1085\n211\n-459\n-1\n204\n776\n\n\n20\n53\n252\n866\n321\n621\n-105\n-858\n602\n-726\n\n\n-35\n108\n323\n-395\n117\n-405\n486\n396\n-934\n1039\n\n\n369\n43\n379\n634\n617\n158\n186\n-290\n-284\n392\n\n\n754\n-187\n-23\n37\n302\n-10\n-229\n24\n-126\n497\n\n\n69\n750\n893\n418\n-664\n-52\n411\n180\n714\n614\n\n\n27\n857\n-676\n443\n468\n-384\n-682\n-489\n-79\n-654\n\n\n-749\n360\n-315\n-324\n154\n-318\n612\n-300\n-1064\n68\n\n\n-366\n-199\n552\n-148\n-1053\n135\n919\n285\n650\n-389\n\n\n761\n-102\n385\n-61\n23\n299\n150\n1340\n-79\n302\n\n\n-433\n236\n253\n460\n-41\n-79\n-137\n25\n-167\n-463\n\n\n438\n528\n-504\n443\n345\n965\n50\n-569\n-655\n31\n\n\n-311\n641\n821\n68\n299\n-83\n724\n-115\n-188\n504\n\n\n1078\n364\n115\n-40\n245\n-939\n708\n-1329\n-759\n-543\n\n\n-615\n344\n203\n890\n486\n-944\n400\n-702\n274\n644\n\n\n374\n1227\n999\n622\n-302\n-145\n524\n204\n288\n-558\n\n\n653\n212\n140\n-221\n1017\n215\n253\n-935\n-44\n-22\n\n\n800\n240\n-81\n575\n443\n224\n214\n-153\n-134\n-524\n\n\n176\n154\n-281\n205\n890\n387\n573\n289\n15\n-513\n\n\n442\n-677\n397\n804\n-356\n-452\n-908\n-619\n150\n-158\n\n\n-299\n536\n496\n-408\n725\n88\n1016\n574\n-271\n375\n\n\n-780\n-30\n-798\n-351\n248\n21\n-57\n386\n-414\n683\n\n\n403\n457\n602\n-790\n1010\n-213\n-136\n156\n-441\n-316\n\n\n1366\n256\n75\n-101\n533\n-519\n-212\n792\n-372\n905\n\n\n531\n1870\n-967\n-32\n-564\n-575\n128\n765\n188\n-64\n\n\n542\n-413\n173\n434\n332\n-483\n-33\n651\n-965\n-823\n\n\n1156\n5\n507\n194\n730\n90\n344\n-17\n-546\n624\n\n\n80\n-467\n298\n-212\n703\n976\n688\n-181\n895\n548\n\n\n-639\n-477\n28\n150\n380\n135\n-118\n-519\n1321\n787\n\n\n20\n8\n-710\n-413\n42\n564\n537\n-1112\n-427\n364\n\n\n-962\n275\n-244\n-462\n952\n-30\n-1020\n-174\n233\n131\n\n\n-207\n318\n410\n-493\n36\n999\n-573\n288\n912\n182\n\n\n\n\n\nNow, what are you going to with that big pile of numbers? Look at it all day long? When you deal with data, it will deal so many numbers to you that you will be overwhelmed by them. That is why we need ways to describe the data in a more manageable fashion.\nThe complete description of the data is always the data itself. Descriptive statistics and other tools for describing data go one step further to summarize aspects of the data. Summaries are a way to compress the important bits of a thing down to a useful and manageable tidbit. It’s like telling your friends why they should watch a movie: you don’t replay the entire movie for them, instead you hit the highlights. Summarizing the data is just like a movie preview, only for data."
  },
  {
    "objectID": "02-Describing_Data.html#look-at-the-data",
    "href": "02-Describing_Data.html#look-at-the-data",
    "title": "2  Describing Data",
    "section": "2.2 Look at the data",
    "text": "2.2 Look at the data\nWe already tried one way of looking at the numbers, and it wasn’t useful. Let’s look at some other ways of looking at the numbers, using graphs.\n\n2.2.1 Stop, plotting time (o o oh) U can plot this\nLet’s turn all of the numbers into dots, then show them in a graph. Note, when we do this, we have not yet summarized anything about the data. Instead, we just look at all of the data in a visual format, rather than looking at the numbers.\n\n\n\n\n\nFigure 2.1: Pretend happiness ratings from 500 people\n\n\n\n\nFigure 2.1 shows 500 measurements of happiness. The graph has two axes. The horizontal x-axis, going from left to right is labeled “Index”. The vertical y-axis, going up and down, is labelled “happiness”. Each dot represents one measurement of every person’s happiness from our pretend study. Before we talk about what we can and cannot see about the data, it is worth mentioning that the way you plot the data will make some things easier to see and some things harder to see. So, what can we now see about the data?\nThere are lots of dots everywhere. It looks like there are 500 of them because the index goes to 500. It looks like some dots go as high as 1000-1500 and as low as -1500. It looks like there are more dots in the middle-ish area of the plot, sort of spread about 0.\n\nTake home: we can see all the numbers at once by putting them in a plot, and that is much easier and more helpful than looking at the raw numbers.\n\nOK, so if these dots represent how happy 500 people are, what can we say about those people? First, the dots are kind of all over the place, so different people have different levels of happiness. Are there any trends? Are more people happy than unhappy, or vice-versa? It’s hard to see that in the graph, so let’s make a different one, called a histogram\n\n\n2.2.2 Histograms\nMaking a histogram will be our first act of officially summarizing something about the data. We will no longer look at the individual bits of data, instead we will see how the numbers group together. Let’s look at Figure 2.2, a histogram of the happiness data, and then explain it.\n\n\n\n\n\nFigure 2.2: A histogram of the happiness ratings\n\n\n\n\nThe dots have disappeared, and now we some bars. Each bar is a summary of the dots, representing the number of dots (frequency count) inside a particular range of happiness, also called bins. For example, how many people gave a happiness rating between 0 and 500? The fifth bar, the one between 0 and 500 on the x-axis, tells you how many. Look how tall that bar is. How tall is it? The height is shown on the y-axis, which provides a frequency count (the number of dots or data points). It looks like around 150 people said their happiness was between 0-500.\nMore generally, we see there are many bins on the x-axis. We have divided the data into bins of 500. Bin #1 goes from -2000 to -1500, bin #2 goes from -1500 to -1000, and so on until the last bin. To make the histogram, we just count up the number of data points falling inside each bin, then plot those frequency counts as a function of the bins. Voila, a histogram.\nWhat does the histogram help us see about the data? First, we can see the shape of data. The shape of the histogram refers to how it goes up and down. The shape tells us where the data is. For example, when the bars are low we know there isn’t much data there. When the bars are high, we know there is more data there. So, where is most of the data? It looks like it’s mostly in the middle two bins, between -500 and 500. We can also see the range of the data. This tells us the minimums and the maximums of the data. Most of the data is between -1500 and +1500, so no infinite sadness or infinite happiness in our data-set.\nWhen you make a histogram you get to choose how wide each bar will be. For example, below are four different histograms of the very same happiness data. What changes is the width of the bins.\n\n\n\n\n\nFigure 2.3: Four histograms of the same data using different bin widths\n\n\n\n\nAll of the histograms have roughly the same overall shape: From left to right, the bars start off small, then go up, then get small again. In other words, as the numbers get closer to zero, they start to occur more frequently. We see this general trend across all the histograms. But, some aspects of the trend fall apart when the bars get really narrow. For example, although the bars generally get taller when moving from -1000 to 0, there are some exceptions and the bars seem to fluctuate a little bit. When the bars are wider, there are less exceptions to the general trend. How wide or narrow should your histogram be? It’s a Goldilocks question. Make it just right for your data."
  },
  {
    "objectID": "02-Describing_Data.html#important-ideas-distribution-central-tendency-and-variance",
    "href": "02-Describing_Data.html#important-ideas-distribution-central-tendency-and-variance",
    "title": "2  Describing Data",
    "section": "2.3 Important Ideas: Distribution, Central Tendency, and Variance",
    "text": "2.3 Important Ideas: Distribution, Central Tendency, and Variance\nLet’s introduce three important terms we will use a lot, distribution, central tendency, and variance. These terms are similar to their everyday meanings (although I suspect most people don’t say central tendency very often).\nDistribution. When you order something from Amazon, where does it come from, and how does it get to your place? That stuff comes from one of Amazon’s distribution centers. They distribute all sorts of things by spreading them around to your doorstep. “To Distribute”” is to spread something. Notice, the data in the histogram is distributed, or spread across the bins. We can also talk about a distribution as a noun. The histogram is a distribution of the frequency counts across the bins. Distributions are very, very, very, very, very important. They can have many different shapes. They can describe data, like in the histogram above. And as we will learn in later chapters, they can produce data. Many times we will be asking questions about where our data came from, and this usually means asking what kind of distribution could have created our data (more on that later.)\nCentral Tendency is all about sameness: What is common about some numbers? For example, is there anything similar about all of the numbers in the histogram? Yes, we can say that most of them are near 0. There is a tendency for most of the numbers to be centered near 0. Notice we are being cautious about our generalization about the numbers. We are not saying they are all 0. We are saying there is a tendency for many of them to be near zero. There are lots of ways to talk about the central tendency of some numbers. There can even be more than one kind of tendency. For example, if lots of the numbers were around -1000, and a similar large amount of numbers were grouped around 1000, we could say there was two tendencies.\nVariance is all about differentness: What is different about some numbers?. For example, is there anything different about all of the numbers in the histogram? YES!!! The numbers are not all the same! When the numbers are not all the same, they must vary. So, the variance in the numbers refers to how the numbers are different. There are many ways to summarize the amount of variance in the numbers, and we discuss these very soon."
  },
  {
    "objectID": "02-Describing_Data.html#measures-of-central-tendency-sameness",
    "href": "02-Describing_Data.html#measures-of-central-tendency-sameness",
    "title": "2  Describing Data",
    "section": "2.4 Measures of Central Tendency (Sameness)",
    "text": "2.4 Measures of Central Tendency (Sameness)\nWe’ve seen that we can get a sense of data by plotting dots in a graph, and by making a histogram. These tools show us what the numbers look like, approximately how big and small they are, and how similar and different they are from another. It is good to get a feeling about the numbers in this way. But, these visual sensitudes are not very precise. In addition to summarizing numbers with graphs, we can summarize numbers using numbers (NO, please not more numbers, we promise numbers can be your friend).\n\n2.4.1 From many numbers to one\nMeasures of central have one important summary goal: to reduce a pile of numbers to a single number that we can look at. We already know that looking at thousands of numbers is hopeless. Wouldn’t it be nice if we could just look at one number instead? We think so. It turns out there are lots of ways to do this. Then, if your friend ever asks the frightening question, “hey, what are all these numbers like?”. You can say they are like this one number right here.\nBut, just like in Indiana Jones and the Last Crusade (highly recommended movie), you must choose your measure of central tendency wisely.\n\n\n2.4.2 Mode\nThe mode is the most frequently occurring number in your measurement. That is it. How do you find it? You have to count the number of times each number appears in your measure, then whichever one occurs the most, is the mode.\n\nExample: 1 1 1 2 3 4 5 6\n\nThe mode of the above set is 1, which occurs three times. Every other number only occurs once.\nOK fine. What happens here:\n\nExample: 1 1 1 2 2 2 3 4 5 6\n\nHmm, now 1 and 2 both occur three times each. What do we do? We say there are two modes, and they are 1 and 2.\nWhy is the mode a measure of central tendency? Well, when we ask, “what are my numbers like”, we can say, “most of the number are, like a 1 (or whatever the mode is)”.\nIs the mode a good measure of central tendency? That depends on your numbers. For example, consider these numbers\n\n1 1 2 3 4 5 6 7 8 9\n\nHere, the mode is 1 again, because there are two 1s, and all of the other numbers occur once. But, are most of the numbers like, a 1. No, they are mostly not 1s.\n“Argh, so should I or should I not use the mode? I thought this class was supposed to tell me what to do?”. There is no telling you what to do. Every time you use a tool in statistics you have to think about what you are doing and justify why what you are doing makes sense. Sorry.\n\n\n2.4.3 Median\nThe median is the exact middle of the data. After all, we are asking about central tendency, so why not go to the center of the data and see where we are. What do you mean middle of the data? Let’s look at these numbers:\n\n1 5 4 3 6 7 9\n\nUmm, OK. So, three is in the middle? Isn’t that kind of arbitrary. Yes. Before we can compute the median, we need to order the numbers from smallest to largest.\n\n1 3 4 5 6 7 9\n\nNow, 5 is in the middle. And, by middle we mean in the middle. There are three numbers to the left of 5, and three numbers to the right. So, five is definitely in the middle.\nOK fine, but what happens when there aren’t an even number of numbers? Then the middle will be missing right? Let’s see:\n\n1 2 3 4 5 6\n\nThere is no number between 3 and 4 in the data, the middle is empty. In this case, we compute the median by figuring out the number in between 3 and 4. So, the median would be 3.5.\nIs the median a good measure of central tendency? Sure, it is often very useful. One property of the median is that it stays in the middle even when some of the other numbers get really weird. For example, consider these numbers:\n\n1 2 3 4 4 4 5 6 6 6 7 7 1000\n\nMost of these numbers are smallish, but the 1000 is a big old weird number, very different from the rest. The median is still 5, because it is in the middle of these ordered numbers. We can also see that five is pretty similar to most of the numbers (except for 1000). So, the median does a pretty good job of representing most of the numbers in the set, and it does so even if one or two of the numbers are very different from the others.\nFinally, outlier is a term will we use to describe numbers that appear in data that are very different from the rest. 1000 is an outlier, because it lies way out there on the number line compared to the other numbers. What to do with outliers is another topic we discuss sometimes throughout this course.\n\n\n2.4.4 Mean\nHave you noticed this is a textbook about statistics that hasn’t used a formula yet? That is about to change, but for those of you with formula anxiety, don’t worry, we will do our best to explain them.\nThe mean is also called the average. And, we’re guessing you might already now what the average of a bunch of numbers is? It’s the sum of the numbers, divided by the number of number right? How do we express that idea in a formula? Just like this:\n\\(Mean = \\bar{X} = \\frac{\\sum_{i=1}^{n} x_{i}}{N}\\)\n“That looks like Greek to me”. Yup. The \\(\\sum\\) symbol is called sigma, and it stands for the operation of summing. The little “i” on the bottom, and the little “n” on the top refers to all of the numbers in the set, from the first number “i” to the last number “n”. The letters are just arbitrary labels, called variables that we use for descriptive purposes. The \\(x_{i}\\) refers to individual numbers in the set. We sum up all of the numbers, then divide the sum by \\(N\\), which is the total number of numbers. Sometimes you will see \\(\\bar{X}\\) to refer to the mean of all of the numbers.\nIn plain English, the formula looks like:\n\\(mean = \\frac{\\text{Sum of my numbers}}{\\text{Count of my numbers}}\\)\n“Well, why didn’t you just say that?”. We just did.\nLet’s compute the mean for these five numbers:\n\n3 7 9 2 6\n\nAdd em up:\n\n3+7+9+2+6 = 27\n\nCount em up:\n\n\\(i_{1}\\) = 3, \\(i_{2}\\) = 7, \\(i_{3}\\) = 9, \\(i_{4}\\) = 2, \\(i_{5}\\) = 6; N=5, because \\(i\\) went from 1 to 5\n\nDivide em:\n\nmean = 27 / 5 = 5.4\n\nOr, to put the numbers in the formula, it looks like this:\n\\(Mean = \\bar{X} = \\frac{\\sum_{i=1}^{n} x_{i}}{N} = \\frac{3+7+9+2+6}{5} = \\frac{27}{5} = 5.4\\)\nOK fine, that is how to compute the mean. But, like we imagined, you probably already knew that, and if you didn’t that’s OK, now you do. What’s next?\nIs the mean a good measure of central tendency? By now, you should know: it depends.\n\n\n2.4.5 What does the mean mean?\nIt is not enough to know the formula for the mean, or to be able to use the formula to compute a mean for a set of numbers. We believe in your ability to add and divide numbers. What you really need to know is what the mean really “means”. This requires that you know what the mean does, and not just how to do it. Puzzled? Let’s explain.\nCan you answer this question: What happens when you divide a sum of numbers by the number of numbers? What are the consequences of doing this? What is the formula doing? What kind of properties does the result give us? FYI, the answer is not that we compute the mean.\nOK, so what happens when you divide any number by another number? Of course, the key word here is divide. We literally carve the number up top in the numerator into pieces. How many times do we split the top number? That depends on the bottom number in the denominator. Watch:\n\\(\\frac{12}{3} = 4\\)\nSo, we know the answer is 4. But, what is really going on here is that we are slicing and dicing up 12 aren’t we. Yes, and we slicing 12 into three parts. It turns out the size of those three parts is 4. So, now we are thinking of 12 as three different pieces \\(12 = 4 + 4 + 4\\). I know this will be obvious, but what kind of properties do our pieces have? You mean the fours? Yup. Well, obviously they are all fours. Yes. The pieces are all the same size. They are all equal. So, division equalizes the numerator by the denominator…\n“Umm, I think I learned this in elementary school, what does this have to do with the mean?”. The number on top of the formula for the mean is just another numerator being divided by a denominator isn’t it. In this case, the numerator is a sum of all the values in your data. What if it was the sum of all of the 500 happiness ratings? The sum of all of them would just be a single number adding up all the different ratings. If we split the sum up into equal parts representing one part for each person’s happiness what would we get? We would get 500 identical and equal numbers for each person. It would be like taking all of the happiness in the world, then dividing it up equally, then to be fair, giving back the same equal amount of happiness to everyone in the world. This would make some people more happy than they were before, and some people less happy right. Of course, that’s because it would be equalizing the distribution of happiness for everybody. This process of equalization by dividing something into equal parts is what the mean does. See, it’s more than just a formula. It’s an idea. This is just the beginning of thinking about these kinds of ideas. We will come back to this idea about the mean, and other ideas, in later chapters.\n\nPro tip: The mean is the one and only number that can take the place of every number in the data, such that when you add up all the equal parts, you get back the original sum of the data.\n\n\n\n2.4.6 All together now\nJust to remind ourselves of the mode, median, and mean, take a look at the next histogram in @fig-meanmodemed. We have overlaid the location of the mean (red), median (green), and mode (blue). For this dataset, the three measures of central tendency all give different answers. The mean is the largest because it is influenced by large numbers, even if they occur rarely. The mode and median are insensitive to large numbers that occur infrequently, so they have smaller values.\n\n\n\n\n\nFigure 2.4: A histogram with the mean (red), the median (green), and the mode (blue)"
  },
  {
    "objectID": "02-Describing_Data.html#measures-of-variation-differentness",
    "href": "02-Describing_Data.html#measures-of-variation-differentness",
    "title": "2  Describing Data",
    "section": "2.5 Measures of Variation (Differentness)",
    "text": "2.5 Measures of Variation (Differentness)\nWhat did you do when you wrote essays in high school about a book you read? Probably compare and contrast something right? When you summarize data, you do the same thing. Measures of central tendency give us something like comparing does, they tell us stuff about what is the same. Measures of variation give us something like contrasting does, they tell us stuff about what is different.\nFirst, we note that whenever you see a bunch of numbers that aren’t the same, you already know there are some differences. This means the numbers vary, and there is variation in the size of the numbers.\n\n2.5.1 The Range\nConsider these 10 numbers, that I already ordered from smallest to largest for you:\n\n1 3 4 5 5 6 7 8 9 24\n\nThe numbers have variation, because they are not all the same. We can use the range to describe the width of the variation. The range refers to the minimum (smallest value) and maximum (largest value) in the set. So, the range would be 1 and 24.\nThe range is a good way to quickly summarize the boundaries of your data in just two numbers. By computing the range we know that none of the data is larger or smaller than the range. And, it can alert you to outliers. For example, if you are expecting your numbers to be between 1 and 7, but you find the range is 1 - 340,500, then you know you have some big numbers that shouldn’t be there, and then you can try to figure out why those numbers occurred (and potentially remove them if something went wrong).\n\n\n2.5.2 The Difference Scores\nIt would be nice to summarize the amount of differentness in the data. Here’s why. If you thought that raw data (lots of numbers) is too big to look at, then you will be frightened to contemplate how many differences there are to look at. For example, these 10 numbers are easy to look at:\n\n1 3 4 5 5 6 7 8 9 24\n\nBut, what about the difference between the numbers, what do those look like? We can compute the difference scores between each number, then put them in a matrix like the one below:\n\n\n\n\n\n\n1\n3\n4\n5\n5\n6\n7\n8\n9\n24\n\n\n\n\n1\n0\n2\n3\n4\n4\n5\n6\n7\n8\n23\n\n\n3\n-2\n0\n1\n2\n2\n3\n4\n5\n6\n21\n\n\n4\n-3\n-1\n0\n1\n1\n2\n3\n4\n5\n20\n\n\n5\n-4\n-2\n-1\n0\n0\n1\n2\n3\n4\n19\n\n\n5\n-4\n-2\n-1\n0\n0\n1\n2\n3\n4\n19\n\n\n6\n-5\n-3\n-2\n-1\n-1\n0\n1\n2\n3\n18\n\n\n7\n-6\n-4\n-3\n-2\n-2\n-1\n0\n1\n2\n17\n\n\n8\n-7\n-5\n-4\n-3\n-3\n-2\n-1\n0\n1\n16\n\n\n9\n-8\n-6\n-5\n-4\n-4\n-3\n-2\n-1\n0\n15\n\n\n24\n-23\n-21\n-20\n-19\n-19\n-18\n-17\n-16\n-15\n0\n\n\n\n\n\nWe are looking at all of the possible differences between each number and every other number. So, in the top left, the difference between 1 and itself is 0. One column over to the right, the difference between 3 and 1 (3-1) is 2, etc. As you can see, this is a 10x10 matrix, which means there are 100 differences to look at. Not too bad, but if we had 500 numbers, then we would have 500*500 = 250,000 differences to look at (go for it if you like looking at that sort of thing).\nPause for a simple question. What would this matrix look like if all of the 10 numbers in our data were the same number? It should look like a bunch of 0s right? Good. In that case, we could easily see that the numbers have no variation.\nBut, when the numbers are different, we can see that there is a very large matrix of difference scores. How can we summarize that? How about we apply what we learned from the previous section on measures of central tendency. We have a lot of differences, so we could ask something like, what is the average difference that we have? So, we could just take all of our differences, and compute the mean difference right? What do you think would happen if we did that?\nLet’s try it out on these three numbers:\n\n1 2 3\n\n\n\n\n\n\n\n1\n2\n3\n\n\n\n\n1\n0\n1\n2\n\n\n2\n-1\n0\n1\n\n\n3\n-2\n-1\n0\n\n\n\n\n\nYou might already guess what is going to happen. Let’s compute the mean:\n\\(\\text{mean of difference scores} = \\frac{0+1+2-1+0+1-2-1+0}{9} = \\frac{0}{9} = 0\\)\nUh oh, we get zero for the mean of the difference scores. This will always happen whenever you take the mean of the difference scores. We can see that there are some differences between the numbers, so using 0 as the summary value for the variation in the numbers doesn’t make much sense.\nFurthermore, you might also notice that the matrices of difference scores are redundant. The diagonal is always zero, and numbers on one side of the diagonal are the same as the numbers on the other side, except their signs are reversed. So, that’s one reason why the difference scores add up to zero.\nThese are little problems that can be solved by computing the variance and the standard deviation. For now, the standard deviation is a just a trick that we use to avoid getting a zero. But, later we will see it has properties that are important for other reasons.\n\n\n2.5.3 The Variance\nVariability, variation, variance, vary, variable, varying, variety. Confused yet? Before we describe the variance, we want to you be OK with how this word is used. First, don’t forget the big picture. We know that variability and variation refers to the big idea of differences between numbers. We can even use the word variance in the same way. When numbers are different, they have variance.\n\n\n\n\n\n\nNote\n\n\n\nThe formulas for variance and standard deviation depend on whether you think your data represents an entire population of numbers, or is sample from the population. We discuss this issue in later on. For now, we divide by N, later we discuss why you will often divide by N-1 instead.\n\n\nThe word variance also refers to a specific summary statistic, the sum of the squared deviations from the mean. Hold on what? Plain English please. The variance is the sum of the squared difference scores, where the difference scores are computed between each score and the mean. What are these scores? The scores are the numbers in the data set. Let’s see the formula in English first:\n\\(variance = \\frac{\\text{Sum of squared difference scores}}{\\text{Number of Scores}}\\)\n\n2.5.3.1 Deviations from the mean, Difference scores from the mean\nWe got a little bit complicated before when we computed the difference scores between all of the numbers in the data. Let’s do it again, but in a more manageable way. This time, we calculate the difference between each score and the mean. The idea here is\n\nWe can figure out how similar our scores are by computing the mean\nThen we can figure out how different our scores are from the mean\n\nThis could tell us, 1) something about whether our scores are really all very close to the mean (which could help us know if the mean is good representative number of the data), and 2) something about how much differences there are in the numbers.\nTake a look at this table:\n\n\n\n\n\nscores\nvalues\nmean\nDifference_from_Mean\n\n\n\n\n1\n1\n4.5\n-3.5\n\n\n2\n6\n4.5\n1.5\n\n\n3\n4\n4.5\n-0.5\n\n\n4\n2\n4.5\n-2.5\n\n\n5\n6\n4.5\n1.5\n\n\n6\n8\n4.5\n3.5\n\n\nSums\n27\n27\n0\n\n\nMeans\n4.5\n4.5\n0\n\n\n\n\n\nThe first column shows we have 6 scores in the data set, and the value columns shows each score. The sum of the values, and the mean is presented on the last two rows. The sum and the mean were obtained by:\n\\(\\frac{1+6+4+2+6+8}{6} = \\frac{27}{6} = 4.5\\).\nThe third column mean, appears a bit silly. We are just listing the mean once for every score. If you think back to our discussion about the meaning of the mean, then you will remember that it equally distributes the total sum across each data point. We can see that here, if we treat each score as the mean, then every score is a 4.5. We can also see that adding up all of the means for each score gives us back 27, which is the sum of the original values. Also, we see that if we find the mean of the mean scores, we get back the mean (4.5 again).\nAll of the action is occurring in the fourth column, Difference_from_Mean. Here, we are showing the difference scores from the mean, using \\(X_{i}-\\bar{X}\\). In other words, we subtracted the mean from each score. So, the first score, 1, is -3.5 from the mean, the second score, 6, is +1.5 from the mean, and so on.\nNow, we can look at our original scores and we can look at their differences from the mean. Notice, we don’t have a matrix of raw difference scores, so it is much easier to look at out. But, we still have a problem:\nWe can see that there are non-zero values in the difference scores, so we know there are a differences in the data. But, when we add them all up, we still get zero, which makes it seem like there are a total of zero differences in the data…Why does this happen…and what to do about it?\n\n\n2.5.3.2 The mean is the balancing point in the data\nOne brief pause here to point out another wonderful property of the mean. It is the balancing point in the data. If you take a pen or pencil and try to balance it on your figure so it lays flat what are you doing? You need to find the center of mass in the pen, so that half of it is on one side, and the other half is on the other side. That’s how balancing works. One side = the other side.\nWe can think of data as having mass or weight to it. If we put our data on our bathroom scale, we could figure out how heavy it was by summing it up. If we wanted to split the data down the middle so that half of the weight was equal to the other half, then we could balance the data on top of a pin. The mean of the data tells you where to put the pin. It is the location in the data, where the numbers on the one side add up to the same sum as the numbers on the other side.\nIf we think this through, it means that the sum of the difference scores from the mean will always add up to zero. This is because the numbers on one side of the mean will always add up to -x (whatever the sum of those numbers is), and the numbers of the other side of the mean will always add up to +x (which will be the same value only positive). And:\n\\(-x + x = 0\\), right.\nRight.\n\n\n2.5.3.3 The squared deviations\nSome devious someone divined a solution to the fact that differences scores from the mean always add to zero. Can you think of any solutions? For example, what could you do to the difference scores so that you could add them up, and they would weigh something useful, that is they would not be zero?\nThe devious solution is to square the numbers. Squaring numbers converts all the negative numbers to positive numbers. For example, \\(2^2 = 4\\), and \\(-2^2 = 4\\). Remember how squaring works, we multiply the number twice: \\(2^2 = 2*2 = 4\\), and \\(-2^2 = -2*-2 = 4\\). We use the term squared deviations to refer to differences scores that have been squared. Deviations are things that move away from something. The difference scores move away from the mean, so we also call them deviations.\nLet’s look at our table again, but add the squared deviations.\n\n\n\n\n\nscores\nvalues\nmean\nDifference_from_Mean\nSquared_Deviations\n\n\n\n\n1\n1\n4.5\n-3.5\n12.25\n\n\n2\n6\n4.5\n1.5\n2.25\n\n\n3\n4\n4.5\n-0.5\n0.25\n\n\n4\n2\n4.5\n-2.5\n6.25\n\n\n5\n6\n4.5\n1.5\n2.25\n\n\n6\n8\n4.5\n3.5\n12.25\n\n\nSums\n27\n27\n0\n35.5\n\n\nMeans\n4.5\n4.5\n0\n5.91666666666667\n\n\n\n\n\nOK, now we have a new column called squared_deviations. These are just the difference scores squared. So, \\(-3.5^2 = 12.25\\), etc. You can confirm for yourself with your cellphone calculator.\nNow that all of the squared deviations are positive, we can add them up. When we do this we create something very special called the sum of squares (SS), also known as the sum of the squared deviations from the mean. We will talk at length about this SS later on in the ANOVA chapter. So, when you get there, remember that you already know what it is, just some sums of some squared deviations, nothing fancy.\n\n\n2.5.3.4 Finally, the variance\nGuess what, we already computed the variance. It already happened, and maybe you didn’t notice. “Wait, I missed that, what happened?”.\nFirst, see if you can remember what we are trying to do here. Take a pause, and see if you can tell yourself what problem we are trying solve.\n\npause\n\nWithout further ado, we are trying to get a summary of the differences in our data. There are just as many difference scores from the mean as there are data points, which can be a lot, so it would be nice to have a single number to look at, something like a mean, that would tell us about the average differences in the data.\nIf you look at the table, you can see we already computed the mean of the squared deviations. First, we found the sum (SS), then below that we calculated the mean = 5.916 repeating. This is the variance. The variance is the mean of the sum of the squared deviations:\n\\(variance = \\frac{SS}{N}\\), where SS is the sum of the squared deviations, and N is the number of observations.\nOK, now what. What do I do with the variance? What does this number mean? Good question. The variance is often an unhelpful number to look at. Why? Because it is not in the same scale as the original data. This is because we squared the difference scores before taking the mean. Squaring produces large numbers. For example, we see a 12.25 in there. That’s a big difference, bigger than any difference between any two original values. What to do? How can we bring the numbers back down to their original unsquared size?\nIf you are thinking about taking the square root, that’s a ding ding ding, correct answer for you. We can always unsquare anything by taking the square root. So, let’s do that to 5.916. \\(\\sqrt{5.916} =\\) 2.4322829.\n\n\n\n2.5.4 The Standard Deviation\nOops, we did it again. We already computed the standard deviation, and we didn’t tell you. The standard deviation is the square root of the variance…At least, it is right now, until we complicate matters for you in the next chapter.\nHere is the formula for the standard deviation:\n\\(\\text{standard deviation} = \\sqrt{Variance} = \\sqrt{\\frac{SS}{N}}\\).\nWe could also expand this to say:\n\\(\\text{standard deviation} = \\sqrt{\\frac{\\sum_{i}^{n}({x_{i}-\\bar{x})^2}}{N}}\\)\nDon’t let those big square root signs put you off. Now, you know what they are doing there. Just bringing our measure of the variance back down to the original size of the data. Let’s look at our table again:\n\n\n\n\n\nscores\nvalues\nmean\nDifference_from_Mean\nSquared_Deviations\n\n\n\n\n1\n1\n4.5\n-3.5\n12.25\n\n\n2\n6\n4.5\n1.5\n2.25\n\n\n3\n4\n4.5\n-0.5\n0.25\n\n\n4\n2\n4.5\n-2.5\n6.25\n\n\n5\n6\n4.5\n1.5\n2.25\n\n\n6\n8\n4.5\n3.5\n12.25\n\n\nSums\n27\n27\n0\n35.5\n\n\nMeans\n4.5\n4.5\n0\n5.91666666666667\n\n\n\n\n\nWe measured the standard deviation as 2.4322829. Notice this number fits right in the with differences scores from the mean. All of the scores are kind of in and around + or - 2.4322829. Whereas, if we looked at the variance, 5.916 is just too big, it doesn’t summarize the actual differences very well.\nWhat does all this mean? Well, if someone told they had some number with a mean of 4.5 (like the values in our table), and a standard deviation of 2.4322829, you would get a pretty good summary of the numbers. You would know that many of the numbers are around 4.5, and you would know that not all of the numbers are 4.5. You would know that the numbers spread around 4.5. You also know that the spread isn’t super huge, it’s only + or - 2.4322829 on average. That’s a good starting point for describing numbers.\nIf you had loads of numbers, you could reduce them down to the mean and the standard deviation, and still be pretty well off in terms of getting a sense of those numbers."
  },
  {
    "objectID": "02-Describing_Data.html#using-descriptive-statistics-with-data",
    "href": "02-Describing_Data.html#using-descriptive-statistics-with-data",
    "title": "2  Describing Data",
    "section": "2.6 Using Descriptive Statistics with data",
    "text": "2.6 Using Descriptive Statistics with data\nRemember, you will be learning how to compute descriptive statistics using software in the labs. Check out the lab manual exercises for descriptives to see some examples of working with real data."
  },
  {
    "objectID": "02-Describing_Data.html#rolling-your-own-descriptive-statistics",
    "href": "02-Describing_Data.html#rolling-your-own-descriptive-statistics",
    "title": "2  Describing Data",
    "section": "2.7 Rolling your own descriptive statistics",
    "text": "2.7 Rolling your own descriptive statistics\nWe spent many paragraphs talking about variation in numbers, and how to use calculate the variance and standard deviation to summarize the average differences between numbers in a data set. The basic process was to 1) calculate some measure of the differences, then 2) average the differences to create a summary. We found that we couldn’t average the raw difference scores, because we would always get a zero. So, we squared the differences from the mean, then averaged the squared differences differences. Finally, we square rooted our measure to bring the summary back down to the scale of the original numbers.\nPerhaps you haven’t heard, but there is more than one way to skin a cat, but we prefer to think of this in terms of petting cats, because some of us love cats. Jokes aside, perhaps you were also thinking that the problem of summing differences scores (so that they don’t equal zero), can be solved in more than one way. Can you think of a different way, besides squaring?\n\n2.7.1 Absolute deviations\nHow about just taking the absolute value of the difference scores. Remember, the absolute value converts any number to a positive value. Check out the following table:\n\n\n\n\n\nscores\nvalues\nmean\nDifference_from_Mean\nAbsolute_Deviations\n\n\n\n\n1\n1\n4.5\n-3.5\n3.5\n\n\n2\n6\n4.5\n1.5\n1.5\n\n\n3\n4\n4.5\n-0.5\n0.5\n\n\n4\n2\n4.5\n-2.5\n2.5\n\n\n5\n6\n4.5\n1.5\n1.5\n\n\n6\n8\n4.5\n3.5\n3.5\n\n\nSums\n27\n27\n0\n13\n\n\nMeans\n4.5\n4.5\n0\n2.16666666666667\n\n\n\n\n\nThis works pretty well too. By converting the difference scores from the mean to positive values, we can now add them up and get a non-zero value (if there are differences). Then, we can find the mean of the sum of the absolute deviations. If we were to map the terms sum of squares (SS), variance and standard deviation onto these new measures based off of the absolute deviation, how would the mapping go? For example, what value in the table corresponds to the SS? That would be the sum of absolute deviations in the last column. How about the variance and standard deviation, what do those correspond to? Remember that the variance is mean (\\(SS/N\\)), and the standard deviation is a square-rooted mean (\\(\\sqrt{SS/N}\\)). In the table above we only have one corresponding mean, the mean of the sum of the absolute deviations. So, we have a variance measure that does not need to be square rooted. We might say the mean absolute deviation, is doing double-duty as a variance and a standard-deviation. Neat.\n\n\n2.7.2 Other sign-inverting operations\nIn principle, we could create lots of different summary statistics for variance that solve the summing to zero problem. For example, we could raise every difference score to any even numbered power beyond 2 (which is the square). We could use, 4, 6, 8, 10, etc. There is an infinity of even numbers, so there is an infinity of possible variance statistics. We could also use odd numbers as powers, and then take their absolute value. Many things are possible. The important aspect to any of this is to have a reason for what you are doing, and to choose a method that works for the data-analysis problem you are trying to solve. Note also, we bring up this general issue because we want you to understand that statistics is a creative exercise. We invent things when we need them, and we use things that have already been invented when they work for the problem at hand."
  },
  {
    "objectID": "02-Describing_Data.html#remember-to-look-at-your-data",
    "href": "02-Describing_Data.html#remember-to-look-at-your-data",
    "title": "2  Describing Data",
    "section": "2.8 Remember to look at your data",
    "text": "2.8 Remember to look at your data\nDescriptive statistics are great and we will use them a lot in the course to describe data. You may suspect that descriptive statistics also have some short-comings. This is very true. They are compressed summaries of large piles of numbers. They will almost always be unable to represent all of the numbers fairly. There are also different kinds of descriptive statistics that you could use, and it sometimes not clear which one’s you should use.\nPerhaps the most important thing you can do when using descriptives is to use them in combination with looking at the data in a graph form. This can help you see whether or not your descriptives are doing a good job of representing the data.\n\n2.8.1 Anscombe’s Quartet\nTo hit this point home, and to get you thinking about the issues we discuss in the next chapter, check this out. It’s called Anscombe’s Quartet, because these interesting graphs and numbers and numbers were produced by Anscombe (1973). In Figure 2.5 you are looking at pairs of measurements. Each graph has an X and Y axis, and each point represents two measurements. Each of the graphs looks very different, right?\n\n\n\n\n\nFigure 2.5: Anscombe’s Quartet\n\n\n\n\nWell, would you be surprised if I told that the descriptive statistics for the numbers in these graphs are exactly the same? It turns out they do have the same descriptive statistics. In the table below I present the mean and variance for the x-values in each graph, and the mean and the variance for the y-values in each graph.\n\n\n\n\n\nquartet\nmean_x\nvar_x\nmean_y\nvar_y\n\n\n\n\n1\n9\n11\n7.500909\n4.127269\n\n\n2\n9\n11\n7.500909\n4.127629\n\n\n3\n9\n11\n7.500000\n4.122620\n\n\n4\n9\n11\n7.500909\n4.123249\n\n\n\n\n\nThe descriptives are all the same! Anscombe put these special numbers together to illustrate the point of graphing your numbers. If you only look at your descriptives, you don’t know what patterns in the data they are hiding. If you look at the graph, then you can get a better understanding.\n\n\n2.8.2 Datasaurus Dozen\nIf you thought that Anscombe’s quartet was neat, you should take a look at the Datasaurus Dozen (Matejka and Fitzmaurice 2017). Scroll down to see the examples. You will be looking at dot plots. The dot plots show many different patterns, including dinosaurs! What’s amazing is that all of the dots have very nearly the same descriptive statistics. Just another reminder to look at your data, it might look like a dinosaur!"
  },
  {
    "objectID": "02-Describing_Data.html#videos",
    "href": "02-Describing_Data.html#videos",
    "title": "2  Describing Data",
    "section": "2.9 Videos",
    "text": "2.9 Videos\n\n2.9.1 Measures of center: Mode\n\n\n\n\n2.9.2 Measures of center: Median and Mean\n\n\n\n\n2.9.3 Standard deviation part I\n\n\n\n\n2.9.4 Standard deviation part II\n\n\n\n\n\n\nAnscombe, F. J. 1973. “Graphs in Statistical Analysis.” American Statistician 27: 17–21.\n\n\nMatejka, Justin, and George Fitzmaurice. 2017. “Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics Through Simulated Annealing.” In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, 1290–94. ACM. https://doi.org/10.1145/3025453.3025912."
  },
  {
    "objectID": "03-Correlation.html#if-something-caused-something-else-to-change-what-would-that-look-like",
    "href": "03-Correlation.html#if-something-caused-something-else-to-change-what-would-that-look-like",
    "title": "3  Correlation",
    "section": "3.1 If something caused something else to change, what would that look like?",
    "text": "3.1 If something caused something else to change, what would that look like?\nBefore we go around determining the causes of happiness, we should prepare ourselves with some analytical tools so that we could identify what causation looks like. If we don’t prepare ourselves for what we might find, then we won’t know how to interpret our own data. Instead, we need to anticipate what the data could look like. Specifically, we need to know what data would look like when one thing does not cause another thing, and what data would look like when one thing does cause another thing. This chapter does some of this preparation. Fair warning: we will find out some tricky things. For example, we can find patterns that look like one thing is causing another, even when that one thing DOES NOT CAUSE the other thing. Hang in there.\n\n3.1.1 Charlie and the Chocolate factory\nLet’s imagine that a person’s supply of chocolate has a causal influence on their level of happiness. Let’s further imagine that, like Charlie, the more chocolate you have the more happy you will be, and the less chocolate you have, the less happy you will be. Finally, because we suspect happiness is caused by lots of other things in a person’s life, we anticipate that the relationship between chocolate supply and happiness won’t be perfect. What do these assumptions mean for how the data should look?\nOur first step is to collect some imaginary data from 100 people. We walk around and ask the first 100 people we meet to answer two questions:\n\nhow much chocolate do you have, and\nhow happy are you.\n\nFor convenience, both the scales will go from 0 to 100. For the chocolate scale, 0 means no chocolate, 100 means lifetime supply of chocolate. Any other number is somewhere in between. For the happiness scale, 0 means no happiness, 100 means all of the happiness, and in between means some amount in between.\nHere is some sample data from the first 10 imaginary subjects.\n\nsubject &lt;- 1:100\nchocolate &lt;- round(1:100*runif(100,.5,1))\nhappiness &lt;- round(1:100*runif(100,.5,1))\n\nthe_df_CC &lt;- data.frame(subject,chocolate,happiness)\n\n\nthe_df_short &lt;- the_df_CC[1:10,]\n\nknitr::kable(the_df_short)\n\n\n\n\nsubject\nchocolate\nhappiness\n\n\n\n\n1\n1\n1\n\n\n2\n1\n1\n\n\n3\n3\n3\n\n\n4\n2\n4\n\n\n5\n3\n3\n\n\n6\n5\n3\n\n\n7\n4\n4\n\n\n8\n7\n5\n\n\n9\n9\n7\n\n\n10\n10\n10\n\n\n\n\n\nWe asked each subject two questions so there are two scores for each subject, one for their chocolate supply, and one for their level of happiness. You might already notice some relationships between amount of chocolate and level of happiness in the table. To make those relationships even more clear, let’s plot all of the data in a graph.\n\n\n3.1.2 Scatter plots\nWhen you have two measurements worth of data, you can always turn them into dots and plot them in a scatter plot. A scatter plot has a horizontal x-axis, and a vertical y-axis. You get to choose which measurement goes on which axis. Let’s put chocolate supply on the x-axis, and happiness level on the y-axis. Figure 3.1 shows 100 dots for each subject.\n\nlibrary(ggplot2)\nggplot(the_df_CC,aes(x=chocolate,y=happiness))+\n  geom_point()+\n  theme_classic()\n\n\n\n\nFigure 3.1: Imaginary data showing a positive correlation between amount of chocolate and amount happiness\n\n\n\n\nYou might be wondering, why are there only 100 dots for the data. Didn’t we collect 100 measures for chocolate, and 100 measures for happiness, shouldn’t there be 200 dots? Nope. Each dot is for one subject, there are 100 subjects, so there are 100 dots.\nWhat do the dots mean? Each dot has two coordinates, an x-coordinate for chocolate, and a y-coordinate for happiness. The first dot, all the way on the bottom left is the first subject in the table, who had close to 0 chocolate and close to zero happiness. You can look at any dot, then draw a straight line down to the x-axis: that will tell you how much chocolate that subject has. You can draw a straight line left to the y-axis: that will tell you how much happiness the subject has.\nNow that we are looking at the scatter plot, we can see many things. The dots are scattered around a bit aren’t they, hence scatter plot. Even when the dot’s don’t scatter, they’re still called scatter plots, perhaps because those pesky dots in real life have so much scatter all the time. More important, the dots show a relationship between chocolate supply and happiness. Happiness is lower for people with smaller supplies of chocolate, and higher for people with larger supplies of chocolate. It looks like the more chocolate you have the happier you will be, and vice-versa. This kind of relationship is called a positive correlation.\n\n\n3.1.3 Positive, Negative, and No-Correlation\nSeeing as we are in the business of imagining data, let’s imagine some more. We’ve already imagined what data would look like if larger chocolate supplies increase happiness. We’ll show that again in a bit. What do you imagine the scatter plot would look like if the relationship was reversed, and larger chocolate supplies decreased happiness. Or, what do you imagine the scatter plot would look like if there was no relationship, and the amount of chocolate that you have doesn’t do anything to your happiness. We invite your imagination to look at Figure 3.2:\n\nsubject_x&lt;-1:100\nchocolate_x&lt;-round(1:100*runif(100,.5,1))\nhappiness_x&lt;-round(1:100*runif(100,.5,1))\n\ndf_positive&lt;-data.frame(subject_x,chocolate_x,happiness_x)\n\nsubject_x&lt;-1:100\nchocolate_x&lt;-round(1:100*runif(100,.5,1))\nhappiness_x&lt;-round(100:1*runif(100,.5,1))\n\ndf_negative&lt;-data.frame(subject_x,chocolate_x,happiness_x)\n\nsubject_x&lt;-1:100\nchocolate_x&lt;-round(runif(100,0,100))\nhappiness_x&lt;-round(runif(100,0,100))\n\ndf_random&lt;-data.frame(subject_x,chocolate_x,happiness_x)\n\nall_data&lt;-rbind(df_positive,df_negative,df_random)\nall_data&lt;-cbind(all_data,correlation=rep(c(\"positive\",\"negative\",\"random\"),each=100))\n\nggplot(all_data,aes(x=chocolate_x,y=happiness_x))+\n  geom_point()+\n  theme_classic()+\n  facet_wrap(~correlation)+\n  xlab(\"chocolate supply\")+\n  ylab(\"happiness\")\n\n\n\n\nFigure 3.2: Three scatterplots showing negative, positive, and zero correlation\n\n\n\n\nThe first panel shows a negative correlation. Happiness goes down as chocolate supply increases. Negative correlation occurs when one thing goes up and the other thing goes down; or, when more of X is less of Y, and vice-versa. The second panel shows a positive correlation. Happiness goes up as chocolate as chocolate supply increases. Positive correlation occurs when both things go up together, and go down together: more of X is more of Y, and vice-versa. The third panel shows no correlation. Here, there doesn’t appear to be any obvious relationship between chocolate supply and happiness. The dots are scattered all over the place, the truest of the scatter plots.\n\n\n\n\n\n\nNote\n\n\n\nWe are wading into the idea that measures of two things can be related, or correlated with one another. It is possible for the relationships to be more complicated than just going up, or going down. For example, we could have a relationship that where the dots go up for the first half of X, and then go down for the second half.\n\n\nZero correlation occurs when one thing is not related in any way to another things: changes in X do not relate to any changes in Y, and vice-versa."
  },
  {
    "objectID": "03-Correlation.html#pearsons-r",
    "href": "03-Correlation.html#pearsons-r",
    "title": "3  Correlation",
    "section": "3.2 Pearson’s r",
    "text": "3.2 Pearson’s r\nIf Beyoncé was a statistician, she might look at these scatter plots and want to “put a number on it”. We think this is a good idea too. We’ve already learned how to create descriptive statistics for a single measure, like chocolate, or happiness (i.e., means, variances, etc.). Is it possible to create a descriptive statistic that summarized the relationship between two measures, all in one number? Can it be done? Karl Pearson to the rescue.\n\n\n\n\n\n\nNote\n\n\n\nThe stories about the invention of various statistics are very interesting, you can read more about them in the book, “The Lady Tasting Tea” (Salsburg 2001)\n\n\nThere’s a statistic for that, and Karl Pearson invented it. Everyone now calls it, “Pearson’s \\(r\\)”. We will find out later that Karl Pearson was a big-wig editor at Biometrika in the 1930s. He took a hating to another big-wig statistician, Sir Ronald Fisher (who we learn about later), and they had some stats fights…why can’t we all just get along in statistics.\nHow does Pearson’s \\(r\\) work? Let’s look again at the first 10 subjects in our fake experiment:\n\n\n\n\n\nsubject\nchocolate\nhappiness\n\n\n\n\n1\n1\n1\n\n\n2\n1\n1\n\n\n3\n3\n3\n\n\n4\n2\n4\n\n\n5\n3\n3\n\n\n6\n5\n3\n\n\n7\n4\n4\n\n\n8\n7\n5\n\n\n9\n9\n7\n\n\n10\n10\n10\n\n\nSums\n45\n41\n\n\nMeans\n4.5\n4.1\n\n\n\n\n\nWhat could we do to these numbers to produce a single summary value that represents the relationship between the chocolate supply and happiness?\n\n3.2.1 The idea of co-variance\n“Oh please no, don’t use the word variance again”. Yes, we’re doing it, we’re going to use the word variance again, and again, until it starts making sense. Remember what variance means about some numbers. It means the numbers have some change in them, they are not all the same, some of them are big, some are small. We can see that there is variance in chocolate supply across the 10 subjects. We can see that there is variance in happiness across the 10 subjects. We also saw in the scatter plot, that happiness increases as chocolate supply increases; which is a positive relationship, a positive correlation. What does this have to do with variance? Well, it means there is a relationship between the variance in chocolate supply, and the variance in happiness levels. The two measures vary together don’t they? When we have two measures that vary together, they are like a happy couple who share their variance. This is what co-variance refers to, the idea that the pattern of varying numbers in one measure is shared by the pattern of varying numbers in another measure.\nCo-variance is very, very, very ,very important. We suspect that the word co-variance is initially confusing, especially if you are not yet fully comfortable with the meaning of variance for a single measure. Nevertheless, we must proceed and use the idea of co-variance over and over again to firmly implant it into your statistical mind (we already said, but redundancy works, it’s a thing).\n\nPro tip: Three-legged race is a metaphor for co-variance. Two people tie one leg to each other, then try to walk. It works when they co-vary their legs together (positive relationship). They can also co-vary in an unhelpful way, when one person tries to move forward exactly when the other person tries to move backward. This is still co-variance (negative relationship). Funny random walking happens when there is no co-variance. This means one person does whatever they want, and so does the other person. There is a lot of variance, but the variance is shared randomly, so it’s just a bunch of legs moving around accomplishing nothing.\n\n\nPro tip #2: Successfully playing paddy-cake occurs when two people coordinate their actions so they have positively shared co-variance."
  },
  {
    "objectID": "03-Correlation.html#turning-the-numbers-into-a-measure-of-co-variance",
    "href": "03-Correlation.html#turning-the-numbers-into-a-measure-of-co-variance",
    "title": "3  Correlation",
    "section": "3.3 Turning the numbers into a measure of co-variance",
    "text": "3.3 Turning the numbers into a measure of co-variance\n“OK, so if you are saying that co-variance is just another word for correlation or relationship between two measures, I’m good with that. I suppose we would need some way to measure that.” Correct, back to our table…notice anything new?\n\n\n\n\n\nsubject\nchocolate\nhappiness\nChocolate_X_Happiness\n\n\n\n\n1\n1\n1\n1\n\n\n2\n1\n1\n1\n\n\n3\n3\n3\n9\n\n\n4\n2\n4\n8\n\n\n5\n3\n3\n9\n\n\n6\n5\n3\n15\n\n\n7\n4\n4\n16\n\n\n8\n7\n5\n35\n\n\n9\n9\n7\n63\n\n\n10\n10\n10\n100\n\n\nSums\n45\n41\n257\n\n\nMeans\n4.5\n4.1\n25.7\n\n\n\n\n\nWe’ve added a new column called Chocolate_X_Happiness, which translates to Chocolate scores multiplied by Happiness scores. Each row in the new column, is the product, or multiplication of the chocolate and happiness score for that row. Yes, but why would we do this?\nLast chapter we took you back to Elementary school and had you think about division. Now it’s time to do the same thing with multiplication. We assume you know how that works. One number times another, means taking the first number, and adding it as many times as the second says to do,\n\\(2*2= 2+2=4\\)\n\\(2*6= 2+2+2+2+2+2 = 12\\), or \\(6+6=12\\), same thing.\nYes, you know all that. But, can you bend multiplication to your will, and make it do your bidding when need to solve a problem like summarizing co-variance? Multiplication is the droid you are looking for.\nWe know how to multiple numbers, and all we have to next is think about the consequences of multiplying sets of numbers together. For example, what happens when you multiply two small numbers together, compared to multiplying two big numbers together? The first product should be smaller than the second product right? How about things like multiplying a small number by a big number? Those products should be in between right?.\nThen next step is to think about how the products of two measures sum together, depending on how they line up. Let’s look at another table:\n\nthe_df_short &lt;- data.frame(scores=1:10,X=1:10, Y=1:10, A=1:10, B=10:1, XY = 1:10*1:10, AB=1:10*10:1)\n\nthe_df_short &lt;- the_df_short %&gt;%\n  rbind(c(\"Sums\",colSums(the_df_short[1:10,2:7]))) %&gt;%\n  rbind(c(\"Means\",colMeans(the_df_short[1:10,2:7])))\nknitr::kable(the_df_short)\n\n\n\n\nscores\nX\nY\nA\nB\nXY\nAB\n\n\n\n\n1\n1\n1\n1\n10\n1\n10\n\n\n2\n2\n2\n2\n9\n4\n18\n\n\n3\n3\n3\n3\n8\n9\n24\n\n\n4\n4\n4\n4\n7\n16\n28\n\n\n5\n5\n5\n5\n6\n25\n30\n\n\n6\n6\n6\n6\n5\n36\n30\n\n\n7\n7\n7\n7\n4\n49\n28\n\n\n8\n8\n8\n8\n3\n64\n24\n\n\n9\n9\n9\n9\n2\n81\n18\n\n\n10\n10\n10\n10\n1\n100\n10\n\n\nSums\n55\n55\n55\n55\n385\n220\n\n\nMeans\n5.5\n5.5\n5.5\n5.5\n38.5\n22\n\n\n\n\n\nLook at the \\(X\\) and \\(Y\\) column. The scores for \\(X\\) and \\(Y\\) perfectly co-vary. When \\(X\\) is 1, \\(Y\\) is 1; when \\(X\\) is 2, \\(Y\\) is 2, etc. They are perfectly aligned. The scores for \\(A\\) and \\(B\\) also perfectly co-vary, just in the opposite manner. When \\(A\\) is 1, \\(B\\) is 10; when \\(A\\) is 2, \\(B\\) is 9, etc. \\(B\\) is a reversed copy of \\(A\\).\nNow, look at the column \\(XY\\). These are the products we get when we multiply the values of \\(X\\) across with the values of \\(Y\\). Also, look at the column \\(AB\\). These are the products we get when we multiply the values of A across with the values of B. So far so good.\nNow, look at the Sums for the \\(XY\\) and \\(AB\\) columns. Not the same. The sum of the \\(XY\\) products is 385, and the sum of the \\(AB\\) products is 220. For this specific set of data, the numbers 385 and 220 are very important. They represent the biggest possible sum of products (385), and the smallest possible sum of products (220). There is no way of re-ordering the numbers 1 to 10, say for \\(X\\), and the numbers 1 to 10 for \\(Y\\), that would ever produce larger or smaller numbers. Don’t believe me? Check this out:\n\nsimulated_sums&lt;-length(0)\nfor(sim in 1:1000){\nX&lt;-sample(1:10)\nY&lt;-sample(1:10)\nsimulated_sums[sim]&lt;-sum(X*Y)\n}\nsim_df&lt;-data.frame(sims=1:1000,simulated_sums)\nggplot(sim_df,aes(x=sims,y=simulated_sums))+\n  geom_point()+\n  theme_classic()+\n  geom_hline(yintercept = 385)+\n  geom_hline(yintercept = 220)\n\n\n\n\nFigure 3.3: Simulated sums of products showing the kinds of values than can be produced by randomly ordering the numbers in X and Y.\n\n\n\n\nFigure 3.3 shows 1000 computer simulations. I convinced my computer to randomly order the numbers 1 to 10 for X, and randomly order the numbers 1 to 10 for Y. Then, I multiplied X and Y, and added the products together. I did this 1000 times. The dots show the sum of the products for each simulation. The two black lines show the maximum possible sum (385), and the minimum possible sum (220), for this set of numbers. Notice, how all of the dots are in between the maximum and minimum possible values. Told you so.\n“OK fine, you told me so…So what, who cares?”. We’ve been looking for a way to summarize the co-variance between two measures right? Well, for these numbers, we have found one, haven’t we. It’s the sum of the products. We know that when the sum of the products is 385, we have found a perfect, positive correlation. We know, that when the sum of the products is 220, we have found a perfect negative correlation. What about the numbers in between. What could we conclude about the correlation if we found the sum of the products to be 350. Well, it’s going to be positive, because it’s close to 385, and that’s perfectly positive. If the sum of the products was 240, that’s going to be negative, because it’s close to the perfectly negatively correlating 220. What about no correlation? Well, that’s going to be in the middle between 220 and 385 right.\nWe have just come up with a data-specific summary measure for the correlation between the numbers 1 to 10 in X, and the numbers 1 to 10 in Y, it’s the sum of the products. We know the maximum (385) and minimum values (220), so we can now interpret any product sum for this kind of data with respect to that scale.\n\nPro tip: When the correlation between two measures increases in the positive direction, the sum of their products increases to its maximum possible value. This is because the bigger numbers in X will tend to line up with the bigger numbers in Y, creating the biggest possible sum of products. When the correlation between two measures increases in the negative direction, the sum of their products decreases to its minimum possible value. This is because the bigger numbers in X will tend to line up with the smaller numbers in Y, creating the smallest possible sum of products. When there is no correlation, the big numbers in X will be randomly lined up with the big and small numbers in Y, making the sum of the products, somewhere in the middle.\n\n\n3.3.1 Co-variance, the measure\nWe took some time to see what happens when you multiply sets of numbers together. We found that \\(big*big = bigger\\) and \\(small*small=\\text{still small}\\), and \\(big*small=\\text{in the middle}\\). The purpose of this was to give you some conceptual idea of how the co-variance between two measures is reflected in the sum of their products. We did something very straightforward. We just multiplied X with Y, and looked at how the product sums get big and small, as X and Y co-vary in different ways.\nNow, we can get a little bit more formal. In statistics, co-variance is not just the straight multiplication of values in X and Y. Instead, it’s the multiplication of the deviations in X from the mean of X, and the deviation in Y from the mean of Y. Remember those difference scores from the mean we talked about last chapter? They’re coming back to haunt you know, but in a good way like Casper the friendly ghost.\nLet’s see what this look like in a table:\n\n\nWarning in rbind(deparse.level, ...): El número de columnas del resultado, 6,\nno es un múltiplo de la longitud del vector 7 del argumento 2\n\n\n\n\n\nsubject\nchocolate\nhappiness\nC_d\nH_d\nCd_x_Hd\n\n\n\n\n1\n1\n1\n-3.5\n-3.1\n10.85\n\n\n2\n1\n1\n-3.5\n-3.1\n10.85\n\n\n3\n3\n3\n-1.5\n-1.1\n1.65\n\n\n4\n2\n4\n-2.5\n-0.1\n0.25\n\n\n5\n3\n3\n-1.5\n-1.1\n1.65\n\n\n6\n5\n3\n0.5\n-1.1\n-0.55\n\n\n7\n4\n4\n-0.5\n-0.1\n0.05\n\n\n8\n7\n5\n2.5\n0.9\n2.25\n\n\n9\n9\n7\n4.5\n2.9\n13.05\n\n\n10\n10\n10\n5.5\n5.9\n32.45\n\n\nSums\n45\n41\n0\n0\n72\n\n\nMeans\n4.5\n4.1\n0\n0\n7.25\n\n\n\n\n\nWe have computed the deviations from the mean for the chocolate scores (column C_d), and the deviations from the mean for the happiness scores (column H_d). Then, we multiplied them together (last column). Finally, you can see the mean of the products listed in the bottom right corner of the table, the official the covariance.\nThe formula for the co-variance is:\n\\(cov(X,Y) = \\frac{\\sum_{i}^{n}(x_{i}-\\bar{X})(y_{i}-\\bar{Y})}{N}\\)\nOK, so now we have a formal single number to calculate the relationship between two variables. This is great, it’s what we’ve been looking for. However, there is a problem. Remember when we learned how to compute just the plain old variance. We looked at that number, and we didn’t know what to make of it. It was squared, it wasn’t in the same scale as the original data. So, we square rooted the variance to produce the standard deviation, which gave us a more interpretable number in the range of our data. The co-variance has a similar problem. When you calculate the co-variance as we just did, we don’t know immediately know its scale. Is a 3 big? is a 6 big? is a 100 big? How big or small is this thing?\nFrom our prelude discussion on the idea of co-variance, we learned the sum of products between two measures ranges between a maximum and minimum value. The same is true of the co-variance. For a given set of data, there is a maximum possible positive value for the co-variance (which occurs when there is perfect positive correlation). And, there is a minimum possible negative value for the co-variance (which occurs when there is a perfect negative correlation). When there is zero co-variation, guess what happens. Zeroes. So, at the very least, when we look at a co-variation statistic, we can see what direction it points, positive or negative. But, we don’t know how big or small it is compared to the maximum or minimum possible value, so we don’t know the relative size, which means we can’t say how strong the correlation is. What to do?\n\n\n3.3.2 Pearson’s r we there yet\nYes, we are here now. Wouldn’t it be nice if we could force our measure of co-variation to be between -1 and +1?\n-1 would be the minimum possible value for a perfect negative correlation. +1 would be the maximum possible value for a perfect positive correlation. 0 would mean no correlation. Everything in between 0 and -1 would be increasingly large negative correlations. Everything between 0 and +1 would be increasingly large positive correlations. It would be a fantastic, sensible, easy to interpret system. If only we could force the co-variation number to be between -1 and 1. Fortunately, for us, this episode is brought to you by Pearson’s \\(r\\), which does precisely this wonderful thing.\nLet’s take a look at a formula for Pearson’s \\(r\\):\n\\(r = \\frac{cov(X,Y)}{\\sigma_{X}\\sigma_{Y}} = \\frac{cov(X,Y)}{SD_{X}SD_{Y}}\\)\nWe see the symbol \\(\\sigma\\) here, that’s more Greek for you. \\(\\sigma\\) is often used as a symbol for the standard deviation (SD). If we read out the formula in English, we see that r is the co-variance of X and Y, divided by the product of the standard deviation of X and the standard deviation of Y. Why are we dividing the co-variance by the product of the standard deviations. This operation has the effect of normalizing the co-variance into the range -1 to 1.\n\n\n\n\n\n\nNote\n\n\n\nBut, we will fill this part in as soon as we can…promissory note to explain the magic. FYI, it’s not magic. Brief explanation here is that dividing each measure by its standard deviation ensures that the values in each measure are in the same range as one another.\n\n\nFor now, we will call this mathematical magic. It works, but we don’t have space to tell you why it works right now.\n\nIt’s worth saying that there are loads of different formulas for computing Pearson’s \\(r\\). You can find them by Googling them. We will probably include more of them here, when we get around to it. However, they all give you the same answer. And, they are all not as pretty as each other. Some of them might even look scary. In other statistics textbook you will often find formulas that are easier to use for calculation purposes. For example, if you only had a pen and paper, you might use one or another formula because it helps you compute the answer faster by hand. To be honest, we are not very interested in teaching you how to plug numbers into formulas. We give one lesson on that here: Put the numbers into the letters, then compute the answer. Sorry to be snarky. Nowadays you have a computer that you should use for this kind of stuff. So, we are more interested in teaching you what the calculations mean, rather than how to do them. Of course, every week we are showing you how to do the calculations in lab with computers, because that is important too.\n\nDoes Pearson’s \\(r\\) really stay between -1 and 1 no matter what? It’s true, take a look at the following simulation. Here I randomly ordered the numbers 1 to 10 for an X measure, and did the same for a Y measure. Then, I computed Pearson’s \\(r\\), and repeated this process 1000 times. As you can see from Figure 3.4 all of the dots are between -1 and 1. Neat huh.\n\nsimulated_sums &lt;- length(0)\nfor(sim in 1:1000){\n  X &lt;- sample(1:10)\n  Y &lt;- sample(1:10)\n  simulated_sums[sim] &lt;- cor(X,Y)\n}\n\nsim_df &lt;- data.frame(sims=1:1000,simulated_sums)\n\nggplot(sim_df, aes(x = sims, y = simulated_sums))+\n  geom_point()+\n  theme_classic()+\n  geom_hline(yintercept = -1)+\n  geom_hline(yintercept = 1)+\n  ggtitle(\"Simulation of 1000 r values\")\n\n\n\n\nFigure 3.4: A simulation of of correlations. Each dot represents the r-value for the correlation between an X and Y variable that each contain the numbers 1 to 10 in random orders. The figure ilustrates that many r-values can be obtained by this random process"
  },
  {
    "objectID": "03-Correlation.html#examples-with-data",
    "href": "03-Correlation.html#examples-with-data",
    "title": "3  Correlation",
    "section": "3.4 Examples with Data",
    "text": "3.4 Examples with Data\nIn the lab for correlation you will be shown how to compute correlations in real data-sets using software. To give you a brief preview, let’s look at some data from the world happiness report (2018).\nThis report measured various attitudes across people from different countries. For example, one question asked about how much freedom people thought they had to make life choices. Another question asked how confident people were in their national government. Figure 3.5 is a scatterplot showing the relationship between these two measures. Each dot represents means for different countries.\n\nlibrary(data.table)\nlibrary(dplyr)\nwhr_data &lt;- fread('data/WHR2018.csv')\n\n# select DVs and filter for NAs\n\nsmaller_df &lt;- whr_data %&gt;%\n               dplyr::select(country,\n                      `Freedom to make life choices`,\n                      `Confidence in national government`) %&gt;%\n               dplyr::filter(!is.na(`Freedom to make life choices`),\n                      !is.na(`Confidence in national government`))\n\n# plot the data with best fit line\n\nggplot(smaller_df, aes(x=`Freedom to make life choices`,\n                     y=`Confidence in national government`))+\n  geom_point(alpha=.5)+\n  geom_smooth(method=lm, se=FALSE)+\n  theme_classic()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nFigure 3.5: Relationship between freedom to make life choices and confidence in national government. Data from the world happiness report for 2018\n\n\n\n\nWe put a blue line on the scatterplot to summarize the positive relationship. It appears that as “freedom to make life choices goes up”, so to does confidence in national government. It’s a positive correlation.\nThe actual correlation, as measured by Pearson’s \\(r\\) is:\n\n# calculate correlation\n\ncor(smaller_df$`Freedom to make life choices`,\n    smaller_df$`Confidence in national government`)\n\n[1] 0.4080963\n\n\nYou will do a lot more of this kind of thing in the lab. Looking at the graph you might start to wonder: Does freedom to make life choices cause changes how confident people are in their national government? Our does it work the other way? Does being confident in your national government give you a greater sense of freedom to make life choices? Or, is this just a random relationship that doesn’t mean anything? All good questions. These data do not provide the answers, they just suggest a possible relationship."
  },
  {
    "objectID": "03-Correlation.html#regression-a-mini-intro",
    "href": "03-Correlation.html#regression-a-mini-intro",
    "title": "3  Correlation",
    "section": "3.5 Regression: A mini intro",
    "text": "3.5 Regression: A mini intro\nWe’re going to spend the next little bit adding one more thing to our understanding of correlation. It’s called linear regression. It sounds scary, and it really is. You’ll find out much later in your Statistics education that everything we will be soon be talking about can be thought of as a special case of regression. But, we don’t want to scare you off, so right now we just introduce the basic concepts.\nFirst, let’s look at a linear regression. This way we can see what we’re trying to learn about. Figure 3.6 shows the same scatter plots as before with something new: lines!\n\nsubject_x&lt;-1:100\nchocolate_x&lt;-round(1:100*runif(100,.5,1))\nhappiness_x&lt;-round(1:100*runif(100,.5,1))\n\ndf_positive&lt;-data.frame(subject_x,chocolate_x,happiness_x)\n\nsubject_x&lt;-1:100\nchocolate_x&lt;-round(1:100*runif(100,.5,1))\nhappiness_x&lt;-round(100:1*runif(100,.5,1))\n\ndf_negative&lt;-data.frame(subject_x,chocolate_x,happiness_x)\n\nsubject_x&lt;-1:100\nchocolate_x&lt;-round(runif(100,0,100))\nhappiness_x&lt;-round(runif(100,0,100))\n\ndf_random&lt;-data.frame(subject_x,chocolate_x,happiness_x)\n\nall_data&lt;-rbind(df_positive,df_negative,df_random)\nall_data&lt;-cbind(all_data,correlation=rep(c(\"positive\",\"negative\",\"random\"),each=100))\n\nggplot(all_data,aes(x=chocolate_x,y=happiness_x))+\n  geom_point()+\n  theme_classic()+\n  geom_smooth(method=\"lm\",se=F)+\n  facet_wrap(~correlation)+\n  xlab(\"chocolate supply\")+\n  ylab(\"happiness\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nFigure 3.6: Three scatterplots showing negative, positive, and a random correlation (where the r-value is expected to be 0), along with the best fit regression line\n\n\n\n\n\n3.5.1 The best fit line\nNotice anything about these blue lines? Hopefully you can see, at least for the first two panels, that they go straight through the data, just like a kebab skewer. We call these lines best fit lines, because according to our definition (soon we promise) there are no other lines that you could draw that would do a better job of going straight throw the data.\nOne big idea here is that we are using the line as a kind of mean to describe the relationship between the two variables. When we only have one variable, that variable exists on a single dimension, it’s 1D. So, it is appropriate that we only have one number, like the mean, to describe it’s central tendency. When we have two variables, and plot them together, we now have a two-dimensional space. So, for two dimensions we could use a bigger thing that is 2d, like a line, to summarize the central tendency of the relationship between the two variables.\nWhat do we want out of our line? Well, if you had a pencil, and a printout of the data, you could draw all sorts of straight lines any way you wanted. Your lines wouldn’t even have to go through the data, or they could slant through the data with all sorts of angles. Would all of those lines be very good a describing the general pattern of the dots? Most of them would not. The best lines would go through the data following the general shape of the dots. Of the best lines, however, which one is the best? How can we find out, and what do we mean by that? In short, the best fit line is the one that has the least error.\n\n\n\n\n\n\nNote\n\n\n\nR code for plotting residuals thanks to Simon Jackson’s blog post: https://drsimonj.svbtle.com/visualising-residuals\n\n\nCheck out this next plot, it shows a line through some dots. But, it also shows some teeny tiny lines. These lines drop down from each dot, and they land on the line. Each of these little lines is called a residual. They show you how far off the line is for different dots. It’s measure of error, it shows us just how wrong the line is. After all, it’s pretty obvious that not all of the dots are on the line. This means the line does not actually represent all of the dots. The line is wrong. But, the best fit line is the least wrong of all the wrong lines.\n\nd &lt;- mtcars\nfit &lt;- lm(mpg ~ hp, data = d)\nd$predicted &lt;- predict(fit)   # Save the predicted values\nd$residuals &lt;- residuals(fit) # Save the residual values\n\nggplot(d, aes(x = hp, y = mpg)) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"lightblue\") +  # Plot regression slope\n  geom_segment(aes(xend = hp, yend = predicted, color=\"red\"), alpha = .5) +  # alpha to fade lines\n  geom_point() +\n  geom_point(aes(y = predicted), shape = 1) +\n  theme_classic()+\n  theme(legend.position=\"none\")+\n  xlab(\"X\")+ylab(\"Y\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n# Quick look at the actual, predicted, and residual values\n#library(dplyr)\n#d %&gt;% select(mpg, predicted, residuals) %&gt;% head()\n\n\n\n\nFigure 3.7: Black dots represent data points. The blue line is the best fit regression line. The white dots are repesent the predicted location of each black dot. The red lines show the error between each black dot and the regression line. The blue line is the best fit line because it minimizes the error shown by the red lines.\n\n\n\n\nThere’s a lot going on in Figure 3.7. First, we are looking at a scatter plot of two variables, an X and Y variable. Each of the black dots are the actual values from these variables. You can see there is a negative correlation here, as X increases, Y tends to decrease. We drew a regression line through the data, that’s the blue line. There’s these little white dots too. This is where the line thinks the black dots should be. The red lines are the important residuals we’ve been talking about. Each black dot has a red line that drops straight down, or straight up from the location of the black dot, and lands directly on the line. We can already see that many of the dots are not on the line, so we already know the line is “off” by some amount for each dot. The red line just makes it easier to see exactly how off the line is.\nThe important thing that is happening here, is that the the blue line is drawn is such a way, that it minimizes the total length of the red lines. For example, if we wanted to know how wrong this line was, we could simply gather up all the red lines, measure how long they are, and then add all the wrongness together. This would give us the total amount of wrongness. We usually call this the error. In fact, we’ve already talked about this idea before when we discussed standard deviation. What we will actually be doing with the red lines, is computing the sum of the squared deviations from the line. That sum is the total amount of error. Now, this blue line here minimizes the sum of the squared deviations. Any other line would produce a larger total error.\nFigure 3.8 is an animation to see this in action. The animations compares the best fit line in blue, to some other possible lines in black. The black line moves up and down. The red lines show the error between the black line and the data points. As the black line moves toward the best fit line, the total error, depicted visually by the grey area shrinks to it’s minimum value. The total error expands as the black line moves away from the best fit line.\n\nknitr::include_graphics(path=\"imgs/gifs/regression-1.gif\")\n\n\n\n\nFigure 3.8: The blue line is the best fit regression line explaining the co-variation among the black dots. The black line moves up and down showing alternative lines that could be drawn. The red lines show the amount of error between each data point and the black line. The total amount of error is depicted by the shaded grey area. The size of the grey area expands as the black line moves away from the best fit line, and shrinks to a minimum as the black line moves toward the best fit line.\n\n\n\n\nWhenever the black line does not overlap with the blue line, it is worse than the best fit line. The blue regression line is like Goldilocks, it’s just right, and it’s in the middle.\nFigure 3.9 shows how the sum of squared deviations (the sum of the squared lengths of the red lines) behaves as we move the line up and down. What’s going on here is that we are computing a measure of the total error as the black line moves through the best fit line. This represents the sum of the squared deviations. In other words, we square the length of each red line from the above animation, then we add up all of the squared red lines, and get the total error (the total sum of the squared deviations). The graph below shows what the total error looks like as the black line approaches then moves away from the best fit line. Notice, the dots in this graph start high on the left side, then they swoop down to a minimum at the bottom middle of the graph. When they reach their minimum point, we have found a line that minimizes the total error. This is the best fit regression line.\n\nd &lt;- mtcars\nfit &lt;- lm(mpg ~ hp, data = d)\nd$predicted &lt;- predict(fit)   # Save the predicted values\nd$residuals &lt;- residuals(fit) # Save the residual values\n\ncoefs&lt;-coef(lm(mpg ~ hp, data = mtcars))\n#coefs[1]\n#coefs[2]\n\nx&lt;-d$hp\nmove_line&lt;-seq(-5,5,.5)\ntotal_error&lt;-c(length(move_line))\ncnt&lt;-0\nfor(i in move_line){\n  cnt&lt;-cnt+1\n  predicted_y &lt;- coefs[2]*x + coefs[1]+i\n  error_y &lt;- (predicted_y-d$mpg)^2\n  total_error[cnt]&lt;-sum(error_y)\n}\ndf&lt;-data.frame(move_line,total_error)\nggplot(df,aes(x=move_line,y=total_error))+\n  geom_point()+\n  theme_classic()+\n  ylab(\"sum of squared deviations\")+\n  xlab(\"change to y-intercept\")\n\n\n\n\nFigure 3.9: A plot of the sum of the squared deviations for different lines moving up and down, through the best fit line. The best fit line occurs at the position that minimizes the sum of the sqaured deviations.\n\n\n\n\nOK, so we haven’t talked about the y-intercept yet. But, what this graph shows us is how the total error behaves as we move the line up and down. The y-intercept here is the thing we change that makes our line move up and down. As you can see the dots go up when we move the line down from 0 to -5, and the dots go up when we move the line up from 0 to +5. The best line, that minimizes the error occurs right in the middle, when we don’t move the blue regression line at all.\n\n\n3.5.2 Lines\nOK, fine you say. So, there is one magic line that will go through the middle of the scatter plot and minimize the sum of the squared deviations. How do I find this magic line? We’ll show you. But, to be completely honest, you’ll almost never do it the way we’ll show you here. Instead, it’s much easier to use software and make your computer do it for. You’ll learn how to that in the labs.\nBefore we show you how to find the regression line, it’s worth refreshing your memory about how lines work, especially in 2 dimensions. Remember this?\n\\(y = ax + b\\), or also \\(y = mx + b\\) (sometimes a or m is used for the slope)\nThis is the formula for a line. Another way of writing it is:\n\\(y = slope * x + \\text{y-intercept}\\)\nThe slope is the slant of the line, and the y-intercept is where the line crosses the y-axis. Let’s look at the lines in Figure 3.10.\n\nggplot()+\n  geom_abline(slope=1,intercept=5,color=\"blue\")+\n  geom_abline(slope=-1, intercept=15, color=\"red\")+\n  lims(x = c(1,20), y = c(0,20))+\n  theme_classic()\n\n\n\n\nFigure 3.10: Two different lines with different y-intercepts (where the line crosses the y-axis), and different slopes. A positive slope makes the line go up from left to right. A negative slope makes the line go down from left to right.\n\n\n\n\nThe formula for the blue line is \\(y = 1*x + 5\\). Let’s talk about that. When x = 0, where is the blue line on the y-axis? It’s at five. That happens because 1 times 0 is 0, and then we just have the five left over. How about when x = 5? In that case y =10. You just need the plug in the numbers to the formula, like this:\n\\(y = 1*x + 5\\) \\(y = 1*5 + 5 = 5+5 =10\\)\nThe point of the formula is to tell you where y will be, for any number of x. The slope of the line tells you whether the line is going to go up or down, as you move from the left to the right. The blue line has a positive slope of one, so it goes up as x goes up. How much does it go up? It goes up by one for everyone one of x! If we made the slope a 2, it would be much steeper, and go up faster. The red line has a negative slope, so it slants down. This means \\(y\\) goes down, as \\(x\\) goes up. When there is no slant, and we want to make a perfectly flat line, we set the slope to 0. This means that y doesn’t go anywhere as x gets bigger and smaller.\nThat’s lines.\n\n\n3.5.3 Computing the best fit line\nIf you have a scatter plot showing the locations of scores from two variables, the real question is how can you find the slope and the y-intercept for the best fit line? What are you going to do? Draw millions of lines, add up the residuals, and then see which one was best? That would take forever. Fortunately, there are computers, and when you don’t have one around, there’s also some handy formulas.\n\n\n\n\n\n\nNote\n\n\n\nIt’s worth pointing out just how much computers have changed everything. Before computers everyone had to do these calculations by hand, such a chore! Aside from the deeper mathematical ideas in the formulas, many of them were made for convenience, to speed up hand calculations, because there were no computers. Now that we have computers, the hand calculations are often just an exercise in algebra. Perhaps they build character. You decide.\n\n\nWe’ll show you the formulas. And, work through one example by hand. It’s the worst, we know. By the way, you should feel sorry for me as I do this entire thing by hand for you.\nHere are two formulas we can use to calculate the slope and the intercept, straight from the data. We won’t go into why these formulas do what they do. These ones are for “easy” calculation.\n\\(intercept = b = \\frac{\\sum{y}\\sum{x^2}-\\sum{x}\\sum{xy}}{n\\sum{x^2}-(\\sum{x})^2}\\)\n\\(slope = m = \\frac{n\\sum{xy}-\\sum{x}\\sum{y}}{n\\sum{x^2}-(\\sum{x})^2}\\)\nIn these formulas, the \\(x\\) and the \\(y\\) refer to the individual scores. Here’s a table showing you how everything fits together.\n\nscores&lt;-c(1,2,3,4,5,6,7)\nx&lt;-c(1,4,3,6,5,7,8)\ny&lt;-c(2,5,1,8,6,8,9)\nx_squared&lt;-x^2\ny_squared&lt;-y^2\nxy&lt;-x*y\n\nall_df&lt;-data.frame(scores,x,y,x_squared,y_squared,xy)\n\nall_df &lt;- all_df %&gt;%\n  rbind(c(\"Sums\",colSums(all_df[1:7,2:6]))) \n\nslope=((sum(y)*sum(x_squared))-(sum(x)*sum(xy)))/((7*sum(x_squared))-sum(x)^2)\nintercept=(7*sum(xy)-sum(x)*sum(y))/(7*sum(x_squared)-sum(x)^2)\n\nknitr::kable(all_df)\n\n\n\n\nscores\nx\ny\nx_squared\ny_squared\nxy\n\n\n\n\n1\n1\n2\n1\n4\n2\n\n\n2\n4\n5\n16\n25\n20\n\n\n3\n3\n1\n9\n1\n3\n\n\n4\n6\n8\n36\n64\n48\n\n\n5\n5\n6\n25\n36\n30\n\n\n6\n7\n8\n49\n64\n56\n\n\n7\n8\n9\n64\n81\n72\n\n\nSums\n34\n39\n200\n275\n231\n\n\n\n\n\nWe see 7 sets of scores for the x and y variable. We calculated \\(x^2\\) by squaring each value of x, and putting it in a column. We calculated \\(y^2\\) by squaring each value of y, and putting it in a column. Then we calculated \\(xy\\), by multiplying each \\(x\\) score with each \\(y\\) score, and put that in a column. Then we added all the columns up, and put the sums at the bottom. These are all the number we need for the formulas to find the best fit line. Here’s what the formulas look like when we put numbers in them:\n\\(intercept = b = \\frac{\\sum{y}\\sum{x^2}-\\sum{x}\\sum{xy}}{n\\sum{x^2}-(\\sum{x})^2} = \\frac{39 * 200 - 34*231}{7*200-34^2} = -.221\\)\n\\(slope = m = \\frac{n\\sum{xy}-\\sum{x}\\sum{y}}{n\\sum{x^2}-(\\sum{x})^2} = \\frac{7*231-34*39}{7*275-34^2} = 1.19\\)\nGreat, now we can check our work, let’s plot the scores in a scatter plot and draw a line through it with slope = 1.19, and a y-intercept of -.221. As shown in Figure 3.11, the line should go through the middle of the dots.\n\nplot_df&lt;-data.frame(x,y)\n\nggplot(plot_df,aes(x=x,y=y))+\n  geom_point()+\n  geom_smooth(method=\"lm\",se=FALSE)+\n  theme_classic()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n#coef(lm(y~x,plot_df))\n\n\n\n\nFigure 3.11: An example regression line with confidence bands going through a few data points in a scatterplot"
  },
  {
    "objectID": "03-Correlation.html#interpreting-correlations",
    "href": "03-Correlation.html#interpreting-correlations",
    "title": "3  Correlation",
    "section": "3.6 Interpreting Correlations",
    "text": "3.6 Interpreting Correlations\nWhat does the presence or the absence of a correlation between two measures mean? How should correlations be interpreted? What kind of inferences can be drawn from correlations? These are all very good questions. A first piece of advice is to use caution when interpreting correlations. Here’s why.\n\n3.6.1 Correlation does not equal causation\nPerhaps you have heard that correlation does not equal causation. Why not? There are lots of reasons why not. However, before listing some of the reasons let’s start with a case where we would expect a causal connection between two measurements. Consider, buying a snake plant for your home. Snake plants are supposed to be easy to take care of because you can mostly ignore them.\nLike most plants, snake plants need some water to stay alive. However, they also need just the right amount of water. Imagine an experiment where 1000 snake plants were grown in a house. Each snake plant is given a different amount of water per day, from zero teaspoons of water per day to 1000 teaspoons of water per day. We will assume that water is part of the causal process that allows snake plants to grow. The amount of water given to each snake plant per day can also be one of our measures. Imagine further that every week the experimenter measures snake plant growth, which will be the second measurement. Now, can you imagine for yourself what a scatter plot of weekly snake plant growth by tablespoons of water would look like?\n\n3.6.1.1 Even when there is causation, there might not be obvious correlation\nThe first plant given no water at all would have a very hard time and eventually die. It should have the least amount of weekly growth. How about the plants given only a few teaspoons of water per day. This could be just enough water to keep the plants alive, so they will grow a little bit but not a lot. If you are imagining a scatter plot, with each dot being a snake plant, then you should imagine some dots starting in the bottom left hand corner (no water & no plant growth), moving up and to the right (a bit of water, and a bit of growth). As we look at snake plants getting more and more water, we should see more and more plant growth, right? “Sure, but only up to a point”. Correct, there should be a trend for a positive correlation with increasing plant growth as amount of water per day increases. But, what happens when you give snake plants too much water? From personal experience, they die. So, at some point, the dots in the scatter plot will start moving back down again. Snake plants that get way too much water will not grow very well.\nThe imaginary scatter plot you should be envisioning could have an upside U shape. Going from left to right, the dot’s go up, they reach a maximum, then they go down again reaching a minimum. Computing Pearson’s \\(r\\) for data like this can give you \\(r\\) values close to zero. The scatter plot could look something like Figure 3.12.\n\nwater&lt;-seq(0,999,1)\ngrowth&lt;-c(seq(0,10,(10/499)),seq(10,0,-(10/499)))\nnoise&lt;-runif(1000,-2,2)\ngrowth&lt;-growth+noise\nsnake_df&lt;-data.frame(growth,water)\n\nggplot(snake_df, aes(x=water,y=growth))+\n  geom_point()+\n  theme_classic()+\n  xlab(\"Water (teaspoons)\")+\n  ggtitle(\"Imaginary snake plant growth \\n as a function of water\")\n\n\n\n\nFigure 3.12: Illustration of a possible relationship between amount of water and snake plant growth. Growth goes up with water, but eventually goes back down as too much water makes snake plants die.\n\n\n\n\nGranted this looks more like an inverted V, than an inverted U, but you get the picture right? There is clearly a relationship between watering and snake plant growth. But, the correlation isn’t in one direction. As a result, when we compute the correlation in terms of Pearson’s r, we get a value suggesting no relationship.\n\ncor(growth,water)\n\n[1] 0.005462981\n\n\nWhat this really means is there is no linear relationship that can be described by a single straight line. When we need lines or curves going in more than one direction, we have a nonlinear relationship.\nThis example illustrates some conundrums in interpreting correlations. We already know that water is needed for plants to grow, so we are rightly expecting there to be a relationship between our measure of amount of water and plant growth. If we look at the first half of the data we see a positive correlation, if we look at the last half of the data we see a negative correlation, and if we look at all of the data we see no correlation. Yikes. So, even when there is a causal connection between two measures, we won’t necessarily obtain clear evidence of the connection just by computing a correlation coefficient.\n\nPro Tip: This is one reason why plotting your data is so important. If you see an upside U shape pattern, then a correlation analysis is probably not the best analysis for your data.\n\n\n\n3.6.1.2 Confounding variable, or Third variable problem\nAnybody can correlate any two things that can be quantified and measured. For example, we could find a hundred people, ask them all sorts of questions like:\n\nhow happy are you\nhow old are you\nhow tall are you\nhow much money do you make per year\nhow long are your eyelashes\nhow many books have you read in your life\nhow loud is your inner voice\n\nLet’s say we found a positive correlation between yearly salary and happiness. Note, we could have just as easily computed the same correlation between happiness and yearly salary. If we found a correlation, would you be willing to infer that yearly salary causes happiness? Perhaps it does play a small part. But, something like happiness probably has a lot of contributing causes. Money could directly cause some people to be happy. But, more likely, money buys people access to all sorts of things, and some of those things might contribute happiness. These “other” things are called third variables. For example, perhaps people living in nicer places in more expensive houses are more happy than people in worse places in cheaper houses. In this scenario, money isn’t causing happiness, it’s the places and houses that money buys. But, even is this were true, people can still be more or less happy in lots of different situations.\nThe lesson here is that a correlation can occur between two measures because of a third variable that is not directly measured. So, just because we find a correlation, does not mean we can conclude anything about a causal connection between two measurements.\n\n\n\n3.6.2 Correlation and Random chance\nAnother very important aspect of correlations is the fact that they can be produced by random chance. This means that you can find a positive or negative correlation between two measures, even when they have absolutely nothing to do with one another. You might have hoped to find zero correlation when two measures are totally unrelated to each other. Although this certainly happens, unrelated measures can accidentally produce spurious correlations, just by chance alone.\nLet’s demonstrate how correlations can occur by chance when there is no causal connection between two measures. Imagine two participants. One is at the North pole with a lottery machine full of balls with numbers from 1 to 10. The other is at the south pole with a different lottery machine full of balls with numbers from 1 to 10. There are an endless supply of balls in the machine, so every number could be picked for any ball. Each participant randomly chooses 10 balls, then records the number on the ball. In this situation we will assume that there is no possible way that balls chosen by the first participant could causally influence the balls chosen by the second participant. They are on the other side of the world. We should assume that the balls will be chosen by chance alone.\nHere is what the numbers on each ball could look like for each participant:\n\nBall&lt;-1:10\nNorth_pole&lt;-round(round(runif(10,1,10)))\nSouth_pole&lt;-round(round(runif(10,1,10)))\n\nthe_df_balls&lt;-data.frame(Ball,North_pole,South_pole)\n\n#the_df_balls &lt;- the_df_balls %&gt;%\n#  rbind(c(\"Sums\",colSums(the_df_balls[1:10,2:3]))) %&gt;%\n#  rbind(c(\"Means\",colMeans(the_df_balls[1:10,2:3])))\nknitr::kable(the_df_balls)\n\n\n\n\nBall\nNorth_pole\nSouth_pole\n\n\n\n\n1\n9\n4\n\n\n2\n5\n6\n\n\n3\n2\n5\n\n\n4\n6\n10\n\n\n5\n3\n8\n\n\n6\n10\n4\n\n\n7\n9\n2\n\n\n8\n1\n6\n\n\n9\n6\n4\n\n\n10\n9\n1\n\n\n\n\n\nIn this one case, if we computed Pearson’s \\(r\\), we would find that \\(r =\\) -0.5543889. But, we already know that this value does not tell us anything about the relationship between the balls chosen in the north and south pole. We know that relationship should be completely random, because that is how we set up the game.\nThe better question here is to ask what can random chance do? For example, if we ran our game over and over again thousands of times, each time choosing new balls, and each time computing the correlation, what would we find?First, we will find fluctuation. The r value will sometimes be positive, sometimes be negative, sometimes be big and sometimes be small. Second, we will see what the fluctuation looks like. This will give us a window into the kinds of correlations that chance alone can produce. Let’s see what happens.\n\n3.6.2.1 Monte-carlo simulation of random correlations\nIt is possible to use a computer to simulate our game as many times as we want. This process is often termed monte-carlo simulation.\nBelow is a script written for the programming language R. We won’t go into the details of the code here. However, let’s briefly explain what is going on. Notice, the part that says for(sim in 1:1000). This creates a loop that repeats our game 1000 times. Inside the loop there are variables named North_pole and South_pole. During each simulation, we sample 10 random numbers (between 1 to 10) into each variable. These random numbers stand for the numbers that would have been on the balls from the lottery machine. Once we have 10 random numbers for each, we then compute the correlation using cor(North_pole,South_pole). Then, we save the correlation value and move on to the next simulation. At the end, we will have 1000 individual Pearson \\(r\\) values.\n\nsimulated_correlations &lt;- length(0)\nfor(sim in 1:1000){\n  North_pole &lt;- runif(10,1,10)\n  South_pole &lt;- runif(10,1,10)\n  simulated_correlations[sim] &lt;- cor(North_pole,South_pole)\n}\n\nsim_df &lt;- data.frame(sims=1:1000,simulated_correlations)\n\nggplot(sim_df, aes(x = sims, y = simulated_correlations))+\n  geom_point()+\n  theme_classic()+\n  geom_hline(yintercept = -1)+\n  geom_hline(yintercept = 1)+\n  ggtitle(\"Simulation of 1000 r values\")\n\n\n\n\nFigure 3.13: Another figure showing a range of r-values that can be obtained by chance.\n\n\n\n\nFigure 3.13 shows the 1000 Pearson \\(r\\) values from the simulation. Does the figure below look familiar to you? We have already conducted a similar kind of simulation before. Each dot in the scatter plot shows the Pearson \\(r\\) for each simulation from 1 to 1000. As you can see the dots are all over of the place, in between the range -1 to 1. The important lesson here is that random chance produced all of these correlations. This means we can find “correlations” in the data that are completely meaningless, and do not reflect any causal relationship between one measure and another.\nLet’s illustrate the idea of finding “random” correlations one more time, with a little movie. This time, we will show you a scatter plot of the random values sampled for the balls chosen from the North and South pole. If there is no relationship we should see dots going everywhere. If there happens to be a positive relationship (purely by chance), we should see the dots going from the bottom left to the top right. If there happens to be a negative relationship (purely by chance), we should see the dots going from the top left down to the bottom right.\nOn more thing to prepare you for the movie. There are three scatter plots below in Figure 3.14, showing negative, positive, and zero correlations between two variables. You’ve already seen this graph before. We are just reminding you that the blue lines are helpful for seeing the correlation.Negative correlations occur when a line goes down from the top left to bottom right. Positive correlations occur when a line goes up from the bottom left to the top right. Zero correlations occur when the line is flat (doesn’t go up or down).\n\nsubject_x&lt;-1:100\nchocolate_x&lt;-round(1:100*runif(100,.5,1))\nhappiness_x&lt;-round(1:100*runif(100,.5,1))\n\ndf_positive&lt;-data.frame(subject_x,chocolate_x,happiness_x)\n\nsubject_x&lt;-1:100\nchocolate_x&lt;-round(1:100*runif(100,.5,1))\nhappiness_x&lt;-round(100:1*runif(100,.5,1))\n\ndf_negative&lt;-data.frame(subject_x,chocolate_x,happiness_x)\n\nsubject_x&lt;-1:100\nchocolate_x&lt;-round(runif(100,0,100))\nhappiness_x&lt;-round(runif(100,0,100))\n\ndf_random&lt;-data.frame(subject_x,chocolate_x,happiness_x)\n\nall_data&lt;-rbind(df_positive,df_negative,df_random)\nall_data&lt;-cbind(all_data,correlation=rep(c(\"positive\",\"negative\",\"random\"),each=100))\n\nggplot(all_data,aes(x=chocolate_x,y=happiness_x))+\n  geom_point()+\n  geom_smooth(method=lm,se=FALSE)+\n  theme_classic()+\n  facet_wrap(~correlation)+\n  xlab(\"chocolate supply\")+\n  ylab(\"happiness\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nFigure 3.14: A reminder of what positive, negative, and zero correlation looks like\n\n\n\n\nOK, now we are ready for the movie. Figure 3.15 shows the process of sampling two sets of numbers randomly, one for the X variable, and one for the Y variable. Each time we sample 10 numbers for each, plot them, then draw a line through them. Remember, these numbers are all completely random, so we should expect, on average that there should be no correlation between the numbers. However, this is not what happens. You can the line going all over the place. Sometimes we find a negative correlation (line goes down), sometimes we see a positive correlation (line goes up), and sometimes it looks like zero correlation (line is more flat).\n\nknitr::include_graphics(path=\"imgs/gifs/corUnifn10-1.gif\")\n\n\n\n\nFigure 3.15: Completely random data points drawn from a uniform distribution with a small sampl-size of 10. The blue line twirls around sometimes showing large correlations that are produced by chance\n\n\n\n\nYou might be thinking this is kind of disturbing. If we know that there should be no correlation between two random variables, how come we are finding correlations? This is a big problem right? I mean, if someone showed me a correlation between two things, and then claimed one thing was related to another, how could know I if it was true. After all, it could be chance! Chance can do that too.\nFortunately, all is not lost. We can look at our simulated data in another way, using a histogram. Remember, just before the movie, we simulated 1000 different correlations using random numbers. By, putting all of those \\(r\\) values into a histogram, we can get a better sense of how chance behaves. We can see what kind of correlations chance is likely or unlikely to produce. Figure 3.16 is a histogram of the simulated \\(r\\) values.\n\nhist(simulated_correlations,breaks=seq(-1,1,.1))\n\n\n\n\nFigure 3.16: A histogram showing the frequency distribution of r-values for completely random values between an X and Y variable (sample-size=10). A rull range of r-values can be obtained by chance alone. Larger r-values are less common than smaller r-values\n\n\n\n\nNotice that this histogram is not flat. Most of the simulated \\(r\\) values are close to zero. Notice, also that the bars get smaller as you move away from zero in the positive or negative direction. The general take home here is that chance can produce a wide range of correlations. However, not all correlations happen very often. For example, the bars for -1 and 1 are very small. Chance does not produce nearly perfect correlations very often. The bars around -.5 and .5 are smaller than the bars around zero, as medium correlations do not occur as often as small correlations by chance alone.\nYou can think of this histogram as the window of chance. It shows what chance often does, and what it often does not do. If you found a correlation under these very same circumstances (e.g., measured the correlation between two sets of 10 random numbers), then you could consult this window. What should you ask the window? How about, could my observed correlation (the one that you found in your data) have come from this window. Let’s say you found a correlation of \\(r = .1\\). Could a .1 have come from the histogram? Well, look at the histogram around where the .1 mark on the x-axis is. Is there a big bar there? If so, this means that chance produces this value fairly often. You might be comfortable with the inference: Yes, this .1 could have been produced by chance, because it is well inside the window of chance. How about \\(r = .5\\)? The bar is much smaller here, you might think, “well, I can see that chance does produce .5 some times, so chance could have produced my .5. Did it? Maybe, maybe not, not sure”. Here, your confidence in a strong inference about the role of chance might start getting a bit shakier.\nHow about an \\(r = .95\\)?. You might see that the bar for .95 is very very small, perhaps too small to see. What does this tell you? It tells you that chance does not produce .95 very often, hardly if at all, pretty much never. So, if you found a .95 in your data, what would you infer? Perhaps you would be comfortable inferring that chance did not produce your .95, after .95 is mostly outside the window of chance.\n\n\n3.6.2.2 Increasing sample-size decreases opportunity for spurious correlation\nBefore moving on, let’s do one more thing with correlations. In our pretend lottery game, each participant only sampled 10 balls each. We found that this could lead to a range of correlations between the numbers randomly drawn from either sides of the pole. Indeed, we even found some correlations that were medium to large in size. If you were a researcher who found such correlations, you might be tempted to believe there was a relationship between your measurements. However, we know in our little game, that those correlations would be spurious, just a product of random sampling.\nThe good news is that, as a researcher, you get to make the rules of the game. You get to determine how chance can play. This is all a little bit metaphorical, so let’s make it concrete.\nWe will see what happens in four different scenarios. First, we will repeat what we already did. Each participant will draw 10 balls, then we compute the correlation, and do this over 1000 times and look at a histogram. Second, we will change the game so each participant draws 50 balls each, and then repeat our simulation. Third, and fourth, we will change the game so each participant draws 100 balls each, and then 1000 balls each, and repeat etc.\nFigure 3.17 shows four different histograms of the Pearson \\(r\\) values in each of the different scenarios. Each scenario involves a different sample-size, from, 10, 50, 100 to 1000.\n\nall_df&lt;-data.frame()\nfor(s_size in  c(10,50,100,1000)){\n  simulated_correlations &lt;- length(0)\n  for(sim in 1:1000){\n    North_pole &lt;- runif(s_size,1,10)\n    South_pole &lt;- runif(s_size,1,10)\n    simulated_correlations[sim] &lt;- cor(North_pole,South_pole)\n  }\nsim_df &lt;- data.frame(sample_size=rep(s_size,1000),sims=1:1000,simulated_correlations)\nall_df&lt;-rbind(all_df,sim_df)\n}\n\n\nggplot(all_df,aes(x=simulated_correlations))+\n  geom_histogram()+\n  facet_wrap(~sample_size)+\n  theme_classic()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 3.17: Four histograms showing the frequency distributions of r-values between completely random X and Y variables as a function of sample-size. The width of the distributions shrink as sample-size increases. Smaller sample-sizes are more likely to produce a wider range of r-values by chance. Larger sample-sizes always produce a narrow range of small r-values\n\n\n\n\nBy inspecting the four histograms you should notice a clear pattern. The width or range of each histogram shrinks as the sample-size increases. What is going on here? Well, we already know that we can think of these histograms as windows of chance. They tell us which \\(r\\) values occur fairly often, which do not. When our sample-size is 10, lots of different \\(r\\) values happen. That histogram is very flat and spread out. However, as the sample-size increases, we see that the window of chance gets pulled in. For example, by the time we get to 1000 balls each, almost all of the Pearson \\(r\\) values are very close to 0.\nOne take home here, is that increasing sample-size narrows the window of chance. So, for example, if you ran a study involving 1000 samples of two measures, and you found a correlation of .5, then you can clearly see in the bottom right histogram that .5 does not occur very often by chance alone. In fact, there is no bar, because it didn’t happen even once in the simulation. As a result, when you have a large sample size like n = 1000, you might be more confident that your observed correlation (say of .5) was not a spurious correlation. If chance is not producing your result, then something else is.\nFinally, notice how your confidence about whether or not chance is mucking about with your results depends on your sample size. If you only obtained 10 samples per measurement, and found \\(r = .5\\), you should not be as confident that your correlation reflects a real relationship. Instead, you can see that \\(r\\)’s of .5 happen fairly often by chance alone.\n\nPro tip: when you run an experiment you get to decide how many samples you will collect, which means you can choose to narrow the window of chance. Then, if you find a relationship in the data you can be more confident that your finding is real, and not just something that happened by chance.\n\n\n\n\n3.6.3 Some more movies\nLet’s ingrain these idea with some more movies. When our sample-size is small (N is small), sampling error can cause all sort “patterns” in the data. This makes it possible, and indeed common, for “correlations” to occur between two sets of numbers. When we increase the sample-size, sampling error is reduced, making it less possible for “correlations” to occur just by chance alone. When N is large, chance has less of an opportunity to operate.\n\n3.6.3.1 Watching how correlation behaves when there is no correlation\nBelow we randomly sample numbers for two variables, plot them, and show the correlation using a line. There are four panels, each showing the number of observations in the samples, from 10, 50, 100, to 1000 in each sample.\nRemember, because we are randomly sampling numbers, there should be no relationship between the X and Y variables. But, as we have been discussing, because of chance, we can sometimes observe a correlation (due to chance). The important thing to watch is how the line behaves across the four panels in Figure 3.18. The line twirls around in all directions when the sample size is 10. It is also moves around quite a bit when the sample size is 50 or 100. It still moves a bit when the sample size is 1000, but much less. In all cases we expect that the line should be flat, but every time we take new samples, sometimes the line shows us pseudo patterns.\n\nknitr::include_graphics(path=\"imgs/gifs/corUnifFourNs-1.gif\")\n\n\n\n\nFigure 3.18: Animation of how correlation behaves for completely random X and Y variables as a function of sample size. The best fit line is not very stable for small sample-sizes, but becomes more reliably flat as sample-size increases.\n\n\n\n\n\nall_df&lt;-data.frame()\nfor(sim in 1:10){\n  for(n in c(10,50,100,1000)){\n  North_pole &lt;- runif(n,1,10)\n  South_pole &lt;- runif(n,1,10)\n  t_df&lt;-data.frame(nsize=rep(n,n),\n                   simulation=rep(sim,n),\n                                  North_pole,\n                                  South_pole)\n  all_df&lt;-rbind(all_df,t_df)\n  }\n}\n\n\nggplot(all_df,aes(x=North_pole,y=South_pole))+\n  geom_point()+\n  geom_smooth(method=lm, se=FALSE)+\n  theme_classic()+\n  facet_wrap(~nsize)+\n  transition_states(\n    simulation,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n\nWhich line should you trust? Well, hopefully you can see that the line for 1000 samples is the most stable. It tends to be very flat every time, and it does not depend so much on the particular sample. The line with 10 observations per sample goes all over the place. The take home here, is that if someone told you that they found a correlation, you should want to know how many observations they hand in their sample. If they only had 10 observations, how could you trust the claim that there was a correlation? You can’t!!! Not now that you know samples that are that small can do all sorts of things by chance alone. If instead, you found out the sample was very large, then you might trust that finding a little bit more. For example, in the above movie you can see that when there are 1000 samples, we never see a strong or weak correlation; the line is always flat. This is because chance almost never produces strong correlations when the sample size is very large.\nIn the above example, we sampled numbers random numbers from a uniform distribution. Many examples of real-world data will come from a normal or approximately normal distribution. We can repeat the above, but sample random numbers from the same normal distribution. There will still be zero actual correlation between the X and Y variables, because everything is sampled randomly. Figure 3.19 shows the same behavior. The computed correlation for small sample-sizes fluctuate wildly, and large sample sizes do not.\n\nknitr::include_graphics(path=\"imgs/gifs/corNormFourNs-1.gif\")\n\n\n\n\nFigure 3.19: Animation of correlation for random values sampled from a normal distribution, rather than a uniform distribution.\n\n\n\n\nOK, so what do things look like when there actually is a correlation between variables?\n\n\n3.6.3.2 Watching correlations behave when there really is a correlation\nSometimes there really are correlations between two variables that are not caused by chance. Figure 3.20 shows a movie of four scatter plots. Each shows the correlation between two variables. Again, we change the sample-size in steps of 10, 50 100, and 1000. The data have been programmed to contain a real positive correlation. So, we should expect that the line will be going up from the bottom left to the top right. However, there is still variability in the data. So this time, sampling error due to chance will fuzz the correlation. We know it is there, but sometimes chance will cause the correlation to be eliminated.\nNotice that in the top left panel (sample-size = 10), the line is twirling around much more than the other panels. Every new set of samples produces different correlations. Sometimes, the line even goes flat or downward. However, as we increase sample-size, we can see that the line doesn’t change very much, it is always going up showing a positive correlation.\n\nknitr::include_graphics(path=\"imgs/gifs/corRealgif-1.gif\")\n\n\n\n\nFigure 3.20: How correlation behaves as a function of sample-size when there is a true correlation between X and Y variables.\n\n\n\n\nThe main takeaway here is that even when there is a positive correlation between two things, you might not be able to see it if your sample size is small. For example, you might get unlucky with the one sample that you measured. Your sample could show a negative correlation, even when the actual correlation is positive! Unfortunately, in the real world we usually only have the sample that we collected, so we always have to wonder if we got lucky or unlucky. Fortunately, if you want to remove luck, all you need to do is collect larger samples. Then you will be much more likely to observe the real pattern, rather the pattern that can be introduced by chance."
  },
  {
    "objectID": "03-Correlation.html#summary",
    "href": "03-Correlation.html#summary",
    "title": "3  Correlation",
    "section": "3.7 Summary",
    "text": "3.7 Summary\nIn this section we have talked about correlation, and started to build some intuitions about inferential statistics, which is the major topic of the remaining chapters. For now, the main ideas are:\n\nWe can measure relationships in data using things like correlation\nThe correlations we measure can be produced by numerous things, so they are hard to to interpret\nCorrelations can be produced by chance, so have the potential to be completely meaningless.\nHowever, we can create a model of exactly what chance can do. The model tells us whether chance is more or less likely to produce correlations of different sizes\nWe can use the chance model to help us make decisions about our own data. We can compare the correlation we found in our data to the model, then ask whether or not chance could have or was likely to have produced our results.\n\n\n\n\n\nSalsburg, David. 2001. The Lady Tasting Tea: How Statistics Revolutionized Science in the Twentieth Century. Macmillan."
  },
  {
    "objectID": "04-SamplesPopulations.html#how-are-probability-and-statistics-different",
    "href": "04-SamplesPopulations.html#how-are-probability-and-statistics-different",
    "title": "4  Probability, Sampling, and Estimation",
    "section": "",
    "text": "What are the chances of a fair coin coming up heads 10 times in a row?\nIf I roll two six sided dice, how likely is it that I’ll roll two sixes?\nHow likely is it that five cards drawn from a perfectly shuffled deck will all be hearts?\nWhat are the chances that I’ll win the lottery?\n\n\n\n\n\n\nIf my friend flips a coin 10 times and gets 10 heads, are they playing a trick on me?\nIf five cards off the top of the deck are all hearts, how likely is it that the deck was shuffled?\nIf the lottery commissioner’s spouse wins the lottery, how likely is it that the lottery was rigged?\n\n\nH H H H H H H H H H H",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Probability, Sampling, and Estimation</span>"
    ]
  },
  {
    "objectID": "04-SamplesPopulations.html#what-does-probability-mean",
    "href": "04-SamplesPopulations.html#what-does-probability-mean",
    "title": "4  Probability, Sampling, and Estimation",
    "section": "4.2 What does probability mean?",
    "text": "4.2 What does probability mean?\nLet’s start with the first of these questions. What is “probability”? It might seem surprising to you, but while statisticians and mathematicians (mostly) agree on what the rules of probability are, there’s much less of a consensus on what the word really means. It seems weird because we’re all very comfortable using words like “chance”, “likely”, “possible” and “probable”, and it doesn’t seem like it should be a very difficult question to answer. If you had to explain “probability” to a five year old, you could do a pretty good job. But if you’ve ever had that experience in real life, you might walk away from the conversation feeling like you didn’t quite get it right, and that (like many everyday concepts) it turns out that you don’t really know what it’s all about.\nSo I’ll have a go at it. Let’s suppose I want to bet on a soccer game between two teams of robots, Arduino Arsenal and C Milan. After thinking about it, I decide that there is an 80% probability of Arduino Arsenal winning. What do I mean by that? Here are three possibilities…\n\nThey’re robot teams, so I can make them play over and over again, and if I did that, Arduino Arsenal would win 8 out of every 10 games on average.\nFor any given game, I would only agree that betting on this game is only “fair” if a $1 bet on C Milan gives a $5 payoff (i.e. I get my $1 back plus a $4 reward for being correct), as would a $4 bet on Arduino Arsenal (i.e., my $4 bet plus a $1 reward).\nMy subjective “belief” or “confidence” in an Arduino Arsenal victory is four times as strong as my belief in a C Milan victory.\n\nEach of these seems sensible. However they’re not identical, and not every statistician would endorse all of them. The reason is that there are different statistical ideologies (yes, really!) and depending on which one you subscribe to, you might say that some of those statements are meaningless or irrelevant. In this section, I give a brief introduction the two main approaches that exist in the literature. These are by no means the only approaches, but they’re the two big ones.\n\n4.2.1 The frequentist view\nThe first of the two major approaches to probability, and the more dominant one in statistics, is referred to as the frequentist view, and it defines probability as a long-run frequency. Suppose we were to try flipping a fair coin, over and over again. By definition, this is a coin that has \\(P(H) = 0.5\\). What might we observe? One possibility is that the first 20 flips might look like this:\nT,H,H,H,H,T,T,H,H,H,H,T,H,H,T,T,T,T,T,H\nIn this case 11 of these 20 coin flips (55%) came up heads. Now suppose that I’d been keeping a running tally of the number of heads (which I’ll call \\(N_H\\)) that I’ve seen, across the first \\(N\\) flips, and calculate the proportion of heads \\(N_H / N\\) every time. Here’s what I’d get (I did literally flip coins to produce this!):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnumber of flips\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nnumber of heads\n0\n1\n2\n3\n4\n4\n4\n5\n6\n7\n\n\nproportion\n.00\n.50\n.67\n.75\n.80\n.67\n.57\n.63\n.67\n.70\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnumber of flips\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\nnumber of heads\n8\n8\n9\n10\n10\n10\n10\n10\n10\n11\n\n\nproportion\n.73\n.67\n.69\n.71\n.67\n.63\n.59\n.56\n.53\n.55\n\n\n\nNotice that at the start of the sequence, the proportion of heads fluctuates wildly, starting at .00 and rising as high as .80. Later on, one gets the impression that it dampens out a bit, with more and more of the values actually being pretty close to the “right” answer of .50. This is the frequentist definition of probability in a nutshell: flip a fair coin over and over again, and as \\(N\\) grows large (approaches infinity, denoted \\(N\\rightarrow \\infty\\)), the proportion of heads will converge to 50%. There are some subtle technicalities that the mathematicians care about, but qualitatively speaking, that’s how the frequentists define probability. Unfortunately, I don’t have an infinite number of coins, or the infinite patience required to flip a coin an infinite number of times. However, I do have a computer, and computers excel at mindless repetitive tasks. So I asked my computer to simulate flipping a coin 1000 times, and then drew a picture of what happens to the proportion \\(N_H / N\\) as \\(N\\) increases. Actually, I did it four times, just to make sure it wasn’t a fluke. The results are shown in Figure 4.1. As you can see, the proportion of observed heads eventually stops fluctuating, and settles down; when it does, the number at which it finally settles is the true probability of heads.\n\n\n\n\n\n\n\n\nFigure 4.1: An illustration of how frequentist probability works. If you flip a fair coin over and over again, the proportion of heads that you’ve seen eventually settles down, and converges to the true probability of 0.5. Each panel shows four different simulated experiments: in each case, we pretend we flipped a coin 1000 times, and kept track of the proportion of flips that were heads as we went along. Although none of these sequences actually ended up with an exact value of .5, if we’d extended the experiment for an infinite number of coin flips they would have.\n\n\n\n\n\nThe frequentist definition of probability has some desirable characteristics. First, it is objective: the probability of an event is necessarily grounded in the world. The only way that probability statements can make sense is if they refer to (a sequence of) events that occur in the physical universe. Second, it is unambiguous: any two people watching the same sequence of events unfold, trying to calculate the probability of an event, must inevitably come up with the same answer.\nHowever, it also has undesirable characteristics. Infinite sequences don’t exist in the physical world. Suppose you picked up a coin from your pocket and started to flip it. Every time it lands, it impacts on the ground. Each impact wears the coin down a bit; eventually, the coin will be destroyed. So, one might ask whether it really makes sense to pretend that an “infinite” sequence of coin flips is even a meaningful concept, or an objective one. We can’t say that an “infinite sequence” of events is a real thing in the physical universe, because the physical universe doesn’t allow infinite anything.\nMore seriously, the frequentist definition has a narrow scope. There are lots of things out there that human beings are happy to assign probability to in everyday language, but cannot (even in theory) be mapped onto a hypothetical sequence of events. For instance, if a meteorologist comes on TV and says, “the probability of rain in Adelaide on 2 November 2048 is 60%” we humans are happy to accept this. But it’s not clear how to define this in frequentist terms. There’s only one city of Adelaide, and only 2 November 2048. There’s no infinite sequence of events here, just a once-off thing. Frequentist probability genuinely forbids us from making probability statements about a single event. From the frequentist perspective, it will either rain tomorrow or it will not; there is no “probability” that attaches to a single non-repeatable event. Now, it should be said that there are some very clever tricks that frequentists can use to get around this. One possibility is that what the meteorologist means is something like this: “There is a category of days for which I predict a 60% chance of rain; if we look only across those days for which I make this prediction, then on 60% of those days it will actually rain”. It’s very weird and counter intuitive to think of it this way, but you do see frequentists do this sometimes.\n\n\n4.2.2 The Bayesian view\nThe Bayesian view of probability is often called the subjectivist view, and it is a minority view among statisticians, but one that has been steadily gaining traction for the last several decades. There are many flavors of Bayesianism, making hard to say exactly what “the” Bayesian view is. The most common way of thinking about subjective probability is to define the probability of an event as the degree of belief that an intelligent and rational agent assigns to that truth of that event. From that perspective, probabilities don’t exist in the world, but rather in the thoughts and assumptions of people and other intelligent beings. However, in order for this approach to work, we need some way of operationalising “degree of belief”. One way that you can do this is to formalize it in terms of “rational gambling”, though there are many other ways. Suppose that I believe that there’s a 60% probability of rain tomorrow. If someone offers me a bet: if it rains tomorrow, then I win $5, but if it doesn’t rain then I lose $5. Clearly, from my perspective, this is a pretty good bet. On the other hand, if I think that the probability of rain is only 40%, then it’s a bad bet to take. Thus, we can operationalize the notion of a “subjective probability” in terms of what bets I’m willing to accept.\nWhat are the advantages and disadvantages to the Bayesian approach? The main advantage is that it allows you to assign probabilities to any event you want to. You don’t need to be limited to those events that are repeatable. The main disadvantage (to many people) is that we can’t be purely objective – specifying a probability requires us to specify an entity that has the relevant degree of belief. This entity might be a human, an alien, a robot, or even a statistician, but there has to be an intelligent agent out there that believes in things. To many people this is uncomfortable: it seems to make probability arbitrary. While the Bayesian approach does require that the agent in question be rational (i.e., obey the rules of probability), it does allow everyone to have their own beliefs; I can believe the coin is fair and you don’t have to, even though we’re both rational. The frequentist view doesn’t allow any two observers to attribute different probabilities to the same event: when that happens, then at least one of them must be wrong. The Bayesian view does not prevent this from occurring. Two observers with different background knowledge can legitimately hold different beliefs about the same event. In short, where the frequentist view is sometimes considered to be too narrow (forbids lots of things that that we want to assign probabilities to), the Bayesian view is sometimes thought to be too broad (allows too many differences between observers).\n\n\n4.2.3 What’s the difference? And who is right?\nNow that you’ve seen each of these two views independently, it’s useful to make sure you can compare the two. Go back to the hypothetical robot soccer game at the start of the section. What do you think a frequentist and a Bayesian would say about these three statements? Which statement would a frequentist say is the correct definition of probability? Which one would a Bayesian do? Would some of these statements be meaningless to a frequentist or a Bayesian? If you’ve understood the two perspectives, you should have some sense of how to answer those questions.\nOkay, assuming you understand the different, you might be wondering which of them is right? Honestly, I don’t know that there is a right answer. As far as I can tell there’s nothing mathematically incorrect about the way frequentists think about sequences of events, and there’s nothing mathematically incorrect about the way that Bayesians define the beliefs of a rational agent. In fact, when you dig down into the details, Bayesians and frequentists actually agree about a lot of things. Many frequentist methods lead to decisions that Bayesians agree a rational agent would make. Many Bayesian methods have very good frequentist properties.\nFor the most part, I’m a pragmatist so I’ll use any statistical method that I trust. As it turns out, that makes me prefer Bayesian methods, for reasons I’ll explain towards the end of the book, but I’m not fundamentally opposed to frequentist methods. Not everyone is quite so relaxed. For instance, consider Sir Ronald Fisher, one of the towering figures of 20th century statistics and a vehement opponent to all things Bayesian, whose paper on the mathematical foundations of statistics referred to Bayesian probability as “an impenetrable jungle [that] arrests progress towards precision of statistical concepts” Fisher (1922, 311). Or the psychologist Paul Meehl, who suggests that relying on frequentist methods could turn you into “a potent but sterile intellectual rake who leaves in his merry path a long train of ravished maidens but no viable scientific offspring” Meehl (1967, 114). The history of statistics, as you might gather, is not devoid of entertainment.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Probability, Sampling, and Estimation</span>"
    ]
  },
  {
    "objectID": "04-SamplesPopulations.html#basic-probability-theory",
    "href": "04-SamplesPopulations.html#basic-probability-theory",
    "title": "4  Probability, Sampling, and Estimation",
    "section": "4.3 Basic probability theory",
    "text": "4.3 Basic probability theory\nIdeological arguments between Bayesians and frequentists notwithstanding, it turns out that people mostly agree on the rules that probabilities should obey. There are lots of different ways of arriving at these rules. The most commonly used approach is based on the work of Andrey Kolmogorov, one of the great Soviet mathematicians of the 20th century. I won’t go into a lot of detail, but I’ll try to give you a bit of a sense of how it works. And in order to do so, I’m going to have to talk about my pants.\n\n4.3.1 Introducing probability distributions\nOne of the disturbing truths about my life is that I only own 5 pairs of pants: three pairs of jeans, the bottom half of a suit, and a pair of tracksuit pants. Even sadder, I’ve given them names: I call them \\(X_1\\), \\(X_2\\), \\(X_3\\), \\(X_4\\) and \\(X_5\\). I really do: that’s why they call me Mister Imaginative. Now, on any given day, I pick out exactly one of pair of pants to wear. Not even I’m so stupid as to try to wear two pairs of pants, and thanks to years of training I never go outside without wearing pants anymore. If I were to describe this situation using the language of probability theory, I would refer to each pair of pants (i.e., each \\(X\\)) as an elementary event. The key characteristic of elementary events is that every time we make an observation (e.g., every time I put on a pair of pants), then the outcome will be one and only one of these events. Like I said, these days I always wear exactly one pair of pants, so my pants satisfy this constraint. Similarly, the set of all possible events is called a sample space. Granted, some people would call it a “wardrobe”, but that’s because they’re refusing to think about my pants in probabilistic terms. Sad.\nOkay, now that we have a sample space (a wardrobe), which is built from lots of possible elementary events (pants), what we want to do is assign a probability of one of these elementary events. For an event \\(X\\), the probability of that event \\(P(X)\\) is a number that lies between 0 and 1. The bigger the value of \\(P(X)\\), the more likely the event is to occur. So, for example, if \\(P(X) = 0\\), it means the event \\(X\\) is impossible (i.e., I never wear those pants). On the other hand, if \\(P(X) = 1\\) it means that event \\(X\\) is certain to occur (i.e., I always wear those pants). For probability values in the middle, it means that I sometimes wear those pants. For instance, if \\(P(X) = 0.5\\) it means that I wear those pants half of the time.\nAt this point, we’re almost done. The last thing we need to recognize is that “something always happens”. Every time I put on pants, I really do end up wearing pants (crazy, right?). What this somewhat trite statement means, in probabilistic terms, is that the probabilities of the elementary events need to add up to 1. This is known as the law of total probability, not that any of us really care. More importantly, if these requirements are satisfied, then what we have is a probability distribution. For example, this is an example of a probability distribution\n\n\n\nWhich pants?\nLabel\nProbability\n\n\n\n\nBlue jeans\n\\(X_1\\)\n\\(P(X_1) = .5\\)\n\n\nGrey jeans\n\\(X_2\\)\n\\(P(X_2) = .3\\)\n\n\nBlack jeans\n\\(X_3\\)\n\\(P(X_3) = .1\\)\n\n\nBlack suit\n\\(X_4\\)\n\\(P(X_4) = 0\\)\n\n\nBlue tracksuit\n\\(X_5\\)\n\\(P(X_5) = .1\\)\n\n\n\nEach of the events has a probability that lies between 0 and 1, and if we add up the probability of all events, they sum to 1. Awesome. We can even draw a nice bar graph to visualize this distribution, as shown in Figure 4.2. And at this point, we’ve all achieved something. You’ve learned what a probability distribution is, and I’ve finally managed to find a way to create a graph that focuses entirely on my pants. Everyone wins!\n\n\n\n\n\n\n\n\nFigure 4.2: A visual depiction of the pants probability distribution. There are five elementary events, corresponding to the five pairs of pants that I own. Each event has some probability of occurring: this probability is a number between 0 to 1. The sum of these probabilities is 1.\n\n\n\n\n\nThe only other thing that I need to point out is that probability theory allows you to talk about non elementary events as well as elementary ones. The easiest way to illustrate the concept is with an example. In the pants example, it’s perfectly legitimate to refer to the probability that I wear jeans. In this scenario, the “Dan wears jeans” event said to have happened as long as the elementary event that actually did occur is one of the appropriate ones; in this case “blue jeans”, “black jeans” or “grey jeans”. In mathematical terms, we defined the “jeans” event \\(E\\) to correspond to the set of elementary events \\((X_1, X_2, X_3)\\). If any of these elementary events occurs, then \\(E\\) is also said to have occurred. Having decided to write down the definition of the \\(E\\) this way, it’s pretty straightforward to state what the probability \\(P(E)\\) is: we just add everything up. In this particular case \\[P(E) = P(X_1) + P(X_2) + P(X_3)\\] and, since the probabilities of blue, grey and black jeans respectively are .5, .3 and .1, the probability that I wear jeans is equal to .9.\nAt this point you might be thinking that this is all terribly obvious and simple and you’d be right. All we’ve really done is wrap some basic mathematics around a few common sense intuitions. However, from these simple beginnings it’s possible to construct some extremely powerful mathematical tools. I’m definitely not going to go into the details in this book, but what I will do is list some of the other rules that probabilities satisfy. These rules can be derived from the simple assumptions that I’ve outlined above, but since we don’t actually use these rules for anything in this book, I won’t do so here.\n\nSome basic rules that probabilities must satisfy. You don’t really need to know these rules in order to understand the analyses that we’ll talk about later in the book, but they are important if you want to understand probability theory a bit more deeply.\n\n\nEnglish\nNotation\n\nFormula\n\n\n\n\nnot \\(A\\)\n\\(P(\\neg A)\\)\n\\(=\\)\n\\(1-P(A)\\)\n\n\n\\(A\\) or \\(B\\)\n\\(P(A \\cup B)\\)\n\\(=\\)\n\\(P(A) + P(B) - P(A \\cap B)\\)\n\n\n\\(A\\) and \\(B\\)\n\\(P(A \\cap B)\\)\n\\(=\\)\n\\(P(A|B) P(B)\\)\n\n\n\nNow that we have the ability to “define” non-elementary events in terms of elementary ones, we can actually use this to construct (or, if you want to be all mathematicallish, “derive”) some of the other rules of probability. These rules are listed above, and while I’m pretty confident that very few of my readers actually care about how these rules are constructed, I’m going to show you anyway: even though it’s boring and you’ll probably never have a lot of use for these derivations, if you read through it once or twice and try to see how it works, you’ll find that probability starts to feel a bit less mysterious, and with any luck a lot less daunting. So here goes. Firstly, in order to construct the rules I’m going to need a sample space \\(X\\) that consists of a bunch of elementary events \\(x\\), and two non-elementary events, which I’ll call \\(A\\) and \\(B\\). Let’s say: \\[\\begin{array}{rcl}\nX &=& (x_1, x_2, x_3, x_4, x_5) \\\\\nA &=& (x_1, x_2, x_3) \\\\\nB &=& (x_3, x_4)\n\\end{array}\\] To make this a bit more concrete, let’s suppose that we’re still talking about the pants distribution. If so, \\(A\\) corresponds to the event “jeans”, and \\(B\\) corresponds to the event “black”: \\[\\begin{array}{rcl}\n\\mbox{``jeans''} &=& (\\mbox{``blue jeans''}, \\mbox{``grey jeans''}, \\mbox{``black jeans''}) \\\\\n\\mbox{``black''} &=& (\\mbox{``black jeans''}, \\mbox{``black suit''})\n\\end{array}\\] So now let’s start checking the rules that I’ve listed in the table.\nIn the first line, the table says that \\[P(\\neg A) = 1- P(A)\\] and what it means is that the probability of “not \\(A\\)” is equal to 1 minus the probability of \\(A\\). A moment’s thought (and a tedious example) make it obvious why this must be true. If \\(A\\) corresponds to the even that I wear jeans (i.e., one of \\(x_1\\) or \\(x_2\\) or \\(x_3\\) happens), then the only meaningful definition of “not \\(A\\)” (which is mathematically denoted as \\(\\neg A\\)) is to say that \\(\\neg A\\) consists of all elementary events that don’t belong to \\(A\\). In the case of the pants distribution it means that \\(\\neg A = (x_4, x_5)\\), or, to say it in English: “not jeans” consists of all pairs of pants that aren’t jeans (i.e., the black suit and the blue tracksuit). Consequently, every single elementary event belongs to either \\(A\\) or \\(\\neg A\\), but not both. Okay, so now let’s rearrange our statement above: \\[P(\\neg A) + P(A) = 1\\] which is a trite way of saying either I do wear jeans or I don’t wear jeans: the probability of “not jeans” plus the probability of “jeans” is 1. Mathematically: \\[\\begin{array}{rcl}\nP(\\neg A) &=&  P(x_4) + P(x_5) \\\\\nP(A) &=& P(x_1) + P(x_2) + P(x_3)\n\\end{array}\\] so therefore \\[\\begin{array}{rcl}\nP(\\neg A) + P(A) &=& P(x_1) + P(x_2) + P(x_3) + P(x_4) + P(x_5) \\\\\n&=& \\sum_{x \\in X} P(x) \\\\\n&=& 1\n\\end{array}\\] Excellent. It all seems to work.\nWow, I can hear you saying. That’s a lot of \\(x\\)s to tell me the freaking obvious. And you’re right: this is freaking obvious. The whole point of probability theory to to formalize and mathematize a few very basic common sense intuitions. So let’s carry this line of thought forward a bit further. In the last section I defined an event corresponding to not A, which I denoted \\(\\neg A\\). Let’s now define two new events that correspond to important everyday concepts: \\(A\\) and \\(B\\), and \\(A\\) or \\(B\\). To be precise:\n\n\n\nEnglish statement:\nMathematical notation:\n\n\n\n\n“\\(A\\) and \\(B\\)” both happen\n\\(A \\cap B\\)\n\n\nat least one of “\\(A\\) or \\(B\\)” happens\n\\(A \\cup B\\)\n\n\n\nSince \\(A\\) and \\(B\\) are both defined in terms of our elementary events (the \\(x\\)s) we’re going to need to try to describe \\(A \\cap B\\) and \\(A \\cup B\\) in terms of our elementary events too. Can we do this? Yes we can The only way that both \\(A\\) and \\(B\\) can occur is if the elementary event that we observe turns out to belong to both \\(A\\) and \\(B\\). Thus “\\(A \\cap B\\)” includes only those elementary events that belong to both \\(A\\) and \\(B\\)… \\[\\begin{array}{rcl}\nA &=& (x_1, x_2, x_3) \\\\\nB &=& (x_3, x_4) \\\\\nA \\cap B & = & (x_3)\n\\end{array}\\] So, um, the only way that I can wear “jeans” \\((x_1, x_2, x_3)\\) and “black pants” \\((x_3, x_4)\\) is if I wear “black jeans” \\((x_3)\\). Another victory for the bloody obvious.\nAt this point, you’re not going to be at all shocked by the definition of \\(A \\cup B\\), though you’re probably going to be extremely bored by it. The only way that I can wear “jeans” or “black pants” is if the elementary pants that I actually do wear belongs to \\(A\\) or to \\(B\\), or to both. So… \\[\\begin{array}{rcl}\nA &=& (x_1, x_2, x_3) \\\\\nB &=& (x_3, x_4) \\\\\nA \\cup B & = & (x_1, x_2, x_3, x_4)\n\\end{array}\\] Oh yeah baby. Mathematics at its finest.\nSo, we’ve defined what we mean by \\(A \\cap B\\) and \\(A \\cup B\\). Now let’s assign probabilities to these events. More specifically, let’s start by verifying the rule that claims that: \\[P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\] Using our definitions earlier, we know that \\(A \\cup B = (x_1, x_2, x_3, x_4)\\), so \\[P(A \\cup B) = P(x_1) + P(x_2) + P(x_3) + P(x_4)\\] and making similar use of the fact that we know what elementary events belong to \\(A\\), \\(B\\) and \\(A \\cap B\\)…. \\[\\begin{array}{rcl}\nP(A)  &=&   P(x_1) + P(x_2) + P(x_3)    \\\\\nP(B) &=&  P(x_3) + P(x_4)  \\\\\nP(A \\cap B) &=&  P(x_3)\n\\end{array}\\] and therefore \\[\\begin{array}{rcl}\nP(A) + P(B) - P(A \\cap B)\n&=&  P(x_1) + P(x_2) + P(x_3) +  P(x_3) + P(x_4) -  P(x_3) \\\\\n&=& P(x_1) + P(x_2) + P(x_3) + P(x_4) \\\\\n&=& P(A \\cup B)\n\\end{array}\\] Done.\nThe next concept we need to define is the notion of “\\(B\\) given \\(A\\)”, which is typically written \\(B | A\\). Here’s what I mean: suppose that I get up one morning, and put on a pair of pants. An elementary event \\(x\\) has occurred. Suppose further I yell out to my wife (who is in the other room, and so cannot see my pants) “I’m wearing jeans today!”. Assuming that she believes that I’m telling the truth, she knows that \\(A\\) is true. Given that she knows that \\(A\\) has happened, what is the conditional probability that \\(B\\) is also true? Well, let’s think about what she knows. Here are the facts:\n\nThe non-jeans events are impossible. If \\(A\\) is true, then we know that the only possible elementary events that could have occurred are \\(x_1\\), \\(x_2\\) and \\(x_3\\) (i.e.,the jeans). The non-jeans events \\(x_4\\) and \\(x_5\\) are now impossible, and must be assigned probability zero. In other words, our sample space has been restricted to the jeans events. But it’s still the case that the probabilities of these these events must sum to 1: we know for sure that I’m wearing jeans.\nShe’s learned nothing about which jeans I’m wearing. Before I made my announcement that I was wearing jeans, she already knew that I was five times as likely to be wearing blue jeans (\\(P(x_1) = 0.5\\)) than to be wearing black jeans (\\(P(x_3) = 0.1\\)). My announcement doesn’t change this… I said nothing about what color my jeans were, so it must remain the case that \\(P(x_1) / P(x_3)\\) stays the same, at a value of 5.\n\nThere’s only one way to satisfy these constraints: set the impossible events to have zero probability (i.e., \\(P(x | A) = 0\\) if \\(x\\) is not in \\(A\\)), and then divide the probabilities of all the others by \\(P(A)\\). In this case, since \\(P(A) = 0.9\\), we divide by 0.9. This gives:\n\n\n\n\n\n\n\n\n\nwhich pants?\nelementary event\nold prob, \\(P(x)\\)\nnew prob, \\(P(x | A)\\)\n\n\n\n\nblue jeans\n\\(x_1\\)\n0.5\n0.556\n\n\ngrey jeans\n\\(x_2\\)\n0.3\n0.333\n\n\nblack jeans\n\\(x_3\\)\n0.1\n0.111\n\n\nblack suit\n\\(x_4\\)\n0\n0\n\n\nblue tracksuit\n\\(x_5\\)\n0.1\n0\n\n\n\nIn mathematical terms, we say that \\[P(x | A) = \\frac{P(x)}{P(A)}\\] if \\(x \\in A\\), and \\(P(x|A) = 0\\) otherwise. And therefore… \\[\\begin{array}{rcl}\nP(B | A) &=& P(x_3 | A)  + P(x_4 | A) \\\\ \\\\\n&=&  \\displaystyle\\frac{P(x_3)}{P(A)} + 0    \\\\ \\\\\n&=& \\displaystyle\\frac{P(x_3)}{P(A)}\n\\end{array}\\] Now, recalling that \\(A \\cap B = (x_3)\\), we can write this as \\[P(B | A) = \\frac{P(A \\cap B)}{P(A)}\\] and if we multiply both sides by \\(P(A)\\) we obtain: \\[P(A \\cap B) = P(B| A) P(A)\\] which is the third rule that we had listed in the table.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Probability, Sampling, and Estimation</span>"
    ]
  },
  {
    "objectID": "04-SamplesPopulations.html#the-binomial-distribution",
    "href": "04-SamplesPopulations.html#the-binomial-distribution",
    "title": "4  Probability, Sampling, and Estimation",
    "section": "4.4 The binomial distribution",
    "text": "4.4 The binomial distribution\nAs you might imagine, probability distributions vary enormously, and there’s an enormous range of distributions out there. However, they aren’t all equally important. In fact, the vast majority of the content in this book relies on one of five distributions: the binomial distribution, the normal distribution, the \\(t\\) distribution, the \\(\\chi^2\\) (“chi-square”) distribution and the \\(F\\) distribution. Given this, what I’ll do over the next few sections is provide a brief introduction to all five of these, paying special attention to the binomial and the normal. I’ll start with the binomial distribution, since it’s the simplest of the five.\n\n4.4.1 Introducing the binomial\nThe theory of probability originated in the attempt to describe how games of chance work, so it seems fitting that our discussion of the binomial distribution should involve a discussion of rolling dice and flipping coins. Let’s imagine a simple “experiment”: in my hot little hand I’m holding 20 identical six-sided dice. On one face of each die there’s a picture of a skull; the other five faces are all blank. If I proceed to roll all 20 dice, what’s the probability that I’ll get exactly 4 skulls? Assuming that the dice are fair, we know that the chance of any one die coming up skulls is 1 in 6; to say this another way, the skull probability for a single die is approximately \\(.167\\). This is enough information to answer our question, so let’s have a look at how it’s done.\nAs usual, we’ll want to introduce some names and some notation. We’ll let \\(N\\) denote the number of dice rolls in our experiment; which is often referred to as the size parameter of our binomial distribution. We’ll also use \\(\\theta\\) to refer to the the probability that a single die comes up skulls, a quantity that is usually called the success probability of the binomial. Finally, we’ll use \\(X\\) to refer to the results of our experiment, namely the number of skulls I get when I roll the dice. Since the actual value of \\(X\\) is due to chance, we refer to it as a random variable. In any case, now that we have all this terminology and notation, we can use it to state the problem a little more precisely. The quantity that we want to calculate is the probability that \\(X = 4\\) given that we know that \\(\\theta = .167\\) and \\(N=20\\). The general “form” of the thing I’m interested in calculating could be written as \\[P(X \\ | \\ \\theta, N)\\] and we’re interested in the special case where \\(X=4\\), \\(\\theta = .167\\) and \\(N=20\\). There’s only one more piece of notation I want to refer to before moving on to discuss the solution to the problem. If I want to say that \\(X\\) is generated randomly from a binomial distribution with parameters \\(\\theta\\) and \\(N\\), the notation I would use is as follows: \\[X \\sim \\mbox{Binomial}(\\theta, N)\\]\nYeah, yeah. I know what you’re thinking: notation, notation, notation. Really, who cares? Very few readers of this book are here for the notation, so I should probably move on and talk about how to use the binomial distribution. To that end, Figure Figure 4.3 plots the binomial probabilities for all possible values of \\(X\\) for our dice rolling experiment, from \\(X=0\\) (no skulls) all the way up to \\(X=20\\) (all skulls). Note that this is basically a bar chart, and is no different to the “pants probability” plot I drew in Figure 4.2. On the horizontal axis we have all the possible events, and on the vertical axis we can read off the probability of each of those events. So, the probability of rolling 4 skulls out of 20 times is about 0.20 (the actual answer is 0.2022036, as we’ll see in a moment). In other words, you’d expect that to happen about 20% of the times you repeated this experiment.\n\n\n\n\n\n\n\n\nFigure 4.3: The binomial distribution with size parameter of N =20 and an underlying success probability of 1/6. Each vertical bar depicts the probability of one specific outcome (i.e., one possible value of X). Because this is a probability distribution, each of the probabilities must be a number between 0 and 1, and the heights of the bars must sum to 1 as well.\n\n\n\n\n\n\n\n4.4.2 Working with the binomial distribution in R\nR has a function called dbinom that calculates binomial probabilities for us. The main arguments to the function are\n\nx This is a number, or vector of numbers, specifying the outcomes whose probability you’re trying to calculate.\nsize This is a number telling R the size of the experiment.\nprob This is the success probability for any one trial in the experiment.\n\nSo, in order to calculate the probability of getting skulls, from an experiment of trials, in which the probability of getting a skull on any one trial is … well, the command I would use is simply this:\n\ndbinom( x = 4, size = 20, prob = 1/6 )\n#&gt; [1] 0.2022036\n\nTo give you a feel for how the binomial distribution changes when we alter the values of \\(\\theta\\) and \\(N\\), let’s suppose that instead of rolling dice, I’m actually flipping coins. This time around, my experiment involves flipping a fair coin repeatedly, and the outcome that I’m interested in is the number of heads that I observe. In this scenario, the success probability is now \\(\\theta = 1/2\\). Suppose I were to flip the coin \\(N=20\\) times. In this example, I’ve changed the success probability, but kept the size of the experiment the same. What does this do to our binomial distribution?\n\n\n\n\n\n\n\n\nFigure 4.4: Two binomial distributions, involving a scenario in which I’m flipping a fair coin, so the underlying success probability is 1/2. In panel (a), we assume I’m flipping the coin N = 20 times. In panel (b) we assume that the coin is flipped N = 100 times.\n\n\n\n\n\nWell, as Figure 4.4 \\(a\\) shows, the main effect of this is to shift the whole distribution, as you’d expect. Okay, what if we flipped a coin \\(N=100\\) times? Well, in that case, we get Figure 4.4 \\(b\\). The distribution stays roughly in the middle, but there’s a bit more variability in the possible outcomes.\nAt this point, I should probably explain the name of the dbinom function. Obviously, the “binom” part comes from the fact that we’re working with the binomial distribution, but the “d” prefix is probably a bit of a mystery. In this section I’ll give a partial explanation: specifically, I’ll explain why there is a prefix. As for why it’s a “d” specifically, you’ll have to wait until the next section. What’s going on here is that R actually provides four functions in relation to the binomial distribution. These four functions are dbinom, pbinom, rbinom and qbinom, and each one calculates a different quantity of interest. Not only that, R does the same thing for every probability distribution that it implements. No matter what distribution you’re talking about, there’s a d function, a p function, r a function and a q function.\nLet’s have a look at what all four functions do. Firstly, all four versions of the function require you to specify the size and prob arguments: no matter what you’re trying to get R to calculate, it needs to know what the parameters are. However, they differ in terms of what the other argument is, and what the output is. So let’s look at them one at a time.\n\nThe d form we’ve already seen: you specify a particular outcome x, and the output is the probability of obtaining exactly that outcome. (the “d” is short for density, but ignore that for now).\nThe p form calculates the cumulative probability. You specify a particular quantile q , and it tells you the probability of obtaining an outcome smaller than or equal to q.\nThe q form calculates the quantiles of the distribution. You specify a probability value p, and it gives you the corresponding percentile. That is, the value of the variable for which there’s a probability p of obtaining an outcome lower than that value.\nThe r form is a random number generator: specifically, it generates n random outcomes from the distribution.\n\nThis is a little abstract, so let’s look at some concrete examples. Again, we’ve already covered dbinom so let’s focus on the other three versions. We’ll start with pbinom, and we’ll go back to the skull-dice example. Again, I’m rolling 20 dice, and each die has a 1 in 6 chance of coming up skulls. Suppose, however, that I want to know the probability of rolling 4 or fewer skulls. If I wanted to, I could use the dbinom function to calculate the exact probability of rolling 0 skulls, 1 skull, 2 skulls, 3 skulls and 4 skulls and then add these up, but there’s a faster way. Instead, I can calculate this using the pbinom function. Here’s the command:\n\npbinom( q= 4, size = 20, prob = 1/6)\n#&gt; [1] 0.7687492\n\nIn other words, there is a 76.9% chance that I will roll 4 or fewer skulls. Or, to put it another way, R is telling us that a value of 4 is actually the 76.9th percentile of this binomial distribution.\nNext, let’s consider the qbinom function. Let’s say I want to calculate the 75th percentile of the binomial distribution. If we’re sticking with our skulls example, I would use the following command to do this:\n\nqbinom( p = 0.75, size = 20, prob = 1/6 )\n#&gt; [1] 4\n\nHm. There’s something odd going on here. Let’s think this through. What the qbinom function appears to be telling us is that the 75th percentile of the binomial distribution is 4, even though we saw from the function that 4 is actually the 76.9th percentile. And it’s definitely the pbinom function that is correct. I promise. The weirdness here comes from the fact that our binomial distribution doesn’t really have a 75th percentile. Not really. Why not? Well, there’s a 56.7% chance of rolling 3 or fewer skulls (you can type pbinom(3, 20, 1/6) to confirm this if you want), and a 76.9% chance of rolling 4 or fewer skulls. So there’s a sense in which the 75th percentile should lie “in between” 3 and 4 skulls. But that makes no sense at all! You can’t roll 20 dice and get 3.9 of them come up skulls. This issue can be handled in different ways: you could report an in between value (or interpolated value, to use the technical name) like 3.9, you could round down (to 3) or you could round up (to 4).\nThe qbinom function rounds upwards: if you ask for a percentile that doesn’t actually exist (like the 75th in this example), R finds the smallest value for which the the percentile rank is at least what you asked for. In this case, since the “true” 75th percentile (whatever that would mean) lies somewhere between 3 and 4 skulls, R Rounds up and gives you an answer of 4. This subtlety is tedious, I admit, but thankfully it’s only an issue for discrete distributions like the binomial. The other distributions that I’ll talk about (normal, \\(t\\), \\(\\chi^2\\) and \\(F\\)) are all continuous, and so R can always return an exact quantile whenever you ask for it.\nFinally, we have the random number generator. To use the rbinom function, you specify how many times R should “simulate” the experiment using the n argument, and it will generate random outcomes from the binomial distribution. So, for instance, suppose I were to repeat my die rolling experiment 100 times. I could get R to simulate the results of these experiments by using the following command:\n\nrbinom( n = 100, size = 20, prob = 1/6 )\n#&gt;   [1] 2 4 5 3 3 4 6 2 2 4 6 3 4 2 2 5 5 5 4 1 3 6 6 1 2 1 1 7 3 6 4 5 5 2 2 5 4\n#&gt;  [38] 4 1 4 3 1 1 4 4 4 1 1 4 2 4 4 3 6 1 5 4 5 2 5 3 4 5 4 4 3 4 5 1 3 4 3 5 3\n#&gt;  [75] 7 3 5 8 5 2 4 3 6 4 4 6 0 2 3 5 1 1 2 4 3 6 1 3 3 4\n\nAs you can see, these numbers are pretty much what you’d expect given the distribution shown in Figure 4.3 . Most of the time I roll somewhere between 1 to 5 skulls. There are a lot of subtleties associated with random number generation using a computer, but for the purposes of this book we don’t need to worry too much about them.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Probability, Sampling, and Estimation</span>"
    ]
  },
  {
    "objectID": "04-SamplesPopulations.html#the-normal-distribution",
    "href": "04-SamplesPopulations.html#the-normal-distribution",
    "title": "4  Probability, Sampling, and Estimation",
    "section": "4.5 The normal distribution",
    "text": "4.5 The normal distribution\nWhile the binomial distribution is conceptually the simplest distribution to understand, it’s not the most important one. That particular honor goes to the normal distribution, which is also referred to as “the bell curve” or a “Gaussian distribution”.\n\n\n\n\n\n\n\n\nFigure 4.5: The normal distribution with mean = 0 and standard deviation = 1. The x-axis corresponds to the value of some variable, and the y-axis tells us something about how likely we are to observe that value. However, notice that the y-axis is labelled Probability Density and not Probability. There is a subtle and somewhat frustrating characteristic of continuous distributions that makes the y axis behave a bit oddly: the height of the curve here isn’t actually the probability of observing a particular x value. On the other hand, it is true that the heights of the curve tells you which x values are more likely (the higher ones!).\n\n\n\n\n\nA normal distribution is described using two parameters, the mean of the distribution \\(\\mu\\) and the standard deviation of the distribution \\(\\sigma\\). The notation that we sometimes use to say that a variable \\(X\\) is normally distributed is as follows: \\[X \\sim \\mbox{Normal}(\\mu,\\sigma)\\] Of course, that’s just notation. It doesn’t tell us anything interesting about the normal distribution itself. The mathematical formula for the normal distribution is:\n\n\n\n\n\nFormula for the normal distribution\n\n\n\n\nThe formula is important enough that everyone who learns statistics should at least look at it, but since this is an introductory text I don’t want to focus on it to much. Instead, we look at how R can be used to work with normal distributions. The R functions for the normal distribution are dnorm(), pnorm(), qnorm() and rnorm(). However, they behave in pretty much exactly the same way as the corresponding functions for the binomial distribution, so there’s not a lot that you need to know. The only thing that I should point out is that the argument names for the parameters are mean and sd. In pretty much every other respect, there’s nothing else to add.\nInstead of focusing on the maths, let’s try to get a sense for what it means for a variable to be normally distributed. To that end, have a look at Figure 4.5, which plots a normal distribution with mean \\(\\mu = 0\\) and standard deviation \\(\\sigma = 1\\). You can see where the name “bell curve” comes from: it looks a bit like a bell. Notice that, unlike the plots that I drew to illustrate the binomial distribution, the picture of the normal distribution in Figure Figure 4.5 shows a smooth curve instead of “histogram-like” bars. This isn’t an arbitrary choice: the normal distribution is continuous, whereas the binomial is discrete. For instance, in the die rolling example from the last section, it was possible to get 3 skulls or 4 skulls, but impossible to get 3.9 skulls.\nWith this in mind, let’s see if we can get an intuition for how the normal distribution works. First, let’s have a look at what happens when we play around with the parameters of the distribution. One parameter we can change is the mean. This will shift the distribution to the right or left. The animation in Figure 4.6 shows a normal distribution with mean = 0, moving up and down from mean = 0 to mean = 5. Note, when you change the mean the whole shape of the distribution does not change, it just shifts from left to right. In the animation the normal distribution bounces up and down a little, but that’s just a quirk of the animation (plus it looks fun that way).\n\n\n\n\n\n\n\n\nFigure 4.6: A normal distribution with a moving mean\n\n\n\n\n\nIn contrast, if we increase the standard deviation while keeping the mean constant, the peak of the distribution stays in the same place, but the distribution gets wider. The animation in Figure 4.7 shows what happens when you start with a small standard deviation (sd = 0.5), and move to larger and larger standard deviation (up to sd = 5). As you can see, the distribution spreads out and becomes wider as the standard deviation increases.\n\n\n\n\n\n\n\n\nFigure 4.7: A normal distribution with a shifting sd.\n\n\n\n\n\nNotice that when we widen the distribution the height of the peak shrinks. This has to happen: in the same way that the heights of the bars that we used to draw a discrete binomial distribution have to sum to 1, the total area under the curve for the normal distribution must equal 1. Before moving on, I want to point out one important characteristic of the normal distribution. Irrespective of what the actual mean and standard deviation are, 68.3% of the area falls within 1 standard deviation of the mean. Similarly, 95.4% of the distribution falls within 2 standard deviations of the mean, and 99.7% of the distribution is within 3 standard deviations.\n\n4.5.1 Probability density\nThere’s something I’ve been trying to hide throughout my discussion of the normal distribution, something that some introductory textbooks omit completely. They might be right to do so: this “thing” that I’m hiding is weird and counter intuitive even by the admittedly distorted standards that apply in statistics. Fortunately, it’s not something that you need to understand at a deep level in order to do basic statistics: rather, it’s something that starts to become important later on when you move beyond the basics. So, if it doesn’t make complete sense, don’t worry: try to make sure that you follow the gist of it.\nThroughout my discussion of the normal distribution, there’s been one or two things that don’t quite make sense. Perhaps you noticed that the \\(y\\)-axis in these figures is labelled “Probability Density” rather than density. Maybe you noticed that I used \\(p(X)\\) instead of \\(P(X)\\) when giving the formula for the normal distribution. Maybe you’re wondering why R uses the “d” prefix for functions like dnorm(). And maybe, just maybe, you’ve been playing around with the dnorm() function, and you accidentally typed in a command like this:\n\ndnorm( x = 1, mean = 1, sd = 0.1 )\n#&gt; [1] 3.989423\n\nAnd if you’ve done the last part, you’re probably very confused. I’ve asked R to calculate the probability that x = 1, for a normally distributed variable with mean = 1 and standard deviation sd = 0.1; and it tells me that the probability is 3.99. But, as we discussed earlier, probabilities can’t be larger than 1. So either I’ve made a mistake, or that’s not a probability.\nAs it turns out, the second answer is correct. What we’ve calculated here isn’t actually a probability: it’s something else. To understand what that something is, you have to spend a little time thinking about what it really means to say that \\(X\\) is a continuous variable. Let’s say we’re talking about the temperature outside. The thermometer tells me it’s 23 degrees, but I know that’s not really true. It’s not exactly 23 degrees. Maybe it’s 23.1 degrees, I think to myself. But I know that that’s not really true either, because it might actually be 23.09 degrees. But, I know that… well, you get the idea. The tricky thing with genuinely continuous quantities is that you never really know exactly what they are.\nNow think about what this implies when we talk about probabilities. Suppose that tomorrow’s maximum temperature is sampled from a normal distribution with mean 23 and standard deviation 1. What’s the probability that the temperature will be exactly 23 degrees? The answer is “zero”, or possibly, “a number so close to zero that it might as well be zero”. Why is this?\nIt’s like trying to throw a dart at an infinitely small dart board: no matter how good your aim, you’ll never hit it. In real life you’ll never get a value of exactly 23. It’ll always be something like 23.1 or 22.99998 or something. In other words, it’s completely meaningless to talk about the probability that the temperature is exactly 23 degrees. However, in everyday language, if I told you that it was 23 degrees outside and it turned out to be 22.9998 degrees, you probably wouldn’t call me a liar. Because in everyday language, “23 degrees” usually means something like “somewhere between 22.5 and 23.5 degrees”. And while it doesn’t feel very meaningful to ask about the probability that the temperature is exactly 23 degrees, it does seem sensible to ask about the probability that the temperature lies between 22.5 and 23.5, or between 20 and 30, or any other range of temperatures.\nThe point of this discussion is to make clear that, when we’re talking about continuous distributions, it’s not meaningful to talk about the probability of a specific value. However, what we can talk about is the probability that the value lies within a particular range of values. To find out the probability associated with a particular range, what you need to do is calculate the “area under the curve”.\nOkay, so that explains part of the story. I’ve explained a little bit about how continuous probability distributions should be interpreted (i.e., area under the curve is the key thing), but I haven’t actually explained what the dnorm() function actually calculates. Equivalently, what does the formula for \\(p(x)\\) that I described earlier actually mean? Obviously, \\(p(x)\\) doesn’t describe a probability, but what is it? The name for this quantity \\(p(x)\\) is a probability density, and in terms of the plots we’ve been drawing, it corresponds to the height of the curve. The densities themselves aren’t meaningful in and of themselves: but they’re “rigged” to ensure that the area under the curve is always interpretable as genuine probabilities. To be honest, that’s about as much as you really need to know for now.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Probability, Sampling, and Estimation</span>"
    ]
  },
  {
    "objectID": "04-SamplesPopulations.html#other-useful-distributions",
    "href": "04-SamplesPopulations.html#other-useful-distributions",
    "title": "4  Probability, Sampling, and Estimation",
    "section": "4.6 Other useful distributions",
    "text": "4.6 Other useful distributions\nThere are many other useful distributions, these include the t distribution, the F distribution, and the chi squared distribution. We will soon discover more about the t and F distributions when we discuss t-tests and ANOVAs in later chapters.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Probability, Sampling, and Estimation</span>"
    ]
  },
  {
    "objectID": "04-SamplesPopulations.html#summary-of-probability",
    "href": "04-SamplesPopulations.html#summary-of-probability",
    "title": "4  Probability, Sampling, and Estimation",
    "section": "4.7 Summary of Probability",
    "text": "4.7 Summary of Probability\nWe’ve talked what probability means, and why statisticians can’t agree on what it means. We talked about the rules that probabilities have to obey. And we introduced the idea of a probability distribution, and spent a good chunk talking about some of the more important probability distributions that statisticians work with. We talked about things like this:\n\nProbability theory versus statistics\nFrequentist versus Bayesian views of probability\nBasics of probability theory\nBinomial distribution, normal distribution\n\nAs you’d expect, this coverage is by no means exhaustive. Probability theory is a large branch of mathematics in its own right, entirely separate from its application to statistics and data analysis. As such, there are thousands of books written on the subject and universities generally offer multiple classes devoted entirely to probability theory. Even the “simpler” task of documenting standard probability distributions is a big topic.Fortunately for you, very little of this is necessary. You’re unlikely to need to know dozens of statistical distributions when you go out and do real world data analysis, and you definitely won’t need them for this book, but it never hurts to know that there’s other possibilities out there.\nPicking up on that last point, there’s a sense in which this whole chapter is something of a digression. Many undergraduate psychology classes on statistics skim over this content very quickly (I know mine did), and even the more advanced classes will often “forget” to revisit the basic foundations of the field. Most academic psychologists would not know the difference between probability and density, and until recently very few would have been aware of the difference between Bayesian and frequentist probability. However, I think it’s important to understand these things before moving onto the applications. For example, there are a lot of rules about what you’re “allowed” to say when doing statistical inference, and many of these can seem arbitrary and weird. However, they start to make sense if you understand that there is this Bayesian/frequentist distinction.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Probability, Sampling, and Estimation</span>"
    ]
  },
  {
    "objectID": "04-SamplesPopulations.html#samples-populations-and-sampling",
    "href": "04-SamplesPopulations.html#samples-populations-and-sampling",
    "title": "4  Probability, Sampling, and Estimation",
    "section": "4.8 Samples, populations and sampling",
    "text": "4.8 Samples, populations and sampling\nRemember, the role of descriptive statistics is to concisely summarize what we do know. In contrast, the purpose of inferential statistics is to “learn what we do not know from what we do”. What kinds of things would we like to learn about? And how do we learn them? These are the questions that lie at the heart of inferential statistics, and they are traditionally divided into two “big ideas”: estimation and hypothesis testing. The goal in this chapter is to introduce the first of these big ideas, estimation theory, but we’ll talk about sampling theory first because estimation theory doesn’t make sense until you understand sampling. So, this chapter divides into sampling theory, and how to make use of sampling theory to discuss how statisticians think about estimation. We have already done lots of sampling, so you are already familiar with some of the big ideas.\nSampling theory plays a huge role in specifying the assumptions upon which your statistical inferences rely. And in order to talk about “making inferences” the way statisticians think about it, we need to be a bit more explicit about what it is that we’re drawing inferences from (the sample) and what it is that we’re drawing inferences about (the population).\nIn almost every situation of interest, what we have available to us as researchers is a sample of data. We might have run experiment with some number of participants; a polling company might have phoned some number of people to ask questions about voting intentions; etc. Regardless: the data set available to us is finite, and incomplete. We can’t possibly get every person in the world to do our experiment; a polling company doesn’t have the time or the money to ring up every voter in the country etc. In our earlier discussion of descriptive statistics, this sample was the only thing we were interested in. Our only goal was to find ways of describing, summarizing and graphing that sample. This is about to change.\n\n4.8.1 Defining a population\nA sample is a concrete thing. You can open up a data file, and there’s the data from your sample. A population, on the other hand, is a more abstract idea. It refers to the set of all possible people, or all possible observations, that you want to draw conclusions about, and is generally much bigger than the sample. In an ideal world, the researcher would begin the study with a clear idea of what the population of interest is, since the process of designing a study and testing hypotheses about the data that it produces does depend on the population about which you want to make statements. However, that doesn’t always happen in practice: usually the researcher has a fairly vague idea of what the population is and designs the study as best he/she can on that basis.\nSometimes it’s easy to state the population of interest. For instance, in the “polling company” example, the population consisted of all voters enrolled at the a time of the study – millions of people. The sample was a set of 1000 people who all belong to that population. In most situations the situation is much less simple. In a typical a psychological experiment, determining the population of interest is a bit more complicated. Suppose I run an experiment using 100 undergraduate students as my participants. My goal, as a cognitive scientist, is to try to learn something about how the mind works. So, which of the following would count as “the population”:\n\nAll of the undergraduate psychology students at the University of Adelaide?\nUndergraduate psychology students in general, anywhere in the world?\nAustralians currently living?\nAustralians of similar ages to my sample?\nAnyone currently alive?\nAny human being, past, present or future?\nAny biological organism with a sufficient degree of intelligence operating in a terrestrial environment?\nAny intelligent being?\n\nEach of these defines a real group of mind-possessing entities, all of which might be of interest to me as a cognitive scientist, and it’s not at all clear which one ought to be the true population of interest.\n\n\n4.8.2 Simple random samples\nIrrespective of how we define the population, the critical point is that the sample is a subset of the population, and our goal is to use our knowledge of the sample to draw inferences about the properties of the population. The relationship between the two depends on the procedure by which the sample was selected. This procedure is referred to as a sampling method, and it is important to understand why it matters.\nTo keep things simple, imagine we have a bag containing 10 chips. Each chip has a unique letter printed on it, so we can distinguish between the 10 chips. The chips come in two colors, black and white.\n\n\n\n\n\n\n\n\nFigure 4.8: Simple random sampling without replacement from a finite population\n\n\n\n\n\nThis set of chips is the population of interest, and it is depicted graphically on the left of Figure 4.8.\nAs you can see from looking at the picture, there are 4 black chips and 6 white chips, but of course in real life we wouldn’t know that unless we looked in the bag. Now imagine you run the following “experiment”: you shake up the bag, close your eyes, and pull out 4 chips without putting any of them back into the bag. First out comes the \\(a\\) chip (black), then the \\(c\\) chip (white), then \\(j\\) (white) and then finally \\(b\\) (black). If you wanted, you could then put all the chips back in the bag and repeat the experiment, as depicted on the right hand side of Figure 4.8. Each time you get different results, but the procedure is identical in each case. The fact that the same procedure can lead to different results each time, we refer to it as a random process. However, because we shook the bag before pulling any chips out, it seems reasonable to think that every chip has the same chance of being selected. A procedure in which every member of the population has the same chance of being selected is called a simple random sample. The fact that we did not put the chips back in the bag after pulling them out means that you can’t observe the same thing twice, and in such cases the observations are said to have been sampled without replacement.\nTo help make sure you understand the importance of the sampling procedure, consider an alternative way in which the experiment could have been run. Suppose that my 5-year old son had opened the bag, and decided to pull out four black chips without putting any of them back in the bag. This biased sampling scheme is depicted in Figure 4.9.\n\n\n\n\n\n\n\n\nFigure 4.9: Biased sampling without replacement from a finite populations.\n\n\n\n\n\nNow consider the evidentiary value of seeing 4 black chips and 0 white chips. Clearly, it depends on the sampling scheme, does it not? If you know that the sampling scheme is biased to select only black chips, then a sample that consists of only black chips doesn’t tell you very much about the population! For this reason, statisticians really like it when a data set can be considered a simple random sample, because it makes the data analysis much easier.\nA third procedure is worth mentioning. This time around we close our eyes, shake the bag, and pull out a chip. This time, however, we record the observation and then put the chip back in the bag. Again we close our eyes, shake the bag, and pull out a chip. We then repeat this procedure until we have 4 chips. Data sets generated in this way are still simple random samples, but because we put the chips back in the bag immediately after drawing them it is referred to as a sample with replacement. The difference between this situation and the first one is that it is possible to observe the same population member multiple times, as illustrated in Figure 4.10.\n\n\n\n\n\n\n\n\nFigure 4.10: Simple random sampling with replacement from a finite population.\n\n\n\n\n\nMost psychology experiments tend to be sampling without replacement, because the same person is not allowed to participate in the experiment twice. However, most statistical theory is based on the assumption that the data arise from a simple random sample with replacement. In real life, this very rarely matters. If the population of interest is large (e.g., has more than 10 entities!) the difference between sampling with- and without- replacement is too small to be concerned with. The difference between simple random samples and biased samples, on the other hand, is not such an easy thing to dismiss.\n\n\n4.8.3 Most samples are not simple random samples\nAs you can see from looking at the list of possible populations that I showed above, it is almost impossible to obtain a simple random sample from most populations of interest. When I run experiments, I’d consider it a minor miracle if my participants turned out to be a random sampling of the undergraduate psychology students at Adelaide university, even though this is by far the narrowest population that I might want to generalize to. A thorough discussion of other types of sampling schemes is beyond the scope of this book, but to give you a sense of what’s out there I’ll list a few of the more important ones:\n\nStratified sampling. Suppose your population is (or can be) divided into several different sub-populations, or strata. Perhaps you’re running a study at several different sites, for example. Instead of trying to sample randomly from the population as a whole, you instead try to collect a separate random sample from each of the strata. Stratified sampling is sometimes easier to do than simple random sampling, especially when the population is already divided into the distinct strata. It can also be more efficient that simple random sampling, especially when some of the sub-populations are rare. For instance, when studying schizophrenia it would be much better to divide the population into two strata (schizophrenic and not-schizophrenic), and then sample an equal number of people from each group. If you selected people randomly, you would get so few schizophrenic people in the sample that your study would be useless. This specific kind of of stratified sampling is referred to as oversampling because it makes a deliberate attempt to over-represent rare groups.\nSnowball sampling is a technique that is especially useful when sampling from a “hidden” or hard to access population, and is especially common in social sciences. For instance, suppose the researchers want to conduct an opinion poll among transgender people. The research team might only have contact details for a few trans folks, so the survey starts by asking them to participate (stage 1). At the end of the survey, the participants are asked to provide contact details for other people who might want to participate. In stage 2, those new contacts are surveyed. The process continues until the researchers have sufficient data. The big advantage to snowball sampling is that it gets you data in situations that might otherwise be impossible to get any. On the statistical side, the main disadvantage is that the sample is highly non-random, and non-random in ways that are difficult to address. On the real life side, the disadvantage is that the procedure can be unethical if not handled well, because hidden populations are often hidden for a reason. I chose transgender people as an example here to highlight this: if you weren’t careful you might end up outing people who don’t want to be outed (very, very bad form), and even if you don’t make that mistake it can still be intrusive to use people’s social networks to study them. It’s certainly very hard to get people’s informed consent before contacting them, yet in many cases the simple act of contacting them and saying “hey we want to study you” can be hurtful. Social networks are complex things, and just because you can use them to get data doesn’t always mean you should.\nConvenience sampling is more or less what it sounds like. The samples are chosen in a way that is convenient to the researcher, and not selected at random from the population of interest. Snowball sampling is one type of convenience sampling, but there are many others. A common example in psychology are studies that rely on undergraduate psychology students. These samples are generally non-random in two respects: firstly, reliance on undergraduate psychology students automatically means that your data are restricted to a single sub-population. Secondly, the students usually get to pick which studies they participate in, so the sample is a self selected subset of psychology students not a randomly selected subset. In real life, most studies are convenience samples of one form or another. This is sometimes a severe limitation, but not always.\n\n\n\n4.8.4 How much does it matter if you don’t have a simple random sample?\nOkay, so real world data collection tends not to involve nice simple random samples. Does that matter? A little thought should make it clear to you that it can matter if your data are not a simple random sample: just think about the difference between Figure 4.8 and Figure 4.9. However, it’s not quite as bad as it sounds. Some types of biased samples are entirely unproblematic. For instance, when using a stratified sampling technique you actually know what the bias is because you created it deliberately, often to increase the effectiveness of your study, and there are statistical techniques that you can use to adjust for the biases you’ve introduced (not covered in this book!). So in those situations it’s not a problem.\nMore generally though, it’s important to remember that random sampling is a means to an end, not the end in itself. Let’s assume you’ve relied on a convenience sample, and as such you can assume it’s biased. A bias in your sampling method is only a problem if it causes you to draw the wrong conclusions. When viewed from that perspective, I’d argue that we don’t need the sample to be randomly generated in every respect: we only need it to be random with respect to the psychologically-relevant phenomenon of interest. Suppose I’m doing a study looking at working memory capacity. In study 1, I actually have the ability to sample randomly from all human beings currently alive, with one exception: I can only sample people born on a Monday. In study 2, I am able to sample randomly from the Australian population. I want to generalize my results to the population of all living humans. Which study is better? The answer, obviously, is study 1. Why? Because we have no reason to think that being “born on a Monday” has any interesting relationship to working memory capacity. In contrast, I can think of several reasons why “being Australian” might matter. Australia is a wealthy, industrialized country with a very well-developed education system. People growing up in that system will have had life experiences much more similar to the experiences of the people who designed the tests for working memory capacity. This shared experience might easily translate into similar beliefs about how to “take a test”, a shared assumption about how psychological experimentation works, and so on. These things might actually matter. For instance, “test taking” style might have taught the Australian participants how to direct their attention exclusively on fairly abstract test materials relative to people that haven’t grown up in a similar environment; leading to a misleading picture of what working memory capacity is.\nThere are two points hidden in this discussion. Firstly, when designing your own studies, it’s important to think about what population you care about, and try hard to sample in a way that is appropriate to that population. In practice, you’re usually forced to put up with a “sample of convenience” (e.g., psychology lecturers sample psychology students because that’s the least expensive way to collect data, and our coffers aren’t exactly overflowing with gold), but if so you should at least spend some time thinking about what the dangers of this practice might be.\nSecondly, if you’re going to criticize someone else’s study because they’ve used a sample of convenience rather than laboriously sampling randomly from the entire human population, at least have the courtesy to offer a specific theory as to how this might have distorted the results. Remember, everyone in science is aware of this issue, and does what they can to alleviate it. Merely pointing out that “the study only included people from group BLAH” is entirely unhelpful, and borders on being insulting to the researchers, who are aware of the issue. They just don’t happen to be in possession of the infinite supply of time and money required to construct the perfect sample. In short, if you want to offer a responsible critique of the sampling process, then be helpful. Rehashing the blindingly obvious truisms that I’ve been rambling on about in this section isn’t helpful.\n\n\n4.8.5 Population parameters and sample statistics\nOkay. Setting aside the thorny methodological issues associated with obtaining a random sample, let’s consider a slightly different issue. Up to this point we have been talking about populations the way a scientist might. To a psychologist, a population might be a group of people. To an ecologist, a population might be a group of bears. In most cases the populations that scientists care about are concrete things that actually exist in the real world.\nStatisticians, however, are a funny lot. On the one hand, they are interested in real world data and real science in the same way that scientists are. On the other hand, they also operate in the realm of pure abstraction in the way that mathematicians do. As a consequence, statistical theory tends to be a bit abstract in how a population is defined. In much the same way that psychological researchers operationalize our abstract theoretical ideas in terms of concrete measurements, statisticians operationalize the concept of a “population” in terms of mathematical objects that they know how to work with. You’ve already come across these objects they’re called probability distributions (remember, the place where data comes from).\nThe idea is quite simple. Let’s say we’re talking about IQ scores. To a psychologist, the population of interest is a group of actual humans who have IQ scores. A statistician “simplifies” this by operationally defining the population as the probability distribution depicted in Figure 4.11 \\(a\\).\n\n\n\n\n\n\n\n\nFigure 4.11: The population distribution of IQ scores (panel a) and two samples drawn randomly from it. In panel b we have a sample of 100 observations, and panel c we have a sample of 10,000 observations.\n\n\n\n\n\nIQ tests are designed so that the average IQ is 100, the standard deviation of IQ scores is 15, and the distribution of IQ scores is normal. These values are referred to as the population parameters because they are characteristics of the entire population. That is, we say that the population mean \\(\\mu\\) is 100, and the population standard deviation \\(\\sigma\\) is 15.\nNow suppose we collect some data. We select 100 people at random and administer an IQ test, giving a simple random sample from the population. The sample would consist of a collection of numbers like this:\n106 101 98 80 74 ... 107 72 100\nEach of these IQ scores is sampled from a normal distribution with mean 100 and standard deviation 15. So if I plot a histogram of the sample, I get something like the one shown in Figure 4.11 \\(b\\). As you can see, the histogram is roughly the right shape, but it’s a very crude approximation to the true population distribution shown in Figure 4.11 \\(a\\). The mean of the sample is fairly close to the population mean 100 but not identical. In this case, it turns out that the people in the sample have a mean IQ of 98.5, and the standard deviation of their IQ scores is 15.9. These sample statistics are properties of the data set, and although they are fairly similar to the true population values, they are not the same. In general, sample statistics are the things you can calculate from your data set, and the population parameters are the things you want to learn about. Later on in this chapter we’ll talk about how you can estimate population parameters using your sample statistics and how to work out how confident you are in your estimates but before we get to that there’s a few more ideas in sampling theory that you need to know about.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Probability, Sampling, and Estimation</span>"
    ]
  },
  {
    "objectID": "04-SamplesPopulations.html#the-law-of-large-numbers",
    "href": "04-SamplesPopulations.html#the-law-of-large-numbers",
    "title": "4  Probability, Sampling, and Estimation",
    "section": "4.9 The law of large numbers",
    "text": "4.9 The law of large numbers\nWe just looked at the results of one fictitious IQ experiment with a sample size of \\(N=100\\). The results were somewhat encouraging: the true population mean is 100, and the sample mean of 98.5 is a pretty reasonable approximation to it. In many scientific studies that level of precision is perfectly acceptable, but in other situations you need to be a lot more precise. If we want our sample statistics to be much closer to the population parameters, what can we do about it?\nThe obvious answer is to collect more data. Suppose that we ran a much larger experiment, this time measuring the IQ’s of 10,000 people. We can simulate the results of this experiment using R, using the rnorm() function, which generates random numbers sampled from a normal distribution. For an experiment with a sample size of n = 10000, and a population with mean = 100 and sd = 15, R produces our fake IQ data using these commands:\n\nIQ &lt;- rnorm(n=10000, mean=100, sd=15) #generate IQ scores\nIQ &lt;- round(IQ) # make round numbers\n\nCool, we just generated 10,000 fake IQ scores. Where did they go? Well, they went into the variable IQ on my computer. You can do the same on your computer too by copying the above code. 10,000 numbers is too many numbers to look at. We can look at the first 100 like this:\n\nprint(IQ[1:100])\n#&gt;   [1] 104  85  99  93 109 106  96 101  92  82  81  81 108  90  84 127  75  94\n#&gt;  [19]  94  69 118  86  91  92  81 105  91 103 112 113 107  96 133  97 106 113\n#&gt;  [37] 110  91  73  92 117 100 120  91  95  93 119 106  98  96 102 102 111 153\n#&gt;  [55] 107  87 104  98 103 107 101 107  96 104  86 121 108 125  92 121 108  90\n#&gt;  [73] 118  90  81 100  81  70 119 110  83 105 121  87  99  92 122  98 105  95\n#&gt;  [91] 103  87  91 109  81  91  81  99  90  73\n\nWe can compute the mean IQ using the command mean(IQ) and the standard deviation using the command sd(IQ), and draw a histogram using hist(). The histogram of this much larger sample is shown in Figure @ref(fig:IQdist)c. Even a moment’s inspections makes clear that the larger sample is a much better approximation to the true population distribution than the smaller one. This is reflected in the sample statistics: the mean IQ for the larger sample turns out to be 99.9, and the standard deviation is 15.1. These values are now very close to the true population.\nI feel a bit silly saying this, but the thing I want you to take away from this is that large samples generally give you better information. I feel silly saying it because it’s so bloody obvious that it shouldn’t need to be said. In fact, it’s such an obvious point that when Jacob Bernoulli – one of the founders of probability theory – formalized this idea back in 1713, he was kind of a jerk about it. Here’s how he described the fact that we all share this intuition:\n\nFor even the most stupid of men, by some instinct of nature, by himself and without any instruction (which is a remarkable thing), is convinced that the more observations have been made, the less danger there is of wandering from one’s goal (see Stigler, 1986, p65).\n\nOkay, so the passage comes across as a bit condescending (not to mention sexist), but his main point is correct: it really does feel obvious that more data will give you better answers. The question is, why is this so? Not surprisingly, this intuition that we all share turns out to be correct, and statisticians refer to it as the law of large numbers. The law of large numbers is a mathematical law that applies to many different sample statistics, but the simplest way to think about it is as a law about averages. The sample mean is the most obvious example of a statistic that relies on averaging (because that’s what the mean is… an average), so let’s look at that. When applied to the sample mean, what the law of large numbers states is that as the sample gets larger, the sample mean tends to get closer to the true population mean. Or, to say it a little bit more precisely, as the sample size “approaches” infinity (written as \\(N \\rightarrow \\infty\\)) the sample mean approaches the population mean (\\(\\bar{X} \\rightarrow \\mu\\)).\nI don’t intend to subject you to a proof that the law of large numbers is true, but it’s one of the most important tools for statistical theory. The law of large numbers is the thing we can use to justify our belief that collecting more and more data will eventually lead us to the truth. For any particular data set, the sample statistics that we calculate from it will be wrong, but the law of large numbers tells us that if we keep collecting more data those sample statistics will tend to get closer and closer to the true population parameters.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Probability, Sampling, and Estimation</span>"
    ]
  },
  {
    "objectID": "04-SamplesPopulations.html#sampling-distributions-and-the-central-limit-theorem",
    "href": "04-SamplesPopulations.html#sampling-distributions-and-the-central-limit-theorem",
    "title": "4  Probability, Sampling, and Estimation",
    "section": "4.10 Sampling distributions and the central limit theorem",
    "text": "4.10 Sampling distributions and the central limit theorem\nThe law of large numbers is a very powerful tool, but it’s not going to be good enough to answer all our questions. Among other things, all it gives us is a “long run guarantee”. In the long run, if we were somehow able to collect an infinite amount of data, then the law of large numbers guarantees that our sample statistics will be correct. But as John Maynard Keynes famously argued in economics, a long run guarantee is of little use in real life:\n\n[**The] long run is a misleading guide to current affairs. In the long run we are all dead. Economists set themselves too easy, too useless a task, if in tempestuous seasons they can only tell us, that when the storm is long past, the ocean is flat again.** Keynes (1923, 80)\n\nAs in economics, so too in psychology and statistics. It is not enough to know that we will eventually arrive at the right answer when calculating the sample mean. Knowing that an infinitely large data set will tell me the exact value of the population mean is cold comfort when my actual data set has a sample size of \\(N=100\\). In real life, then, we must know something about the behavior of the sample mean when it is calculated from a more modest data set!\n\n4.10.1 Sampling distribution of the sample means\n“Oh no, what is the sample distribution of the sample means? Is that even allowed in English?”. Yes, unfortunately, this is allowed. The sampling distribution of the sample means is the next most important thing you will need to understand. IT IS SO IMPORTANT THAT IT IS NECESSARY TO USE ALL CAPS. It is only confusing at first because it’s long and uses sampling and sample in the same phrase.\nDon’t worry, we’ve been prepping you for this. You know what a distribution is right? It’s where numbers comes from. It makes some numbers occur more or less frequently, or the same as other numbers. You know what a sample is right? It’s the numbers we take from a distribution. So, what could the sampling distribution of the sample means refer to?\nFirst, what do you think the sample means refers to? Well, if you took a sample of numbers, you would have a bunch of numbers…then, you could compute the mean of those numbers. The sample mean is the mean of the numbers in the sample. That is all. So, what is this distribution you speak of? Well, what if you took a bunch of samples, put one here, put one there, put some other ones other places. You have a lot of different samples of numbers. You could compute the mean for each them. Then you would have a bunch of means. What do those means look like? Well, if you put them in a histogram, you could find out. If you did that, you would be looking at (roughly) a distribution, AKA the sampling distribution of the sample means.\n“I’m following along sort of, why would I want to do this instead of watching Netflix…”. Because, the sampling distribution of the sample means gives you another window into chance. A very useful one that you can control, just like your remote control, by pressing the right design buttons.\n\n\n4.10.2 Seeing the pieces\nTo make a sampling distribution of the sample means, we just need the following:\n\nA distribution to take numbers from\nA bunch of different samples from the distribution\nThe means of each of the samples\nGet all of the sample means, and plot them in a histogram\n\n\nQuestion for yourself: What do you think the sampling distribution of the sample means will look like? Will it tend to look the shape of the distribution that the samples came from? Or not? Good question, think about it.\n\nLet’s do those four things. We will sample numbers from the uniform distribution. Figure 4.12 shows the uniform distribution for sampling the set of integers from 1 to 10:\n\n\n\n\n\n\n\n\nFigure 4.12: A uniform distribution illustrating the probabilites of sampling the numbers 1 to 10. In a uniform distribution, all numbers have an equal probability of being sampled, so the line is flat indicating all numbers have the same probability\n\n\n\n\n\nFigure 4.13 animates the process of taking a bunch of samples from the uniform distribution. We will set our sample-size to 20. It’s easier to see how the sample mean behaves in a movie. Each histogram shows a new sample. The red line shows where the mean of the sample is. The samples are all very different from each other, but the red line doesn’t move around very much, it always stays near the middle. However, the red line does move around a little bit, and this variance is what we call the sampling distribution of the sample mean.\n\n\n\n\n\n\n\n\nFigure 4.13: Animiation showing histograms for different samples of size 20 from the uniform distribution. The red line shows the mean of each sample.\n\n\n\n\n\nOK, what have we got here? We have an animation of 10 different samples. Each sample has 20 observations and these are summarized in each of histograms that show up in the animation. Each histogram has a red line. The red line shows you where the mean of each sample is located. So, we have found the sample means for the 10 different samples from a uniform distribution.\nFirst question. Are the sample means all the same? The answer is no. They are all kind of similar to each other though, they are all around five plus or minus a few numbers. This is interesting. Although all of our samples look pretty different from one another, the means of our samples look more similar than different.\nSecond question. What should we do with the means of our samples? Well, how about we collect them them all, and then plot a histogram of them. This would allow us to see what the distribution of the sample means looks like. The next histogram is just this. Except, rather than taking 10 samples, we will take 10,000 samples. For each of them we will compute the means. So, we will have 10,000 means. Figure 4.14 shows the histogram of the sample means:\n\n\n\n\n\n\n\n\nFigure 4.14: A histogram showing the sample means for 10,000 samples, each size 20, from the uniform distribution of numbers from 1 to 10. The expected mean is 5.5, and the histogram is centered on 5.5. The mean of each sample is not always 5.5 because of sampling error or chance\n\n\n\n\n\n“Wait what? This doesn’t look right. I thought we were taking samples from a uniform distribution. Uniform distributions are flat. THIS DOES NOT LOOK LIKE A FLAT DISTRIBTUION, WHAT IS GOING ON, AAAAAGGGHH.”. We feel your pain.\nRemember, we are looking at the distribution of sample means. It is indeed true that the distribution of sample means does not look the same as the distribution we took the samples from. Our distribution of sample means goes up and down. In fact, this will almost always be the case for distributions of sample means. This fact is called the central limit theorem, which we talk about later.\nFor now, let’s talk about about what’s happening. Remember, we have been sampling numbers between the range 1 to 10. We are supposed to get each number with roughly equal frequency, because we are sampling from a uniform distribution. So, let’s say we took a sample of 10 numbers, and happened to get one of each from 1 to 10.\n1 2 3 4 5 6 7 8 9 10\nWhat is the mean of those numbers? Well, its 1+2+3+4+5+6+7+8+9+10 = 55 / 10 = 5.5. Imagine if we took a bigger sample, say of 20 numbers, and again we got exactly 2 of each number. What would the mean be? It would be (1+2+3+4+5+6+7+8+9+10)*2 = 110 / 20 = 5.5. Still 5.5. You can see here, that the mean value of our uniform distribution is 5.5. Now that we know this, we might expect that most of our samples will have a mean near this number. We already know that every sample won’t be perfect, and it won’t have exactly an equal amount of every number. So, we will expect the mean of our samples to vary a little bit. The histogram that we made shows the variation. Not surprisingly, the numbers vary around the value 5.5.\n\n\n4.10.3 Sampling distributions exist for any sample statistic!\nOne thing to keep in mind when thinking about sampling distributions is that any sample statistic you might care to calculate has a sampling distribution. For example, suppose that each time you sampled some numbers from an experiment you wrote down the largest number in the experiment. Doing this over and over again would give you a very different sampling distribution, namely the sampling distribution of the maximum. You could calculate the smallest number, or the mode, or the median, of the variance, or the standard deviation, or anything else from your sample. Then, you could repeat many times, and produce the sampling distribution of those statistics. Neat!\nJust for fun here are some different sampling distributions for different statistics. We will take a normal distribution with mean = 100, and standard deviation =20. Then, we’ll take lots of samples with n = 50 (50 observations per sample). We’ll save all of the sample statistics, then plot their histograms in Figure 4.15. Let’s do it:\n\n\n\n\n\n\n\n\nFigure 4.15: Each panel shows a histogram of a different sampling statistic\n\n\n\n\n\nWe just computed 4 different sampling distributions, for the mean, standard deviation, maximum value, and the median. If you just look quickly at these histograms you might think they all basically look the same. Hold up now. It’s very important to look at the x-axes. They are different. For example, the sample mean goes from about 90 to 110, whereas the standard deviation goes from 15 to 25.\nThese sampling distributions are super important, and worth thinking about. What should you think about? Well, here’s a clue. These distributions are telling you what to expect from your sample. Critically, they are telling you what you should expect from a sample, when you take one from the specific distribution that we used (normal distribution with mean =100 and SD = 20). What have we learned. We’ve learned a tonne. We’ve learned that we can expect our sample to have a mean somewhere between 90 and 108ish. Notice, the sample means are never more extreme. We’ve learned that our sample will usually have some variance, and that the the standard deviation will be somewhere between 15 and 25 (never much more extreme than that). We can see that sometime we get some big numbers, say between 120 and 180, but not much bigger than that. And, we can see that the median is pretty similar to the mean. If you ever took a sample of 50 numbers, and your descriptive statistics were inside these windows, then perhaps they came from this kind of normal distribution. If your sample statistics are very different, then your sample probably did not come this distribution. By using simulation, we can find out what samples look like when they come from distributions, and we can use this information to make inferences about whether our sample came from particular distributions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Probability, Sampling, and Estimation</span>"
    ]
  },
  {
    "objectID": "04-SamplesPopulations.html#the-central-limit-theorem",
    "href": "04-SamplesPopulations.html#the-central-limit-theorem",
    "title": "4  Probability, Sampling, and Estimation",
    "section": "4.11 The central limit theorem",
    "text": "4.11 The central limit theorem\nOK, so now you’ve seen lots of sampling distributions, and you know what the sampling distribution of the mean is. Here, we’ll focus on how the sampling distribution of the mean changes as a function of sample size.\nIntuitively, you already know part of the answer: if you only have a few observations, the sample mean is likely to be quite inaccurate (you’ve already seen it bounce around): if you replicate a small experiment and recalculate the mean you’ll get a very different answer. In other words, the sampling distribution is quite wide. If you replicate a large experiment and recalculate the sample mean you’ll probably get the same answer you got last time, so the sampling distribution will be very narrow.\nLet’s give ourselves a nice movie to see everything in action. We’re going to sample numbers from a normal distribution. Figure 4.16 has four panels, each panel represents a different sample size (n), including sample-sizes of 10, 50, 100, and 1000. The red line shows the shape of the normal distribution. The grey bars show a histogram of each of the samples that we take. The red line shows the mean of an individual sample (the middle of the grey bars). As you can see, the red line moves around a lot, especially when the sample size is small (10).\nThe new bits are the blue bars and the blue lines. The blue bars represent the sampling distribution of the sample mean. For example, in the panel for sample-size 10, we see a bunch of blue bars. This is a histogram of 10 sample means, taken from 10 samples of size 10. In the 50 panel, we see a histogram of 50 sample means, taken from 50 samples of size 50, and so on. The blue line in each panel is the mean of the sample means (“aaagh, it’s a mean of means”, yes it is).\n\n\n\n\n\n\n\n\nFigure 4.16: Animation of samples (grey histogram shows frequency counts of data in each sample), and the sampling distribution of the mean (histogram of the sampling means for many samples). Each sample is taken from the normal distribution shown in red. The moving red line is the mean of an individual sample. The blue line is the mean of the blue histogram, which represents the sampling distribution of the mean for many samples\n\n\n\n\n\nWhat should you notice? Notice that the range of the blue bars shrinks as sample size increases. The sampling distribution of the mean is quite wide when the sample-size is 10, it narrows as sample-size increases to 50 and 100, and it’s just one bar, right in the middle when sample-size goes to 1000. What we are seeing is that the mean of the sampling distribution approaches the mean of the population as sample-size increases.\nSo, the sampling distribution of the mean is another distribution, and it has some variance. It varies more when sample-size is small, and varies less when sample-size is large. We can quantify this effect by calculating the standard deviation of the sampling distribution, which is referred to as the standard error. The standard error of a statistic is often denoted SE, and since we’re usually interested in the standard error of the sample mean, we often use the acronym SEM. As you can see just by looking at the movie, as the sample size \\(N\\) increases, the SEM decreases.\nOkay, so that’s one part of the story. However, there’s something we’ve been glossing over a little bit. We’ve seen it already, but it’s worth looking at it one more time. Here’s the thing: no matter what shape your population distribution is, as \\(N\\) increases the sampling distribution of the mean starts to look more like a normal distribution. This is the central limit theorem.\nTo see the central limit theorem in action, we are going to look at some histograms of sample means from different kinds of distributions. It is very important to recognize that you are looking at distributions of sample means, not distributions of individual samples.\nHere we go, Figure 4.17 shows sampling from a normal distribution. The red line is the normal distribution where each sample is drawn from. The mean for each sample of numbers is computed, and the distribution of sample means is shown by the blue bars. Note that the shape of red line and the blue bars are similar, they both look like a normal distribution.\n\n\n\n\n\n\n\n\nFigure 4.17: Comparison of two normal distributions, and histograms for the sampling distribution of the mean for different samples-sizes. The range of sampling distribution of the mean shrinks as sample-size increases\n\n\n\n\n\nLet’s do it again. This time we will sample from a flat uniform distribution shown by the red line. However, Figure 4.18 shows the distribution of sample means represented by the blue bars is not flat, it looks like a normal distribution.\n\n\n\n\n\n\n\n\nFigure 4.18: Illustration that the shape of the sampling distribution of the mean is normal, even when the samples come from a non-normal (uniform in this case) distribution\n\n\n\n\n\nOne more time with an exponential distribution (shown in red) where smaller numbers are more likely to be sampled than larger numbers. Even though way more of the numbers in a given sample will be smaller than larger, according to Figure 4.19 the sampling distribution of the mean does not look the red line. Instead, the sampling distribution of the mean looks like a bell-shaped normal curve. This is the central limit theorem in action.\n\n\n\n\n\n\n\n\nFigure 4.19: Illustration that the shape of the sampling distribution of the mean is normal, even when the samples come from an exponential distribution\n\n\n\n\n\nOn the basis of these figures, it seems like we have evidence for all of the following claims about the sampling distribution of the mean:\n\nThe mean of the sampling distribution is the same as the mean of the population\nThe standard deviation of the sampling distribution (i.e., the standard error) gets smaller as the sample size increases\nThe shape of the sampling distribution becomes normal as the sample size increases\n\nAs it happens, not only are all of these statements true, there is a very famous theorem in statistics that proves all three of them, known as the central limit theorem. Among other things, the central limit theorem tells us that if the population distribution has mean \\(\\mu\\) and standard deviation \\(\\sigma\\), then the sampling distribution of the mean also has mean \\(\\mu\\), and the standard error of the mean is \\[\\mbox{SEM} = \\frac{\\sigma}{ \\sqrt{N} }\\] Because we divide the population standard deviation \\(\\sigma\\) by the square root of the sample size \\(N\\), the SEM gets smaller as the sample size increases. It also tells us that the shape of the sampling distribution becomes normal.\nThis result is useful for all sorts of things. It tells us why large experiments are more reliable than small ones, and because it gives us an explicit formula for the standard error it tells us how much more reliable a large experiment is. It tells us why the normal distribution is, well, normal. In real experiments, many of the things that we want to measure are actually averages of lots of different quantities (e.g., arguably, “general” intelligence as measured by IQ is an average of a large number of “specific” skills and abilities), and when that happens, the averaged quantity should follow a normal distribution. Because of this mathematical law, the normal distribution pops up over and over again in real data.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Probability, Sampling, and Estimation</span>"
    ]
  },
  {
    "objectID": "04-SamplesPopulations.html#z-scores",
    "href": "04-SamplesPopulations.html#z-scores",
    "title": "4  Probability, Sampling, and Estimation",
    "section": "4.12 z-scores",
    "text": "4.12 z-scores\nWe are now in a position to combine some of things we’ve been talking about in this chapter, and introduce you to a new tool, z-scores. It turns out we won’t use z-scores very much in this textbook. However, you can’t take a class on statistics and not learn about z-scores.\nWe are going to look at a normal distribution in Figure 4.20, and draw lines through the distribution at 0, +/- 1, +/-2, and +/- 3 standard deviations from the mean:\n\n\n\n\n\n\n\n\nFigure 4.20: A normal distribution. Each line represents a standard deviation from the mean. The labels show the proportions of scores that fall between each bar.\n\n\n\n\n\nThe figure shows a normal distribution with mean = 0, and standard deviation = 1. We’ve drawn lines at each of the standard deviations: -3, -2, -1, 0, 1, 2, and 3. We also show some numbers in the labels, in between each line. These numbers are proportions. For example, we see the proportion is .341 for scores that fall between the range 0 and 1. Scores between 0 and 1 occur 34.1% of the time. Scores in between -1 and 1, occur 68.2% of the time, that’s more than half of the scores. Scores between 1 and occur about 13.6% of the time, and scores between 2 and 3 occur even less, only 2.1% of the time.\nNormal distributions always have these properties, even when they have different means and standard deviations. For example, take a look at the normal distribution in Figure 4.21 that has a mean = 100, and standard deviation = 25.\n\n\n\n\n\n\n\n\nFigure 4.21: A normal distribution. Each line represents a standard deviation from the mean. The labels show the proportions of scores that fall between each bar.\n\n\n\n\n\nNow we are looking at a normal distribution with mean = 100 and standard deviation = 25. Notice that the region between 100 and 125 contains 34.1% of the scores. This region is 1 standard deviation away from the mean (the standard deviation is 25, the mean is 100, so 25 is one whole standard deviation away from 100). As you can see, the very same proportions occur between each of the standard deviations, as they did when our standard deviation was set to 1 (with a mean of 0).\n\n4.12.1 Idea behind z-scores\nSometimes it can be convenient to transform your original scores into different scores that are easier to work with. For example, if you have a bunch of proportions, like .3, .5, .6, .7, you might want to turn them into percentages like 30%, 50%, 60%, and 70%. To do that you multiply the proportions by a constant of 100. If you want to turn percentages back into proportions, you divide by a constant of 100. This kind of transformation just changes the scale of the numbers from between 0-1, and between 0-100. Otherwise, the pattern in the numbers stays the same.\nThe idea behind z-scores is a similar kind of transformation. The idea is to express each raw score in terms of it’s standard deviation. For example, if I told you I got a 75% on test, you wouldn’t know how well I did compared to the rest of the class. But, if I told you that I scored 2 standard deviations above the mean, you’d know I did quite well compared to the rest of the class, because you know that most scores (if they are distributed normally) fall below 2 standard deviations of the mean.\nWe also know, now thanks to the central limit theorem, that many of our measures, such as sample means, will be distributed normally. So, it can often be desirable to express the raw scores in terms of their standard deviations.\nLet’s see how this looks in a table without showing you any formulas. We will look at some scores that come from a normal distribution with mean = 100, and standard deviation = 25. We will list some raw scores, along with the z-scores\n\n\n\n\n\nraw\nz\n\n\n\n\n25\n-3\n\n\n50\n-2\n\n\n75\n-1\n\n\n100\n0\n\n\n125\n1\n\n\n150\n2\n\n\n175\n3\n\n\n\n\n\nRemember, the mean is 100, and the standard deviation is 25. How many standard deviations away from the mean is a score of 100? The answer is 0, it’s right on the mean. You can see the z-score for 100, is 0. How many standard deviations is 125 away from the mean? Well the standard deviation is 25, 125 is one whole 25 away from 100, that’s a total of 1 standard deviation, so the z-score for 125 is 1. The z-score for 150 is 2, because 150 is two 25s away from 100. The z-score for 50 is -2, because 50 is two 25s away from 100 in the opposite direction. All we are doing here is re-expressing the raw scores in terms of how many standard deviations they are from the mean. Remember, the mean is always right on target, so the center of the z-score distribution is always 0.\n\n\n4.12.2 Calculating z-scores\nTo calculate z-scores all you have to do is figure out how many standard deviations from the mean each number is. Let’s say the mean is 100, and the standard deviation is 25. You have a score of 97. How many standard deviations from the mean is 97?\nFirst compute the difference between the score and the mean:\n\\(97-100 = -3\\)\nAlright, we have a total difference of -3. How many standard deviations does -3 represent if 1 standard deviation is 25? Clearly -3 is much smaller than 25, so it’s going to be much less than 1. To figure it out, just divide -3 by the standard deviation.\n\\(\\frac{-3}{25} = -.12\\)\nOur z-score for 97 is -.12.\nHere’s the general formula:\n\\(z = \\frac{\\text{raw score} - \\text{mean}}{\\text{standard deviation}}\\)\nSo, for example if we had these 10 scores from a normal distribution with mean = 100, and standard deviation =25\n\n#&gt;  [1]  64.01  74.31 115.35 113.49  72.17  96.15 100.01 117.77 103.08  78.17\n\nThe z-scores would be:\n\n#&gt;  [1] -1.4396 -1.0276  0.6140  0.5396 -1.1132 -0.1540  0.0004  0.7108  0.1232\n#&gt; [10] -0.8732\n\nOnce you have the z-scores, you could use them as another way to describe your data. For example, now just by looking at a score you know if it is likely or unlikely to occur, because you know how the area under the normal curve works. z-scores between -1 and 1 happen pretty often, scores greater than 1 or -1 still happen fairly often, but not as often. And, scores bigger than 2 or -2 don’t happen very often. This is a convenient thing to do if you want to look at your numbers and get a general sense of how often they happen.\nUsually you do not know the mean or the standard deviation of the population that you are drawing your sample scores from. So, you could use the mean and standard deviation of your sample as an estimate, and then use those to calculate z-scores.\nFinally, z-scores are also called standardized scores, because each raw score is described in terms of it’s standard deviation. This may well be the last time we talk about z-scores in this book. You might wonder why we even bothered telling you about them. First, it’s worth knowing they are a thing. Second, they become important as your statistical prowess becomes more advanced. Third, some statistical concepts, like correlation, can be re-written in terms of z-scores, and this illuminates aspects of those statistics. Finally, they are super useful when you are dealing with a normal distribution that has a known mean and standard deviation.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Probability, Sampling, and Estimation</span>"
    ]
  },
  {
    "objectID": "04-SamplesPopulations.html#estimating-population-parameters",
    "href": "04-SamplesPopulations.html#estimating-population-parameters",
    "title": "4  Probability, Sampling, and Estimation",
    "section": "4.13 Estimating population parameters",
    "text": "4.13 Estimating population parameters\nLet’s pause for a moment to get our bearings. We’re about to go into the topic of estimation. What is that, and why should you care? First, population parameters are things about a distribution. For example, distributions have means. The mean is a parameter of the distribution. The standard deviation of a distribution is a parameter. Anything that can describe a distribution is a potential parameter.\nOK fine, who cares? This I think, is a really good question. There are some good concrete reasons to care. And there are some great abstract reasons to care. Unfortunately, most of the time in research, it’s the abstract reasons that matter most, and these can be the most difficult to get your head around.\n\n4.13.1 Concrete population parameters\nFirst some concrete reasons. There are real populations out there, and sometimes you want to know the parameters of them. For example, if you are a shoe company, you would want to know about the population parameters of feet size. As a first pass, you would want to know the mean and standard deviation of the population. If your company knew this, and other companies did not, your company would do better (assuming all shoes are made equal). Why would your company do better, and how could it use the parameters? Here’s one good reason. As a shoe company you want to meet demand with the right amount of supply. If you make too many big or small shoes, and there aren’t enough people to buy them, then you’re making extra shoes that don’t sell. If you don’t make enough of the most popular sizes, you’ll be leaving money on the table. Right? Yes. So, what would be an optimal thing to do? Perhaps, you would make different amounts of shoes in each size, corresponding to how the demand for each shoe size. You would know something about the demand by figuring out the frequency of each size in the population. You would need to know the population parameters to do this.\nFortunately, it’s pretty easy to get the population parameters without measuring the entire population. Who has time to measure every-bodies feet? Nobody, that’s who. Instead, you would just need to randomly pick a bunch of people, measure their feet, and then measure the parameters of the sample. If you take a big enough sample, we have learned that the sample mean gives a very good estimate of the population mean. We will learn shortly that a version of the standard deviation of the sample also gives a good estimate of the standard deviation of the population. Perhaps shoe-sizes have a slightly different shape than a normal distribution. Here too, if you collect a big enough sample, the shape of the distribution of the sample will be a good estimate of the shape of the populations. All of these are good reasons to care about estimating population parameters. But, do you run a shoe company? Probably not.\n\n\n4.13.2 Abstract population parameters\nEven when we think we are talking about something concrete in Psychology, it often gets abstract right away. Instead of measuring the population of feet-sizes, how about the population of human happiness. We all think we know what happiness is, everyone has more or less of it, there are a bunch of people, so there must be a population of happiness right? Perhaps, but it’s not very concrete. The first problem is figuring out how to measure happiness. Let’s use a questionnaire. Consider these questions:\n\nHow happy are you right now on a scale from 1 to 7? How happy are you in general on a scale from 1 to 7? How happy are you in the mornings on a scale from 1 to 7? How happy are you in the afternoons on a scale from 1 to 7?\n\n\n= very unhappy\n= unhappy\n= sort of unhappy\n= in the middle\n= sort of happy\n= happy\n= very happy\n\nForget about asking these questions to everybody in the world. Let’s just ask them to lots of people (our sample). What do you think would happen? Well, obviously people would give all sorts of answers right. We could tally up the answers and plot them in a histogram. This would show us a distribution of happiness scores from our sample. “Great, fantastic!”, you say. Yes, fine and dandy.\nSo, on the one hand we could say lots of things about the people in our sample. We could say exactly who says they are happy and who says they aren’t, after all they just told us!\nBut, what can we say about the larger population? Can we use the parameters of our sample (e.g., mean, standard deviation, shape etc.) to estimate something about a larger population. Can we infer how happy everybody else is, just from our sample? HOLD THE PHONE.\n\n4.13.2.1 Complications with inference\nBefore listing a bunch of complications, let me tell you what I think we can do with our sample. Provided it is big enough, our sample parameters will be a pretty good estimate of what another sample would look like. Because of the following discussion, this is often all we can say. But, that’s OK, as you see throughout this book, we can work with that!\nProblem 1: Multiple populations: If you looked at a large sample of questionnaire data you will find evidence of multiple distributions inside your sample. People answer questions differently. Some people are very cautious and not very extreme. Their answers will tend to be distributed about the middle of the scale, mostly 3s, 4s, and 5s. Some people are very bi-modal, they are very happy and very unhappy, depending on time of day. These people’s answers will be mostly 1s and 2s, and 6s and 7s, and those numbers look like they come from a completely different distribution. Some people are entirely happy or entirely unhappy. Again, these two “populations” of people’s numbers look like two different distributions, one with mostly 6s and 7s, and one with mostly 1s and 2s. Other people will be more random, and their scores will look like a uniform distribution. So, is there a single population with parameters that we can estimate from our sample? Probably not. Could be a mixture of lots of populations with different distributions.\nProblem 2: What do these questions measure?: If the whole point of doing the questionnaire is to estimate the population’s happiness, we really need wonder if the sample measurements actually tell us anything about happiness in the first place. Some questions: Are people accurate in saying how happy they are? Does the measure of happiness depend on the scale, for example, would the results be different if we used 0-100, or -100 to +100, or no numbers? Does the measure of happiness depend on the wording in the question? Does a measure like this one tell us everything we want to know about happiness (probably not), what is it missing (who knows? probably lots). In short, nobody knows if these kinds of questions measure what we want them to measure. We just hope that they do. Instead, we have a very good idea of the kinds of things that they actually measure. It’s really quite obvious, and staring you in the face. Questionnaire measurements measure how people answer questionnaires. In other words, how people behave and answer questions when they are given a questionnaire. This might also measure something about happiness, when the question has to do about happiness. But, it turns out people are remarkably consistent in how they answer questions, even when the questions are total nonsense, or have no questions at all (just numbers to choose!) Maul (2017).\nThe take home complications here are that we can collect samples, but in Psychology, we often don’t have a good idea of the populations that might be linked to these samples. There might be lots of populations, or the populations could be different depending on who you ask. Finally, the “population” might not be the one you want it to be.\n\n\n\n4.13.3 Experiments and Population parameters\nOK, so we don’t own a shoe company, and we can’t really identify the population of interest in Psychology, can’t we just skip this section on estimation? After all, the “population” is just too weird and abstract and useless and contentious. HOLD THE PHONE AGAIN!\nIt turns out we can apply the things we have been learning to solve lots of important problems in research. These allow us to answer questions with the data that we collect. Parameter estimation is one of these tools. We just need to be a little bit more creative, and a little bit more abstract to use the tools.\nHere is what we know already. The numbers that we measure come from somewhere, we have called this place “distributions”. Distributions control how the numbers arrive. Some numbers happen more than others depending on the distribution. We assume, even if we don’t know what the distribution is, or what it means, that the numbers came from one. Second, when get some numbers, we call it a sample. This entire chapter so far has taught you one thing. When your sample is big, it resembles the distribution it came from. And, when your sample is big, it will resemble very closely what another big sample of the same thing will look like. We can use this knowledge!\nVery often as Psychologists what we want to know is what causes what. We want to know if X causes something to change in Y. Does eating chocolate make you happier? Does studying improve your grades? There a bazillions of these kinds of questions. And, we want answers to them.\nI’ve been trying to be mostly concrete so far in this textbook, that’s why we talk about silly things like chocolate and happiness, at least they are concrete. Let’s give a go at being abstract. We can do it.\nSo, we want to know if X causes Y to change. What is X? What is Y? X is something you change, something you manipulate, the independent variable. Y is something you measure. So, we will be taking samples from Y. “Oh I get it, we’ll take samples from Y, then we can use the sample parameters to estimate the population parameters of Y!” NO, not really, but yes sort of. We will take sample from Y, that is something we absolutely do. In fact, that is really all we ever do, which is why talking about the population of Y is kind of meaningless. We’re more interested in our samples of Y, and how they behave.\nSo, what would happen if we removed X from the universe altogether, and then took a big sample of Y. We’ll pretend Y measures something in a Psychology experiment. So, we know right away that Y is variable. When we take a big sample, it will have a distribution (because Y is variable). So, we can do things like measure the mean of Y, and measure the standard deviation of Y, and anything else we want to know about Y. Fine. What would happen if we replicated this measurement. That is, we just take another random sample of Y, just as big as the first. What should happen is that our first sample should look a lot like our second example. After all, we didn’t do anything to Y, we just took two big samples twice. Both of our samples will be a little bit different (due to sampling error), but they’ll be mostly the same. The bigger our samples, the more they will look the same, especially when we don’t do anything to cause them to be different. In other words, we can use the parameters of one sample to estimate the parameters of a second sample, because they will tend to be the same, especially when they are large.\nWe are now ready for step two. You want to know if X changes Y. What do you do? You make X go up and take a big sample of Y then look at it. You make X go down, then take a second big sample of Y and look at it. Next, you compare the two samples of Y. If X does nothing then what should you find? We already discussed that in the previous paragraph. If X does nothing, then both of your big samples of Y should be pretty similar. However, if X does something to Y, then one of your big samples of Y will be different from the other. You will have changed something about Y. Maybe X makes the mean of Y change. Or maybe X makes the variation in Y change. Or, maybe X makes the whole shape of the distribution change. If we find any big changes that can’t be explained by sampling error, then we can conclude that something about X caused a change in Y! We could use this approach to learn about what causes what!\nThe very important idea is still about estimation, just not population parameter estimation exactly. We know that when we take samples they naturally vary. So, when we estimate a parameter of a sample, like the mean, we know we are off by some amount. When we find that two samples are different, we need to find out if the size of the difference is consistent with what sampling error can produce, or if the difference is bigger than that. If the difference is bigger, then we can be confident that sampling error didn’t produce the difference. So, we can confidently infer that something else (like an X) did cause the difference. This bit of abstract thinking is what most of the rest of the textbook is about. Determining whether there is a difference caused by your manipulation. There’s more to the story, there always is. We can get more specific than just, is there a difference, but for introductory purposes, we will focus on the finding of differences as a foundational concept.\n\n\n4.13.4 Interim summary\nWe’ve talked about estimation without doing any estimation, so in the next section we will do some estimating of the mean and of the standard deviation. Formally, we talk about this as using a sample to estimate a parameter of the population. Feel free to think of the “population” in different ways. It could be concrete population, like the distribution of feet-sizes. Or, it could be something more abstract, like the parameter estimate of what samples usually look like when they come from a distribution.\n\n\n4.13.5 Estimating the population mean\nSuppose we go to Brooklyn and 100 of the locals are kind enough to sit through an IQ test. The average IQ score among these people turns out to be \\(\\bar{X}=98.5\\). So what is the true mean IQ for the entire population of Brooklyn? Obviously, we don’t know the answer to that question. It could be \\(97.2\\), but if could also be \\(103.5\\). Our sampling isn’t exhaustive so we cannot give a definitive answer. Nevertheless if forced to give a “best guess” I’d have to say \\(98.5\\). That’s the essence of statistical estimation: giving a best guess. We’re using the sample mean as the best guess of the population mean.\nIn this example, estimating the unknown population parameter is straightforward. I calculate the sample mean, and I use that as my estimate of the population mean. It’s pretty simple, and in the next section we’ll explain the statistical justification for this intuitive answer. However, for the moment let’s make sure you recognize that the sample statistic and the estimate of the population parameter are conceptually different things. A sample statistic is a description of your data, whereas the estimate is a guess about the population. With that in mind, statisticians often use different notation to refer to them. For instance, if true population mean is denoted \\(\\mu\\), then we would use \\(\\hat\\mu\\) to refer to our estimate of the population mean. In contrast, the sample mean is denoted \\(\\bar{X}\\) or sometimes \\(m\\). However, in simple random samples, the estimate of the population mean is identical to the sample mean: if I observe a sample mean of \\(\\bar{X} = 98.5\\), then my estimate of the population mean is also \\(\\hat\\mu = 98.5\\). To help keep the notation clear, here’s a handy table:\n\n\n\n\n\n\n\n\nSymbol\nWhat is it?\nDo we know what it is?\n\n\n\n\n\\(\\bar{X}\\)\nSample mean\nYes, calculated from the raw data\n\n\n\\(\\mu\\)\nTrue population mean\nAlmost never known for sure\n\n\n\\(\\hat{\\mu}\\)\nEstimate of the population mean\nYes, identical to the sample mean\n\n\n\n\n\n4.13.6 Estimating the population standard deviation\nSo far, estimation seems pretty simple, and you might be wondering why I forced you to read through all that stuff about sampling theory. In the case of the mean, our estimate of the population parameter (i.e. \\(\\hat\\mu\\)) turned out to identical to the corresponding sample statistic (i.e. \\(\\bar{X}\\)). However, that’s not always true. To see this, let’s have a think about how to construct an estimate of the population standard deviation, which we’ll denote \\(\\hat\\sigma\\). What shall we use as our estimate in this case? Your first thought might be that we could do the same thing we did when estimating the mean, and just use the sample statistic as our estimate. That’s almost the right thing to do, but not quite.\nHere’s why. Suppose I have a sample that contains a single observation. For this example, it helps to consider a sample where you have no intuitions at all about what the true population values might be, so let’s use something completely fictitious. Suppose the observation in question measures the cromulence of my shoes. It turns out that my shoes have a cromulence of 20. So here’s my sample:\n20\nThis is a perfectly legitimate sample, even if it does have a sample size of \\(N=1\\). It has a sample mean of 20, and because every observation in this sample is equal to the sample mean (obviously!) it has a sample standard deviation of 0. As a description of the sample this seems quite right: the sample contains a single observation and therefore there is no variation observed within the sample. A sample standard deviation of \\(s = 0\\) is the right answer here. But as an estimate of the population standard deviation, it feels completely insane, right? Admittedly, you and I don’t know anything at all about what “cromulence” is, but we know something about data: the only reason that we don’t see any variability in the sample is that the sample is too small to display any variation! So, if you have a sample size of \\(N=1\\), it feels like the right answer is just to say “no idea at all”.\nNotice that you don’t have the same intuition when it comes to the sample mean and the population mean. If forced to make a best guess about the population mean, it doesn’t feel completely insane to guess that the population mean is 20. Sure, you probably wouldn’t feel very confident in that guess, because you have only the one observation to work with, but it’s still the best guess you can make.\nLet’s extend this example a little. Suppose I now make a second observation. My data set now has \\(N=2\\) observations of the cromulence of shoes, and the complete sample now looks like this:\n20, 22\nThis time around, our sample is just large enough for us to be able to observe some variability: two observations is the bare minimum number needed for any variability to be observed! For our new data set, the sample mean is \\(\\bar{X}=21\\), and the sample standard deviation is \\(s=1\\). What intuitions do we have about the population? Again, as far as the population mean goes, the best guess we can possibly make is the sample mean: if forced to guess, we’d probably guess that the population mean cromulence is 21. What about the standard deviation? This is a little more complicated. The sample standard deviation is only based on two observations, and if you’re at all like me you probably have the intuition that, with only two observations, we haven’t given the population “enough of a chance” to reveal its true variability to us. It’s not just that we suspect that the estimate is wrong: after all, with only two observations we expect it to be wrong to some degree. The worry is that the error is systematic.\nIf the error is systematic, that means it is biased. For example, imagine if the sample mean was always smaller than the population mean. If this was true (it’s not), then we couldn’t use the sample mean as an estimator. It would be biased, we’d be using the wrong number.\nIt turns out the sample standard deviation is a biased estimator of the population standard deviation. We can sort of anticipate this by what we’ve been discussing. When the sample size is 1, the standard deviation is 0, which is obviously to small. When the sample size is 2, the standard deviation becomes a number bigger than 0, but because we only have two sample, we suspect it might still be too small. Turns out this intuition is correct.\nIt would be nice to demonstrate this somehow. There are in fact mathematical proofs that confirm this intuition, but unless you have the right mathematical background they don’t help very much. Instead, what I’ll do is use R to simulate the results of some experiments. With that in mind, let’s return to our IQ studies. Suppose the true population mean IQ is 100 and the standard deviation is 15. I can use the rnorm() function to generate the the results of an experiment in which I measure \\(N=2\\) IQ scores, and calculate the sample standard deviation. If I do this over and over again, and plot a histogram of these sample standard deviations, what I have is the sampling distribution of the standard deviation. I’ve plotted this distribution in Figure 4.22.\n\n\n\n\n\n\n\n\nFigure 4.22: The sampling distribution of the sample standard deviation for a two IQ scores experiment. The true population standard deviation is 15 (dashed line), but as you can see from the histogram, the vast majority of experiments will produce a much smaller sample standard deviation than this. On average, this experiment would produce a sample standard deviation of only 8.5, well below the true value! In other words, the sample standard deviation is a biased estimate of the population standard deviation.\n\n\n\n\n\nEven though the true population standard deviation is 15, the average of the sample standard deviations is only 8.5. Notice that this is a very different from when we were plotting sampling distributions of the sample mean, those were always centered around the mean of the population.\nNow let’s extend the simulation. Instead of restricting ourselves to the situation where we have a sample size of \\(N=2\\), let’s repeat the exercise for sample sizes from 1 to 10. If we plot the average sample mean and average sample standard deviation as a function of sample size, you get the following results.\n@fig-estimatorbiasA shows the sample mean as a function of sample size. Notice it’s a flat line. The sample mean doesn’t underestimate or overestimate the population mean. It is an unbiased estimate!\n\n\n\n\n\n\n\n\nFigure 4.23: An illustration of the fact that the sample mean is an unbiased estimator of the population mean.\n\n\n\n\n\nFigure 4.24 shows the sample standard deviation as a function of sample size. Notice it is not a flat line. The sample standard deviation systematically underestimates the population standard deviation!\n\n\n\n\n\n\n\n\nFigure 4.24: An illustration of the fact that the the sample standard deviation is a biased estimator of the population standard deviation.\n\n\n\n\n\nIn other words, if we want to make a “best guess” (\\(\\hat\\sigma\\), our estimate of the population standard deviation) about the value of the population standard deviation \\(\\sigma\\), we should make sure our guess is a little bit larger than the sample standard deviation \\(s\\).\nThe fix to this systematic bias turns out to be very simple. Here’s how it works. Before tackling the standard deviation, let’s look at the variance. If you recall from the second chapter, the sample variance is defined to be the average of the squared deviations from the sample mean. That is: \\[s^2 = \\frac{1}{N} \\sum_{i=1}^N (X_i - \\bar{X})^2\\] The sample variance \\(s^2\\) is a biased estimator of the population variance \\(\\sigma^2\\). But as it turns out, we only need to make a tiny tweak to transform this into an unbiased estimator. All we have to do is divide by \\(N-1\\) rather than by \\(N\\). If we do that, we obtain the following formula: \\[\\hat\\sigma^2 = \\frac{1}{N-1} \\sum_{i=1}^N (X_i - \\bar{X})^2\\] This is an unbiased estimator of the population variance \\(\\sigma\\).\nA similar story applies for the standard deviation. If we divide by \\(N-1\\) rather than \\(N\\), our estimate of the population standard deviation becomes: \\[\\hat\\sigma = \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^N (X_i - \\bar{X})^2}\\].\nIt is worth pointing out that software programs make assumptions for you, about which variance and standard deviation you are computing. Some programs automatically divide by \\(N-1\\), some do not. You need to check to figure out what they are doing. Don’t let the software tell you what to do. Software is for you telling it what to do.\nOne final point: in practice, a lot of people tend to refer to \\(\\hat{\\sigma}\\) (i.e., the formula where we divide by \\(N-1\\)) as the sample standard deviation. Technically, this is incorrect: the sample standard deviation should be equal to \\(s\\) (i.e., the formula where we divide by \\(N\\)). These aren’t the same thing, either conceptually or numerically. One is a property of the sample, the other is an estimated characteristic of the population. However, in almost every real life application, what we actually care about is the estimate of the population parameter, and so people always report \\(\\hat\\sigma\\) rather than \\(s\\).\n\n\n\n\n\n\nNote\n\n\n\nNote, whether you should divide by N or N-1 also depends on your philosophy about what you are doing. For example, if you don’t think that what you are doing is estimating a population parameter, then why would you divide by N-1? Also, when N is large, it doesn’t matter too much. The difference between a big N, and a big N-1, is just -1.\n\n\nThis is the right number to report, of course, it’s that people tend to get a little bit imprecise about terminology when they write it up, because “sample standard deviation” is shorter than “estimated population standard deviation”. It’s no big deal, and in practice I do the same thing everyone else does. Nevertheless, I think it’s important to keep the two concepts separate: it’s never a good idea to confuse “known properties of your sample” with “guesses about the population from which it came”. The moment you start thinking that \\(s\\) and \\(\\hat\\sigma\\) are the same thing, you start doing exactly that.\nTo finish this section off, here’s another couple of tables to help keep things clear:\n\n\n\n\n\n\n\n\nSymbol\nWhat is it?\nDo we know what it is?\n\n\n\n\n\\(s^2\\)\nSample variance\nYes, calculated from the raw data\n\n\n\\(\\sigma^2\\)\nPopulation variance\nAlmost never known for sure\n\n\n\\(\\hat{\\sigma}^2\\)\nEstimate of the population variance\nYes, but not the same as the sample variance",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Probability, Sampling, and Estimation</span>"
    ]
  },
  {
    "objectID": "04-SamplesPopulations.html#estimating-a-confidence-interval",
    "href": "04-SamplesPopulations.html#estimating-a-confidence-interval",
    "title": "4  Probability, Sampling, and Estimation",
    "section": "4.14 Estimating a confidence interval",
    "text": "4.14 Estimating a confidence interval\n\nStatistics means never having to say you’re certain – Unknown origin\n\nUp to this point in this chapter, we’ve outlined the basics of sampling theory which statisticians rely on to make guesses about population parameters on the basis of a sample of data. As this discussion illustrates, one of the reasons we need all this sampling theory is that every data set leaves us with some of uncertainty, so our estimates are never going to be perfectly accurate. The thing that has been missing from this discussion is an attempt to quantify the amount of uncertainty in our estimate. It’s not enough to be able guess that the mean IQ of undergraduate psychology students is 115 (yes, I just made that number up). We also want to be able to say something that expresses the degree of certainty that we have in our guess. For example, it would be nice to be able to say that there is a 95% chance that the true mean lies between 109 and 121. The name for this is a confidence interval for the mean.\nArmed with an understanding of sampling distributions, constructing a confidence interval for the mean is actually pretty easy. Here’s how it works. Suppose the true population mean is \\(\\mu\\) and the standard deviation is \\(\\sigma\\). I’ve just finished running my study that has \\(N\\) participants, and the mean IQ among those participants is \\(\\bar{X}\\). We know from our discussion of the central limit theorem that the sampling distribution of the mean is approximately normal. We also know from our discussion of the normal distribution that there is a 95% chance that a normally-distributed quantity will fall within two standard deviations of the true mean. To be more precise, we can use the qnorm() function to compute the 2.5th and 97.5th percentiles of the normal distribution\n\nqnorm( p = c(.025, .975) ) [1] -1.959964 1.959964\n\nOkay, so I lied earlier on. The more correct answer is that a 95% chance that a normally-distributed quantity will fall within 1.96 standard deviations of the true mean.\nNext, recall that the standard deviation of the sampling distribution is referred to as the standard error, and the standard error of the mean is written as SEM. When we put all these pieces together, we learn that there is a 95% probability that the sample mean \\(\\bar{X}\\) that we have actually observed lies within 1.96 standard errors of the population mean. Oof, that is a lot of mathy talk there. We’ll clear it up, don’t worry.\nMathematically, we write this as: \\[\\mu - \\left( 1.96 \\times \\mbox{SEM} \\right) \\ \\leq \\  \\bar{X}\\  \\leq \\  \\mu + \\left( 1.96 \\times \\mbox{SEM} \\right)\\] where the SEM is equal to \\(\\sigma / \\sqrt{N}\\), and we can be 95% confident that this is true.\nHowever, that’s not answering the question that we’re actually interested in. The equation above tells us what we should expect about the sample mean, given that we know what the population parameters are. What we want is to have this work the other way around: we want to know what we should believe about the population parameters, given that we have observed a particular sample. However, it’s not too difficult to do this. Using a little high school algebra, a sneaky way to rewrite our equation is like this: \\[\\bar{X} -  \\left( 1.96 \\times \\mbox{SEM} \\right) \\ \\leq \\ \\mu  \\ \\leq  \\ \\bar{X} +  \\left( 1.96 \\times \\mbox{SEM}\\right)\\] What this is telling is is that the range of values has a 95% probability of containing the population mean \\(\\mu\\). We refer to this range as a 95% confidence interval, denoted \\(\\mbox{CI}_{95}\\). In short, as long as \\(N\\) is sufficiently large – large enough for us to believe that the sampling distribution of the mean is normal – then we can write this as our formula for the 95% confidence interval: \\[\\mbox{CI}_{95} = \\bar{X} \\pm \\left( 1.96 \\times \\frac{\\sigma}{\\sqrt{N}} \\right)\\] Of course, there’s nothing special about the number 1.96: it just happens to be the multiplier you need to use if you want a 95% confidence interval. If I’d wanted a 70% confidence interval, I could have used the qnorm() function to calculate the 15th and 85th quantiles:\n\nqnorm( p = c(.15, .85) ) [1] -1.036433 1.036433\n\nand so the formula for \\(\\mbox{CI}_{70}\\) would be the same as the formula for \\(\\mbox{CI}_{95}\\) except that we’d use 1.04 as our magic number rather than 1.96.\n\n4.14.1 A slight mistake in the formula\nAs usual, I lied. The formula that I’ve given above for the 95% confidence interval is approximately correct, but I glossed over an important detail in the discussion. Notice my formula requires you to use the standard error of the mean, SEM, which in turn requires you to use the true population standard deviation \\(\\sigma\\).\nYet, before we stressed the fact that we don’t actually know the true population parameters. Because we don’t know the true value of \\(\\sigma\\), we have to use an estimate of the population standard deviation \\(\\hat{\\sigma}\\) instead. This is pretty straightforward to do, but this has the consequence that we need to use the quantiles of the \\(t\\)-distribution rather than the normal distribution to calculate our magic number; and the answer depends on the sample size. Plus, we haven’t really talked about the \\(t\\) distribution yet.\nWhen we use the \\(t\\) distribution instead of the normal distribution, we get bigger numbers, indicating that we have more uncertainty. And why do we have that extra uncertainty? Well, because our estimate of the population standard deviation \\(\\hat\\sigma\\) might be wrong! If it’s wrong, it implies that we’re a bit less sure about what our sampling distribution of the mean actually looks like… and this uncertainty ends up getting reflected in a wider confidence interval.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Probability, Sampling, and Estimation</span>"
    ]
  },
  {
    "objectID": "04-SamplesPopulations.html#summary",
    "href": "04-SamplesPopulations.html#summary",
    "title": "4  Probability, Sampling, and Estimation",
    "section": "4.15 Summary",
    "text": "4.15 Summary\nIn this chapter I’ve covered two main topics. The first half of the chapter talks about sampling theory, and the second half talks about how we can use sampling theory to construct estimates of the population parameters. The section breakdown looks like this:\n\nBasic ideas about samples, sampling and populations\nStatistical theory of sampling: the law of large numbers, sampling distributions and the central limit theorem.\nEstimating means and standard deviations\nconfidence intervals\n\nAs always, there’s a lot of topics related to sampling and estimation that aren’t covered in this chapter, but for an introductory psychology class this is fairly comprehensive I think. For most applied researchers you won’t need much more theory than this. One big question that I haven’t touched on in this chapter is what you do when you don’t have a simple random sample. There is a lot of statistical theory you can draw on to handle this situation, but it’s well beyond the scope of this book.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Probability, Sampling, and Estimation</span>"
    ]
  },
  {
    "objectID": "04-SamplesPopulations.html#videos",
    "href": "04-SamplesPopulations.html#videos",
    "title": "5  Probabilidad, muestreo y estimación",
    "section": "5.15 Videos",
    "text": "5.15 Videos\n\n5.15.1 Introducción a la probabilidad\nJeff tiene varios videos más sobre probabilidad que podés ver en su lista de reproducción de estadística.\n\n\n\n\n5.15.2 Teorema de Chebychev\n\n\n\n\n5.15.3 Puntajes Z\n\n\n\n\n5.15.4 Distribución normal I\n\n\n\n\n5.15.5 Distribución normal II\n\n\n\n\n\n\nFisher, R. A. 1922. «On the Mathematical Foundation of Theoretical Statistics». Philosophical Transactions of the Royal Society A 222: 309-68.\n\n\nKeynes, John Maynard. 1923. A Tract on Monetary Reform. London: Macmillan and Company.\n\n\nMeehl, P. H. 1967. «Theory Testing in Psychology and Physics: A Methodological Paradox». Philosophy of Science 34: 103-15. https://doi.org/10.1086/288135."
  },
  {
    "objectID": "05-Foundation_Inference.html#brief-review-of-experiments",
    "href": "05-Foundation_Inference.html#brief-review-of-experiments",
    "title": "5  Foundations for inference",
    "section": "5.1 Brief review of Experiments",
    "text": "5.1 Brief review of Experiments\nIn chapter one we talked a about research methods and experiments. Experiments are a structured way of collecting data that can permit inferences about causality. If we wanted to know whether something like watching cats on YouTube increases happiness we would need an experiment. We already found out that just finding a bunch of people and measuring number of hours watching cats, and level of happiness, and correlating the two will not permit inferences about causation. For one, the causal flow could be reversed. Maybe being happy causes people to watch more cat videos. We need an experiment.\nAn experiment has two parts. A manipulation and a measurement. The manipulation is under the control of the experimenter. Manipulations are also called independent variables. For example, we could manipulate time spent watching cat videos: 1 hour versus 2 hours of cat videos. The measurement is the data that is collected. We could measure how happy people are after watching cat videos on a scale from 1 to 100. Measurements are also called dependent variables. So, in a basic experiment like the one above, we take measurements of happiness from people in one of two experimental conditions defined by the independent variable. Let’s say we ran 50 subjects. 25 subjects would be randomly assigned to watch 1 hour of cat videos, and the other 25 subjects would be randomly assigned to watch 2 hours of cat videos. We would measure happiness for each subject at the end of the videos. Then we could look at the data.\nWhat would we want to look at? If watching cat videos caused a change in happiness, then we would expect the measures of happiness for people watching 1 hour of cat videos to be different from the measures of happiness for people watching 2 hours of cat videos. If watching cat videos does not change happiness, then we would expect no differences in measures of happiness between conditions. Causal forces cause change, and the experiment is set up to detect the change.\nNow we can state one overarching question, how do we know if the data changed between conditions? If we can be confident that there was a change between conditions, we can infer that our manipulation caused a changed in the measurement. If we cannot be confident there was a change, then we cannot infer that our manipulation caused a change in the measurement. We need to build some change detection tools so we can know a change when we find one.\n“Hold on, if we are just looking for a change, wouldn’t that be easy to see by looking at the numbers and seeing if they are different, what’s so hard about that?”. Good question. Now we must take a detour. The short answer is that there will always be change in the data (remember variance)."
  },
  {
    "objectID": "05-Foundation_Inference.html#the-data-came-from-a-distribution",
    "href": "05-Foundation_Inference.html#the-data-came-from-a-distribution",
    "title": "5  Foundations for inference",
    "section": "5.2 The data came from a distribution",
    "text": "5.2 The data came from a distribution\nIn the last chapter we discussed samples and distributions, and the idea that you can take samples from distributions. So, from now on when you see a bunch of numbers, you should wonder, “where did these numbers come from?”. What caused some kinds of numbers to happen more than other kinds of numbers. The answer to this question requires us to again veer off into the abstract world of distributions. A distribution a place where numbers can come from. The distribution sets the constraints. It determines what numbers are likely to occur, and what numbers are not likely to occur. Distributions are abstract ideas. But, they can be made concrete, and we can draw them with pictures that you have seen already, called histograms.\nThe next bit might seem slightly repetitive from the previous chapter. We again look at sampling numbers from a uniform distribution. We show that individual samples can look quite different from each other. Much of the beginning part of this chapter will already be familiar to you, but we take the concepts in a slightly different direction. The direction is how to make inferences about the role of chance in your experiment.\n\n5.2.1 Uniform distribution\nAs a reminder from last chapter, Figure 5.1 shows that the shape of a uniform distribution is completely flat.\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\ndf &lt;- data.frame(a=1:10,b=seq(.1,1,.1))\ndf$a &lt;- as.factor(df$a)\n\nggplot(df,aes(x=a,y=b))+\n  geom_point(color=\"white\")+\n  geom_hline(yintercept=.1)+\n  scale_y_continuous(limits=c(0,1), breaks=seq(0,1, by = .1))+\n  theme_classic(base_size = 18)+\n  ylab(\"Probability\")+\n  xlab(\"Number\")+\n  ggtitle(\"Uniform distribution for numbers 1 to 10\")\n\n\n\n\nFigure 5.1: Uniform distribution showing that the numbers from 1 to 10 have an equal probability of being sampled\n\n\n\n\nOK, so that doesn’t look like much. What is going on here? The y-axis is labelled probability, and it goes from 0 to 1. The x-axis is labelled Number, and it goes from one to 10. There is a horizontal line drawn straight through. This line tells you the probability of each number from 1 to 10. Notice the line is flat. This means all of the numbers have the same probability of occurring. More specifically, there are 10 numbers from 1 to 10 (1,2,3,4,5,6,7,8,9,10), and they all have an equal chance of occurring. 1/10 = .1, which is the probability indicated by the horizontal line.\n“So what?”. Imagine that this uniform distribution is a number generating machine. It spits out numbers, but it spits out each number with the probability indicated by the line. If this distribution was going to start spitting out numbers, it would spit out 10% 1s, 10% 2s, 10% 3s, and so on, up to 10% 10s. Wanna see what that would look like? Let’s make it spit out 100 numbers and put them in Table 5.1.\n\na&lt;-matrix(round(runif(100,1,10)),ncol=10)\nknitr::kable(a)\n\n\n\nTable 5.1: 100 numbers randomly sampled from a uniform distribution.\n\n\n4\n5\n3\n8\n4\n6\n10\n6\n7\n8\n\n\n8\n5\n8\n6\n9\n2\n6\n8\n3\n2\n\n\n2\n4\n5\n5\n8\n4\n4\n9\n6\n9\n\n\n8\n2\n9\n8\n6\n10\n3\n3\n7\n3\n\n\n8\n9\n7\n9\n5\n6\n3\n5\n2\n9\n\n\n7\n8\n5\n8\n5\n9\n8\n1\n3\n2\n\n\n3\n9\n9\n6\n3\n2\n8\n7\n10\n7\n\n\n6\n3\n8\n2\n5\n2\n4\n5\n7\n4\n\n\n4\n6\n10\n3\n9\n1\n1\n1\n3\n7\n\n\n4\n4\n9\n9\n2\n5\n3\n10\n2\n10\n\n\n\n\n\n\nWe used the uniform distribution to generate these numbers. Officially, we call this sampling from a distribution. Sampling is what you do at a grocery store when there is free food. You can keep taking more. However, if you take all of the samples, then what you have is called the population. We’ll talk more about samples and populations as we go along.\nBecause we used the uniform distribution to create numbers, we already know where our numbers came from. However, we can still pretend for the moment that someone showed up at your door, showed you these numbers, and then you wondered where they came from. Can you tell just by looking at these numbers that they came from a uniform distribution? What would need to look at? Perhaps you would want to know if all of the numbers occur with roughly equal frequency, after all they should have right? That is, if each number had the same chance of occurring, we should see that each number occurs roughly the same number of times.\nWe already know what a histogram is, so we can put our sample of 100 numbers into a histogram and see what the counts look like. If all of the numbers from 1 to 10 occur with equal frequency, then each individual number should occur about 10 times. Figure 5.2 shows the histogram:\n\nhist(a)\n\n\n\n\nFigure 5.2: Histogram of 100 numbers randomly sampled from the uniform distribution containing the integers from 1 to 10\n\n\n\n\nUh oh, as you can see, not all of the number occurred 10 times each. All of the bars are not the same height. This shows that randomly sampling numbers from this distribution does not guarantee that our numbers will be exactly like the distribution they came from. We can call this sampling error, or sampling variability.\n\n\n5.2.2 Not all samples are the same, they are usually quite different\nLet’s look at sampling error more closely. We will sample 20 numbers from the uniform distribution. We should expect that each number between 1 and 10 occurs about two times each. As before, this expectation can be visualized in a histogram. To get a better sense of sampling error, let’s repeat the above process ten times. Figure 5.3 has 10 histograms, each showing what 10 different samples of twenty numbers looks like:\n\npossible_integers &lt;- round(runif(20*10,1,10))\ndf &lt;- data.frame(possible_integers, sample=rep(1:10,each=20))\n\nggplot(df,aes(x=possible_integers))+\n  geom_histogram(bins=10, color=\"white\")+\n  theme_classic()+\n  scale_x_continuous(breaks=seq(1,10,1))+\n  facet_wrap(~sample, nrow=2)\n\n\n\n\nFigure 5.3: Histograms for 10 different samples from the uniform distribution. Each sample contains 20 numbers. The histograms all look quite different. The differences between the samples are due to sampling error or random chance.\n\n\n\n\nYou might notice right away that none of the histograms are the same. Even though we are randomly taking 20 numbers from the very same uniform distribution, each sample of 20 numbers comes out different. This is sampling variability, or sampling error.\nFigure 5.4 shows an animated version of the process of repeatedly choosing 20 new random numbers and plotting a histogram. The horizontal line shows the flat-line shape of the uniform distribution. The line crosses the y-axis at 2; and, we expect that each number (from 1 to 10) should occur about 2 times each in a sample of 20. However, each sample bounces around quite a bit, due to random chance.\n\nknitr::include_graphics(path=\"imgs/gifs/sampleUnifExpected-1.gif\")\n\n\n\n\nFigure 5.4: Animation of histograms for different samples of 20 from Uniform distribution (numbers 1 to 10). The black lines shows the expected number of times each number from 1 to 10 should occur. The fact that each number does not occur 2 times each illustrates the error associated with sampling\n\n\n\n\nLooking at the above histograms shows us that figuring out where our numbers came from can be difficult. In the real world, our measurements are samples. We usually only have the luxury of getting one sample of measurements, rather than repeating our own measurements 10 times or more. If you look at the histograms, you will see that some of them look like they could have come from the uniform distribution: most of the bars are near two, and they all fall kind of on a flat line. But, if you happen to look at a different sample, you might see something that is very bumpy, with some numbers happening way more than others. This could suggest to you that those numbers did not come from a uniform distribution (they’re just too bumpy). But let me remind you, all of these samples came from a uniform distribution, this is what samples from that distribution look like. This is what chance does to samples, it makes the individual data points noisy.\n\n\n5.2.3 Large samples are more like the distribution they came from\nLet’s refresh the question. Which of the two samples in Figure 5.5 do you think came from a uniform distribution?\n\na&lt;-round(runif(20*2,1,10))\ndf&lt;-data.frame(a,sample=rep(1:2,each=20))\n\nggplot(df,aes(x=a))+\n  geom_histogram(bins=10, color=\"white\")+\n  facet_wrap(~sample)+\n  theme_classic()+\n  scale_x_continuous(breaks=seq(1,10,1))\n\n\n\n\nFigure 5.5: Which of these two samples came from a uniform distribution?\n\n\n\n\nThe answer is that they both did. But, neither of them look like they did.\nCan we improve things, and make it easier to see if a sample came from a uniform distribution? Yes, we can. All we need to do is increase the sample-size. We will often use the letter n to refer to sample-size. N is the number of observations in the sample.\nSo let’s increase the number of observations in each sample from 20 to 100. We will again create 10 samples (each with 100 observations), and make histograms for each of them. All of these samples will be drawn from the very same uniform distribution. This, means we should expect each number from 1 to 10 to occur about 10 times in each sample. The histograms are shown in Figure 5.6.\n\na&lt;-sample(1:10,100*10,replace=T)\ndf&lt;-data.frame(a,sample=rep(1:10,each=100))\n\nggplot(df,aes(x=a))+\n  geom_histogram(bins=10, color=\"white\")+\n  facet_wrap(~sample, nrow=2)+\n  theme_classic()+\n  ylim(0,20)+\n  scale_x_continuous(breaks=seq(1,10,1))\n\n\n\n\nFigure 5.6: Histograms for different samples from a uniform distribution. N = 100 for each sample.\n\n\n\n\nAgain, most of these histograms don’t look very flat, and all of the bars seem to be going up or down, and they are not exactly at 10 each. So, we are still dealing with sampling error. It’s a pain. It’s always there.\nLet’s bump up the \\(N\\) from 100 to 1000 observations per sample. Now we should expect every number to appear about 100 times each. What happens?\n\na&lt;-sample(1:10,1000*10,replace=T)\ndf&lt;-data.frame(a,sample=rep(1:10,each=1000))\n\nggplot(df,aes(x=a))+\n geom_histogram(bins=10, color=\"white\")+\n  facet_wrap(~sample, nrow=2)+\n  theme_classic()+\n  ylim(0,200)+\n  scale_x_continuous(breaks=seq(1,10,1))\n\n\n\n\nFigure 5.7: Histograms for different samples from a uniform distribution. N = 1000 for each sample.\n\n\n\n\nFigure 5.7 shows the histograms are starting to flatten out. The bars are still not perfectly at 100, because there is still sampling error (there always will be). But, if you found a histogram that looked flat and knew that the sample contained many observations, you might be more confident that those numbers came from a uniform distribution.\nJust for fun let’s make the samples really big. Say 100,000 observations per sample. Here, we should expect that each number occurs about 10,000 times each. What happens?\n\na&lt;-sample(1:10,100000*10,replace=T)\ndf&lt;-data.frame(a,sample=rep(1:10,each=100000))\n\nggplot(df,aes(x=a))+\n  geom_histogram(bins=10, color=\"white\")+\n  facet_wrap(~sample, nrow=2)+\n  theme_classic()+\n  ylim(0,20000)+\n  scale_x_continuous(breaks=seq(1,10,1))\n\n\n\n\nFigure 5.8: Histograms for different samples from a uniform distribution. N = 100,000 for each sample.\n\n\n\n\nFigure 5.8 shows that the histograms for each sample are starting to look the same. They all have 100,000 observations, and this gives chance enough opportunity to equally distribute the numbers, roughly making sure that they all occur very close to the same amount of times. As you can see, the bars are all very close to 10,000, which is where they should be if the sample came from a uniform distribution.\n\n\n\n\n\n\nPro tip\n\n\n\nThe pattern behind a sample will tend to stabilize as sample-size increases. Small samples will have all sorts of patterns because of sampling error (chance).\n\n\nBefore getting back to the topic of experiments that we started with, let’s ask two more questions. First, which of the two samples in Figure 5.9 do you think came from a uniform distribution? FYI, each of these samples had 20 observations each.\n\na&lt;-c(sample(1:10,20,replace=T),round(rnorm(20,5,2.5)))\ndf&lt;-data.frame(a,sample=rep(1:2,each=20))\n\nggplot(df,aes(x=a))+\n  geom_histogram(bins=10, color=\"white\")+\n  facet_wrap(~sample)+\n  theme_classic()+\n  scale_x_continuous(breaks=seq(1,10,1))\n\n\n\n\nFigure 5.9: Which of these samples came from a uniform distribution?\n\n\n\n\nIf you are not confident in the answer, this is because sampling error (randomness) is fuzzing with the histograms.\nHere is the very same question, only this time we will take 1,000 observations for each sample. Which histogram in Figure 5.10 do you think came from a uniform distribution, which one did not?\n\na&lt;-c(sample(1:10,1000,replace=T),round(rnorm(1000,5,1.25)))\ndf&lt;-data.frame(a,sample=rep(1:2,each=1000))\n\nggplot(df,aes(x=a))+\n  geom_histogram(bins=10, color=\"white\")+\n  facet_wrap(~sample)+\n  theme_classic()+\n  scale_x_continuous(breaks=seq(0,10,1))\n\n\n\n\nFigure 5.10: Which of these samples came from a uniform distribution?\n\n\n\n\nNow that we have increased N, we can see the pattern in each sample becomes more obvious. The histogram for sample 1 has bars near 100, not perfectly flat, but it resembles a uniform distribution. The histogram for sample 2 is not flat looking at all.\nCongratulations to Us! We have just made some statistical inferences without using formulas!\n“We did?” Yes, by looking at our two samples we have inferred that sample 2 did not come from a uniform distribution. We have also inferred that sample 1 could have come form a uniform distribution. Fantastic. These are the same kinds of inferences we will be making for the rest of the course. We will be looking at some numbers, wondering where they came from, then we will arrange the numbers in such a way so that we can make inferences about the kind of distribution they came from. That’s it."
  },
  {
    "objectID": "05-Foundation_Inference.html#is-there-a-difference",
    "href": "05-Foundation_Inference.html#is-there-a-difference",
    "title": "5  Foundations for inference",
    "section": "5.3 Is there a difference?",
    "text": "5.3 Is there a difference?\nLet’s get back to experiments. In an experiment we want to know if an independent variable (our manipulation) causes a change in a dependent variable (measurement). If this occurs, then we will expect to see some differences in our measurement as a function of the manipulation.\nConsider the light switch example:\n\nLight Switch Experiment: You manipulate the switch up (condition 1 of independent variable), light goes on (measurement). You manipulate the switch down (condition 2 of independent variable), light goes off (another measurement). The measurement (light) changes (goes off and on) as a function of the manipulation (moving switch up or down).\nYou can see the change in measurement between the conditions, it is as obvious as night and day. So, when you conduct a manipulation, and can see the difference (change) in your measure, you can be pretty confident that your manipulation is causing the change.\n\nnote: to be cautious we can say “something” about your manipulation is causing the change, it might not be what you think it is if your manipulation is very complicated and involves lots of moving parts.\n\n\n\n5.3.1 Chance can produce differences\nDo you think random chance can produce the appearance of differences, even when there really aren’t any? I hope so. We have already shown that the process of sampling numbers from a distribution is a chancy process that produces different samples. Different samples are different, so yes, chance can produce differences. This can muck up our interpretation of experiments.\nLet’s conduct a fictitious experiment where we expect to find no differences, because we will manipulate something that shouldn’t do anything. Here’s the set-up:\nYou are the experimenter standing in front of a gumball machine. It is very big, has thousands of gumballs. 50% of the gumballs are green, and 50% are red. You want to find out if picking gumballs with your right hand vs. your left hand will cause you to pick more green gumballs. Plus, you will be blindfolded the entire time. The independent variable is Hand: right hand vs. left hand. The dependent variable is the measurement of the color of each gumball.\nYou run the experiment as follows. 1) put on blind fold. 2) pick 10 gumballs randomly with left hand, set them aside. 3) pick 10 gumballs randomly with right hand, set them aside. 4) count the number of green and red gumballs chosen by your left hand, and count the number of green and red gumballs chosen by your right hand. Hopefully you will agree that your hands will not be able to tell the difference between the gumballs. If you don’t agree, we will further stipulate the gumballs are completely identical in every way except their color, so it would be impossible to tell them apart using your hands. So, what should happen in this experiment?\n“Umm, maybe you get 5 red gum balls and 5 green balls from your left hand, and also from your right hand?”. Sort of yes, this is what you would usually get. But, it is not all that you can get. Here is some data showing what happened from one pretend experiment:\n\n\n\n\n\nhand\ngumball\n\n\n\n\nleft\n1\n\n\nleft\n1\n\n\nleft\n0\n\n\nleft\n1\n\n\nleft\n0\n\n\nleft\n0\n\n\nleft\n1\n\n\nleft\n1\n\n\nleft\n1\n\n\nleft\n0\n\n\nright\n1\n\n\nright\n1\n\n\nright\n0\n\n\nright\n0\n\n\nright\n1\n\n\nright\n0\n\n\nright\n0\n\n\nright\n0\n\n\nright\n0\n\n\nright\n1\n\n\n\n\n\n“What am I looking at here”. This is a long-format table. Each row is one gumball. The first column tells you what hand was used. The second column tells you what kind of gumball. We will say 1s stand for green gum balls, and 0s stand for red gumballs. So, did your left hand cause you to pick more green gumballs than your right hand?\nIt would be easier to look at the data using a bar graph (Figure 5.11). To keep things simple, we only count the green gumballs (the other gumballs must be red). So, all we need to do is sum up the 1s. The 0s won’t add anything.\n\nsum_df&lt;-aggregate(gumball~hand,df,sum)\nggplot(sum_df,aes(x=hand,y=gumball))+\n  ylab(\"# of green gumballs\")+\n  geom_bar(stat=\"identity\")+\n  theme_classic(base_size=20)\n\n\n\n\nFigure 5.11: Counts of green gumballs picked randomly by each hand.\n\n\n\n\nOh look, the bars are not the same. One hand picked more green gum balls than the other. Does this mean that one of your hands secretly knows how to find green gumballs? No, it’s just another case of sampling error, that thing we call luck or chance. The difference here is caused by chance, not by the manipulation (which hand you use). Major problem for inference alert. We run experiments to look for differences so we can make inferences about whether our manipulations cause change in our measures. However, this example demonstrates that we can find differences by chance. How can we know if a difference is real, or just caused by chance?\n\n\n5.3.2 Differences due to chance can be simulated\nRemember when we showed that chance can produce correlations. We also showed that chance is restricted in its ability to produce correlations. For example, chance more often produces weak correlations than strong correlations. Remember the window of chance? We found out before that correlations falling outside the window of chance were very unlikely. We can do the same thing for differences. Let’s find out just what chance can do in our experiment. Once we know what chance is capable of we will be in a better position to judge whether our manipulation caused a difference, or whether it could have been chance.\nThe first thing to do is pretend you conduct the gumball experiment 10 times in a row. This will produce 10 different sets of results. Figure 5.12 shows bar graphs for each replication of the experiment. Now we can look at whether the left hand chose more green gumballs than red gumballs.\n\nhand &lt;- rep(rep(c(\"left\",\"right\"),each=10),10)\nexperiment &lt;- rep(1:10,each=20)\ngumball &lt;- rbinom(20*10,1,.5)\ndf &lt;- data.frame(experiment,hand,gumball)\nsum_df &lt;- aggregate(gumball~experiment*hand,df,sum)\n\nggplot(sum_df, aes(x=hand,y=gumball))+\n  geom_bar(stat = \"identity\")+\n  theme_classic()+\n  facet_wrap(~experiment, nrow = 2)\n\n\n\n\nFigure 5.12: 10 simulated replications of picking gumballs. Each replication gives a slightly different answer. Any difference between the bars is due to chance, or sampling error. This shows that chance alone can produce differences, just by the act of sampling.\n\n\n\n\nThese 10 experiments give us a better look at what chance can do. It should also mesh well with your expectations. If everything is determined by chance (as we have made it so), then sometimes your left hand will choose more green balls, sometimes your right hand will choose more green gumballs, and sometimes they will choose the same amount of gumballs. Right? Right."
  },
  {
    "objectID": "05-Foundation_Inference.html#chance-makes-some-differences-more-likely-than-others",
    "href": "05-Foundation_Inference.html#chance-makes-some-differences-more-likely-than-others",
    "title": "5  Foundations for inference",
    "section": "5.4 Chance makes some differences more likely than others",
    "text": "5.4 Chance makes some differences more likely than others\nOK, we have seen that chance can produce differences here. But, we still don’t have a good idea about what chance usually does and doesn’t do. For example, if we could find the window of opportunity here, we would be able find out that chance usually does not produce differences of a certain large size. If we knew what the size was, then if we ran experiment and our difference was bigger than what chance can do, we could be confident that chance did not produce our difference.\nLet’s think about our measure of green balls in terms of a difference. For example, in each experiment we counted the green balls for the left and right hand. What we really want to know is if there is a difference between them. So, we can calculate the difference score. Let’s decide that the difference score = # of green gumballs in left hand - # of green gumballs in right hand. Figure 5.13 redraws the 10 bar graphs from above; however, now there is only one bar for each experiment. This bar represents the difference in number of green gumballs drawn by the left and right hand.\n\ndifferences&lt;-sum_df[sum_df$hand ==\"left\",]$gumball-\n  sum_df[sum_df$hand ==\"right\",]$gumball\n\ndif_df&lt;-data.frame(experiment=c(1:10),differences)\ndif_df$experiment&lt;-as.factor(dif_df$experiment)\n\nggplot(dif_df,aes(y=differences,x=experiment))+\n  geom_bar(stat=\"identity\")+\n  geom_hline(yintercept = 0)+\n  theme_classic(base_size = 20)+\n  ylab(\"Difference Scores\")\n\n\n\n\nFigure 5.13: A look at the differences between number of each kind of gumball for the different replications. The difference should be zero, but sampling error produces non-zero differences.\n\n\n\n\nMissing bars mean that there were an equal number of green gumballs chosen by the left and right hands (difference score is 0). A positive value means that more green gumballs were chosen by the left than right hand. A negative value means that more green gumballs were chosen by the right than left hand. Note that if we decided (and we get to decide) to calculate the difference in reverse (right hand - left hand), the signs of the differences scores would flip around.\nWe are starting to see more of the differences that chance can produce. The difference scores are mostly between -2 to +2. We could get an even better impression by running this pretend experiment 100 times instead of only 10 times. The results are shown in Figure 5.14.\n\nhand &lt;- rep(rep(c(\"left\", \"right\"), each = 10), 100)\nexperiment &lt;- rep(1:100, each = 20)\ngumball &lt;- rbinom(20 * 100, 1, .5)\ndf &lt;- data.frame(experiment, hand, gumball)\nsum_df &lt;- aggregate(gumball ~ experiment * hand, df, sum)\n\ndifferences &lt;- sum_df[sum_df$hand == \"left\", ]$gumball -\n  sum_df[sum_df$hand == \"right\", ]$gumball\n\ndif_df &lt;- data.frame(experiment = c(1:100), differences)\ndif_df$experiment &lt;- as.factor(dif_df$experiment)\n\nggplot(dif_df, aes(y = differences, x = experiment)) +\n  geom_bar(stat = \"identity\") +\n  theme_classic(base_size=20) +\n  ylab(\"Difference Scores\")\n\n\n\n\nFigure 5.14: Replicating the experiment 100 times, and looking at the differences each time. There are mnay kinds of differences that chance alone can produce.\n\n\n\n\nOoph, we just ran so many simulated experiments that the x-axis is unreadable, but it goes from 1 to 100. Each bar represents the difference of number of green balls chosen randomly by the left or right hand. Beginning to notice anything? Look at the y-axis, this shows the size of the difference. Yes, there are lots of bars of different sizes, this shows us that many kinds of differences do occur by chance. However, the y-axis is also restricted. It does not go from -10 to +10. Big differences greater than 5 or -5 don’t happen very often.\nNow that we have a method for simulating differences due to chance, let’s run 10,000 simulated experiments. But, instead of plotting the differences in a bar graph for each experiment, how about we look at the histogram of difference scores. The histogram in Figure 5.15 provides a clearer picture about which differences happen most often, and which ones do not. This will be another window into observing what kinds of differences chance is capable of producing.\n\nhand&lt;-rep(rep(c(\"left\",\"right\"),each=10),10000)\nexperiment&lt;-rep(1:10000,each=20)\ngumball&lt;-rbinom(20*10000,1,.5)\ndf&lt;-data.frame(experiment,hand,gumball)\nsum_df&lt;-aggregate(gumball~experiment*hand,df,sum)\n\ndifferences&lt;-sum_df[sum_df$hand ==\"left\",]$gumball-\n  sum_df[sum_df$hand ==\"right\",]$gumball\n\nhist(differences,breaks=seq(-10,10,1))\n\n\n\n\nFigure 5.15: A histogram of the differences obtained by chance over 10,000 replications. The most frequency difference is 0, which is what we expect by chance. But the differences can be as large as -10 or +10. Larger differences occur less often by chance. Chance can’t do everything.\n\n\n\n\nOur computer simulation allows us to force chance to operate hundreds of times, each time it produces a difference. We record the difference, then at the end of the simulation we plot the histogram of the differences. The histogram begins to show us the where the differences came from. Remember the idea that numbers come from a distribution, and the distribution says how often each number occurs. We are looking at one of these distributions. It is showing us that chance produces some differences more often than others. First, chance usually produces 0 differences, that’s the biggest bar in the middle. Chance also produces larger differences, but as the differences get larger (positive or negative), they occur less frequently. The shape of this histogram is your chance window, it tells you what chance can do, it tells you what chance usually does, and what it usually does not do.\nYou can use this chance window to help you make inferences. If you ran yourself in the gumball experiment and found that your left hand chose 2 more green gumballs than red gumballs, would you conclude that you left hand was special, and caused you to choose more green gumballs? Hopefully not. You could look at the chance window and see that differences of size +2 do happen fairly often by chance alone. You should not be surprised if you got a +2 difference. However, what if your left chose 5 more green gumballs than red gumballs. Well, chance doesn’t do this very often, you might think something is up with your left hand. If you got a whopping 9 more green gumballs than red gumballs, you might really start to wonder. This is the kind of thing that could happen (it’s possible), but virtually never happens by chance. When you get things that almost never happen by chance, you can be more confident that the difference reflects a causal force that is not chance."
  },
  {
    "objectID": "05-Foundation_Inference.html#the-crump-test",
    "href": "05-Foundation_Inference.html#the-crump-test",
    "title": "5  Foundations for inference",
    "section": "5.5 The Crump Test",
    "text": "5.5 The Crump Test\nWe are going to be doing a lot of inference throughout the rest of this course. Pretty much all of it will come down to one question. Did chance produce the differences in my data? We will be talking about experiments mostly, and in experiments we want to know if our manipulation caused a difference in our measurement. But, we measure things that have natural variability, so every time we measure things we will always find a difference. We want to know if the difference we found (between our experimental conditions) could have been produced by chance. If chance is a very unlikely explanation of our observed difference, we will make the inference that chance did not produce the difference, and that something about our experimental manipulation did produce the difference. This is it (for this textbook).\n\n\n\n\n\n\nNote\n\n\n\nStatistics is not only about determining whether chance could have produced a pattern in the observed data. The same tools we are talking about here can be generalized to ask whether any kind of distribution could have produced the differences. This allows comparisons between different models of the data, to see which one was the most likely, rather than just rejecting the unlikely ones (e.g., chance). But, we’ll leave those advanced topics for another textbook.\n\n\nThis chapter is about building intuitions for making these kinds of inferences about the role of chance in your data. It’s not clear to me what are the best things to say, to build up your intuitions for how to do statistical inference. So, this chapter tries different things, some of them standard, and some of them made up. What you are about to read, is a made up way of doing statistical inference, without using the jargon that we normally use to talk about it. The goal is to do things without formulas, and without probabilities, and just work with some ideas using simulations to see what happens. We will look at what chance can do, then we will talk about what needs to happen in your data in order for you to be confident that chance didn’t do it.\n\n5.5.1 Intuitive methods\nWarning, this is an unofficial statistical test made up by Matt Crump. It makes sense to him (me), and if it turns out someone else already made this up, then Crump didn’t do his homework, and we will change the name of this test to it’s original author later on. The point of this test is to show how simple arithmetic operations that you already understand can be used to create a statistic tool for inference. This test uses:\n\nSampling numbers randomly from a distribution\nAdding and subtracting\nDivision, to find the mean\nCounting\nGraphing and drawing lines\nNO FORMULAS\n\n\n\n5.5.2 Part 1: Frequency based intuition about occurrence\nQuestion: How many times does something need to happen for it to happen a lot? Or, how many times does something need to happen for it to happen not very much, or even really not at all? Small enough for you to not worry about it at all happening to you?\nWould you go outside everyday if you thought that you would get hit by lightning 1 out of 10 times? I wouldn’t. You’d probably be hit by lightning more than once per month, you’d be dead pretty quickly. 1 out of 10 is a lot (to me, maybe not to you, there’s no right answer here).\nWould you go outside everyday if you thought that you would get hit by lightning 1 out of every 100 days? Jeez, that’s a tough one. What would I even do? If I went out everyday I’d probably be dead in a year! Maybe I would go out 2 or 3 times per year, I’m risky like that, but I’d probably live longer if I stayed at home forever. It would massively suck.\nWould you go outside everyday if you thought you would get hit by lightning 1 out of every 1000 days? Well, you’d probably be dead in 3-6 years if you did that. Are you a gambler? Maybe go out once per month, still sucks.\nWould you go outside everyday if you thought lightning would get you 1 out every 10,000 days? 10,000 is a bigger number, harder to think about. It translates to getting hit about once every 27 years. Ya, I’d probably go out 150 days per year, and keep my fingers crossed.\nWould you go outside everyday if you thought lightning would get you 1 out every 100,000 days? How many years is that? It’s about 273 years. With those odds, I’d probably go out all the time and forget about being hit by lightning. It doesn’t happen very often.\nThe point of considering these questions is to get a sense for yourself of what happens a lot, and what doesn’t happen a lot, and how you would make important decisions based on what happens a lot and what doesn’t.\n\n\n5.5.3 Part 2: Simulating chance\nThis next part could happen a bunch of ways, I’ll make loads of assumptions that I won’t defend, and I won’t claim the Crump test has problems. I will claim it helps us make an inference about whether chance could have produced some differences in data. We’ve already been introduced to simulating things, so we’ll do that again. Here is what we will do. I am a cognitive psychologist who happens to be measuring X. Because of prior research in the field, I know that when I measure X, my samples will tend to have a particular mean and standard deviation. Let’s say the mean is usually 100, and the standard deviation is usually 15. In this case, I don’t care about using these numbers as estimates of the population parameters, I’m just thinking about what my samples usually look like. What I want to know is how they behave when I sample them. I want to see what kind of samples happen a lot, and what kind of samples don’t happen a lot. Now, I also live in the real world, and in the real world when I run experiments to see what changes X, I usually only have access to some number of participants, who I am very grateful too, because they participate in my experiments. Let’s say I usually can run 20 subjects in each condition in my experiments. Let’s keep the experiment simple, with two conditions, so I will need 40 total subjects.\nI would like to learn something to help me with inference. One thing I would like to learn is what the sampling distribution of the sample mean looks like. This distribution tells me what kinds of mean values happen a lot, and what kinds don’t happen very often. But, I’m actually going to skip that bit. Because what I’m really interested in is what the sampling distribution of the difference between my sample means looks like. After all, I am going to run an experiment with 20 people in one condition, and 20 people in the other. Then I am going to calculate the mean for group A, and the mean for group B, and I’m going to look a the difference. I will probably find a difference, but my question is, did my manipulation cause this difference, or is this the kind of thing that happens a lot by chance. If I knew what chance can do, and how often it produces differences of particular sizes, I could look at the difference I observed, then look at what chance can do, and then I can make a decision! If my difference doesn’t happen a lot (we’ll get to how much not a lot is in a bit), then I might be willing to believe that my manipulation caused a difference. If my difference happens all the time by chance alone, then I wouldn’t be inclined to think my manipulation caused the difference, because it could have been chance.\nSo, here’s what we’ll do, even before running the experiment. We’ll do a simulation. We will sample numbers for group A and Group B, then compute the means for group A and group B, then we will find the difference in the means between group A and group B. But, we will do one very important thing. We will pretend that we haven’t actually done a manipulation. If we do this (do nothing, no manipulation that could cause a difference), then we know that only sampling error could cause any differences between the mean of group A and group B. We’ve eliminated all other causes, only chance is left. By doing this, we will be able to see exactly what chance can do. More importantly, we will see the kinds of differences that occur a lot, and the kinds that don’t occur a lot.\nBefore we do the simulation, we need to answer one question. How much is a lot? We could pick any number for a lot. I’m going to pick 10,000. That is a lot. If something happens only 1 times out 10,000, I am willing to say that is not a lot.\nOK, now we have our number, we are going to simulate the possible mean differences between group A and group B that could arise by chance. We do this 10,000 times. This gives chance a lot of opportunity to show us what it does do, and what it does not do.\nThis is what I did: I sampled 20 numbers into group A, and 20 into group B. The numbers both came from the same normal distribution, with mean = 100, and standard deviation = 15. Because the samples are coming from the same distribution, we expect that on average they will be similar (but we already know that samples differ from one another). Then, I compute the mean for each sample, and compute the difference between the means. I save the mean difference score, and end up with 10,000 of them. Then, I draw the histogram in Figure 5.16.\n\ndifference &lt;- length(10000)\nfor (i in 1:10000) {\n  difference[i] &lt;- mean(rnorm(20, 100, 15) - rnorm(20, 100, 15))\n}\nplot_df &lt;- data.frame(sim = 1:10000, difference)\nggplot(plot_df, aes(x = difference)) +\n  geom_histogram(bins = 100, color = \"white\") +\n  theme_classic() +\n  ggtitle(\n    \"Histogram of mean differences between two samples (n=10) \\n\n          both drawn from the same normal distribution (u=100, sd=20\"\n  ) +\n  xlab(\"mean difference\")\n\n\n\n\nFigure 5.16: Histogram of mean differences arising by chance.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nOf course, we might recognize that chance could do a difference greater than 15. We just didn’t give it the opportunity. We only ran the simulation 10,000 times. If we ran it a million times, maybe a difference greater than 15 or even 20 would happen a couple times. If we ran it a bazillion gazillion times, maybe a difference greater than 30 would happen a couple times. If we go out to infinity, then chance might produce all sorts of bigger differences once in a while. But, we’ve already decided that 1/10,000 is not a lot. So things that happen 0 out of 10,000 times, like differences greater than 15, are considered to be extremely unlikely.\n\n\nNow we can see what chance can do to the size of our mean difference. The x-axis shows the size of the mean difference. We took our samples from the sample distribution, so the difference between them should usually be 0, and that’s what we see in the histogram.\nPause for a second. Why should the mean differences usually be zero, wasn’t the population mean = 100, shouldn’t they be around 100? No. The mean of group A will tend to be around 100, and the mean of group B will tend be around 100. So, the difference score will tend to be 100-100 = 0. That is why we expect a mean difference of zero when the samples are drawn from the same population.\nSo, differences near zero happen the most, that’s good, that’s what we expect. Bigger or smaller differences happen increasingly less often. Differences greater than 15 or -15 never happen at all. For our purposes, it looks like chance only produces differences between -15 to 15.\nOK, let’s ask a couple simple questions. What was the biggest negative number that occurred in the simulation? We’ll use R for this. All of the 10,000 difference scores are stored in a variable I made called difference. If we want to find the minimum value, we use the min function. Here’s the result.\n\nmin(difference)\n\n[1] -18.02757\n\n\nOK, so what was the biggest positive number that occurred? Let’s use the max function to find out. It finds the biggest (maximum) value in the variable. FYI, we’ve just computed the range, the minimum and maximum numbers in the data. Remember we learned that before. Anyway, here’s the max.\n\nmax(difference)\n\n[1] 18.50951\n\n\nBoth of these extreme values only occurred once. Those values were so rare we couldn’t even see them on the histogram, the bar was so small. Also, these biggest negative and positive numbers are pretty much the same size if you ignore their sign, which makes sense because the distribution looks roughly symmetrical.\nSo, what can we say about these two numbers for the min and max? We can say the min happens 1 times out of 10,000. We can say the max happens 1 times out of 10,000. Is that a lot of times? Not to me. It’s not a lot.\nSo, how often does a difference of 30 (much larger larger than the max) occur out of 10,000. We really can’t say, 30s didn’t occur in the simulation. Going with what we got, we say 0 out of 10,000. That’s never.\nWe’re about to move into part three, which involves drawing decision lines and talking about them. The really important part about part 3 is this. What would you say if you ran this experiment once, and found a mean difference of 30? I would say it happens 0 times of out 10,000 by chance. I would say chance did not produce my difference of 30. That’s what I would say. We’re going to expand upon this right now.\n\n\n5.5.4 Part 3: Judgment and Decision-making\nRemember, we haven’t even conducted an experiment. We’re just simulating what could happen if we did conduct an experiment. We made a histogram. We can see that chance produces some differences more than others, and that chance never produced really big differences. What should we do with this information?\nWhat we are going to do is talk about judgment and decision making. What kind of judgment and decision making? Well, when you finally do run an experiment, you will get two means for group A and B, and then you will need to make some judgments, and perhaps even a decision, if you are so inclined. You will need to judge whether chance (sampling error) could have produced the difference you observed. If you judge that it did it not, you might make the decision to tell people that your experimental manipulation actually works. If you judge that it could have been chance, you might make a different decision. These are important decisions for researchers. Their careers can depend on them. Also, their decisions matter for the public. Nobody wants to hear fake news from the media about scientific findings.\nSo, what we are doing is preparing to make those judgments. We are going to draw up a plan, before we even see the data, for how we will make judgments and decisions about what we find. This kind of planning is extremely important, because we discuss in part 4, that your planning can help you design an even better experiment than the one you might have been intending to run. This kind of planning can also be used to interpret other people’s results, as a way of double-checking checking whether you believe those results are plausible.\nThe thing about judgement and decision making is that reasonable people disagree about how to do it, unreasonable people really disagree about it, and statisticians and researchers disagree about how to do it. I will propose some things that people will disagree with. That’s OK, these things still make sense. And, the disagreeable things point to important problems that are very real for any “real” statistical inference test.\nLet’s talk about some objective facts from our simulation of 10,000 things that we definitely know to be true. For example, we can draw some lines on the graph, and label some different regions. We’ll talk about two kinds of regions.\n\nRegion of chance. Chance did it. Chance could have done it\nRegion of not chance. Chance didn’t do it. Chance couldn’t have done it.\n\nThe regions are defined by the minimum value and the maximum value. Chance never produced a smaller or bigger number. The region inside the range is what chance did do, and the the region outside the range on both sides is what chance never did. It looks like Figure 5.17:\n\nggplot(plot_df, aes(x = difference)) +\n  annotate(\n    \"rect\",\n    xmin = min(difference),\n    xmax = max(difference),\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.5,\n    fill = \"red\"\n  ) +\n  geom_rect(aes(\n    xmin = -Inf,\n    xmax = min(difference),\n    ymin = 0,\n    ymax = Inf\n  ),\n  alpha = .5,\n  fill = \"lightgreen\") +\n  geom_rect(aes(\n    xmin = max(difference),\n    xmax = Inf,\n    ymin = 0,\n    ymax = Inf\n  ),\n  alpha = .5,\n  fill = \"lightgreen\") +\n  geom_histogram(bins = 50, color = \"white\") +\n  theme_classic() +\n  geom_vline(xintercept = min(difference)) +\n  geom_vline(xintercept = max(difference)) +\n  ggtitle(\n    \"Histogram of mean differences between two samples (n=10) \\n\n          both drawn from the same normal distribution (u=100, sd=20)\"\n  ) +\n  xlim(-30, 30) +\n  geom_label(data = data.frame(x = 0, y = 250, label = \"CHANCE\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = -25, y = 250, label = \"NOT \\n CHANCE\"),\n             aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = 25, y = 250, label = \"NOT \\n CHANCE\"),\n             aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = min(difference),\n    y = 750,\n    label = paste0(\"min \\n\", round(min(difference)))\n  ),\n  aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = max(difference),\n    y = 750,\n    label = paste0(\"max \\n\", round(max(difference)))\n  ),\n  aes(x = x, y = y, label = label)) +\n  xlab(\"mean difference\")\n\n\n\n\nFigure 5.17: Applying decision boundaries to the histogrm of mean differences. The boundaries identify what differences chance did or did not produce in the simulation.\n\n\n\n\nWe have just drawn some lines, and shaded some regions, and made one plan we could use to make decisions. How would the decisions work. Let’s say you ran the experiment and found a mean difference between groups A and B of 25. Where is 25 in the figure? It’s in the green part. What does the green part say? NOT CHANCE. What does this mean. It means chance never made a difference of 25. It did that 0 out of 10,000 times. If we found a difference of 25, perhaps we could confidently conclude that chance did not cause the difference. If I found a difference of 25 with this kind of data, I’d be pretty confident chance did not cause the difference; and, I would give myself license to consider that my experimental manipulation may be causing the difference.\nWhat about a difference of +10? That’s in the red part, where chance lives. Chance could have done a difference of +10 because we can see that it did do that sometimes. The red part is the window of what chance did in our simulation. Anything inside the window could have been a difference caused by chance. If I found a difference of +10, I’d say, “ya, it coulda been chance.” I would also be less confident that the difference was only caused by my experimental manipulation.\nStatistical inference could be this easy. The number you get from your experiment could be in the chance window (then you can’t rule out chance as a cause), or it could be outside the chance window (then you can rule out chance). Case closed. Let’s all go home.\n\n5.5.4.1 Grey areas\nSo what’s the problem? Depending on who you are, and what kinds of risks you’re willing to take, there might not be a problem. But, if you are just even a little bit risky then there is a problem that makes clear judgments about the role of chance difficult. We would like to say chance did or did not cause our difference. But, we’re really always in the position of admitting that it could have sometimes, or wouldn’t have most times. These are wishy washy statements, they are in between yes or no. That’s OK. Grey is a color too, let’s give grey some respect.\n“What grey areas are you talking about?, I only see red or green, am I grey blind?”. Let’s look at where some grey areas might be. I say might be, because people disagree about where the grey is. People have different comfort levels with grey. Figure 5.18 shows my opinion on grey areas.\n\nggplot(plot_df, aes(x = difference)) +\n  annotate(\n    \"rect\",\n    xmin = min(difference),\n    xmax = max(difference),\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.5,\n    fill = \"red\"\n  ) +\n  annotate(\n    \"rect\",\n    xmin = min(difference),\n    xmax = min(difference) + 10,\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.7,\n    fill = \"light grey\"\n  ) +\n  annotate(\n    \"rect\",\n    xmin = max(difference) - 10,\n    xmax = max(difference),\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.7,\n    fill = \"light grey\"\n  ) +\n  geom_rect(aes(\n    xmin = -Inf,\n    xmax = min(difference),\n    ymin = 0,\n    ymax = Inf\n  ),\n  alpha = .5,\n  fill = \"lightgreen\") +\n  geom_rect(aes(\n    xmin = max(difference),\n    xmax = Inf,\n    ymin = 0,\n    ymax = Inf\n  ),\n  alpha = .5,\n  fill = \"lightgreen\") +\n  geom_histogram(bins = 50, color = \"white\") +\n  theme_classic() +\n  geom_vline(xintercept = min(difference)) +\n  geom_vline(xintercept = max(difference)) +\n  geom_vline(xintercept = min(difference) + 10) +\n  geom_vline(xintercept = max(difference) - 10) +\n  ggtitle(\n    \"Histogram of mean differences between two samples (n=10) \\n\n          both drawn from the same normal distribution (u=100, sd=20)\"\n  ) +\n  xlim(-30, 30) +\n  geom_label(data = data.frame(x = 0, \n                               y = 250, \n                               label = \"CHANCE\"), \n             aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = -25, \n                               y = 250, \n                               label = \"NOT \\n CHANCE\"),\n             aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = 25, \n                               y = 250, \n                               label = \"NOT \\n CHANCE\"),\n             aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = min(difference),\n                               y = 750,\n                               label = paste0(\"min \\n\",round(min(difference)))\n                               ), \n             aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = max(difference),\n                               y = 750,\n                               label = paste0(\"max \\n\", round(max(difference)))\n                            ),\n                            aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = -15, y = 250,\n                               label = \"?\"),\n             aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = 15, y = 250,\n                               label = \"?\"),\n             aes(x = x, y = y, label = label)) +\n  xlab(\"mean difference\")\n\n\n\n\nFigure 5.18: The question marks refer to an area where you have some uncertainty. Differences inside the question mark region do not happen very often by chance. When you find differences of these sizes, should you reject the idea that chance caused your difference? You will always have some uncertainty associated with this decision because it is clear that chance could have caused the difference. But, chance usually does not produce differences of these sizes.\n\n\n\n\nI made two grey areas, and they are reddish grey, because we are still in the chance window. There are question marks (?) in the grey areas. Why? The question marks reflect some uncertainty that we have about those particular differences. For example, if you found a difference that was in a grey area, say a 15. 15 is less than the maximum, which means chance did create differences of around 15. But, differences of 15 don’t happen very often.\nWhat can you conclude or say about this 15 you found? Can you say without a doubt that chance did not produce the difference? Of course not, you know that chance could have. Still, it’s one of those things that doesn’t happen a lot. That makes chance an unlikely explanation. Instead of thinking that chance did it, you might be willing to take a risk and say that your experimental manipulation caused the difference. You’d be making a bet that it wasn’t chance…but, could be a safe bet, since you know the odds are in your favor.\nYou might be thinking that your grey areas aren’t the same as the ones I’ve drawn. Maybe you want to be more conservative, and make them smaller. Or, maybe you’re more risky, and would make them bigger. Or, maybe you’d add some grey area going in a little bit to the green area (after all, chance could probably produce some bigger differences sometimes, and to avoid those you would have to make the grey area go a bit into the green area).\nAnother thing to think about is your decision policy. What will you do, when your observed difference is in your grey area? Will you always make the same decision about the role of chance? Or, will you sometimes flip-flop depending on how you feel. Perhaps, you think that there shouldn’t be a strict policy, and that you should accept some level of uncertainty. The difference you found could be a real one, or it might not. There’s uncertainty, hard to avoid that.\nSo let’s illustrate one more kind of strategy for making decisions. We just talked about one that had some lines, and some regions. This makes it seem like a binary choice: we can either rule out, or not rule out the role of chance. Another perspective is that everything is a different shade of grey, like in Figure 5.19.\n\nggplot(plot_df, aes(x = difference)) +\n  # annotate(\"rect\", xmin=min(difference), xmax=max(difference), ymin=0, ymax=Inf, alpha=0.5, fill=\"red\") +\n  annotate(\n    \"rect\",\n    xmin = min(difference),\n    xmax = min(difference) + 10,\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.7,\n    fill = \"light grey\"\n  ) +\n  annotate(\n    \"rect\",\n    xmin = max(difference) - 10,\n    xmax = max(difference),\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.7,\n    fill = \"light grey\"\n  ) +\n  geom_rect(aes(\n    xmin = -Inf,\n    xmax = min(difference),\n    ymin = 0,\n    ymax = Inf\n  ),\n  alpha = .5,\n  fill = \"lightgreen\") +\n  geom_rect(aes(\n    xmin = max(difference),\n    xmax = Inf,\n    ymin = 0,\n    ymax = Inf\n  ),\n  alpha = .5,\n  fill = \"lightgreen\") +\n  geom_histogram(bins = 50, color = \"white\", aes(fill = ..count..)) +\n  theme_classic() +\n  geom_vline(xintercept = min(difference)) +\n  geom_vline(xintercept = max(difference)) +\n  geom_vline(xintercept = min(difference) + 10) +\n  geom_vline(xintercept = max(difference) - 10) +\n  ggtitle(\n    \"Histogram of mean differences between two samples (n=10) \\n\n          both drawn from the same normal distribution (u=100, sd=20)\"\n  ) +\n  xlim(-30, 30) +\n  geom_label(data = data.frame(x = 0, y = 250, label = \"CHANCE\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = -25, y = 250, label = \"NOT \\n CHANCE\"),\n             aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = 25, y = 250, label = \"NOT \\n CHANCE\"),\n             aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = min(difference),\n    y = 750,\n    label = paste0(\"min \\n\", round(min(difference)))\n  ),\n  aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = max(difference),\n    y = 750,\n    label = paste0(\"max \\n\", round(max(difference)))\n  ),\n  aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = -15, y = 250,\n                               label = \"?\"),\n             aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = 15, y = 250,\n                               label = \"?\"),\n             aes(x = x, y = y, label = label)) +\n  xlab(\"mean difference\")\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\nFigure 5.19: The shading of the blue bars indicates levels of confidence in whether a difference could have been produced by chance. Darker bars represent increased confidence that the difference was not produced by chance. Bars get darker as the mean difference increases in absolute value.\n\n\n\n\nOK, so I made it shades of blue (because it was easier in R). Now we can see two decision plans at the same time. Notice that as the bars get shorter, they also get become a darker stronger blue. The color can be used as a guide for your confidence. That is, your confidence in the belief that your manipulation caused the difference rather than chance. If you found a difference near a really dark bar, those don’t happen often by chance, so you might be really confident that chance didn’t do it. If you find a difference near a slightly lighter blue bar, you might be slightly less confident. That is all. You run your experiment, you get your data, then you have some amount of confidence that it wasn’t produced by chance. This way of thinking is elaborated to very interesting degrees in the Bayesian world of statistics. We don’t wade too much into that, but mention it a little bit here and there. It’s worth knowing it’s out there.\n\n\n5.5.4.2 Making decisions and being wrong\nNo matter how you plan to make decisions about your data, you will always be prone to making some mistakes. You might call one finding real, when in fact it was caused by chance. This is called a type I error, or a false positive. You might ignore one finding, calling it chance, when in fact it wasn’t chance (even though it was in the window). This is called a type II error, or a false negative.\nHow you make decisions can influence how often you make errors over time. If you are a researcher, you will run lots of experiments, and you will make some amount of mistakes over time. If you do something like the very strict method of only accepting results as real when they are in the “no chance” zone, then you won’t make many type I errors. Pretty much all of your result will be real. But, you’ll also make type II errors, because you will miss things real things that your decision criteria says are due to chance. The opposite also holds. If you are willing to be more liberal, and accept results in the grey as real, then you will make more type I errors, but you won’t make as many type II errors. Under the decision strategy of using these cutoff regions for decision-making there is a necessary trade-off. The Bayesian view get’s around this a little bit. Bayesians talk about updating their beliefs and confidence over time. In that view, all you ever have is some level of confidence about whether something is real, and by running more experiments you can increase or decrease your level of confidence. This, in some fashion, avoids some trade-off between type I and type II errors.\nRegardless, there is another way to reduce type I and type II errors, and to increase your confidence in your results, even before you do the experiment. It’s called “knowing how to design a good experiment”.\n\n\n\n5.5.5 Part 4: Experiment Design\nWe’ve seen what chance can do. Now we run an experiment. We manipulate something between groups A and B, get the data, calculate the group means, then look at the difference. Then we cross all of our finger and toes, and hope beyond hope that the difference is big enough to not be caused by chance. That’s a lot of hope.\nHere’s the thing, we don’t often know how strong our manipulation is in the first place. So, even if it can cause a change, we don’t necessarily know how much change it can cause. That’s why we’re running the experiment. Many manipulations in Psychology are not strong enough to cause big changes. This is a problem for detecting these smallish causal forces. In our fake example, you could easily manipulate something that has a tiny influence, and will never push the mean difference past say 5 or 10. In our simulation, we need something more like a 15 or 17 or a 21, or hey, a 30 would be great, chance never does that. Let’s say your manipulation is listening to music or not listening to music. Music listening might change something about X, but if it only changes X by +5, you’ll never be able to confidently say it wasn’t chance. And, it’s not that easy to completely change music and make music super strong in the music condition so it really causes a change in X compared to the no music condition.\nEXPERIMENT DESIGN TO THE RESCUE! Newsflash, it is often possible to change how you run your experiment so that it is more sensitive to smaller effects. How do you think we can do this? Here is a hint. It’s the stuff you learned about the sampling distribution of the sample mean, and the role of sample-size. What happens to the sampling distribution of the sample mean when N (sample size)? The distribution gets narrower and narrower, and starts to look the a single number (the hypothetical mean of the hypothetical population). That’s great. If you switch to thinking about mean difference scores, like the distribution we created in this test, what do you think will happen to that distribution as we increase N? It will will also shrink. As we increase N to infinity, it will shrink to 0. Which means that, when N is infinity, chance never produces any differences at all. We can use this.\nFor example, we could run our experiment with 20 subjects in each group. Or, we could decide to invest more time and run 40 subjects in each group, or 80, or 150. When you are the experimenter, you get to decide the design. These decisions matter big time. Basically, the more subjects you have, the more sensitive your experiment. With bigger N, you will be able to reliably detect smaller mean differences, and be able to confidently conclude that chance did not produce those small effects.\nCheck out the histograms in Figure 5.20. This is the same simulation as before, but with four different sample-sizes: 20, 40, 80, 160. We are doubling our sample-size across each simulation just to see what happens to the width of the chance window.\n\nns &lt;- c(20, 40, 80, 160)\nsims &lt;- 5000\nall_df &lt;- data.frame(sample = c(replicate(sims,mean(rnorm(ns[1], 100, 20)) - mean(rnorm(ns[1], 100, 20))),\n                                replicate(sims,mean(rnorm(ns[2], 100, 20)) - mean(rnorm(ns[2], 100, 20))),\n                                replicate(sims,mean(rnorm(ns[3], 100, 20)) - mean(rnorm(ns[3], 100, 20))),\n                                replicate(sims,mean(rnorm(ns[3], 100, 20)) - mean(rnorm(ns[3], 100, 20)))),\n                     sample_size = rep(ns, each = sims)\n)\n\nggplot(all_df, aes(x = sample)) +\n  geom_histogram(bins = 50, color = \"white\") +\n  facet_wrap( ~ sample_size) + xlim(-30, 30) +\n  theme_classic() + ggtitle(\"Sampling distribution of mean differences by N\") +\n  ylab(\"Counts\") +\n  xlab(\"Mean Difference\")\n\n\n\n\nFigure 5.20: The range or width of the differences produced by chance shrinks as sample-size increases.\n\n\n\n\nThere you have it. The sampling distribution of the mean differences shrinks toward 0 as sample-size increases. This means if you run an experiment with a larger sample-size, you will be able to detect smaller mean differences, and be confident they aren’t due to chance. Table 5.2 contains the minimum and maximum values that chance produced across the four sample-sizes:\n\n#library(plyr)\n#the_range&lt;-ddply(all_df,.(sample_size), summarize, smallest=min(sample),\n#                                                  biggest=max(sample))\nthe_range &lt;- all_df %&gt;%\n  group_by(sample_size) %&gt;%\n  summarize(smallest=min(sample),\n            largest=max(sample))\n\nknitr::kable(the_range)\n\n\n\nTable 5.2: The smallest and largest mean differences produced by chance as a function of sample-size.\n\n\nsample_size\nsmallest\nlargest\n\n\n\n\n20\n-21.01155\n20.68919\n\n\n40\n-15.81295\n15.31347\n\n\n80\n-11.32289\n10.77098\n\n\n160\n-11.92454\n10.78809\n\n\n\n\n\n\nThe table shows the range of chance behavior is very wider for smaller N and narrower for larger N. Consider what this narrowing means for your experiment design. For example, one aspect of the design is the choice of sample size, N, or in a psychology experiment the number of participants.\nIf it turns out your manipulation will cause a difference of +11, then what should you do? Run an experiment with N = 20 people? I hope not. If you did that, you could get a mean difference of +11 fairly often by chance. However, if you ran the experiment with 160 people, then you would definitely be able to say that +11 was not due to chance, it would be outside the range of what chance can do. You could even consider running the experiment with 80 subjects. A +11 there wouldn’t happen often by chance, and you’d be cost-effective, spending less time on the experiment.\nThe point is: the design of the experiment determines the sizes of the effects it can detect. If you want to detect a small effect. Make your sample size bigger. It’s really important to say this is not the only thing you can do. You can also make your cell-sizes bigger. For example, often times we take several measurements from a single subject. The more measurements you take (cell-size), the more stable your estimate of the subject’s mean. We discuss these issues more later. You can also make a stronger manipulation, when possible.\n\n\n5.5.6 Part 5: I have the power\n\nBy the power of greyskull, I HAVE THE POWER - He-man\n\nThe last topic in this section is called power. Later we will define power in terms of some particular ideas about statistical inference. Here, we will just talk about the big idea. And, we’ll show how to make sure your design has 100% power. Because, why not. Why run a design that doesn’t have the power?\nThe big idea behind power is the concept of sensitivity. The concept of sensitivity assumes that there is something to be sensitive to. That is, there is some real difference that can be measured. So, the question is, how sensitive is your experiment? We’ve already seen that the number of subjects (sample-size), changes the sensitivity of the design. More subjects = more sensitivity to smaller effects.\nLet’s take a look at one more plot. What we will do is simulate a measure of sensitivity across a whole bunch of sample sizes, from 10 to 300. We’ll do this in steps of 10. For each simulation, we’ll compute the mean differences as we have done. But, rather than showing the histogram, we’ll just compute the smallest value and the largest value. This is a pretty good measure of the outer reach of chance. Then we’ll plot those values as a function of sample size and see what we’ve got.\n\n# all_df &lt;- data.frame()\n# for (sims in 1:1000) {\n#   for (n in seq(10, 300, 10)) {\n#     sample &lt;- mean(rnorm(n, 100, 20)) - mean(rnorm(n, 100, 20))\n#     t_df &lt;- data.frame(sims = sims,\n#                        sample,\n#                        sample_size = n)\n#     all_df &lt;- rbind(all_df, t_df)\n#   }\n# }\n\nall_df &lt;- data.frame(\n  sample = c(sapply(seq(10, 300, 10), FUN = function(x) \n    replicate(1000, mean(rnorm(x,100,20) - mean(rnorm(x, 100,20)))))),\n  sample_size = rep(seq(10, 300, 10), each = 1000)\n)\n\n#the_range&lt;-ddply(all_df,.(sample_size), summarize, smallest=min(sample),\n#                                                  biggest=max(sample))\n\nthe_range &lt;- all_df %&gt;%\n  group_by(sample_size) %&gt;%\n  summarize(smallest = min(sample),\n            biggest = max(sample))\n\n\nggplot(the_range, aes(x = sample_size, y = smallest)) +\n  geom_point() +\n  geom_line() +\n  geom_point(aes(x = sample_size, y = biggest)) +\n  geom_line(aes(x = sample_size, y = biggest)) +\n  geom_hline(yintercept = 0) +\n  theme_classic(base_size=20) +\n  ylab(\"Most extreme mean difference\") +\n  xlab(\"Sample Size\")\n\n\n\n\nFigure 5.21: A graph of the maximum and minimum mean differences produced by chance as a function of sample-size. The range narrows as sample-size increases showing that chance alone produces a smaller range of mean differences as sample-size increases.\n\n\n\n\nFigure 5.21 shows a reasonably precise window of sensitivity as a function of sample size. For each sample size, we can see the maximum difference that chance produced and the minimum difference. In those simulations, chance never produced bigger or smaller differences. So, each design is sensitive to any difference that is underneath the bottom line, or above the top line.\nHere’s another way of putting it. Which of the sample sizes will be sensitive to a difference of +10 or -10. That is, if a difference of +10 or -10 was observed, then we could very confidently say that the difference was not due to chance, because according to these simulations, chance never produced differences that big. To help us see which ones are sensitive, Figure 5.22 draws horizontal lines at -10 and +10.\n\nggplot(the_range, aes(x=sample_size,y=smallest))+\n  geom_point()+\n  geom_line()+\n  geom_point(aes(x=sample_size,y=biggest))+\n  geom_line(aes(x=sample_size,y=biggest))+\n  geom_hline(yintercept=0)+\n  geom_hline(yintercept=10, color=\"red\")+\n  geom_hline(yintercept=-10, color=\"red\")+\n  theme_classic(base_size=20)+\n  ylab(\"Most extreme mean difference\")+\n  xlab(\"Sample Size\")\n\n\n\n\nFigure 5.22: The red line represents the size of a mean difference that a researcher may be interested in detecting. All of the dots outside (above or below) the red line represent designs with small sample-sizes. When a difference of 10 occurs for these designs, we can rule out chance with confidence. The dots between the red lines represent designs with larger sample-sizes. These designs never produce differences as large as 10, so when those differences occur, we can be confident chance did not produce them.\n\n\n\n\nBased on visual guesstimation, the designs with sample-size &gt;= 100 are all sensitive to real differences of 10. Designs with sample-size &gt; 100 all failed to produce extreme differences outside of the red lines by chance alone. If these designs were used, and if an effect of 10 or larger was observed, then we could be confident that chance alone did not produce the effect. Designing your experiment so that you know it is sensitive to the thing you are looking for is the big idea behind power.\n\n\n5.5.7 Summary of Crump Test\nWhat did we learn from this so-called fake Crump test that nobody uses? Well, we learned the basics of what we’ll be doing moving forward. And, we did it all without any hard math or formulas. We sampled numbers, we computed means, we subtracted means between experimental conditions, then we repeated that process many times and counted up the mean differences and put them in a histogram. This showed us what chance do in an experiment. Then, we discussed how to make decisions around these facts. And, we showed how we can control the role of chance just by changing things like sample size."
  },
  {
    "objectID": "05-Foundation_Inference.html#the-randomization-test-permutation-test",
    "href": "05-Foundation_Inference.html#the-randomization-test-permutation-test",
    "title": "5  Foundations for inference",
    "section": "5.6 The randomization test (permutation test)",
    "text": "5.6 The randomization test (permutation test)\nWelcome to the first official inferential statistic in this textbook. Up till now we have been building some intuitions for you. Next, we will get slightly more formal and show you how we can use random chance to tell us whether our experimental finding was likely due to chance or not. We do this with something called a randomization test. The ideas behind the randomization test are the very same ideas behind the rest of the inferential statistics that we will talk about in later chapters. And, surprise, we have already talked about all of the major ideas already. Now, we will just put the ideas together, and give them the name randomization test.\nHere’s the big idea. When you run an experiment and collect some data you get to find out what happened that one time. But, because you ran the experiment only once, you don’t get to find out what could have happened. The randomization test is a way of finding out what could have happened. And, once you know that, you can compare what did happen in your experiment, with what could have happened.\n\n5.6.1 Pretend example does chewing gum improve your grades?\nLet’s say you run an experiment to find out if chewing gum causes students to get better grades on statistics exams. You randomly assign 20 students to the chewing gum condition, and 20 different students to the no-chewing gum condition. Then, you give everybody statistics tests and measure their grades. If chewing gum causes better grades, then the chewing gum group should have higher grades on average than the group who did not chew gum.\nLet’s say the data looked like this:\n\ngum &lt;- round(runif(20, 70, 100))\nno_gum &lt;- round(runif(20, 40, 90))\n\ngum_df &lt;- data.frame(student = seq(1:20), gum, no_gum)\n\ngum_df &lt;- gum_df %&gt;%\n  rbind(c(\"Sums\", colSums(gum_df[, 2:3]))) %&gt;%\n  rbind(c(\"Means\", colMeans(gum_df[, 2:3])))\nknitr::kable(gum_df)\n\n\n\n\nstudent\ngum\nno_gum\n\n\n\n\n1\n90\n45\n\n\n2\n70\n76\n\n\n3\n72\n69\n\n\n4\n74\n73\n\n\n5\n89\n77\n\n\n6\n87\n49\n\n\n7\n78\n83\n\n\n8\n98\n53\n\n\n9\n96\n41\n\n\n10\n81\n55\n\n\n11\n87\n71\n\n\n12\n75\n74\n\n\n13\n80\n85\n\n\n14\n88\n89\n\n\n15\n92\n52\n\n\n16\n81\n82\n\n\n17\n72\n72\n\n\n18\n78\n72\n\n\n19\n95\n46\n\n\n20\n98\n48\n\n\nSums\n1681\n1312\n\n\nMeans\n84.05\n65.6\n\n\n\n\n\nSo, did the students chewing gum do better than the students who didn’t chew gum? Look at the mean test performance at the bottom of the table. The mean for students chewing gum was 84.05, and the mean for students who did not chew gum was 65.6. Just looking at the means, it looks like chewing gum worked!\n“STOP THE PRESSES, this is silly”. We already know this is silly because we are making pretend data. But, even if this was real data, you might think, “Chewing gum won’t do anything, this difference could have been caused by chance, I mean, maybe the better students just happened to be put into the chewing group, so because of that their grades were higher, chewing gum didn’t do anything…”. We agree. But, let’s take a closer look. We already know how the data come out. What we want to know is how they could have come out, what are all the possibilities?\nFor example, the data would have come out a bit different if we happened to have put some of the students from the gum group into the no gum group, and vice versa. Think of all the ways you could have assigned the 40 students into two groups, there are lots of ways. And, the means for each group would turn out differently depending on how the students are assigned to each group.\nPractically speaking, it’s not possible to run the experiment every possible way, that would take too long. But, we can nevertheless estimate how all of those experiments might have turned out using simulation.\nHere’s the idea. We will take the 40 measurements (exam scores) that we found for all the students. Then we will randomly take 20 of them and pretend they were in the gum group, and we’ll take the remaining 20 and pretend they were in the no gum group. Then we can compute the means again to find out what would have happened. We can keep doing this over and over again. Every time computing what happened in that version of the experiment.\n\n5.6.1.1 Doing the randomization\nBefore we do that, let’s show how the randomization part works. We’ll use fewer numbers to make the process easier to look at. Here are the first 5 exam scores for students in both groups.\n\ngum_df_small &lt;- gum_df[1:5, ]\ngum_df_small$gum &lt;- as.numeric(gum_df_small$gum)\ngum_df_small$no_gum &lt;- as.numeric(gum_df_small$no_gum)\n\ngum_df_small &lt;- gum_df_small %&gt;%\n  rbind(c(\"Sums\", colSums(gum_df_small[, 2:3]))) %&gt;%\n  rbind(c(\"Means\", colMeans(gum_df_small[, 2:3])))\nknitr::kable(gum_df_small)\n\n\n\n\nstudent\ngum\nno_gum\n\n\n\n\n1\n90\n45\n\n\n2\n70\n76\n\n\n3\n72\n69\n\n\n4\n74\n73\n\n\n5\n89\n77\n\n\nSums\n395\n340\n\n\nMeans\n79\n68\n\n\n\n\n\nThings could have turned out differently if some of the subjects in the gum group were switched with the subjects in the no gum group. Here’s how we can do some random switching. We will do this using R.\n\nall_scores       &lt;- c(gum[1:5],no_gum[1:5])\nrandomize_scores &lt;- sample(all_scores)\nnew_gum          &lt;- randomize_scores[1:5]\nnew_no_gum       &lt;- randomize_scores[6:10]\nprint(new_gum)\n\n[1] 70 77 76 72 73\n\nprint(new_no_gum)\n\n[1] 69 90 89 74 45\n\n\nWe have taken the first 5 numbers from the original data, and put them all into a variable called all_scores. Then we use the sample function in R to shuffle the scores. Finally, we take the first 5 scores from the shuffled numbers and put them into a new variable called new_gum. Then, we put the last five scores into the variable new_no_gum. Then we printed them, so we can see them.\nIf we do this a couple of times and put them in a table, we can indeed see that the means for gum and no gum would be different if the subjects were shuffled around. Check it out:\n\n\n\n\n\nstudent\ngum\nno_gum\ngum2\nno_gum2\ngum3\nno_gum3\n\n\n\n\n1\n90\n45\n90\n45\n89\n77\n\n\n2\n70\n76\n72\n77\n90\n72\n\n\n3\n72\n69\n89\n70\n45\n73\n\n\n4\n74\n73\n73\n74\n69\n76\n\n\n5\n89\n77\n69\n76\n70\n74\n\n\nSums\n395\n340\n393\n342\n363\n372\n\n\nMeans\n79\n68\n78.6\n68.4\n72.6\n74.4\n\n\n\n\n\n\n\n5.6.1.2 Simulating the mean differences across the different randomizations\nIn our pretend experiment we found that the mean for students chewing gum was 84.05, and the mean for students who did not chew gum was 65.6. The mean difference (gum - no gum) was 18.45. This is a pretty big difference. This is what did happen. But, what could have happened? If we tried out all of the experiments where different subjects were switched around, what does the distribution of the possible mean differences look like? Let’s find out. This is what the randomization test is all about.\nWhen we do our randomization test we will measure the mean difference in exam scores between the gum group and the no gum group. Every time we randomize we will save the mean difference.\nLet’s look at a short animation of what is happening in the randomization test. Figure 5.23 shows data from a different fake experiment, but the principles are the same. We’ll return to the gum no gum experiment after the animation. The animation is showing three important things. First, the purple dots show the mean scores in two groups (didn’t study vs study). It looks like there is a difference, as 1 dot is lower than the other. We want to know if chance could produce a difference this big. At the beginning of the animation, the light green and red dots show the individual scores from each of 10 subjects in the design (the purple dots are the means of these original scores). Now, during the randomizations, we randomly shuffle the original scores between the groups. You can see this happening throughout the animation, as the green and red dots appear in different random combinations. The moving yellow dots show you the new means for each group after the randomization. The differences between the yellow dots show you the range of differences that chance could produce.\n\nknitr::include_graphics(path=\"imgs/gifs/randomizationTest-1.gif\")\n\n\n\n\nFigure 5.23: Animation of a randomization test. The purple dots represent the location of the original sample means in each condition. The yellow dots represent the means of each randomized sample. The blue and red dots show how the original scores are shuffled across each randomization.\n\n\n\n\n\nstudy &lt;- round(runif(10, 80, 100))\nno_study &lt;- round(runif(10, 40, 90))\n\nstudy_df &lt;- data.frame(student = seq(1:10), study, no_study)\nmean_original &lt;- data.frame(IV = c(\"studied\", \"didnt_study\"),\n                            means = c(mean(study), mean(no_study)))\nt_df &lt;- data.frame(\n  sims = rep(1, 20),\n  IV = rep(c(\"studied\", \"didnt_study\"), each = 10),\n  values = c(study, no_study),\n  rand_order = rep(c(0, 1), each = 10)\n)\n\nraw_df &lt;- t_df\nfor (i in 2:10) {\n  #all&lt;-sample(c(study,no_study))\n  #mean_study[i]&lt;-mean(all[1:20])\n  #mean_no_study[i]&lt;-mean(all[21:40])\n  #t_df&lt;-data.frame(sims=rep(i,20),\n  #                 IV=rep(c(\"studied\",\"didnt_study\"),each=10),\n  #                 values=c(all[1:20],all[21:40]))\n  new_index &lt;- sample(1:20)\n  t_df$values &lt;- t_df$values[new_index]\n  t_df$rand_order &lt;- t_df$rand_order[new_index]\n  t_df$sims &lt;- rep(i, 20)\n  raw_df &lt;- rbind(raw_df, t_df)\n}\n\nraw_df$rand_order &lt;- as.factor(raw_df$rand_order)\nrand_df &lt;- aggregate(values ~ sims * IV, raw_df, mean)\nnames(rand_df) &lt;- c(\"sims\", \"IV\", \"means\")\n\n#rand_df &lt;- data.frame(sims=rep(1:10,2),means=c(mean_study,mean_no_study),\n#  IV=rep(c(\"studied\",\"didnt_study\"),each=10))\n\na &lt;- ggplot(raw_df, aes(\n  x = IV,\n  y = values,\n  color = rand_order,\n  size = 3\n)) +\n  geom_point(stat = \"identity\", alpha = .5) +\n  geom_point(\n    data = mean_original,\n    aes(x = IV, y = means),\n    stat = \"identity\",\n    shape = 21,\n    size = 6,\n    color = \"black\",\n    fill = \"mediumorchid2\"\n  ) +\n  geom_point(\n    data = rand_df,\n    aes(x = IV, y = means),\n    stat = \"identity\",\n    shape = 21,\n    size = 6,\n    color = \"black\",\n    fill = \"gold\"\n  ) +\n  theme_classic(base_size = 15) +\n  coord_cartesian(ylim = c(40, 100)) +\n  theme(legend.position = \"none\") +\n  ggtitle(\n    \"Randomization test: Original Means (purple),\n          \\n Randomized means (yellow)\n          \\n Original scores (red,greenish)\"\n  ) +\n  transition_states(sims,\n                    transition_length = 1,\n                    state_length = 2) + enter_fade() +\n  exit_shrink() +\n  ease_aes('sine-in-out')\n\nanimate(a, nframes = 100, fps = 5)\n\nWe are engaging in some visual statistical inference. By looking at the range of motion of the yellow dots, we are watching what kind of differences chance can produce. In this animation, the purple dots, representing the original difference, are generally outside of the range of chance. The yellow dots don’t move past the purple dots, as a result chance is an unlikely explanation of the difference.\nIf the purple dots were inside the range of the yellow dots, then when would know that chance is capable of producing the difference we observed, and that it does so fairly often. As a result, we should not conclude the manipulation caused the difference, because it could have easily occurred by chance.\nLet’s return to the gum example. After we randomize our scores many times, and computed the new means, and the mean differences, we will have loads of mean differences to look at, which we can plot in a histogram. The histogram gives a picture of what could have happened. Then, we can compare what did happen with what could have happened.\nHere’s the histogram of the mean differences from the randomization test. For this simulation, we randomized the results from the original experiment 1000 times. This is what could have happened. The blue line in Figure 5.24 shows where the observed difference lies on the x-axis.\n\nmean_differences &lt;- length(10000)\nfor (i in 1:10000) {\n  all &lt;- sample(c(gum, no_gum))\n  mean_differences[i] &lt;- mean(all[1:20]) - mean(all[21:40])\n}\n\nrand_df &lt;- data.frame(sims = 1:10000, mean_differences)\n\nggplot(rand_df, aes(x = mean_differences)) +\n  geom_histogram(color = \"white\") +\n  theme_classic() +\n  geom_vline(color = \"blue\", xintercept = (mean(gum) - mean(no_gum)))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 5.24: A histogram of simulated mean differences for a randomization test\n\n\n\n\nWhat do you think? Could the difference represented by the blue line have been caused by chance? My answer is probably not. The histogram shows us the window of chance. The blue line is not inside the window. This means we can be pretty confident that the difference we observed was not due to chance.\nWe are looking at another window of chance. We are seeing a histogram of the kinds of mean differences that could have occurred in our experiment, if we had assigned our subjects to the gum and no gum groups differently. As you can see, the mean differences range from negative to positive. The most frequent difference is 0. Also, the distribution appears to be symmetrical about zero, which shows we had roughly same the chances of getting a positive or negative difference. Also, notice that as the differences get larger (in the positive or negative direction, they become less frequent). The blue line shows us the observed difference, this is the one we found in our fake experiment. Where is it? It’s way out to the right. It is is well outside the histogram. In other words, when we look at what could have happened, we see that what did happen doesn’t occur very often.\nIMPORTANT: In this case, when we speak of what could have happened. We are talking about what could have happened by chance. When we compare what did happen to what chance could have done, we can get a better idea of whether our result was caused by chance.\n\nOK, let’s pretend we got a much smaller mean difference when we first ran the experiment. We can draw new lines (blue and red) to represent a smaller mean that we might have found.\n\nggplot(rand_df, aes(x = mean_differences)) +\n  geom_histogram(color = \"white\") +\n  theme_classic() +\n  geom_vline(color = \"blue\", xintercept = 10) +\n  geom_vline(color = \"red\", xintercept = 5)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 5.25: Would you expect a mean difference represented by the blue line to occur more or less often by chance compared to the mean difference represented by the red line?\n\n\n\n\nLook at the blue line in Figure 5.25. If you found a mean difference of 10, would you be convinced that your difference was not caused by chance? As you can see, the blue line is inside the chance window. Notably, differences of +10 don’t very often. You might infer that your difference was not likely to be due to chance (but you might be a little bit skeptical, because it could have been). How about the red line? The red line represents a difference of +5. If you found a difference of +5 here, would you be confident that your difference was not caused by chance? I wouldn’t be. The red line is totally inside the chance window, this kind of difference happens fairly often. I’d need some more evidence to consider the claim the some independent variable actually caused the difference. I’d be much more comfortable assuming that sampling error probably caused the difference.\n\n\n\n5.6.2 Take homes so far\nHave you noticed that we haven’t used any formulas yet, but we have been able to accomplish inferential statistics. We will see some formulas as we progress, but these aren’t as the idea behind the formulas.\nInferential statistics is an attempt to solve the problem: where did my data from?. In the randomization test example, our question was: where did the differences between the means in my data come from?. We know that the differences could be produced by chance alone. We simulated what chance can due using randomization. Then we plotted what chance can do using a histogram. Then, we used to picture to help us make an inference. Did our observed difference come from the distribution, or not? When the observed difference is clearly inside the chance distribution, then we can infer that our difference could have been produced by chance. When the observed difference is not clearly inside the chance distribution, then we can infer that our difference was probably not produced by chance.\nIn my opinion, these pictures are very, very helpful. If one of our goals is to help ourselves summarize a bunch of complicated numbers to arrive at an inference, then the pictures do a great job. We don’t even need a summary number, we just need to look at the picture and see if the observed difference is inside or outside of the window. This is what it is all about. Creating intuitive and meaningful ways to make inferences from our data. As we move forward, the main thing that we will do is formalize our process, and talk more about “standard” inferential statistics. For example, rather than looking at a picture (which is a good thing to do), we will create some helpful numbers. For example, what if you wanted to the probability that your difference could have been produced by chance? That could be a single number, like 95%. If there was a 95% probability that chance can produce the difference you observed, you might not be very confident that something like your experimental manipulation was causing the difference. If there was only 1% probability that chance could produce your difference, then you might be more confident that chance did not produce the difference; and, you might instead be comfortable with the possibility that your experimental manipulation actually caused the difference. So, how can we arrive at those numbers? In order to get there we will introduce you to some more foundational tools for statistical inference."
  },
  {
    "objectID": "05-Foundation_Inference.html#videos",
    "href": "05-Foundation_Inference.html#videos",
    "title": "6  Fundamentos de la inferencia",
    "section": "6.7 Videos",
    "text": "6.7 Videos\n\n6.7.1 Hipótesis nula y alternativa\n\n\n\n\n6.7.2 Tipos de Errores\n\n\n:::::"
  },
  {
    "objectID": "06-ttests.html#check-your-confidence-in-your-mean",
    "href": "06-ttests.html#check-your-confidence-in-your-mean",
    "title": "6  t-tests",
    "section": "6.1 Check your confidence in your mean",
    "text": "6.1 Check your confidence in your mean\nWe’ve talked about getting a sample of data. We know we can find the mean, we know we can find the standard deviation. We know we can look at the data in a histogram. These are all useful things to do for us to learn something about the properties of our data.\nYou might be thinking of the mean and standard deviation as very different things that we would not put together. The mean is about central tendency (where most of the data is), and the standard deviation is about variance (where most of the data isn’t). Yes, they are different things, but we can use them together to create useful new things.\nWhat if I told you my sample mean was 50, and I told you nothing else about my sample. Would you be confident that most of the numbers were near 50? Would you wonder if there was a lot of variability in the sample, and many of the numbers were very different from 50. You should wonder all of those things. The mean alone, just by itself, doesn’t tell you anything about well the mean represents all of the numbers in the sample.\nIt could be a representative number, when the standard deviation is very small, and all the numbers are close to 50. It could be a non-representative number, when the standard deviation is large, and many of the numbers are not near 50. You need to know the standard deviation in order to be confident in how well the mean represents the data.\nHow can we put the mean and the standard deviation together, to give us a new number that tells us about confidence in the mean?\nWe can do this using a ratio:\n\\(\\frac{mean}{\\text{standard deviation}}\\)\nThink about what happens here. We are dividing a number by a number. Look at what happens:\n\\(\\frac{number}{\\text{same number}} = 1\\)\n\\(\\frac{number}{\\text{smaller number}} = \\text{big number}\\)\ncompared to:\n\\(\\frac{number}{\\text{bigger number}} = \\text{smaller number}\\)\nImagine we have a mean of 50, and a truly small standard deviation of 1. What do we get with our formula?\n\\(\\frac{50}{1} = 50\\)\nImagine we have a mean of 50, and a big standard deviation of 100. What do we get with our formula?\n\\(\\frac{50}{100} = 0.5\\)\nNotice, when we have a mean paired with a small standard deviation, our formula gives us a big number, like 50. When we have a mean paired with a large standard deviation, our formula gives us a small number, like 0.5. These numbers can tell us something about confidence in our mean, in a general way. We can be 50 confident in our mean in the first case, and only 0.5 (not at a lot) confident in the second case.\nWhat did we do here? We created a descriptive statistic by dividing the mean by the standard deviation. And, we have a sense of how to interpret this number, when it’s big we’re more confident that the mean represents all of the numbers, when it’s small we are less confident. This is a useful kind of number, a ratio between what we think about our sample (the mean), and the variability in our sample (the standard deviation). Get used to this idea. Almost everything that follows in this textbook is based on this kind of ratio. We will see that our ratio turns into different kinds of “statistics”, and the ratios will look like this in general:\n\\(\\text{name of statistic} = \\frac{\\text{measure of what we know}}{\\text{measure of what we don't know}}\\)\nor, to say it using different words:\n\\(\\text{name of statistic} = \\frac{\\text{measure of effect}}{\\text{measure of error}}\\)\nIn fact, this is the general formula for the t-test. Big surprise!"
  },
  {
    "objectID": "06-ttests.html#one-sample-t-test-a-new-t-test",
    "href": "06-ttests.html#one-sample-t-test-a-new-t-test",
    "title": "6  t-tests",
    "section": "6.2 One-sample t-test: A new t-test",
    "text": "6.2 One-sample t-test: A new t-test\nNow we are ready to talk about t-test. We will talk about three of them. We start with the one-sample t-test.\nCommonly, the one-sample t-test is used to estimate the chances that your sample came from a particular population. Specifically, you might want to know whether the mean that you found from your sample, could have come from a particular population having a particular mean.\nStraight away, the one-sample t-test becomes a little confusing (and I haven’t even described it yet). Officially, it uses known parameters from the population, like the mean of the population and the standard deviation of the population. However, most times you don’t know those parameters of the population! So, you have to estimate them from your sample. Remember from the chapters on descriptive statistics and sampling, our sample mean is an unbiased estimate of the population mean. And, our sample standard deviation (the one where we divide by n-1) is an unbiased estimate of the population standard deviation. When Gosset developed the t-test, he recognized that he could use these estimates from his samples, to make the t-test. Here is the formula for the one sample t-test, we first use words, and then become more specific:\n\n6.2.1 Formulas for one-sample t-test\n\\(\\text{name of statistic} = \\frac{\\text{measure of effect}}{\\text{measure of error}}\\)\n\\(\\text{t} = \\frac{\\text{measure of effect}}{\\text{measure of error}}\\)\n\\(\\text{t} = \\frac{\\text{Mean difference}}{\\text{standard error}}\\)\n\\(\\text{t} = \\frac{\\bar{X}-u}{S_{\\bar{X}}}\\)\n\\(\\text{t} = \\frac{\\text{Sample Mean  - Population Mean}}{\\text{Sample Standard Error}}\\)\n\\(\\text{Estimated Standard Error} = \\text{Standard Error of Sample} = \\frac{s}{\\sqrt{N}}\\)\nWhere, s is the sample standard deviation.\nSome of you may have gone cross-eyed looking at all of this. Remember, we’ve seen it before when we divided our mean by the standard deviation in the first bit. The t-test is just a measure of a sample mean, divided by the standard error of the sample mean. That is it.\n\n\n6.2.2 What does t represent?\n\\(t\\) gives us a measure of confidence, just like our previous ratio for dividing the mean by a standard deviations. The only difference with \\(t\\), is that we divide by the standard error of mean (remember, this is also a standard deviation, it is the standard deviation of the sampling distribution of the mean)\n\n\n\n\n\n\nNote\n\n\n\nWhat does the t in t-test stand for? Apparently nothing. Gosset originally labelled it z. And, Fisher later called it t, perhaps because t comes after s, which is often used for the sample standard deviation.\n\n\n\\(t\\) is a property of the data that you collect. You compute it with a sample mean, and a sample standard error (there’s one more thing in the one-sample formula, the population mean, which we get to in a moment). This is why we call \\(t\\), a sample-statistic. It’s a statistic we compute from the sample.\nWhat kinds of numbers should we expect to find for these \\(ts\\)? How could we figure that out?\nLet’s start small and work through some examples. Imagine your sample mean is 5. You want to know if it came from a population that also has a mean of 5. In this case, what would \\(t\\) be? It would be zero: we first subtract the sample mean from the population mean, \\(5-5=0\\). Because the numerator is 0, \\(t\\) will be zero. So, \\(t\\) = 0, occurs, when there is no difference.\nLet’s say you take another sample, do you think the mean will be 5 every time, probably not. Let’s say the mean is 6. So, what can \\(t\\) be here? It will be a positive number, because 6-5= +1. But, will \\(t\\) be +1? That depends on the standard error of the sample. If the standard error of the sample is 1, then \\(t\\) could be 1, because 1/1 = 1.\nIf the sample standard error is smaller than 1, what happens to \\(t\\)? It gets bigger right? For example, 1 divided by 0.5 = 2. If the sample standard error was 0.5, \\(t\\) would be 2. And, what could we do with this information? Well, it be like a measure of confidence. As \\(t\\) gets bigger we could be more confident in the mean difference we are measuring.\nCan \\(t\\) be smaller than 1? Sure, it can. If the sample standard error is big, say like 2, then \\(t\\) will be smaller than one (in our case), e.g., 1/2 = .5. The direction of the difference between the sample mean and population mean, can also make the \\(t\\) become negative. What if our sample mean was 4. Well, then \\(t\\) will be negative, because the mean difference in the numerator will be negative, and the number in the bottom (denominator) will always be positive (remember why, it’s the standard error, computed from the sample standard deviation, which is always positive because of the squaring that we did.).\nSo, that is some intuitions about what the kinds of values t can take. \\(t\\) can be positive or negative, and big or small.\nLet’s do one more thing to build our intuitions about what \\(t\\) can look like. How about we sample some numbers and then measure the sample mean and the standard error of the mean, and then plot those two things against each each. This will show us how a sample mean typically varies with respect to the standard error of the mean.\nIn Figure 6.1, I pulled 1,000 samples of \\(N = 10\\) from a normal distribution (mean = 0, sd = 1). Each time I measured the mean and standard error of the sample. That gave two descriptive statistics for each sample, letting us plot each sample as dot in a scatter plot.\n\n\n\n\n\nFigure 6.1: A scatter plot with sample mean on the x-axis, and standard error of the mean on the y-axis\n\n\n\n\nWhat we get is a cloud of dots. You might notice the cloud has a circular quality. There’s more dots in the middle, and fewer dots as they radiate out from the middle. The dot cloud shows us the general range of the sample mean, for example most of the dots are in between -1 and 1. Similarly, the range for the sample standard error is roughly between .2 and .5. Remember, each dot represents one sample.\nWe can look at the same data a different way. For example, rather than using a scatter plot, we can divide the mean for each dot by the standard error for each dot. Figure 6.2 shows the result in a histogram.\n\n\n\n\n\nFigure 6.2: A histogram of the sample means divided by the sample standard errors, this is a t-distribution.\n\n\n\n\nInteresting, we can see the histogram is shaped like a normal curve. It is centered on 0, which is the most common value. As values become more extreme, they become less common. If you remember, our formula for \\(t\\), was the mean divided by the standard error of the mean. That’s what we did here. This histogram is showing you a \\(t\\)-distribution.\n\n\n6.2.3 Calculating t from data\nLet’s briefly calculate a t-value from a small sample. Let’s say we had 10 students do a true/false quiz with 5 questions on it. There’s a 50% chance of getting each answer correct.\nEvery student completes the 5 questions, we grade them, and then we find their performance (mean percent correct). What we want to know is whether the students were guessing. If they were all guessing, then the sample mean should be about 50%, it shouldn’t be different from chance, which is 50%. Let’s look at Table 6.1.\n\n\n\n\nTable 6.1: Calculating the t-value for a one-sample test.\n\n\nstudents\nscores\nmean\nDifference_from_Mean\nSquared_Deviations\n\n\n\n\n1\n50\n61\n-11\n121\n\n\n2\n70\n61\n9\n81\n\n\n3\n60\n61\n-1\n1\n\n\n4\n40\n61\n-21\n441\n\n\n5\n80\n61\n19\n361\n\n\n6\n30\n61\n-31\n961\n\n\n7\n90\n61\n29\n841\n\n\n8\n60\n61\n-1\n1\n\n\n9\n70\n61\n9\n81\n\n\n10\n60\n61\n-1\n1\n\n\nSums\n610\n610\n0\n2890\n\n\nMeans\n61\n61\n0\n289\n\n\n\n\n\nsd\n17.92\n\n\n\n\n\nSEM\n5.67\n\n\n\n\n\nt\n1.94003527336861\n\n\n\n\n\n\nYou can see the scores column has all of the test scores for each of the 10 students. We did the things we need to do to compute the standard deviation.\nRemember the sample standard deviation is the square root of the sample variance, or:\n\\(\\text{sample standard deviation} = \\sqrt{\\frac{\\sum_{i}^{n}({x_{i}-\\bar{x})^2}}{N-1}}\\)\n\\(\\text{sd} = \\sqrt{\\frac{2890}{10-1}} = 17.92\\)\nThe standard error of the mean, is the standard deviation divided by the square root of N\n\\(\\text{SEM} = \\frac{s}{\\sqrt{N}} = \\frac{17.92}{10} = 5.67\\)\n\\(t\\) is the difference between our sample mean (61), and our population mean (50, assuming chance), divided by the standard error of the mean.\n\\(\\text{t} = \\frac{\\bar{X}-u}{S_{\\bar{X}}} = \\frac{\\bar{X}-u}{SEM} = \\frac{61-50}{5.67} = 1.94\\)\nAnd, that is you how calculate \\(t\\), by hand. It’s a pain. I was annoyed doing it this way. In the lab, you learn how to calculate \\(t\\) using software, so it will just spit out \\(t\\). For example in R, all you have to do is this:\n\nt.test(scores, mu=50)\n#&gt; \n#&gt;  One Sample t-test\n#&gt; \n#&gt; data:  scores\n#&gt; t = 1.9412, df = 9, p-value = 0.08415\n#&gt; alternative hypothesis: true mean is not equal to 50\n#&gt; 95 percent confidence interval:\n#&gt;  48.18111 73.81889\n#&gt; sample estimates:\n#&gt; mean of x \n#&gt;        61\n\n\n\n6.2.4 How does t behave?\nIf \\(t\\) is just a number that we can compute from our sample (it is), what can we do with it? How can we use \\(t\\) for statistical inference?\nRemember back to the chapter on sampling and distributions, that’s where we discussed the sampling distribution of the sample mean. Remember, we made a lot of samples, then computed the mean for each sample, then we plotted a histogram of the sample means. Later, in that same section, we mentioned that we could generate sampling distributions for any statistic. For each sample, we could compute the mean, the standard deviation, the standard error, and now even \\(t\\), if we wanted to. We could generate 10,000 samples, and draw four histograms, one for each sampling distribution for each statistic.\nThis is exactly what I did, and the results are shown in the four panels of Figure 6.3 below. I used a sample size of 20, and drew random observations for each sample from a normal distribution, with mean = 0, and standard deviation = 1. Let’s look at the sampling distributions for each of the statistics. \\(t\\) was computed assuming with the population mean assumed to be 0.\n\n\n\n\n\nFigure 6.3: Sampling distributions for the mean, standard deviation, standard error of the mean, and \\(t\\).\n\n\n\n\nWe see four sampling distributions. This is how statistical summaries of these summaries behave. We have used the word chance windows before. These are four chance windows, measuring different aspects of the sample. In this case, all of the samples came from the same normal distribution. Because of sampling error, each sample is not identical. The means are not identical, the standard deviations are not identical, sample standard error of the means are not identical, and the \\(t\\)s of the samples are not identical. They all have some variation, as shown by the histograms. This is how samples of size 20 behave.\nWe can see straight away, that in this case, we are unlikely to get a sample mean of 2. That’s way outside the window. The range for the sampling distribution of the mean is around -.5 to +.5, and is centered on 0 (the population mean, would you believe!).\nWe are unlikely to get sample standard deviations of between .6 and 1.5, that is a different range, specific to the sample standard deviation.\nSame thing with the sample standard error of the mean, the range here is even smaller, mostly between .1, and .3. You would rarely find a sample with a standard error of the mean greater than .3. Virtually never would you find one of say 1 (for this situation).\nNow, look at \\(t\\). It’s range is basically between -3 and +3 here. 3s barely happen at all. You pretty much never see a 5 or -5 in this situation.\nAll of these sampling windows are chance windows, and they can all be used in the same way as we have used similar sampling distributions before (e.g., Crump Test, and Randomization Test) for statistical inference. For all of them we would follow the same process:\n\nGenerate these distributions\nLook at your sample statistics for the data you have (mean, SD, SEM, and \\(t\\))\nFind the likelihood of obtaining that value or greater\nObtain that probability\nSee if you think your sample statistics were probable or improbable.\n\nWe’ll formalize this in a second. I just want you to know that what you will be doing is something that you have already done before. For example, in the Crump test and the Randomization test we focused on the distribution of mean differences. We could do that again here, but instead, we will focus on the distribution of \\(t\\) values. We then apply the same kinds of decision rules to the \\(t\\) distribution, as we did for the other distributions. Below you will see a graph you have already seen, except this time it is a distribution of \\(t\\)s, not mean differences:\nRemember, if we obtained a single \\(t\\) from one sample we collected, we could consult the chance window in Figure 6.4 below to find out whether the \\(t\\) we obtained from the sample was likely or unlikely to occur by chance.\n\n\n\n\n\nFigure 6.4: Applying decision criteria to the \\(t\\)-distribution. Histogram of \\(t\\)s from samples (n=20) drawn from the same normal distribution (u=0, sd=1)\n\n\n\n\n\n\n6.2.5 Making a decision\nFrom our early example involving the TRUE/FALSE quizzes, we are now ready to make some kind of decision about what happened there. We found a mean difference of 11. We found a \\(t\\) = 1.9411765. The probability of this \\(t\\) or larger occurring is \\(p\\) = 0.0841503. We were testing the idea that our sample mean of 61 could have come from a normal distribution with mean = 50. The \\(t\\) test tells us that the \\(t\\) for our sample, or a larger one, would happen with p = 0.0841503. In other words, chance can do it a kind of small amount of time, but not often. In English, this means that all of the students could have been guessing, but it wasn’t that likely that were just guessing.\nThe next \\(t\\)-test is called a paired samples t-test. And, spoiler alert, we will find out that a paired samples t-test is actually a one-sample t-test in disguise (WHAT!), yes it is. If the one-sample \\(t\\)-test didn’t make sense to you, read the next section."
  },
  {
    "objectID": "06-ttests.html#paired-samples-t-test",
    "href": "06-ttests.html#paired-samples-t-test",
    "title": "6  t-tests",
    "section": "6.3 Paired-samples t-test",
    "text": "6.3 Paired-samples t-test\nFor me (Crump), many analyses often boil down to a paired samples t-test. It just happens that many things I do reduce down to a test like this.\nI am a cognitive psychologist, I conduct research about how people do things like remember, pay attention, and learn skills. There are lots of Psychologists like me, who do very similar things.\nWe all often conduct the same kinds of experiments. They go like this, and they are called repeated measures designs. They are called repeated measures designs, because we measure how one person does something more than once, we repeat the measure.\nSo, I might measure somebody doing something in condition A, and measure the same person doing something in Condition B, and then I see that same person does different things in the two conditions. I repeatedly measure the same person in both conditions. I am interested in whether the experimental manipulation changes something about how people perform the task in question.\n\n6.3.1 Mehr, Song, and Spelke (2016)\nWe will introduce the paired-samples t-test with an example using real data, from a real study. Mehr, Song, and Spelke (2016) were interested in whether singing songs to infants helps infants become more sensitive to social cues. For example, infants might need to learn to direct their attention toward people as a part of learning how to interact socially with people. Perhaps singing songs to infants aids this process of directing attention. When an infant hears a familiar song, they might start to pay more attention to the person singing that song, even after they are done singing the song. The person who sang the song might become more socially important to the infant. You will learn more about this study in the lab for this week. This example, prepares you for the lab activities. Here is a brief summary of what they did.\nFirst, parents were trained to sing a song to their infants. After many days of singing this song to the infants, a parent came into the lab with their infant. In the first session, parents sat with their infants on their knees, so the infant could watch two video presentations. There were two videos. Each video involved two unfamiliar new people the infant had never seen before. Each new person in the video (the singers) sang one song to the infant. One singer sang the “familiar” song the infant had learned from their parents. The other singer sang an “unfamiliar” song the infant had not hear before.\nThere were two really important measurement phases: the baseline phase, and the test phase.\nThe baseline phase occurred before the infants saw and heard each singer sing a song. During the baseline phase, the infants watched a video of both singers at the same time. The researchers recorded the proportion of time that the infant looked at each singer. The baseline phase was conducted to determine whether infants had a preference to look at either person (who would later sing them a song).\nThe test phase occurred after infants saw and heard each song, sung by each singer. During the test phase, each infant had an opportunity to watch silent videos of both singers. The researchers measured the proportion of time the infants spent looking at each person. The question of interest, was whether the infants would spend a greater proportion of time looking at the singer who sang the familiar song, compared to the singer who sang the unfamiliar song.\nThere is more than one way to describe the design of this study. We will describe it like this. It was a repeated measures design, with one independent (manipulation) variable called Viewing phase: Baseline versus Test. There was one dependent variable (the measurement), which was proportion looking time (to singer who sung familiar song). This was a repeated measures design because the researchers measured proportion looking time twice (they repeated the measure), once during baseline (before infants heard each singer sing a song), and again during test (after infants head each singer sing a song).\nThe important question was whether infants would change their looking time, and look more at the singer who sang the familiar song during the test phase, than they did during the baseline phase. This is a question about a change within individual infants. In general, the possible outcomes for the study are:\n\nNo change: The difference between looking time toward the singer of the familiar song during baseline and test is zero, no difference.\nPositive change: Infants will look longer toward the singer of the familiar song during the test phase (after they saw and heard the singers), compared to the baseline phase (before they saw and heard the singers). This is a positive difference if we use the formula: Test Phase Looking time - Baseline phase looking time (to familiar song singer).\nNegative change: Infants will look longer toward the singer of the unfamiliar song during the test phase (after they saw and heard the singers), compared to the baseline phase (before they saw and heard the singers). This is a negative difference if we use the same formula: Test Phase Looking time - Baseline phase looking time (to familiar song singer).\n\n\n\n6.3.2 The data\nLet’s take a look at the data for the first 5 infants in the study. This will help us better understand some properties of the data before we analyze it. We will see that the data is structured in a particular way that we can take advantage of with a paired samples t-test. Note, we look at the first 5 infants to show how the computations work. The results of the paired-samples t-test change when we use all of the data from the study.\nHere is a table of the data:\n\n\n\n\n\ninfant\nBaseline\nTest\n\n\n\n\n1\n0.44\n0.60\n\n\n2\n0.41\n0.68\n\n\n3\n0.75\n0.72\n\n\n4\n0.44\n0.28\n\n\n5\n0.47\n0.50\n\n\n\n\n\nThe table shows proportion looking times toward the singer of the familiar song during the Baseline and Test phases. Notice there are five different infants, (1 to 5). Each infant is measured twice, once during the Baseline phase, and once during the Test phase. To repeat from before, this is a repeated-measures design, because the infants are measured repeatedly (twice in this case). Or, this kind of design is also called a paired-samples design. Why? because each participant comes with a pair of samples (two samples), one for each level of the design.\nGreat, so what are we really interested in here? We want to know if the mean looking time toward the singer of the familiar song for the Test phase is higher than the Baseline phase. We are comparing the two sample means against each other and looking for a difference. We already know that differences could be obtained by chance alone, simply because we took two sets of samples, and we know that samples can be different. So, we are interested in knowing whether chance was likely or unlikely to have produced any difference we might observe.\nIn other words, we are interested in looking at the difference scores between the baseline and test phase for each infant. The question here is, for each infant, did their proportion looking time to the singer of the familiar song, increase during the test phase as compared to the baseline phase.\n\n\n6.3.3 The difference scores\nLet’s add the difference scores to the table of data so it is easier to see what we are talking about. The first step in creating difference scores is to decide how you will take the difference, there are two options:\n\nTest phase score - Baseline Phase Score\nBaseline phase score - Test Phase score\n\nLet’s use the first formula. Why? Because it will give us positive differences when the test phase score is higher than the baseline phase score. This makes a positive score meaningful with respect to the study design, we know (because we defined it to be this way), that positive scores will refer to longer proportion looking times (to singer of familiar song) during the test phase compared to the baseline phase.\n\n\n\n\n\ninfant\nBaseline\nTest\ndifferences\n\n\n\n\n1\n0.44\n0.60\n0.16\n\n\n2\n0.41\n0.68\n0.27\n\n\n3\n0.75\n0.72\n-0.03\n\n\n4\n0.44\n0.28\n-0.16\n\n\n5\n0.47\n0.50\n0.03\n\n\n\n\n\nThere we have it, the difference scores. The first thing we can do here is look at the difference scores, and ask how many infants showed the effect of interest. Specifically, how many infants showed a positive difference score. We can see that three of five infants showed a positive difference (they looked more at the singer of the familiar song during the test than baseline phase), and two the infants showed the opposite effect (negative difference, they looked more at the singer of the familiar song during baseline than test).\n\n\n6.3.4 The mean difference\nAs we have been discussing, the effect of interest in this study is the mean difference between the baseline and test phase proportion looking times. We can calculate the mean difference, by finding the mean of the difference scores. Let’s do that, in fact, for fun let’s calculate the mean of the baseline scores, the test scores, and the difference scores.\n\n\n\n\n\ninfant\nBaseline\nTest\ndifferences\n\n\n\n\n1\n0.44\n0.6\n0.16\n\n\n2\n0.41\n0.68\n0.27\n\n\n3\n0.75\n0.72\n-0.03\n\n\n4\n0.44\n0.28\n-0.16\n\n\n5\n0.47\n0.5\n0.03\n\n\nSums\n2.51\n2.78\n0.27\n\n\nMeans\n0.502\n0.556\n0.054\n\n\n\n\n\nWe can see there was a positive mean difference of 0.054, between the test and baseline phases.\nCan we rush to judgment and conclude that infants are more socially attracted to individuals who have sung them a familiar song? I would hope not based on this very small sample. First, the difference in proportion looking isn’t very large, and of course we recognize that this difference could have been produced by chance.\nWe will more formally evaluate whether this difference could have been caused by chance with the paired-samples t-test. But, before we do that, let’s again calculate \\(t\\) and discuss what \\(t\\) tells us over and above what our measure of the mean of the difference scores tells us.\n\n\n6.3.5 Calculate t\nOK, so how do we calculate \\(t\\) for a paired-samples \\(t\\)-test? Surprise, we use the one-sample t-test formula that you already learned about! Specifically, we use the one-sample \\(t\\)-test formula on the difference scores. We have one sample of difference scores (you can see they are in one column), so we can use the one-sample \\(t\\)-test on the difference scores. Specifically, we are interested in comparing whether the mean of our difference scores came from a distribution with mean difference = 0. This is a special distribution we refer to as the null distribution. It is the distribution no differences. Of course, this null distribution can produce differences due to to sampling error, but those differences are not caused by any experimental manipulation, they caused by the random sampling process.\nWe calculate \\(t\\) in a moment. Let’s now consider again why we want to calculate \\(t\\)? Why don’t we just stick with the mean difference we already have?\nRemember, the whole concept behind \\(t\\), is that it gives an indication of how confident we should be in our mean. Remember, \\(t\\) involves a measure of the mean in the numerator, divided by a measure of variation (standard error of the sample mean) in the denominator. The resulting \\(t\\) value is small when the mean difference is small, or when the variation is large. So small \\(t\\)-values tell us that we shouldn’t be that confident in the estimate of our mean difference. Large \\(t\\)-values occur when the mean difference is large and/or when the measure of variation is small. So, large \\(t\\)-values tell us that we can be more confident in the estimate of our mean difference. Let’s find \\(t\\) for the mean difference scores. We use the same formulas as we did last time:\n\n\n\n\n\n\n\n\n\n\n\n\n\ninfant\nBaseline\nTest\ndifferences\ndiff_from_mean\nSquared_differences\n\n\n\n\n1\n0.44\n0.6\n0.16\n0.106\n0.011236\n\n\n2\n0.41\n0.68\n0.27\n0.216\n0.046656\n\n\n3\n0.75\n0.72\n-0.03\n-0.084\n0.00705600000000001\n\n\n4\n0.44\n0.28\n-0.16\n-0.214\n0.045796\n\n\n5\n0.47\n0.5\n0.03\n-0.024\n0.000575999999999999\n\n\nSums\n2.51\n2.78\n0.27\n0\n0.11132\n\n\nMeans\n0.502\n0.556\n0.054\n0\n0.022264\n\n\n\n\n\n\nsd\n0.167\n\n\n\n\n\n\nSEM\n0.075\n\n\n\n\n\n\nt\n0.72\n\n\n\n\n\nIf we did this test using R, we would obtain almost the same numbers (there is a little bit of rounding in the table).\n\n#&gt; \n#&gt;  One Sample t-test\n#&gt; \n#&gt; data:  differences\n#&gt; t = 0.72381, df = 4, p-value = 0.5092\n#&gt; alternative hypothesis: true mean is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  -0.1531384  0.2611384\n#&gt; sample estimates:\n#&gt; mean of x \n#&gt;     0.054\n\nHere is a quick write up of our t-test results, t(4) = .72, p = .509.\nWhat does all of that tell us? There’s a few things we haven’t gotten into much yet. For example, the 4 represents degrees of freedom, which we discuss later. The important part, the \\(t\\) value should start to be a little bit more meaningful. We got a kind of small t-value didn’t we. It’s .72. What can we tell from this value? First, it is positive, so we know the mean difference is positive. The sign of the \\(t\\)-value is always the same as the sign of the mean difference (ours was +0.054). We can also see that the p-value was .509. We’ve seen p-values before. This tells us that our \\(t\\) value or larger, occurs about 50.9% of the time… Actually it means more than this. And, to understand it, we need to talk about the concept of two-tailed and one-tailed tests.\n\n\n6.3.6 Interpreting \\(t\\)s\nRemember what it is we are doing here. We are evaluating whether our sample data could have come from a particular kind of distribution. The null distribution of no differences. This is the distribution of \\(t\\)-values that would occur for samples of size 5, with a mean difference of 0, and a standard error of the sample mean of .075 (this is the SEM that we calculated from our sample). We can see what this particular null-distribution looks like in Figure 6.5.\n\n\n\n\n\nFigure 6.5: A distribution of \\(t\\)-values that can occur by chance alone, when there is no difference between the sample and a population\n\n\n\n\nThe \\(t\\)-distribution above shows us the kinds of values \\(t\\) will will take by chance alone, when we measure the mean differences for pairs of 5 samples (like our current). \\(t\\) is most likely to be zero, which is good, because we are looking at the distribution of no-differences, which should most often be 0! But, sometimes, due to sampling error, we can get \\(t\\)s that are bigger than 0, either in the positive or negative direction. Notice the distribution is symmetrical, a \\(t\\) from the null-distribution will be positive half of the time, and negative half of the time, that is what we would expect by chance.\nSo, what kind of information do we want know when we find a particular \\(t\\) value from our sample? We want to know how likely the \\(t\\) value like the one we found occurs just by chance. This is actually a subtly nuanced kind of question. For example, any particular \\(t\\) value doesn’t have a specific probability of occurring. When we talk about probabilities, we are talking about ranges of probabilities. Let’s consider some probabilities. We will use the letter \\(p\\), to talk about the probabilities of particular \\(t\\) values.\n\nWhat is the probability that \\(t\\) is zero or positive or negative? The answer is p=1, or 100%. We will always have a \\(t\\) value that is zero or non-zero…Actually, if we can’t compute the t-value, for example when the standard deviation is undefined, I guess then we would have a non-number. But, assuming we can calculate \\(t\\), then it will always be 0 or positive or negative.\nWhat is the probability of \\(t\\) = 0 or greater than 0? The answer is p=.5, or 50%. 50% of \\(t\\)-values are 0 or greater.\nWhat is the of \\(t\\) = 0 or smaller than 0? The answer is p=.5, or 50%. 50% of \\(t\\)-values are 0 or smaller.\n\nWe can answer all of those questions just by looking at our t-distribution, and dividing it into two equal regions, the left side (containing 50% of the \\(t\\) values), and the right side containing 50% of the \\(t\\)-values).\nWhat if we wanted to take a more fine-grained approach, let’s say we were interested in regions of 10%. What kinds of \\(t\\)s occur 10% of the time. We would apply lines like the following. Notice, the likelihood of bigger numbers (positive or negative) gets smaller, so we have to increase the width of the bars for each of the intervals between the bars to contain 10% of the \\(t\\)-values, it looks like Figure 6.6.\n\n\n\n\n\nFigure 6.6: Splitting the t distribution up into regions each containing 10% of the \\(t\\)-values. The width between the bars narrows as they approach the center of the distribution, where there are more \\(t\\)-values.\n\n\n\n\nConsider the probabilities (\\(p\\)) of \\(t\\) for the different ranges.\n\n\\(t\\) &lt;= -1.5 (\\(t\\) is less than or equal to -1.5), \\(p\\) = 10%\n-1.5 &gt;= \\(t\\) &lt;= -0.9 (\\(t\\) is equal to or between -1.5 and -.9), \\(p\\) = 10%\n-.9 &gt;= \\(t\\) &lt;= -0.6 (\\(t\\) is equal to or between -.9 and -.6), \\(p\\) = 10%\n\\(t\\) &gt;= 1.5 (\\(t\\) is greater than or equal to 1.5), \\(p\\) = 10%\n\nNotice, that the \\(p\\)s are always 10%. \\(t\\)s occur in these ranges with 10% probability.\n\n\n6.3.7 Getting the p-values for \\(t\\)-values\nYou might be wondering where I am getting some of these values from. For example, how do I know that 10% of \\(t\\) values (for this null distribution) have a value of approximately 1.5 or greater than 1.5? The answer is I used R to tell me.\nIn most statistics textbooks the answer would be: there is a table at the back of the book where you can look these things up…This textbook has no such table. We could make one for you. And, we might do that. But, we didn’t do that yet…\nSo, where do these values come from, how can you figure out what they are? The complicated answer is that we are not going to explain the math behind finding these values because, 1) the authors (some of us) admittedly don’t know the math well enough to explain it, and 2) it would sidetrack us to much, 3) you will learn how to get these numbers in the lab with software, 4) you will learn how to get these numbers in lab without the math, just by doing a simulation, and 5) you can do it in R, or excel, or you can use an online calculator.\nThis is all to say that you can find the \\(t\\)s and their associated \\(p\\)s using software. But, the software won’t tell you what these values mean. That’s we are doing here. You will also see that software wants to know a few more things from you, such as the degrees of freedom for the test, and whether the test is one-tailed or two tailed. We haven’t explained any of these things yet. That’s what we are going to do now. Note, we explain degrees of freedom last. First, we start with a one-tailed test.\n\n\n6.3.8 One-tailed tests\nA one-tailed test is sometimes also called a directional test. It is called a directional test, because a researcher might have a hypothesis in mind suggesting that the difference they observe in their means is going to have a particular direction, either a positive difference, or a negative difference.\nTypically, a researcher would set an alpha criterion. The alpha criterion describes a line in the sand for the researcher. Often, the alpha criterion is set at \\(p = .05\\). What does this mean? Figure 6.7 shows the \\(t\\)-distribution and the alpha criterion.\n\n\n\n\n\nFigure 6.7: The critical value of t for an alpha criterion of 0.05. 5% of all ts are at this value or larger\n\n\n\n\nThe figure shows that \\(t\\) values of +2.13 or greater occur 5% of the time. Because the t-distribution is symmetrical, we also know that \\(t\\) values of -2.13 or smaller also occur 5% of the time. Both of these properties are true under the null distribution of no differences. This means, that when there really are no differences, a researcher can expect to find \\(t\\) values of 2.13 or larger 5% of the time.\nLet’s review and connect some of the terms:\n\nalpha criterion: the criterion set by the researcher to make decisions about whether they believe chance did or did not cause the difference. The alpha criterion here is set to \\(p = .05\\).\nCritical \\(t\\). The critical \\(t\\) is the \\(t\\)-value associated with the alpha-criterion. In this case for a one-tailed test, it is the \\(t\\) value where 5% of all \\(t\\)s are this number or greater. In our example, the critical \\(t\\) is 2.13. 5% of all \\(t\\) values (with degrees of freedom = 4) are +2.13, or greater than +2.13.\nObserved \\(t\\). The observed \\(t\\) is the one that you calculated from your sample. In our example about the infants, the observed \\(t\\) was \\(t\\) (4) = 0.72.\np-value. The \\(p\\)-value is the probability of obtaining the observed \\(t\\) value or larger. Now, you could look back at our previous example, and find that the \\(p\\)-value for \\(t\\) (4) = .72, was \\(p = .509\\) . HOWEVER, this p-value was not calculated for a one-directional test…(we talk about what .509 means in the next section).\n\nFigure 6.8 shows what the \\(p\\)-value for \\(t\\) (4) = .72 using a one-directional test would would look like:\n\n\n\n\n\nFigure 6.8: A case where the observed value of t is much less than the critical value for a one-directional t-test.\n\n\n\n\nLet’s take this one step at a time. We have located the observed \\(t\\) of .72 on the graph. We shaded the right region all grey. What we see is that the grey region represents .256 or 25.6% of all \\(t\\) values. In other words, 25.6% of \\(t\\) values are .72 or larger than .72. You could expect, by chance alone, to a find a \\(t\\) value of .72 or larger, 25.6% of the time. That’s fairly often. We did find a \\(t\\) value of .72. Now that you know this kind of \\(t\\) value or larger occurs 25.6% of the time, would you be confident that the mean difference was not due to chance? Probably not, given that chance can produce this difference fairly often.\nFollowing the “standard” decision making procedure, we would claim that our \\(t\\) value was not statistically significant, because it was not large enough. If our observed value was larger than the critical \\(t\\) (larger than 2.13), defined by our alpha criterion, then we would claim that our \\(t\\) value was statistically significant. This would be equivalent to saying that we believe it is unlikely that the difference we observed was due to chance. In general, for any observed \\(t\\) value, the associated \\(p\\)-value tells you how likely a \\(t\\) of the observed size or larger would be observed. The \\(p\\)-value always refers to a range of \\(t\\)-values, never to a single \\(t\\)-value. Researchers use the alpha criterion of .05, as a matter of convenience and convention. There are other ways to interpret these values that do not rely on a strict (significant versus not) dichotomy.\n\n\n6.3.9 Two-tailed tests\nOK, so that was one-tailed tests… What are two tailed tests? The \\(p\\)-value that we originally calculated from our paired-samples \\(t\\)-test was for a 2-tailed test. Often, the default is that the \\(p\\)-value is for a two-tailed test.\nThe two-tailed test, is asking a more general question about whether a difference is likely to have been produced by chance. The question is: what is probability of any difference. It is also called a non-directional test, because here we don’t care about the direction or sign of the difference (positive or negative), we just care if there is any kind of difference.\nThe same basic things as before are involved. We define an alpha criterion (\\(\\alpha = 0.05\\)). And, we say that any observed \\(t\\) value that has a probability of \\(p\\) &lt;.05 (\\(p\\) is less than .05) will be called statistically significant, and ones that are more likely (\\(p\\) &gt;.05, \\(p\\) is greater than .05) will be called null-results, or not statistically significant. The only difference is how we draw the alpha range. Before it was on the right side of the \\(t\\) distribution (we were conducting a one-sided test remember, so we were only interested in one side).\nFigure 6.9 shows what the most extreme 5% of the \\(t\\)-values are when we ignore their sign (whether they are positive or negative).\n\n\n\n\n\nFigure 6.9: Critical values for a two-tailed test. Each line represents the location where 2.5% of all \\(t\\)s are larger or smaller than critical value. The total for both tails is 5%\n\n\n\n\nHere is what we are seeing. A distribution of no differences (the null, which is what we are looking at), will produce \\(t\\)s that are 2.78 or greater 2.5% of the time, and \\(t\\)s that are -2.78 or smaller 2.5% of the time. 2.5% + 2.5% is a total of 5% of the time. We could also say that \\(t\\)s larger than +/- 2.78 occur 5% of the time.\nAs a result, the critical \\(t\\) value is (+/-) 2.78 for a two-tailed test. As you can see, the two-tailed test is blind to the direction or sign of the difference. Because of this, the critical \\(t\\) value is also higher for a two-tailed test, than for the one-tailed test that we did earlier. Hopefully, now you can see why it is called a two-tailed test. There are two tails of the distribution, one on the left and right, both shaded in green.\n\n\n6.3.10 One or two tailed, which one?\nNow that you know there are two kinds of tests, one-tailed, and two-tailed, which one should you use? There is some conventional wisdom on this, but also some debate. In the end, it is up to you to be able to justify your choice and why it is appropriate for you data. That is the real answer.\nThe conventional answer is that you use a one-tailed test when you have a theory or hypothesis that is making a directional prediction (the theory predicts that the difference will be positive, or negative). Similarly, use a two-tailed test when you are looking for any difference, and you don’t have a theory that makes a directional prediction (it just makes the prediction that there will be a difference, either positive or negative).\nAlso, people appear to choose one or two-tailed tests based on how risky they are as researchers. If you always ran one-tailed tests, your critical \\(t\\) values for your set alpha criterion would always be smaller than the critical \\(t\\)s for a two-tailed test. Over the long run, you would make more type I errors, because the criterion to detect an effect is a lower bar for one than two tailed tests.\n\nRemember type 1 errors occur when you reject the idea that chance could have caused your difference. You often never know when you make this error. It happens anytime that sampling error was the actual cause of the difference, but a researcher dismisses that possibility and concludes that their manipulation caused the difference.\n\nSimilarly, if you always ran two-tailed tests, even when you had a directional prediction, you would make fewer type I errors over the long run, because the \\(t\\) for a two-tailed test is higher than the \\(t\\) for a one-tailed test. It seems quite common for researchers to use a more conservative two-tailed test, even when they are making a directional prediction based on theory. In practice, researchers tend to adopt a standard for reporting that is common in their field. Whether or not the practice is justifiable can sometimes be an open question. The important task for any researcher, or student learning statistics, is to be able to justify their choice of test.\n\n\n6.3.11 Degrees of freedom\nBefore we finish up with paired-samples \\(t\\)-tests, we should talk about degrees of freedom. Our sense is that students don’t really understand degrees of freedom very well. If you are reading this textbook, you are probably still wondering what is degrees of freedom, seeing as we haven’t really talked about it all.\nFor the \\(t\\)-test, there is a formula for degrees of freedom. For the one-sample and paired sample \\(t\\)-tests, the formula is:\n\\(\\text{Degrees of Freedom} = \\text{df} = n-1\\). Where n is the number of samples in the test.\nIn our paired \\(t\\)-test example, there were 5 infants. Therefore, degrees of freedom = 5-1 = 4.\nOK, that’s a formula. Who cares about degrees of freedom, what does the number mean? And why do we report it when we report a \\(t\\)-test… you’ve probably noticed the number in parentheses e.g., \\(t\\)(4)=.72, the 4 is the \\(df\\), or degrees of freedom.\nDegrees of freedom is both a concept, and a correction. The concept is that if you estimate a property of the numbers, and you use this estimate, you will be forcing some constraints on your numbers.\nConsider the numbers: 1, 2, 3. The mean of these numbers is 2. Now, let’s say I told you that the mean of three numbers is 2. Then, how many of these three numbers have freedom? Funny question right. What we mean is, how many of the three numbers could be any number, or have the freedom to be any number.\nThe first two numbers could be any number. But, once those two numbers are set, the final number (the third number), MUST be a particular number that makes the mean 2. The first two numbers have freedom. The third number has no freedom.\nTo illustrate. Let’s freely pick two numbers: 51 and -3. I used my personal freedom to pick those two numbers. Now, if our three numbers are 51, -3, and x, and the mean of these three numbers is 2. There is only one solution, x has to be -42, otherwise the mean won’t be 2. This is one way to think about degrees of freedom. The degrees of freedom for these three numbers is n-1 = 3-1= 2, because 2 of the numbers can be free, but the last number has no freedom, it becomes fixed after the first two are decided.\nNow, statisticians often apply degrees of freedom to their calculations, especially when a second calculation relies on an estimated value. For example, when we calculate the standard deviation of a sample, we first calculate the mean of the sample right! By estimating the mean, we are fixing an aspect of our sample, and so, our sample now has n-1 degrees of freedom when we calculate the standard deviation (remember for the sample standard deviation, we divide by n-1…there’s that n-1 again.)\n\n6.3.11.1 Simulating how degrees of freedom affects the \\(t\\) distribution\nThere are at least two ways to think the degrees of freedom for a \\(t\\)-test. For example, if you want to use math to compute aspects of the \\(t\\) distribution, then you need the degrees of freedom to plug in to the formula… If you want to see the formulas I’m talking about, scroll down on the \\(t\\)-test wikipedia page and look for the probability density or cumulative distribution functions…We think that is quite scary for most people, and one reason why degrees of freedom are not well-understood.\nIf we wanted to simulate the \\(t\\) distribution we could more easily see what influence degrees of freedom has on the shape of the distribution. Remember, \\(t\\) is a sample statistic, it is something we measure from the sample. So, we could simulate the process of measuring \\(t\\) from many different samples, then plot the histogram of \\(t\\) to show us the simulated \\(t\\) distribution.\n\n\n\n\n\nFigure 6.10: The width of the t distribution shrinks as sample size and degrees of freedom (from 4 to 100) increases.\n\n\n\n\nIn Figure 6.10 notice that the red distribution for \\(df = 4\\), is a little bit shorter, and a little bit wider than the bluey-green distribution for \\(df = 100\\). As degrees of freedom increase the \\(t\\) distribution gets taller (in the middle), and narrower in the range. It gets more peaky. Can you guess the reason for this? Remember, we are estimating a sample statistic, and degrees of freedom is really just a number that refers to the number of subjects (well minus one). And, we already know that as we increase \\(n\\), our sample statistics become better estimates (less variance) of the distributional parameters they are estimating. So, \\(t\\) becomes a better estimate of it’s “true” value as sample size increase, resulting in a more narrow distribution of \\(t\\)s.\nThere is a slightly different \\(t\\) distribution for every degrees of freedom, and the critical regions associated with 5% of the extreme values are thus slightly different every time. This is why we report the degrees of freedom for each t-test, they define the distribution of \\(t\\) values for the sample-size in question. Why do we use n-1 and not n? Well, we calculate \\(t\\) using the sample standard deviation to estimate the standard error or the mean, that estimate uses n-1 in the denominator, so our \\(t\\) distribution is built assuming n-1. That’s enough for degrees of freedom…"
  },
  {
    "objectID": "06-ttests.html#the-paired-samples-t-test-strikes-back",
    "href": "06-ttests.html#the-paired-samples-t-test-strikes-back",
    "title": "6  t-tests",
    "section": "6.4 The paired samples t-test strikes back",
    "text": "6.4 The paired samples t-test strikes back\nYou must be wondering if we will ever be finished talking about paired samples t-tests… why are we doing round 2, oh no! Don’t worry, we’re just going to 1) remind you about what we were doing with the infant study, and 2) do a paired samples t-test on the entire data set and discuss.\nRemember, we were wondering if the infants would look longer toward the singer who sang the familiar song during the test phase compared to the baseline phase. We showed you data from 5 infants, and walked through the computations for the \\(t\\)-test. As a reminder, it looked like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\ninfant\nBaseline\nTest\ndifferences\ndiff_from_mean\nSquared_differences\n\n\n\n\n1\n0.44\n0.6\n0.16\n0.106\n0.011236\n\n\n2\n0.41\n0.68\n0.27\n0.216\n0.046656\n\n\n3\n0.75\n0.72\n-0.03\n-0.084\n0.00705600000000001\n\n\n4\n0.44\n0.28\n-0.16\n-0.214\n0.045796\n\n\n5\n0.47\n0.5\n0.03\n-0.024\n0.000575999999999999\n\n\nSums\n2.51\n2.78\n0.27\n0\n0.11132\n\n\nMeans\n0.502\n0.556\n0.054\n0\n0.022264\n\n\n\n\n\n\nsd\n0.167\n\n\n\n\n\n\nSEM\n0.075\n\n\n\n\n\n\nt\n0.72\n\n\n\n\n#&gt; \n#&gt;  One Sample t-test\n#&gt; \n#&gt; data:  round(differences, digits = 2)\n#&gt; t = 0.72381, df = 4, p-value = 0.5092\n#&gt; alternative hypothesis: true mean is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  -0.1531384  0.2611384\n#&gt; sample estimates:\n#&gt; mean of x \n#&gt;     0.054\n\nLet’s write down the finding one more time: The mean difference was 0.054, \\(t\\)(4) = .72, \\(p\\) =.509. We can also now confirm, that the \\(p\\)-value was from a two-tailed test. So, what does this all really mean.\nWe can say that a \\(t\\) value with an absolute of .72 or larger occurs 50.9% of the time. More precisely, the distribution of no differences (the null), will produce a \\(t\\) value this large or larger 50.9% of the time. In other words, chance alone good have easily produced the \\(t\\) value from our sample, and the mean difference we observed or .054, could easily have been a result of chance.\nLet’s quickly put all of the data in the \\(t\\)-test, and re-run the test using all of the infant subjects.\n\n#&gt; \n#&gt;  One Sample t-test\n#&gt; \n#&gt; data:  differences\n#&gt; t = 2.4388, df = 31, p-value = 0.02066\n#&gt; alternative hypothesis: true mean is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  0.01192088 0.13370412\n#&gt; sample estimates:\n#&gt; mean of x \n#&gt; 0.0728125\n\nNow we get a very different answer. We would summarize the results saying the mean difference was .073, t(31) = 2.44, p = 0.020. How many total infants were their? Well the degrees of freedom was 31, so there must have been 32 infants in the study. Now we see a much smaller \\(p\\)-value. This was also a two-tailed test, so we that observing a \\(t\\) value of 2.4 or greater (absolute value) only occurs 2% of the time. In other words, the distribution of no differences will produce the observed t-value very rarely. So, it is unlikely that the observed mean difference of .073 was due to chance (it could have been due to chance, but that is very unlikely). As a result, we can be somewhat confident in concluding that something about seeing and hearing a unfamiliar person sing a familiar song, causes an infant to draw their attention toward the singer, and this potentially benefits social learning on the part of the infant."
  },
  {
    "objectID": "06-ttests.html#independent-samples-t-test-the-return-of-the-t-test",
    "href": "06-ttests.html#independent-samples-t-test-the-return-of-the-t-test",
    "title": "6  t-tests",
    "section": "6.5 Independent samples t-test: The return of the t-test?",
    "text": "6.5 Independent samples t-test: The return of the t-test?\nIf you’ve been following the Star Wars references, we are on last movie (of the original trilogy)… the independent t-test. This is were basically the same story plays out as before, only slightly different.\nRemember there are different \\(t\\)-tests for different kinds of research designs. When your design is a between-subjects design, you use an independent samples t-test. Between-subjects design involve different people or subjects in each experimental condition. If there are two conditions, and 10 people in each, then there are 20 total people. And, there are no paired scores, because every single person is measured once, not twice, no repeated measures. Because there are no repeated measures we can’t look at the difference scores between conditions one and two. The scores are not paired in any meaningful way, to it doesn’t make sense to subtract them. So what do we do?\nThe logic of the independent samples t-test is the very same as the other \\(t\\)-tests. We calculated the means for each group, then we find the difference. That goes into the numerator of the t formula. Then we get an estimate of the variation for the denominator. We divide the mean difference by the estimate of the variation, and we get \\(t\\). It’s the same as before.\nThe only wrinkle here is what goes into the denominator? How should we calculate the estimate of the variance? It would be nice if we could do something very straightforward like this, say for an experiment with two groups A and B:\n\\(t = \\frac{\\bar{A}-\\bar{B}}{(\\frac{SEM_A+SEM_B}{2})}\\)\nIn plain language, this is just:\n\nFind the mean difference for the top part\nCompute the SEM (standard error of the mean) for each group, and average them together to make a single estimate, pooling over both samples.\n\nThis would be nice, but unfortunately, it turns out that finding the average of two standard errors of the mean is not the best way to do it. This would create a biased estimator of the variation for the hypothesized distribution of no differences. We won’t go into the math here, but instead of the above formula, we an use a different one that gives as an unbiased estimate of the pooled standard error of the sample mean. Our new and improved \\(t\\) formula would look like this:\n\\(t = \\frac{\\bar{X_A}-\\bar{X_B}}{s_p * \\sqrt{\\frac{1}{n_A} + \\frac{1}{n_B}}}\\)\nand, \\(s_p\\), which is the pooled sample standard deviation is defined as, note the $s$es in the formula are variances:\n\\(s_p = \\sqrt{\\frac{(n_A-1)s_A^2 + (n_B-1)s^2_B}{n_A +n_B -2}}\\)\nBelieve you me, that is so much more formula than I wanted to type out. Shall we do one independent \\(t\\)-test example by hand, just to see the computations? Let’s do it…but in a slightly different way than you expect. I show the steps using R. I made some fake scores for groups A and B. Then, I followed all of the steps from the formula, but made R do each of the calculations. This shows you the needed steps by following the code. At the end, I print the \\(t\\)-test values I computed “by hand”, and then the \\(t\\)-test value that the R software outputs using the \\(t\\)-test function. You should be able to get the same values for \\(t\\), if you were brave enough to compute \\(t\\) by hand.\n\n\n## By \"hand\" using R r code\na &lt;- c(1,2,3,4,5)\nb &lt;- c(3,5,4,7,9)\n\nmean_difference &lt;- mean(a)-mean(b) # compute mean difference\n\nvariance_a &lt;- var(a) # compute variance for A\nvariance_b &lt;- var(b) # compute variance for B\n\n# Compute top part and bottom part of sp formula\n\nsp_numerator &lt;- (4*variance_a + 4* variance_b) \nsp_denominator &lt;- 5+5-2\nsp &lt;- sqrt(sp_numerator/sp_denominator) # compute sp\n\n\n# compute t following formulat\n\nt &lt;- mean_difference / ( sp * sqrt( (1/5) +(1/5) ) )\n\nt # print results\n#&gt; [1] -2.017991\n\n\n# using the R function t.test\nt.test(a,b, paired=FALSE, var.equal = TRUE)\n#&gt; \n#&gt;  Two Sample t-test\n#&gt; \n#&gt; data:  a and b\n#&gt; t = -2.018, df = 8, p-value = 0.0783\n#&gt; alternative hypothesis: true difference in means is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  -5.5710785  0.3710785\n#&gt; sample estimates:\n#&gt; mean of x mean of y \n#&gt;       3.0       5.6"
  },
  {
    "objectID": "06-ttests.html#simulating-data-for-t-tests",
    "href": "06-ttests.html#simulating-data-for-t-tests",
    "title": "6  t-tests",
    "section": "6.6 Simulating data for t-tests",
    "text": "6.6 Simulating data for t-tests\nAn “advanced” topic for \\(t\\)-tests is the idea of using R to conduct simulations for \\(t\\)-tests.\nIf you recall, \\(t\\) is a property of a sample. We calculate \\(t\\) from our sample. The \\(t\\) distribution is the hypothetical behavior of our sample. That is, if we had taken thousands upon thousands of samples, and calculated \\(t\\) for each one, and then looked at the distribution of those \\(t\\)’s, we would have the sampling distribution of \\(t\\)!\nIt can be very useful to get in the habit of using R to simulate data under certain conditions, to see how your sample data, and things like \\(t\\) behave. Why is this useful? It mainly prepares you with some intuitions about how sampling error (random chance) can influence your results, given specific parameters of your design, such as sample-size, the size of the mean difference you expect to find in your data, and the amount of variation you might find. These methods can be used formally to conduct power-analyses. Or more informally for data sense.\n\n6.6.1 Simulating a one-sample t-test\nHere are the steps you might follow to simulate data for a one sample \\(t\\)-test.\n\nMake some assumptions about what your sample (that you might be planning to collect) might look like. For example, you might be planning to collect 30 subjects worth of data. The scores of those data points might come from a normal distribution (mean = 50, sd = 10).\nsample simulated numbers from the distribution, then conduct a \\(t\\)-test on the simulated numbers. Save the statistics you want (such as \\(t\\)s and \\(p\\)s), and then see how things behave.\n\nLet’s do this a couple different times. First, let’s simulate samples with N = 30, taken from a normal (mean= 50, sd = 25). We’ll do a simulation with 1000 simulations. For each simulation, we will compare the sample mean with a population mean of 50. There should be no difference on average here. Figure 6.11 is the null distribution that we are simulating.\n\n\n\n\n\nFigure 6.11: The distribution of \\(t\\)-values under the null. These are the \\(t\\) values that are produced by chance alone.\n\n\n\n\n\n\n\n\n\nFigure 6.12: The distribution of \\(p\\)-values that are observed is flat under the null.\n\n\n\n\nNeat. We see both a \\(t\\) distribution, that looks like \\(t\\) distribution as it should. And we see the \\(p\\) distribution. This shows us how often we get \\(t\\) values of particular sizes. You may find it interesting that the \\(p\\)-distribution is flat under the null, which we are simulating here. This means that you have the same chances of a getting a \\(t\\) with a p-value between 0 and 0.05, as you would for getting a \\(t\\) with a p-value between .90 and .95. Those ranges are both ranges of 5%, so there are an equal amount of \\(t\\) values in them by definition.\nHere’s another way to do the same simulation in R, using the replicate function, instead a for loop:\n\n\n\n\n\nFigure 6.13: Simulating \\(t\\)s in R.\n\n\n\n\n\n\n\n\n\nFigure 6.14: Simulating \\(p\\)s in R.\n\n\n\n\n\n\n6.6.2 Simulating a paired samples t-test\nThe code below is set up to sample 10 scores for condition A and B from the same normal distribution. The simulation is conducted 1000 times, and the \\(t\\)s and \\(p\\)s are saved and plotted for each.\n\n\nsave_ps &lt;- length(1000)\nsave_ts &lt;- length(1000)\nfor ( i in 1:1000 ){\n  condition_A &lt;- rnorm(10,10,5)\n  condition_B &lt;- rnorm(10,10,5)\n  differences &lt;- condition_A - condition_B\n  t_test &lt;- t.test(differences, mu=0)\n  save_ps[i] &lt;- t_test$p.value\n  save_ts[i] &lt;- t_test$statistic\n}\n\n\n\n\n\n\nFigure 6.15: 1000 simulated ts from the null distribution\n\n\n\n\n\n\n\n\n\nFigure 6.16: 1000 simulated ps from the null distribution\n\n\n\n\nAccording to the simulation. When there are no differences between the conditions, and the samples are being pulled from the very same distribution, you get these two distributions for \\(t\\) and \\(p\\). These again show how the null distribution of no differences behaves.\nFor any of these simulations, if you rejected the null-hypothesis (that your difference was only due to chance), you would be making a type I error. If you set your alpha criteria to \\(\\alpha = .05\\), we can ask how many type I errors were made in these 1000 simulations. The answer is:\n\nlength(save_ps[save_ps&lt;.05])\n#&gt; [1] 58\nlength(save_ps[save_ps&lt;.05])/1000\n#&gt; [1] 0.058\n\nWe happened to make 58. The expectation over the long run is 5% type I error rates (if your alpha is .05).\nWhat happens if there actually is a difference in the simulated data, let’s set one condition to have a larger mean than the other:\n\n\nsave_ps &lt;- length(1000)\nsave_ts &lt;- length(1000)\nfor ( i in 1:1000 ){\n  condition_A &lt;- rnorm(10,10,5)\n  condition_B &lt;- rnorm(10,13,5)\n  differences &lt;- condition_A - condition_B\n  t_test &lt;- t.test(differences, mu=0)\n  save_ps[i] &lt;- t_test$p.value\n  save_ts[i] &lt;- t_test$statistic\n}\n\n\n\n\n\n\nFigure 6.17: 1000 ts when there is a true difference\n\n\n\n\n\n\n\n\n\nFigure 6.18: 1000 ps when there is a true difference\n\n\n\n\nNow you can see that the \\(p\\)-value distribution is skewed to the left. This is because when there is a true effect, you will get p-values that are less than .05 more often. Or, rather, you get larger \\(t\\) values than you normally would if there were no differences.\nIn this case, we wouldn’t be making a type I error if we rejected the null when p was smaller than .05. How many times would we do that out of our 1000 experiments?\n\nlength(save_ps[save_ps&lt;.05])\n#&gt; [1] 210\nlength(save_ps[save_ps&lt;.05])/1000\n#&gt; [1] 0.21\n\nWe happened to get 210 simulations where p was less than .05, that’s only 0.21 experiments. If you were the researcher, would you want to run an experiment that would be successful only 0.21 of the time? I wouldn’t. I would run a better experiment.\nHow would you run a better simulated experiment? Well, you could increase \\(n\\), the number of subjects in the experiment. Let’s increase \\(n\\) from 10 to 100, and see what happens to the number of “significant” simulated experiments.\n\n\nsave_ps &lt;- length(1000)\nsave_ts &lt;- length(1000)\nfor ( i in 1:1000 ){\n  condition_A &lt;- rnorm(100,10,5)\n  condition_B &lt;- rnorm(100,13,5)\n  differences &lt;- condition_A - condition_B\n  t_test &lt;- t.test(differences, mu=0)\n  save_ps[i] &lt;- t_test$p.value\n  save_ts[i] &lt;- t_test$statistic\n}\n\n\n\n\n\n\nFigure 6.19: 1000 ts for n =100, when there is a true effect\n\n\n\n\n\n#&gt; [1] 991\n#&gt; [1] 0.991\n\n\n\n\nFigure 6.20: 1000 ps for n =100, when there is a true effect\n\n\n\n\nCool, now almost all of the experiments show a \\(p\\)-value of less than .05 (using a two-tailed test, that’s the default in R). See, you could use this simulation process to determine how many subjects you need to reliably find your effect.\n\n\n6.6.3 Simulating an independent samples t.test\nJust change the t.test function like so… this is for the null, assuming no difference between groups.\n\n\nsave_ps &lt;- length(1000)\nsave_ts &lt;- length(1000)\nfor ( i in 1:1000 ){\n  group_A &lt;- rnorm(10,10,5)\n  group_B &lt;- rnorm(10,10,5)\n  t_test &lt;- t.test(group_A, group_B, paired=FALSE, var.equal=TRUE)\n  save_ps[i] &lt;- t_test$p.value\n  save_ts[i] &lt;- t_test$statistic\n}\n\n\n\n\n\n\nFigure 6.21: 1000 ts for n =100, when there is a true effect\n\n\n\n\n\n#&gt; [1] 45\n#&gt; [1] 0.045\n\n\n\n\nFigure 6.22: 1000 ps for n =100, when there is a true effect"
  },
  {
    "objectID": "06-ttests.html#videos",
    "href": "06-ttests.html#videos",
    "title": "7  Pruebas t",
    "section": "7.6 Videos",
    "text": "7.6 Videos\n\n7.6.1 Test de una o dos colas\n\n\n\n\n\n\nMehr, Samuel A, Lee Ann Song, and Elizabeth S Spelke. 2016. “For 5-Month-Old Infants, Melodies Are Social.” Psychological Science 27 (4): 486–501. https://doi.org/10.1177/0956797615626691.\n\n\nStudent, A. 1908. “The Probable Error of a Mean.” Biometrika 6: 1–2.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Pruebas t</span>"
    ]
  },
  {
    "objectID": "07-ANOVA.html#anova-is-analysis-of-variance",
    "href": "07-ANOVA.html#anova-is-analysis-of-variance",
    "title": "7  ANOVA",
    "section": "7.1 ANOVA is Analysis of Variance",
    "text": "7.1 ANOVA is Analysis of Variance\nANOVA stands for Analysis Of Variance. It is a widely used technique for assessing the likelihood that differences found between means in sample data could be produced by chance. You might be thinking, well don’t we have \\(t\\)-tests for that? Why do we need the ANOVA, what do we get that’s new that we didn’t have before?\nWhat’s new with the ANOVA, is the ability to test a wider range of means beyond just two. In all of the \\(t\\)-test examples we were always comparing two things. For example, we might ask whether the difference between two sample means could have been produced by chance. What if our experiment had more than two conditions or groups? We would have more than 2 means. We would have one mean for each group or condition. That could be a lot depending on the experiment. How would we compare all of those means? What should we do, run a lot of \\(t\\)-tests, comparing every possible combination of means? Actually, you could do that. Or, you could do an ANOVA.\nIn practice, we will combine both the ANOVA test and \\(t\\)-tests when analyzing data with many sample means (from more than two groups or conditions). Just like the \\(t\\)-test, there are different kinds of ANOVAs for different research designs. There is one for between-subjects designs, and a slightly different one for repeated measures designs. We talk about both, beginning with the ANOVA for between-subjects designs."
  },
  {
    "objectID": "07-ANOVA.html#one-factor-anova",
    "href": "07-ANOVA.html#one-factor-anova",
    "title": "7  ANOVA",
    "section": "7.2 One-factor ANOVA",
    "text": "7.2 One-factor ANOVA\nThe one-factor ANOVA is sometimes also called a between-subjects ANOVA, an independent factor ANOVA, or a one-way ANOVA (which is a bit of a misnomer as we discuss later). The critical ingredient for a one-factor, between-subjects ANOVA, is that you have one independent variable, with at least two-levels. When you have one IV with two levels, you can run a \\(t\\)-test. You can also run an ANOVA. Interestingly, they give you almost the exact same results. You will get a \\(p\\)-value from both tests that is identical (they are really doing the same thing under the hood). The \\(t\\)-test gives a \\(t\\)-value as the important sample statistic. The ANOVA gives you the \\(F\\)-value (for Fisher, the inventor of the test) as the important sample statistic. It turns out that \\(t^2\\) equals \\(F\\), when there are only two groups in the design. They are the same test. Side-note, it turns out they are all related to Pearson’s r too (but we haven’t written about this relationship yet in this textbook).\nRemember that \\(t\\) is computed directly from the data. It’s like a mean and standard error that we measure from the sample. In fact it’s the mean difference divided by the standard error of the sample. It’s just another descriptive statistic isn’t it.\nThe same thing is true about \\(F\\). \\(F\\) is computed directly from the data. In fact, the idea behind \\(F\\) is the same basic idea that goes into making \\(t\\). Here is the general idea behind the formula, it is again a ratio of the effect we are measuring (in the numerator), and the variation associated with the effect (in the denominator).\n\\(\\text{name of statistic} = \\frac{\\text{measure of effect}}{\\text{measure of error}}\\)\n\\(\\text{F} = \\frac{\\text{measure of effect}}{\\text{measure of error}}\\)\nThe difference with \\(F\\), is that we use variances to describe both the measure of the effect and the measure of error. So, \\(F\\) is a ratio of two variances.\nRemember what we said about how these ratios work. When the variance associated with the effect is the same size as the variance associated with sampling error, we will get two of the same numbers, this will result in an \\(F\\)-value of 1. When the variance due to the effect is larger than the variance associated with sampling error, then \\(F\\) will be greater than 1. When the variance associated with the effect is smaller than the variance associated with sampling error, \\(F\\) will be less than one.\nLet’s rewrite in plainer English. We are talking about two concepts that we would like to measure from our data. 1) A measure of what we can explain, and 2) a measure of error, or stuff about our data we can’t explain. So, the \\(F\\) formula looks like this:\n\\(\\text{F} = \\frac{\\text{Can Explain}}{\\text{Can't Explain}}\\)\nWhen we can explain as much as we can’t explain, \\(F\\) = 1. This isn’t that great of a situation for us to be in. It means we have a lot of uncertainty. When we can explain much more than we can’t we are doing a good job, \\(F\\) will be greater than 1. When we can explain less than what we can’t, we really can’t explain very much, \\(F\\) will be less than 1. That’s the concept behind making \\(F\\).\nIf you saw an \\(F\\) in the wild, and it was .6. Then you would automatically know the researchers couldn’t explain much of their data. If you saw an \\(F\\) of 5, then you would know the researchers could explain 5 times more than the couldn’t, that’s pretty good. And the point of this is to give you an intuition about the meaning of an \\(F\\)-value, even before you know how to compute it.\n\n7.2.1 Computing the \\(F\\)-value\nFisher’s ANOVA is very elegant in my opinion. It starts us off with a big problem we always have with data. We have a lot of numbers, and there is a lot of variation in the numbers, what to do? Wouldn’t it be nice to split up the variation into to kinds, or sources. If we could know what parts of the variation were being caused by our experimental manipulation, and what parts were being caused by sampling error, we would be making really good progress. We would be able to know if our experimental manipulation was causing more change in the data than sampling error, or chance alone. If we could measure those two parts of the total variation, we could make a ratio, and then we would have an \\(F\\) value. This is what the ANOVA does. It splits the total variation in the data into two parts. The formula is:\nTotal Variation = Variation due to Manipulation + Variation due to sampling error\nThis is a nice idea, but it is also vague. We haven’t specified our measure of variation. What should we use?\nRemember the sums of squares that we used to make the variance and the standard deviation? That’s what we’ll use. Let’s take another look at the formula, using sums of squares for the measure of variation:\n\\(SS_\\text{total} = SS_\\text{Effect} + SS_\\text{Error}\\)\n\n\n7.2.2 SS Total\nThe total sums of squares, or \\(SS\\text{Total}\\) is a way of thinking about all of the variation in a set of data. It’s pretty straightforward to measure. No tricky business. All we do is find the difference between each score and the grand mean, then we square the differences and add them all up.\nLet’s imagine we had some data in three groups, A, B, and C. For example, we might have 3 scores in each group. The data could look like this:\n\nlibrary(dplyr)\nscores &lt;- c(20, 11, 2, 6, 2, 7, 2, 11, 2)\ngroups &lt;- as.character(rep(c(\"A\", \"B\", \"C\"), each = 3))\ndiff &lt;- scores - mean(scores)\ndiff_squared &lt;- diff ^ 2\ndf &lt;- data.frame(groups, scores, diff, diff_squared)\ndf$groups &lt;- as.character(df$groups)\n\ndf &lt;- df %&gt;%\n  rbind(c(\"Sums\", colSums(df[1:9, 2:4]))) %&gt;%\n  rbind(c(\"Means\", colMeans(df[1:9, 2:4])))\n\nknitr::kable(df)\n\n\n\n\ngroups\nscores\ndiff\ndiff_squared\n\n\n\n\nA\n20\n13\n169\n\n\nA\n11\n4\n16\n\n\nA\n2\n-5\n25\n\n\nB\n6\n-1\n1\n\n\nB\n2\n-5\n25\n\n\nB\n7\n0\n0\n\n\nC\n2\n-5\n25\n\n\nC\n11\n4\n16\n\n\nC\n2\n-5\n25\n\n\nSums\n63\n0\n302\n\n\nMeans\n7\n0\n33.5555555555556\n\n\n\n\n\nThe data is organized in long format, so that each row is a single score. There are three scores for the A, B, and C groups. The mean of all of the scores is called the Grand Mean. It’s calculated in the table, the Grand Mean = 7.\nWe also calculated all of the difference scores from the Grand Mean. The difference scores are in the column titled diff. Next, we squared the difference scores, and those are in the next column called diff_squared.\nRemember, the difference scores are a way of measuring variation. They represent how far each number is from the Grand Mean. If the Grand Mean represents our best guess at summarizing the data, the difference scores represent the error between the guess and each actual data point. The only problem with the difference scores is that they sum to zero (because the mean is the balancing point in the data). So, it is convenient to square the difference scores, this turns all of them into positive numbers. The size of the squared difference scores still represents error between the mean and each score. And, the squaring operation exacerbates the differences as the error grows larger (squaring a big number makes a really big number, squaring a small number still makes a smallish number).\nOK fine! We have the squared deviations from the grand mean, we know that they represent the error between the grand mean and each score. What next? SUM THEM UP!\nWhen you add up all of the individual squared deviations (difference scores) you get the sums of squares. That’s why it’s called the sums of squares (SS).\nNow, we have the first part of our answer:\n\\(SS_\\text{total} = SS_\\text{Effect} + SS_\\text{Error}\\)\n\\(SS_\\text{total} = 302\\) and\n\\(302 = SS_\\text{Effect} + SS_\\text{Error}\\)\nWhat next? If you think back to what you learned about algebra, and solving for X, you might notice that we don’t really need to find the answers to both missing parts of the equation. We only need one, and we can solve for the other. For example, if we found \\(SS_\\text{Effect}\\), then we could solve for \\(SS_\\text{Error}\\).\n\n\n7.2.3 SS Effect\n\\(SS_\\text{Total}\\) gave us a number representing all of the change in our data, how all the scores are different from the grand mean.\nWhat we want to do next is estimate how much of the total change in the data might be due to the experimental manipulation. For example, if we ran an experiment that causes causes change in the measurement, then the means for each group will be different from other. As a result, the manipulation forces change onto the numbers, and this will naturally mean that some part of the total variation in the numbers is caused by the manipulation.\nThe way to isolate the variation due to the manipulation (also called effect) is to look at the means in each group, and calculate the difference scores between each group mean and the grand mean, and then sum the squared deviations to find \\(SS_\\text{Effect}\\).\nConsider this table, showing the calculations for \\(SS_\\text{Effect}\\).\n\nlibrary(dplyr)\nscores &lt;- c(20, 11, 2, 6, 2, 7, 2, 11, 2)\nmeans &lt;- c(11, 11, 11, 5, 5, 5, 5, 5, 5)\ngroups &lt;- as.character(rep(c(\"A\", \"B\", \"C\"), each = 3))\ndiff &lt;- means - mean(scores)\ndiff_squared &lt;- diff ^ 2\ndf &lt;- data.frame(groups, scores, means, diff, diff_squared)\ndf$groups &lt;- as.character(df$groups)\n\ndf &lt;- df %&gt;%\n  rbind(c(\"Sums\", colSums(df[1:9, 2:5]))) %&gt;%\n  rbind(c(\"Means\", colMeans(df[1:9, 2:5])))\n\nknitr::kable(df)\n\n\n\n\ngroups\nscores\nmeans\ndiff\ndiff_squared\n\n\n\n\nA\n20\n11\n4\n16\n\n\nA\n11\n11\n4\n16\n\n\nA\n2\n11\n4\n16\n\n\nB\n6\n5\n-2\n4\n\n\nB\n2\n5\n-2\n4\n\n\nB\n7\n5\n-2\n4\n\n\nC\n2\n5\n-2\n4\n\n\nC\n11\n5\n-2\n4\n\n\nC\n2\n5\n-2\n4\n\n\nSums\n63\n63\n0\n72\n\n\nMeans\n7\n7\n0\n8\n\n\n\n\n\nNotice we created a new column called means. For example, the mean for group A was 11. You can see there are three 11s, one for each observation in row A. The means for group B and C happen to both be 5. So, the rest of the numbers in the means column are 5s.\nWhat we are doing here is thinking of each score in the data from the viewpoint of the group means. The group means are our best attempt to summarize the data in those groups. From the point of view of the mean, all of the numbers are treated as the same. The mean doesn’t know how far off it is from each score, it just knows that all of the scores are centered on the mean.\n\nLet’s pretend you are the mean for group A. That means you are an 11. Someone asks you “hey, what’s the score for the first data point in group A?”. Because you are the mean, you say, I know that, it’s 11. “What about the second score?”…it’s 11… they’re all 11, so far as I can tell…“Am I missing something…”, asked the mean.\n\nNow that we have converted each score to it’s mean value we can find the differences between each mean score and the grand mean, then square them, then sum them up. We did that, and found that the \\(SS_\\text{Effect} = 72\\).\n\\(SS_\\text{Effect}\\) represents the amount of variation that is caused by differences between the means. I also refer to this as the amount of variation that the researcher can explain (by the means, which represent differences between groups or conditions that were manipulated by the researcher).\nNotice also that \\(SS_\\text{Effect} = 72\\), and that 72 is smaller than \\(SS_\\text{total} = 302\\). That is very important. \\(SS_\\text{Effect}\\) by definition can never be larger than \\(SS_\\text{total}\\).\n\n\n7.2.4 SS Error\nGreat, we made it to SS Error. We already found SS Total, and SS Effect, so now we can solve for SS Error just like this:\n\\(SS_\\text{total} = SS_\\text{Effect} + SS_\\text{Error}\\)\nswitching around:\n$ SS_ = SS_ - SS_ $\n$ SS_ = 302 - 72 = 230 $\nWe could stop here and show you the rest of the ANOVA, we’re almost there. But, the next step might not make sense unless we show you how to calculate \\(SS_\\text{Error}\\) directly from the data, rather than just solving for it. We should do this just to double-check our work anyway.\n\nscores &lt;- c(20, 11, 2, 6, 2, 7, 2, 11, 2)\nmeans &lt;- c(11, 11, 11, 5, 5, 5, 5, 5, 5)\ngroups &lt;- as.character(rep(c(\"A\", \"B\", \"C\"), each = 3))\ndiff &lt;- means - scores\ndiff_squared &lt;- diff ^ 2\ndf &lt;- data.frame(groups, scores, means, diff, diff_squared)\ndf$groups &lt;- as.character(df$groups)\n\ndf &lt;- df %&gt;%\n  rbind(c(\"Sums\", colSums(df[1:9, 2:5]))) %&gt;%\n  rbind(c(\"Means\", colMeans(df[1:9, 2:5])))\n\nknitr::kable(df)\n\n\n\n\ngroups\nscores\nmeans\ndiff\ndiff_squared\n\n\n\n\nA\n20\n11\n-9\n81\n\n\nA\n11\n11\n0\n0\n\n\nA\n2\n11\n9\n81\n\n\nB\n6\n5\n-1\n1\n\n\nB\n2\n5\n3\n9\n\n\nB\n7\n5\n-2\n4\n\n\nC\n2\n5\n3\n9\n\n\nC\n11\n5\n-6\n36\n\n\nC\n2\n5\n3\n9\n\n\nSums\n63\n63\n0\n230\n\n\nMeans\n7\n7\n0\n25.5555555555556\n\n\n\n\n\nAlright, we did almost the same thing as we did to find \\(SS_\\text{Effect}\\). Can you spot the difference? This time for each score we first found the group mean, then we found the error in the group mean estimate for each score. In other words, the values in the \\(diff\\) column are the differences between each score and it’s group mean. The values in the diff_squared column are the squared deviations. When we sum up the squared deviations, we get another Sums of Squares, this time it’s the \\(SS_\\text{Error}\\). This is an appropriate name, because these deviations are the ones that the group means can’t explain!\n\n\n7.2.5 Degrees of freedom\nDegrees of freedom come into play again with ANOVA. This time, their purpose is a little bit more clear. \\(Df\\)s can be fairly simple when we are doing a relatively simple ANOVA like this one, but they can become complicated when designs get more complicated.\nLet’s talk about the degrees of freedom for the \\(SS_\\text{Effect}\\) and \\(SS_\\text{Error}\\).\nThe formula for the degrees of freedom for \\(SS_\\text{Effect}\\) is\n\\(df_\\text{Effect} = \\text{Groups} -1\\), where Groups is the number of groups in the design.\nIn our example, there are 3 groups, so the df is 3-1 = 2. You can think of the df for the effect this way. When we estimate the grand mean (the overall mean), we are taking away a degree of freedom for the group means. Two of the group means can be anything they want (they have complete freedom), but in order for all three to be consistent with the Grand Mean, the last group mean has to be fixed.\nThe formula for the degrees of freedom for \\(SS_\\text{Error}\\) is\n\\(df_\\text{Error} = \\text{scores} - \\text{groups}\\), or the number of scores minus the number of groups. We have 9 scores and 3 groups, so our \\(df\\) for the error term is 9-3 = 6. Remember, when we computed the difference score between each score and its group mean, we had to compute three means (one for each group) to do that. So, that reduces the degrees of freedom by 3. 6 of the difference scores could be anything they want, but the last 3 have to be fixed to match the means from the groups.\n\n\n7.2.6 Mean Squared Error\nOK, so we have the degrees of freedom. What’s next? There are two steps left. First we divide the \\(SS\\)es by their respective degrees of freedom to create something new called Mean Squared Error. Let’s talk about why we do this.\nFirst of all, remember we are trying to accomplish this goal:\n\\(\\text{F} = \\frac{\\text{measure of effect}}{\\text{measure of error}}\\)\nWe want to build a ratio that divides a measure of an effect by a measure of error. Perhaps you noticed that we already have a measure of an effect and error! How about the \\(SS_\\text{Effect}\\) and \\(SS_\\text{Error}\\). They both represent the variation due to the effect, and the leftover variation that is unexplained. Why don’t we just do this?\n\\(\\frac{SS_\\text{Effect}}{SS_\\text{Error}}\\)\nWell, of course you could do that. What would happen is you can get some really big and small numbers for your inferential statistic. And, the kind of number you would get wouldn’t be readily interpretable like a \\(t\\) value or a \\(z\\) score.\nThe solution is to normalize the \\(SS\\) terms. Don’t worry, normalize is just a fancy word for taking the average, or finding the mean. Remember, the SS terms are all sums. And, each sum represents a different number of underlying properties.\nFor example, the SS_ represents the sum of variation for three means in our study. We might ask the question, well, what is the average amount of variation for each mean…You might think to divide SS_ by 3, because there are three means, but because we are estimating this property, we divide by the degrees of freedom instead (# groups - 1 = 3-1 = 2). Now we have created something new, it’s called the \\(MSE_\\text{Effect}\\).\n\\(MSE_\\text{Effect} = \\frac{SS_\\text{Effect}}{df_\\text{Effect}}\\)\n\\(MSE_\\text{Effect} = \\frac{72}{2} = 36\\)\nThis might look alien and seem a bit complicated. But, it’s just another mean. It’s the mean of the sums of squares for the effect. If this reminds you of the formula for the variance, good memory. The \\(SME_\\text{Effect}\\) is a measure variance for the change in the data due to changes in the means (which are tied to the experimental conditions).\nThe \\(SS_\\text{Error}\\) represents the sum of variation for nine scores in our study. That’s a lot more scores, so the \\(SS_\\text{Error}\\) is often way bigger than than \\(SS_\\text{Effect}\\). If we left our SSes this way and divided them, we would almost always get numbers less than one, because the \\(SS_\\text{Error}\\) is so big. What we need to do is bring it down to the average size. So, we might want to divide our \\(SS_\\text{Error}\\) by 9, after all there were nine scores. However, because we are estimating this property, we divide by the degrees of freedom instead (scores-groups) = 9-3 = 6). Now we have created something new, it’s called the \\(MSE_\\text{Error}\\).\n\\(MSE_\\text{Error} = \\frac{SS_\\text{Error}}{df_\\text{Error}}\\)\n\\(MSE_\\text{Error} = \\frac{230}{6} = 38.33\\)\n\n\n7.2.7 Calculate F\nNow that we have done all of the hard work, calculating \\(F\\) is easy:\n\\(\\text{F} = \\frac{\\text{measure of effect}}{\\text{measure of error}}\\)\n\\(\\text{F} = \\frac{MSE_\\text{Effect}}{MSE_\\text{Error}}\\)\n\\(\\text{F} = \\frac{36}{38.33} = .939\\)\nDone!\n\n\n7.2.8 The ANOVA TABLE\nYou might suspect we aren’t totally done here. We’ve walked through the steps of computing \\(F\\). Remember, \\(F\\) is a sample statistic, we computed \\(F\\) directly from the data. There were a whole bunch of pieces we needed, the dfs, the SSes, the MSEs, and then finally the F.\nAll of these little pieces are conveniently organized by ANOVA tables. ANOVA tables look like this:\n\nlibrary(xtable)\naov_out &lt;- aov(scores ~ groups, df[1:9, ])\nsummary_out &lt;- summary(aov_out)\n\nknitr::kable(xtable(summary_out))\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\ngroups\n2\n72\n36.00000\n0.9391304\n0.4417359\n\n\nResiduals\n6\n230\n38.33333\nNA\nNA\n\n\n\n\n\nYou are looking at the print-out of an ANOVA summary table from R. Notice, it had columns for \\(Df\\), \\(SS\\) (Sum Sq), \\(MSE\\) (Mean Sq), \\(F\\), and a \\(p\\)-value. There are two rows. The groups row is for the Effect (what our means can explain). The Residuals row is for the Error (what our means can’t explain). Different programs give slightly different labels, but they are all attempting to present the same information in the ANOVA table. There isn’t anything special about the ANOVA table, it’s just a way of organizing all the pieces. Notice, the MSE for the effect (36) is placed above the MSE for the error (38.333), and this seems natural because we divide 36/38.33 in or to get the \\(F\\)-value!"
  },
  {
    "objectID": "07-ANOVA.html#what-does-f-mean",
    "href": "07-ANOVA.html#what-does-f-mean",
    "title": "7  ANOVA",
    "section": "7.3 What does F mean?",
    "text": "7.3 What does F mean?\nWe’ve just noted that the ANOVA has a bunch of numbers that we calculated straight from the data. All except one, the \\(p\\)-value. We did not calculate the \\(p\\)-value from the data. Where did it come from, what does it mean? How do we use this for statistical inference. Just so you don’t get too worried, the \\(p\\)-value for the ANOVA has the very same general meaning as the \\(p\\)-value for the \\(t\\)-test, or the \\(p\\)-value for any sample statistic. It tells us that the probability that we would observe our test statistic or larger, under the distribution of no differences (the null).\nAs we keep saying, \\(F\\) is a sample statistic. Can you guess what we do with sample statistics in this textbook? We did it for the Crump Test, the Randomization Test, and the \\(t\\)-test… We make fake data, we simulate it, we compute the sample statistic we are interested in, then we see how it behaves over many replications or simulations.\nLet’s do that for \\(F\\). This will help you understand what \\(F\\) really is, and how it behaves. We are going to created the sampling distribution of \\(F\\). Once we have that you will be able to see where the \\(p\\)-values come from. It’s the same basic process that we followed for the \\(t\\) tests, except we are measuring \\(F\\) instead of \\(t\\).\nHere is the set-up, we are going to run an experiment with three levels. In our imaginary experiment we are going to test whether a new magic pill can make you smarter. The independent variable is the number of magic pills you take: 1, 2, or 3. We will measure your smartness using a smartness test. We will assume the smartness test has some known properties, the mean score on the test is 100, with a standard deviation of 10 (and the distribution is normal).\nThe only catch is that our magic pill does NOTHING AT ALL. The fake people in our fake experiment will all take sugar pills that do absolutely nothing to their smartness. Why would we want to simulate such a bunch of nonsense? The answer is that this kind of simulation is critical for making inferences about chance if you were to conduct a real experiment.\nHere are some more details for the experiment. Each group will have 10 different subjects, so there will be a total of 30 subjects. We are going to run this experiment 10,000 times. Each time drawing numbers randomly from the very same normal distribution. We are going to calculate \\(F\\) from our sample data every time, and then we are going to draw the histogram of \\(F\\)-values. Figure 7.1 shows the sampling distribution of \\(F\\) for our situation.\n\nlibrary(ggplot2)\n\nsave_F &lt;- length(10000)\nfor (i in 1:10000) {\n  smartness   &lt;- rnorm(30, 100, 10)\n  pill_group  &lt;- as.factor(rep(1:3, each = 10))\n  simulations &lt;- rep(i, each = 30)\n  sample_df   &lt;- data.frame(simulations, pill_group, smartness)\n  aov.out &lt;- summary(aov(smartness ~ pill_group, sample_df))\n  save_F[i] &lt;- aov.out[[1]]$`F value`[1]\n}\n\nplot_df &lt;- data.frame(sims = 1:10000, save_F)\nplot_df &lt;- plot_df[plot_df$save_F &lt; 10, ]\n\nggplot(plot_df, aes(x = save_F)) +\n  geom_histogram(color = \"white\", bins = 100) +\n  theme_classic()\n\n\n\n\nFigure 7.1: A simulation of 10,000 experiments from a null distribution where there is no differences. The histogram shows 10,000 \\(F\\)-values, one for each simulation. These are values that F can take in this situation. All of these \\(F\\)-values were produced by random sampling error.\n\n\n\n\nLet’s note a couple things about the \\(F\\) distribution. 1) The smallest value is 0, and there are no negative values. Does this make sense? \\(F\\) can never be negative because it is the ratio of two variances, and variances are always positive because of the squaring operation. So, yes, it makes sense that the sampling distribution of \\(F\\) is always 0 or greater. 2) it does not look normal. No it does not. \\(F\\) can have many different looking shapes, depending on the degrees of freedom in the numerator and denominator. However, these aspects are too important for now.\nRemember, before we talked about some intuitive ideas for understanding \\(F\\), based on the idea that \\(F\\) is a ratio of what we can explain (variance due to mean differences), divided by what we can’t explain (the error variance). When the error variance is higher than the effect variance, then we will always get an \\(F\\)-value less than one. You can see that we often got \\(F\\)-values less than one in the simulation. This is sensible, after all we were simulating samples coming from the very same distribution. On average there should be no differences between the means. So, on average the part of the total variance that is explained by the means should be less than one, or around one, because it should be roughly the same as the amount of error variance (remember, we are simulating no differences).\nAt the same time, we do see that some \\(F\\)-values are larger than 1. There are little bars that we can see going all the way up to about 5. If you were to get an \\(F\\)-value of 5, you might automatically think, that’s a pretty big \\(F\\)-value. Indeed it kind of is, it means that you can explain 5 times more of variance than you can’t explain. That seems like a lot. You can also see that larger \\(F\\)-values don’t occur very often. As a final reminder, what you are looking at is how the \\(F\\)-statistic (measured from each of 10,000 simulated experiments) behaves when the only thing that can cause differences in the means is random sampling error. Just by chance sometimes the means will be different. You are looking at another chance window. These are the \\(F\\)s that chance can produce.\n\n7.3.1 Making Decisions\nWe can use the sampling distribution of \\(F\\) (for the null) to make decisions about the role of chance in a real experiment. For example, we could do the following.\n\nSet an alpha criterion of \\(p\\) = 0.05\nFind out the critical value for \\(F\\), for our particular situation (with our \\(df\\)s for the numerator and denominator).\n\nLet’s do that. I’ve drawn the line for the critical value onto the histogram in Figure 7.2:\n\nggplot(plot_df, aes(x = save_F)) +\n  geom_histogram(color = \"white\", bins = 100) +\n  theme_classic() +\n  geom_vline(xintercept = qf(.95, 2, 27)) +\n  annotate(\n    \"rect\",\n    xmin = qf(.95, 2, 27),\n    xmax = Inf,\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.5,\n    fill = \"green\"\n  ) +\n  geom_label(data = data.frame(\n    x = qf(.95, 2, 27),\n    y = 500,\n    label = round(qf(.95, 2, 27), digits = 2)\n  ), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = 7.5, y = 500, label = \"5% of F-values\"),\n             aes(x = x, y = y, label = label))\n\n\n\n\nFigure 7.2: The critical value for \\(F\\) where 5% of all \\(F\\)-values lie beyond this point\n\n\n\n\nAlright, now we can see that only 5% of all \\(F\\)-values from from this sampling distribution will be 3.35 or larger. We can use this information.\nHow would we use it? Imagine we ran a real version of this experiment. And, we really used some pills that just might change smartness. If we ran the exact same design, with 30 people in total (10 in each group), we could set an \\(F\\) criterion of 3.35 for determining whether any of our results reflected a causal change in smartness due to the pills, and not due to random chance. For example, if we found an \\(F\\)-value of 3.34, which happens, just less than 5% of the time, we might conclude that random sampling error did not produce the differences between our means. Instead, we might be more confident that the pills actually did something, after all an \\(F\\)-value of 3.34 doesn’t happen very often, it is unlikely (only 5 times out of 100) to occur by chance.\n\n\n7.3.2 Fs and means\nUp to here we have been building your intuition for understanding \\(F\\). We went through the calculation of \\(F\\) from sample data. We went through the process of simulating thousands of \\(F\\)s to show you the null distribution. We have not talked so much about what researchers really care about…The MEANS! The actual results from the experiment. Were the means different? that’s often what people want to know. So, now we will talk about the means, and \\(F\\), together.\nNotice, if I told you I ran an experiment with three groups, testing whether some manipulation changes the behavior of the groups, and I told you that I found a big \\(F\\)!, say an \\(F\\) of 6!. And, that the \\(F\\) of 6 had a \\(p\\)-value of .001. What would you know based on that information alone? You would only know that Fs of 6 don’t happen very often by chance. In fact they only happen 0.1% of the time, that’s hardly at all. If someone told me those values, I would believe that the results they found in their experiment were not likely due to chance. However, I still would not know what the results of the experiment were! Nobody told us what the means were in the different groups, we don’t know what happened!\nIMPORTANT: even though we don’t know what the means were, we do know something about them, whenever we get \\(F\\)-values and \\(p\\)-values like that (big \\(F\\)s, and very small associated \\(p\\)s)… Can you guess what we know? I’ll tell you. We automatically know that there must have been some differences between the means. If there was no differences between the means, then the variance explained by the means (the numerator for \\(F\\)) would not be very large. So, we know that there must be some differences, we just don’t know what they are. Of course, if we had the data, all we would need to do is look at the means for the groups (the ANOVA table doesn’t report this, we need to do it as a separate step).\n\n7.3.2.1 ANOVA is an omnibus test\nThis property of the ANOVA is why the ANOVA is sometimes called the omnibus test. Omnibus is a fun word, it sounds like a bus I’d like to ride. The meaning of omnibus, according to the dictionary, is “comprising several items”. The ANOVA is, in a way, one omnibus test, comprising several little tests.\nFor example, if you had three groups, A, B, and C. You get could differences between\n\nA and B\nB and C\nA and C\n\nThat’s three possible differences you could get. You could run separate \\(t\\)-tests, to test whether each of those differences you might have found could have been produced by chance. Or, you could run an ANOVA, like what we have been doing, to ask one more general question about the differences. Here is one way to think about what the omnibus test is testing:\nHypothesis of no differences anywhere: $ A = B = C $\nAny differences anywhere:\n\n$ A B = C $\n$ A = B C $\n$ A C = B $\n\nThe \\(\\neq\\) symbol means “does not equal”, it’s an equal sign with a cross through it (no equals allowed!).\nHow do we put all of this together. Generally, when we get a small \\(F\\)-value, with a large \\(p\\)-value, we will not reject the hypothesis of no differences. We will say that we do not have evidence that the means of the three groups are in any way different, and the differences that are there could easily have been produced by chance. When we get a large F with a small \\(p\\)-value (one that is below our alpha criterion), we will generally reject the hypothesis of no differences. We would then assume that at least one group mean is not equal to one of the others. That is the omnibus test. Rejecting the null in this way is rejecting the idea there are no differences. But, the \\(F\\) test still does not tell you which of the possible group differences are the ones that are different.\n\n\n7.3.2.2 Looking at a bunch of group means\nWe just ran 10,000 experiments and we didn’t even once look at the group means for any of the experiments. Different patterns of group means under the null are shown in Figure 7.3 for a subset of 10 random simulations.\n\n#library(dplyr)\n#library(ggplot2)\n\nall_df &lt;- data.frame()\n\nfor (i in 1:10) {\n  smartness   &lt;- rnorm(30, 100, 10)\n  pill_group  &lt;- as.factor(rep(1:3, each = 10))\n  simulations &lt;- rep(i, each = 30)\n  sample_df   &lt;- data.frame(simulations, pill_group, smartness)\n  all_df      &lt;- rbind(all_df, sample_df)\n}\n\n#print(all_df[1:50,])\n\nall_df$simulations  &lt;- as.factor(all_df$simulations)\n\nplot_df2 &lt;- all_df %&gt;%\n  dplyr::group_by(simulations, pill_group) %&gt;%\n  dplyr::summarise(group_means = mean(smartness),\n                   group_SE = sd(smartness) / sqrt(length(smartness)))\n\n`summarise()` has grouped output by 'simulations'. You can override using the\n`.groups` argument.\n\n#print(plot_df2[1:10,])\n\nggplot(data = plot_df2, aes(x = pill_group, y = group_means, color = simulations)) +\n  geom_point() +\n  geom_errorbar(aes(ymin = group_means - group_SE, ymax = group_means +\n                      group_SE)) +\n  theme_classic() +\n  ylab(\"Mean Smartness\") +\n  facet_wrap( ~ simulations, nrow=2)\n\n\n\n\nFigure 7.3: Different patterns of group means under the null (all scores for each group sampled from the same distribution).\n\n\n\n\nWhoa, that’s a lot to look at. What is going on here? Each little box represents the outcome of a simulated experiment. The dots are the means for each group (whether subjects took 1 , 2, or 3 magic pills). The y-axis shows the mean smartness for each group. The error bars are standard errors of the mean.\nYou can see that each of the 10 experiments turn out different. Remember, we sampled 10 numbers for each group from the same normal distribution with mean = 100, and sd = 10. So, we know that the correct means for each sample should actually be 100 every single time. However, they are not 100 every single time because of?…sampling error (Our good friend that we talk about all the time).\nFor most of the simulations the error bars are all overlapping, this suggests visually that the means are not different. However, some of them look like they are not overlapping so much, and this would suggest that they are different. This is the siren song of chance (sirens lured sailors to their deaths at sea…beware of the siren call of chance). If we concluded that any of these sets of means had a true difference, we would be committing a type I error. Because we made the simulation, we know that none of these means are actually different. But, when you are running a real experiment, you don’t get to know this for sure.\n\n\n7.3.2.3 Looking at bar graphs\nLet’s look at the exact same graph as above, but this time use bars to visually illustrate the means, instead of dots. We’ll re-do our simulation of 10 experiments, so the pattern will be a little bit different:\n\nall_df &lt;- data.frame()\n\nfor (i in 1:10) {\n  smartness   &lt;- rnorm(30, 100, 10)\n  pill_group  &lt;- as.factor(rep(1:3, each = 10))\n  simulations &lt;- rep(i, each = 30)\n  sample_df   &lt;- data.frame(simulations, pill_group, smartness)\n  all_df      &lt;- rbind(all_df, sample_df)\n}\n\nall_df$simulations  &lt;- as.factor(all_df$simulations)\n\nplot_df2 &lt;- all_df %&gt;%\n  dplyr::group_by(simulations, pill_group) %&gt;%\n  dplyr::summarize(group_means = mean(smartness),\n                   group_SE = sd(smartness) / sqrt(length(smartness)))\n\n`summarise()` has grouped output by 'simulations'. You can override using the\n`.groups` argument.\n\nggplot(plot_df2, aes(x = pill_group, y = group_means, fill = simulations)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  geom_errorbar(aes(ymin = group_means - group_SE, ymax = group_means +\n                      group_SE)) +\n  theme_classic() +\n  ylab(\"Mean Smartness\") +\n  facet_wrap( ~ simulations, nrow=2) +\n  coord_cartesian(ylim = c(90, 110))\n\n\n\n\nFigure 7.4: Different patterns of group means under the null (all scores for each group sampled from the same distribution).\n\n\n\n\nIn Figure 7.4 the heights of the bars display the means for each pill group. The pattern across simulations is generally the same. Some of the fake experiments look like there might be differences, and some of them don’t.\n\n\n7.3.2.4 What mean differences look like when \\(F\\) is less than 1\nWe are now giving you some visual experience looking at what means look like from a particular experiment. This is for your stats intuition. We’re trying to improve your data senses.\nWhat we are going to do now is similar to what we did before. Except this time we are going to look at 10 simulated experiments, where all of the \\(F\\)-values were less than 1. All of these \\(F\\)-values would also be associated with fairly large \\(p\\)-values. When F is less than one, we would not reject the hypothesis of no differences. So, when we look at patterns of means when F is less than 1, we should see mostly the same means, and no big differences.\n\nall_df &lt;- data.frame()\ncounter &lt;- 0\nfor (i in 1:100) {\n  smartness   &lt;- rnorm(30, 100, 10)\n  pill_group  &lt;- as.factor(rep(1:3, each = 10))\n  simulations &lt;- rep(i, each = 30)\n  sample_df   &lt;- data.frame(simulations, pill_group, smartness)\n  aov.out &lt;- summary(aov(smartness ~ pill_group, sample_df))\n  the_f &lt;- aov.out[[1]]$`F value`[1]\n  if (the_f &lt; 1) {\n    all_df &lt;- rbind(all_df, sample_df)\n    counter &lt;- counter + 1\n  }\n  if (counter == 10) {\n    break\n  }\n}\n\nall_df$simulations  &lt;- as.factor(all_df$simulations)\n\nplot_df &lt;- all_df %&gt;%\n  dplyr::group_by(simulations, pill_group) %&gt;%\n  dplyr::summarise(means = mean(smartness),\n                   SEs = sd(smartness) / sqrt(length(smartness)))\n\n`summarise()` has grouped output by 'simulations'. You can override using the\n`.groups` argument.\n\nggplot(plot_df, aes(x = pill_group, y = means, fill = simulations)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  geom_errorbar(aes(ymin = means - SEs, ymax = means + SEs)) +\n  theme_classic() +\n  facet_wrap( ~ simulations, nrow=2) +\n  ylab(\"Mean Smartness\") +\n  coord_cartesian(ylim = c(85, 115))\n\n\n\n\nFigure 7.5: Different patterns of group means under the null (sampled from same distribution) when F is less than 1.\n\n\n\n\nIn Figure 7.5 the numbers in the panels now tell us which simulations actually produced \\(F\\)s of less than 1.\nWe see here that all the bars aren’t perfectly flat, that’s OK. What’s more important is that for each panel, the error bars for each mean are totally overlapping with all the other error bars. We can see visually that our estimate of the mean for each sample is about the same for all of the bars. That’s good, we wouldn’t make any type I errors here.\n\n\n7.3.2.5 What mean differences look like when F &gt; 3.35\nEarlier we found that the critical value for \\(F\\) in our situation was 3.35, this was the location on the \\(F\\) distribution where only 5% of \\(F\\)s were 3.35 or greater. We would reject the hypothesis of no differences whenever \\(F\\) was greater than 3.35. In this case, whenever we did that, we would be making a type I error. That is because we are simulating the distribution of no differences (remember all of our sample means are coming from the exact same distribution). So, now we can take a look at what type I errors look like. In other words, we can run some simulations and look at the pattern in the means, only when \\(F\\) happens to be 3.35 or greater (this only happens 5% of the time, so we might have to let the computer simulate for a while). Let’s see what that looks like:\n\nall_df &lt;- data.frame()\ncounter &lt;- 0\nfor (i in 1:1000) {\n  smartness   &lt;- rnorm(30, 100, 10)\n  pill_group  &lt;- as.factor(rep(1:3, each = 10))\n  simulations &lt;- rep(i, each = 30)\n  sample_df   &lt;- data.frame(simulations, pill_group, smartness)\n  aov.out &lt;- summary(aov(smartness ~ pill_group, sample_df))\n  the_f &lt;- aov.out[[1]]$`F value`[1]\n  if (the_f &gt; 3.35) {\n    all_df &lt;- rbind(all_df, sample_df)\n    counter &lt;- counter + 1\n  }\n  if (counter == 10) {\n    break\n  }\n}\n\nall_df$simulations  &lt;- as.factor(all_df$simulations)\n\nplot_df &lt;- all_df %&gt;%\n  dplyr::group_by(simulations, pill_group) %&gt;%\n  dplyr::summarise(means = mean(smartness),\n                   SEs = sd(smartness) / sqrt(length(smartness)))\n\n`summarise()` has grouped output by 'simulations'. You can override using the\n`.groups` argument.\n\nggplot(plot_df, aes(x = pill_group, y = means, fill = simulations)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  geom_errorbar(aes(ymin = means - SEs, ymax = means + SEs)) +\n  theme_classic() +\n  facet_wrap( ~ simulations, nrow = 2) +\n  ylab(\"Mean Smartness\") +\n  coord_cartesian(ylim = c(85, 115))\n\n\n\n\nFigure 7.6: Different patterns of group means under the null when F is above critical value (these are all type I Errors).\n\n\n\n\nThe numbers in the panels now tell us which simulations actually produced \\(F\\)s that were greater than 3.35\nWhat do you notice about the pattern of means inside each panel of Figure 7.6? Now, every the panels show at least one mean that is different from the others. Specifically, the error bars for one mean do not overlap with the error bars for one or another mean. This is what mistakes looks like. These are all type I errors. They are insidious. When they happen to you by chance, the data really does appear to show a strong pattern, your \\(F\\)-value is large, and your \\(p\\)-value is small! It is easy to be convinced by a type I error (it’s the siren song of chance)."
  },
  {
    "objectID": "07-ANOVA.html#anova-on-real-data",
    "href": "07-ANOVA.html#anova-on-real-data",
    "title": "7  ANOVA",
    "section": "7.4 ANOVA on Real Data",
    "text": "7.4 ANOVA on Real Data\nWe’ve covered many fundamentals about the ANOVA, how to calculate the necessary values to obtain an \\(F\\)-statistic, and how to interpret the \\(F\\)-statistic along with it’s associate \\(p\\)-value once we have one. In general, you will be conducting ANOVAs and playing with \\(F\\)s and \\(p\\)s using software that will automatically spit out the numbers for you. It’s important that you understand what the numbers mean, that’s why we’ve spent time on the concepts. We also recommend that you try to compute an ANOVA by hand at least once. It builds character, and let’s you know that you know what you are doing with the numbers.\nBut, we’ve probably also lost the real thread of all this. The core thread is that when we run an experiment we use our inferential statistics, like ANOVA, to help us determine whether the differences we found are likely due to chance or not. In general, we like to find out that the differences that we find are not due to chance, but instead to due to our manipulation.\nSo, we return to the application of the ANOVA to a real data set with a real question. This is the same one that you will be learning about in the lab. We give you a brief overview here so you know what to expect.\n\n7.4.1 Tetris and bad memories\nYup, you read that right. The research you will learn about tests whether playing Tetris after watching a scary movie can help prevent you from having bad memories from the movie (James et al. 2015). Sometimes in life people have intrusive memories, and they think about things they’d rather not have to think about. This research looks at one method that could reduce the frequency of intrusive memories.\nHere’s what they did. Subjects watched a scary movie, then at the end of the week they reported how many intrusive memories about the movie they had. The mean number of intrusive memories was the measurement (the dependent variable). This was a between-subjects experiment with four groups. Each group of subjects received a different treatment following the scary movie. The question was whether any of these treatments would reduce the number of intrusive memories. All of these treatments occurred after watching the scary movie:\n\nNo-task control: These participants completed a 10-minute music filler task after watching the scary movie.\nReactivation + Tetris: These participants were shown a series of images from the trauma film to reactivate the traumatic memories (i.e., reactivation task). Then, participants played the video game Tetris for 12 minutes.\nTetris Only: These participants played Tetris for 12 minutes, but did not complete the reactivation task.\nReactivation Only: These participants completed the reactivation task, but did not play Tetris.\n\nFor reasons we elaborate on in the lab, the researchers hypothesized that the Reactivation+Tetris group would have fewer intrusive memories over the week than the other groups.\nLet’s look at the findings. Note you will learn how to do all of these steps in the lab. For now, we just show the findings and the ANOVA table. Then we walk through how to interpret it.\n\nlibrary(data.table)\n\nall_data &lt;- fread(\"data/Jamesetal2015Experiment2.csv\")\n\nall_data$Condition &lt;- as.factor(all_data$Condition)\nlevels(all_data$Condition) &lt;- c(\"Control\",\n                                \"Reactivation+Tetris\",\n                                \"Tetris_only\",\n                                \"Reactivation_only\")\n\n# get means and SEs\ndescriptive_df &lt;- all_data %&gt;%\n  dplyr::group_by(Condition) %&gt;%\n  dplyr::summarise(\n    means = mean(Days_One_to_Seven_Number_of_Intrusions),\n    SEs = sd(Days_One_to_Seven_Number_of_Intrusions) /\n      sqrt(length(Days_One_to_Seven_Number_of_Intrusions))\n  )\n\n# Make the plot\nggplot(descriptive_df, aes(x = Condition, y = means)) +\n  geom_bar(stat = \"identity\", aes(fill = Condition)) + # add means\n  geom_errorbar(aes(ymin = means - SEs,               # add error bars\n                    ymax = means + SEs), width = .1) +\n  geom_point(\n    data = all_data,\n    aes(x = Condition, y = Days_One_to_Seven_Number_of_Intrusions),\n    alpha = .5\n  ) +\n  geom_point(alpha = .25) +\n  ylab(\"Intrusive Memories (Mean for Week)\")+\n  theme_classic()\n\n\n\n\nFigure 7.7: Mean number of intrusive memories per week as a function of experimental treatments.\n\n\n\n\nOOooh, look at that. We did something fancy. Figure 7.7 shows the data from the four groups. The height of each bar shows the mean intrusive memories for the week. The dots show the individual scores for each subject in each group (useful to to the spread of the data). The error bars show the standard errors of the mean.\nWhat can we see here? Right away it looks like there is some support for the research hypothesis. The green bar, for the Reactivation + Tetris group had the lowest mean number of intrusive memories. Also, the error bar is not overlapping with any of the other error bars. This implies that the mean for the Reactivation + Tetris group is different from the means for the other groups. And, this difference is probably not very likely by chance.\nWe can now conduct the ANOVA on the data to ask the omnibus question. If we get a an \\(F\\)-value with an associated \\(p\\)-value of less than .05 (the alpha criterion set by the authors), then we can reject the hypothesis of no differences. Let’s see what happens:\n\nlibrary(xtable)\naov_out&lt;-aov(Days_One_to_Seven_Number_of_Intrusions ~ Condition, all_data)\nsummary_out&lt;-summary(aov_out)\n\nknitr::kable(xtable(summary_out))\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\nCondition\n3\n114.8194\n38.27315\n3.794762\n0.0140858\n\n\nResiduals\n68\n685.8333\n10.08578\nNA\nNA\n\n\n\n\n\nWe see the ANOVA table, it’s up there. We could report the results from the ANOVA table like this:\n\nThere was a significant main effect of treatment condition, F(3, 68) = 3.79, MSE = 10.08, p=0.014.\n\nWe called this a significant effect because the \\(p\\)-value was less than 0.05. In other words, the \\(F\\)-value of 3.79 only happens 1.4% of the time when the null is true. Or, the differences we observed in the means only occur by random chance (sampling error) 1.4% of the time. Because chance rarely produces this kind of result, the researchers made the inference that chance DID NOT produce their differences, instead, they were inclined to conclude that the Reactivation + Tetris treatment really did cause a reduction in intrusive memories. That’s pretty neat.\n\n\n7.4.2 Comparing means after the ANOVA\nRemember that the ANOVA is an omnibus test, it just tells us whether we can reject the idea that all of the means are the same. The F-test (synonym for ANOVA) that we just conducted suggested we could reject the hypothesis of no differences. As we discussed before, that must mean that there are some differences in the pattern of means.\nGenerally after conducting an ANOVA, researchers will conduct follow-up tests to compare differences between specific means. We will talk more about this practice throughout the textbook. There are many recommended practices for follow-up tests, and there is a lot of debate about what you should do. We are not going to wade into this debate right now. Instead we are going to point out that you need to do something to compare the means of interest after you conduct the ANOVA, because the ANOVA is just the beginning…It usually doesn’t tell you want you want to know. You might wonder why bother conducting the ANOVA in the first place…Not a terrible question at all. A good question. You will see as we talk about more complicated designs, why ANOVAs are so useful. In the present example, they are just a common first step. There are required next steps, such as what we do next.\nHow can you compare the difference between two means, from a between-subjects design, to determine whether or not the difference you observed is likely or unlikely to be produced by chance? We covered this one already, it’s the independent \\(t\\)-test. We’ll do a couple \\(t\\)-tests, showing the process.\n\n7.4.2.1 Control vs. Reactivation+Tetris\nWhat we really want to know is if Reactivation+Tetris caused fewer intrusive memories…but compared to what? Well, if it did something, the Reactivation+Tetris group should have a smaller mean than the Control group. So, let’s do that comparison:\n\ncomparison_df &lt;- all_data %&gt;% \n                  filter(Condition %in% c('Control','Reactivation+Tetris')==TRUE)\n                        \nt.test(Days_One_to_Seven_Number_of_Intrusions ~ Condition, \n       comparison_df,\n       var.equal=TRUE)\n\n\n    Two Sample t-test\n\ndata:  Days_One_to_Seven_Number_of_Intrusions by Condition\nt = 2.9893, df = 34, p-value = 0.005167\nalternative hypothesis: true difference in means between group Control and group Reactivation+Tetris is not equal to 0\n95 percent confidence interval:\n 1.031592 5.412852\nsample estimates:\n            mean in group Control mean in group Reactivation+Tetris \n                         5.111111                          1.888889 \n\n\nWe found that there was a significant difference between the control group (M=5.11) and Reactivation + Tetris group (M=1.89), t(34) = 2.99, p=0.005.\nAbove you just saw an example of reporting another \\(t\\)-test. This sentences does an OK job of telling the reader everything they want to know. It has the means for each group, and the important bits from the \\(t\\)-test.\nMore important, as we suspected the difference between the control and Reactivation + Tetris group was likely not due to chance.\n\n\n7.4.2.2 Control vs. Tetris_only\nNow we can really start wondering what caused the difference. Was it just playing Tetris? Does just playing Tetris reduce the number of intrusive memories during the week? Let’s compare that to control:\n\ncomparison_df &lt;- all_data %&gt;% \n                  filter(Condition %in% c('Control','Tetris_only')==TRUE)\n                        \nt.test(Days_One_to_Seven_Number_of_Intrusions ~ Condition, \n       comparison_df,\n       var.equal=TRUE)\n\n\n    Two Sample t-test\n\ndata:  Days_One_to_Seven_Number_of_Intrusions by Condition\nt = 1.0129, df = 34, p-value = 0.3183\nalternative hypothesis: true difference in means between group Control and group Tetris_only is not equal to 0\n95 percent confidence interval:\n -1.230036  3.674480\nsample estimates:\n    mean in group Control mean in group Tetris_only \n                 5.111111                  3.888889 \n\n\nHere we did not find a significant difference. We found that no significant difference between the control group (M=5.11) and Tetris Only group (M=3.89), t(34) = 2.99, p=0.318.\nSo, it seems that not all of the differences between our means are large enough to be called statistically significant. In particular, the difference here, or larger, happens by chance 31.8% of the time.\nYou could go on doing more comparisons, between all of the different pairs of means. Each time conducting a \\(t\\)-test, and each time saying something more specific about the patterns across the means than you get to say with the omnibus test provided by the ANOVA.\nUsually, it is the pattern of differences across the means that you as a researcher are primarily interested in understanding. Your theories will make predictions about how the pattern turns out (e.g., which specific means should be higher or lower and by how much). So, the practice of doing comparisons after an ANOVA is really important for establishing the patterns in the means."
  },
  {
    "objectID": "07-ANOVA.html#anova-summary",
    "href": "07-ANOVA.html#anova-summary",
    "title": "7  ANOVA",
    "section": "7.5 ANOVA Summary",
    "text": "7.5 ANOVA Summary\nWe have just finished a rather long introduction to the ANOVA, and the \\(F\\)-test. The next couple of chapters continue to explore properties of the ANOVA for different kinds of experimental designs. In general, the process to follow for all of the more complicated designs is very similar to what we did here, which boils down to two steps:\n\nconduct the ANOVA on the data\nconduct follow-up tests, looking at differences between particular means\n\nSo what’s next…the ANOVA for repeated measures designs. See you in the next chapter.\n\n\n\n\nJames, Ella L, Michael B Bonsall, Laura Hoppitt, Elizabeth M Tunbridge, John R Geddes, Amy L Milton, and Emily A Holmes. 2015. “Computer Game Play Reduces Intrusive Memories of Experimental Trauma via Reconsolidation-Update Mechanisms.” Psychological Science 26 (8): 1201–15. https://doi.org/10.1177/0956797615583071.\n\n\nSalsburg, David. 2001. The Lady Tasting Tea: How Statistics Revolutionized Science in the Twentieth Century. Macmillan."
  },
  {
    "objectID": "08-RMANOVA.html#repeated-measures-design",
    "href": "08-RMANOVA.html#repeated-measures-design",
    "title": "8  Repeated Measures ANOVA",
    "section": "8.1 Repeated measures design",
    "text": "8.1 Repeated measures design\nLet’s use the exact same toy example from the previous chapter, but let’s convert it to a repeated measures design.\nLast time, we imagined we had some data in three groups, A, B, and C, such as in Table 8.1:\n\nscores &lt;- c(20,11,2,6,2,7,2,11,2)\ngroups &lt;- as.character(rep(c(\"A\",\"B\",\"C\"), each=3))\ndf&lt;-data.frame(groups,scores)\n\nknitr::kable(df)\n\n\n\nTable 8.1: Example data for three groups.\n\n\ngroups\nscores\n\n\n\n\nA\n20\n\n\nA\n11\n\n\nA\n2\n\n\nB\n6\n\n\nB\n2\n\n\nB\n7\n\n\nC\n2\n\n\nC\n11\n\n\nC\n2\n\n\n\n\n\n\nThe above table represents a between-subject design where each score involves a unique subject.\nLet’s change things up a tiny bit, and imagine we only had 3 subjects in total in the experiment. And, that each subject contributed data to the three levels of the independent variable, A, B, and C. Before we called the IV groups, because there were different groups of subjects. Let’s change that to conditions, because now the same group of subjects participates in all three conditions. Table 8.2 shows a within-subjects (repeated measures) version of this experiment:\n\nscores &lt;- c(20,11,2,6,2,7,2,11,2)\nconditions &lt;- as.character(rep(c(\"A\",\"B\",\"C\"), each=3))\nsubjects &lt;-rep(1:3,3)\ndf&lt;-data.frame(subjects,conditions,scores)\n\nknitr::kable(df)\n\n\n\nTable 8.2: Example data for a repeated measures design with three conditions, where each subject contributes data in each condition.\n\n\nsubjects\nconditions\nscores\n\n\n\n\n1\nA\n20\n\n\n2\nA\n11\n\n\n3\nA\n2\n\n\n1\nB\n6\n\n\n2\nB\n2\n\n\n3\nB\n7\n\n\n1\nC\n2\n\n\n2\nC\n11\n\n\n3\nC\n2"
  },
  {
    "objectID": "08-RMANOVA.html#partitioning-the-sums-of-squares",
    "href": "08-RMANOVA.html#partitioning-the-sums-of-squares",
    "title": "8  Repeated Measures ANOVA",
    "section": "8.2 Partitioning the Sums of Squares",
    "text": "8.2 Partitioning the Sums of Squares\nTime to introduce a new name for an idea you learned about last chapter, it’s called partitioning the sums of squares. Sometimes an obscure new name can be helpful for your understanding of what is going on. ANOVAs are all about partitioning the sums of squares. We already did some partitioning in the last chapter. What do we mean by partitioning?\nImagine you had a big empty house with no rooms in it. What would happen if you partitioned the house? What would you be doing? One way to partition the house is to split it up into different rooms. You can do this by adding new walls and making little rooms everywhere. That’s what partitioning means, to split up.\nThe act of partitioning, or splitting up, is the core idea of ANOVA. To use the house analogy. Our total sums of squares (SS Total) is our big empty house. We want to split it up into little rooms. Before we partitioned SS Total using this formula:\n\\(SS_\\text{TOTAL} = SS_\\text{Effect} + SS_\\text{Error}\\)\nRemember, the \\(SS_\\text{Effect}\\) was the variance we could attribute to the means of the different groups, and \\(SS_\\text{Error}\\) was the leftover variance that we couldn’t explain. \\(SS_\\text{Effect}\\) and \\(SS_\\text{Error}\\) are the partitions of \\(SS_\\text{TOTAL}\\), they are the little rooms.\nIn the between-subjects case above, we got to split \\(SS_\\text{TOTAL}\\) into two parts. What is most interesting about the repeated-measures design, is that we get to split \\(SS_\\text{TOTAL}\\) into three parts, there’s one more partition. Can you guess what the new partition is? Hint: whenever we have a new way to calculate means in our design, we can always create a partition for those new means. What are the new means in the repeated measures design?\nHere is the new idea for partitioning \\(SS_\\text{TOTAL}\\) in a repeated-measures design:\n\\(SS_\\text{TOTAL} = SS_\\text{Effect} + SS_\\text{Subjects} +SS_\\text{Error}\\)\nWe’ve added \\(SS_\\text{Subjects}\\) as the new idea in the formula. What’s the idea here? Well, because each subject was measured in each condition, we have a new set of means. These are the means for each subject, collapsed across the conditions. For example, subject 1 has a mean (mean of their scores in conditions A, B, and C); subject 2 has a mean (mean of their scores in conditions A, B, and C); and subject 3 has a mean (mean of their scores in conditions A, B, and C). There are three subject means, one for each subject, collapsed across the conditions. And, we can now estimate the portion of the total variance that is explained by these subject means.\nWe just showed you a “formula” to split up \\(SS_\\text{TOTAL}\\) into three parts, but we called the formula an idea. We did that because the way we wrote the formula is a little bit misleading, and we need to clear something up. Before we clear the thing up, we will confuse you just a little bit. Be prepared to be confused a little bit.\nFirst, we need to introduce you to some more terms. It turns out that different authors use different words to describe parts of the ANOVA. This can be really confusing. For example, we described the SS formula for a between subjects design like this:\n\\(SS_\\text{TOTAL} = SS_\\text{Effect} + SS_\\text{Error}\\)\nHowever, the very same formula is often written differently, using the words between and within in place of effect and error, it looks like this:\n\\(SS_\\text{TOTAL} = SS_\\text{Between} + SS_\\text{Within}\\)\nWhoa, hold on a minute. Haven’t we switched back to talking about a between-subjects ANOVA. YES! Then why are we using the word within, what does that mean? YES! We think this is very confusing for people. Here the word within has a special meaning. It does not refer to a within-subjects design. Let’s explain. First, \\(SS_\\text{Between}\\) (which we have been calling \\(SS_\\text{Effect}\\)) refers to variation between the group means, that’s why it is called \\(SS_\\text{Between}\\). Second, and most important, \\(SS_\\text{Within}\\) (which we have been calling \\(SS_\\text{Error}\\)), refers to the leftover variation within each group mean. Specifically, it is the variation between each group mean and each score in the group. “AAGGH, you’ve just used the word between to describe within group variation!”. Yes! We feel your pain. Remember, for each group mean, every score is probably off a little bit from the mean. So, the scores within each group have some variation. This is the within group variation, and it is why the leftover error that we can’t explain is often called \\(SS_\\text{Within}\\).\nOK. So why did we introduce this new confusing way of talking about things? Why can’t we just use \\(SS_\\text{Error}\\) to talk about this instead of \\(SS_\\text{Within}\\), which you might (we do) find confusing. We’re getting there, but perhaps Figure 8.1 will help out.\n\nknitr::include_graphics(\"imgs/figures/SS_ANOVA.png\")\n\n\n\n\nFigure 8.1: Illustration showing how the total sums of squares are partitioned differently for a between versus repeated-measures design\n\n\n\n\nThe figure lines up the partitioning of the Sums of Squares for both between-subjects and repeated-measures designs. In both designs, \\(SS_\\text{Total}\\) is first split up into two pieces \\(SS_\\text{Effect (between-groups)}\\) and \\(SS_\\text{Error (within-groups)}\\). At this point, both ANOVAs are the same. In the repeated measures case we split the \\(SS_\\text{Error (within-groups)}\\) into two more littler parts, which we call \\(SS_\\text{Subjects (error variation about the subject mean)}\\) and \\(SS_\\text{Error (left-over variation we can't explain)}\\).\nSo, when we earlier wrote the formula to split up SS in the repeated-measures design, we were kind of careless in defining what we actually meant by \\(SS_\\text{Error}\\), this was a little too vague:\n\\(SS_\\text{TOTAL} = SS_\\text{Effect} + SS_\\text{Subjects} +SS_\\text{Error}\\)\nThe critical feature of the repeated-measures ANOVA, is that the \\(SS_\\text{Error}\\) that we will later use to compute the MSE in the denominator for the \\(F\\)-value, is smaller in a repeated-measures design, compared to a between subjects design. This is because the \\(SS_\\text{Error (within-groups)}\\) is split into two parts, \\(SS_\\text{Subjects (error variation about the subject mean)}\\) and \\(SS_\\text{Error (left-over variation we can't explain)}\\).\nTo make this more clear, consider Figure 8.2:\n\nknitr::include_graphics(\"imgs/figures/SS_RMANOVA.png\")\n\n\n\n\nFigure 8.2: Close-up showing that the Error term is split into two parts in the repeated measures design\n\n\n\n\nAs we point out, the \\(SS_\\text{Error (left-over)}\\) in the green circle will be a smaller number than the \\(SS_\\text{Error (within-group)}\\). That’s because we are able to subtract out the \\(SS_\\text{Subjects}\\) part of the \\(SS_\\text{Error (within-group)}\\). As we will see shortly, this can have the effect of producing larger F-values when using a repeated-measures design compared to a between-subjects design."
  },
  {
    "objectID": "08-RMANOVA.html#calculating-the-rm-anova",
    "href": "08-RMANOVA.html#calculating-the-rm-anova",
    "title": "8  Repeated Measures ANOVA",
    "section": "8.3 Calculating the RM ANOVA",
    "text": "8.3 Calculating the RM ANOVA\nNow that you are familiar with the concept of an ANOVA table (remember the table from last chapter where we reported all of the parts to calculate the \\(F\\)-value?), we can take a look at the things we need to find out to make the ANOVA table. Figure 8.3 presents an abstract for the repeated-measures ANOVA table. It shows us all the thing we need to calculate to get the \\(F\\)-value for our data.\n\nknitr::include_graphics(\"imgs/figures/RMANOVA_table.png\")\n\n\n\n\nFigure 8.3: Equations for computing the ANOVA table for a repeated measures design\n\n\n\n\nSo, what we need to do is calculate all the \\(SS\\)es that we did before for the between-subjects ANOVA. That means the next three steps are identical to the ones you did before. In fact, I will just basically copy the next three steps to find \\(SS_\\text{TOTAL}\\), \\(SS_\\text{Effect}\\) , and \\(SS_\\text{Error (within-conditions)}\\). After that we will talk about splitting up \\(SS_\\text{Error (within-conditions)}\\) into two parts, this is the new thing for this chapter. Here we go!\n\n8.3.1 SS Total\nThe total sums of squares, or \\(SS\\text{Total}\\) measures the total variation in a set of data. All we do is find the difference between each score and the grand mean, then we square the differences and add them all up.\n\nscores &lt;- c(20,11,2,6,2,7,2,11,2)\nconditions &lt;- as.character(rep(c(\"A\",\"B\",\"C\"), each=3))\nsubjects &lt;-rep(1:3,3)\n\ndiff &lt;-scores-mean(scores)\ndiff_squared &lt;-diff^2\ndf&lt;-data.frame(subjects,conditions,scores,diff, diff_squared)\ndf$conditions&lt;-as.character(df$conditions)\ndf$subjects&lt;-as.character(df$subjects)\n\ndf &lt;- df %&gt;%\n  rbind(c(\"Sums\",\"\", colSums(df[1:9,3:5]))) %&gt;%\n  rbind(c(\"Means\",\"\",colMeans(df[1:9,3:5])))\n\nknitr::kable(df)\n\n\n\n\nsubjects\nconditions\nscores\ndiff\ndiff_squared\n\n\n\n\n1\nA\n20\n13\n169\n\n\n2\nA\n11\n4\n16\n\n\n3\nA\n2\n-5\n25\n\n\n1\nB\n6\n-1\n1\n\n\n2\nB\n2\n-5\n25\n\n\n3\nB\n7\n0\n0\n\n\n1\nC\n2\n-5\n25\n\n\n2\nC\n11\n4\n16\n\n\n3\nC\n2\n-5\n25\n\n\nSums\n\n63\n0\n302\n\n\nMeans\n\n7\n0\n33.5555555555556\n\n\n\n\n\nThe mean of all of the scores is called the Grand Mean. It’s calculated in the table, the Grand Mean = 7.\nWe also calculated all of the difference scores from the Grand Mean. The difference scores are in the column titled diff. Next, we squared the difference scores, and those are in the next column called diff_squared.\nWhen you add up all of the individual squared deviations (difference scores) you get the sums of squares. That’s why it’s called the sums of squares (SS).\nNow, we have the first part of our answer:\n\\(SS_\\text{total} = SS_\\text{Effect} + SS_\\text{Error}\\)\n\\(SS_\\text{total} = 302\\) and\n\\(302 = SS_\\text{Effect} + SS_\\text{Error}\\)\n\n\n8.3.2 SS Effect\n\\(SS_\\text{Total}\\) gave us a number representing all of the change in our data, how they all are different from the grand mean.\nWhat we want to do next is estimate how much of the total change in the data might be due to the experimental manipulation. For example, if we ran an experiment that causes causes change in the measurement, then the means for each group will be different from other, and the scores in each group will be different from each. As a result, the manipulation forces change onto the numbers, and this will naturally mean that some part of the total variation in the numbers is caused by the manipulation.\nThe way to isolate the variation due to the manipulation (also called effect) is to look at the means in each group, and the calculate the difference scores between each group mean and the grand mean, and then the squared deviations to find the sum for \\(SS_\\text{Effect}\\).\nConsider this table, showing the calculations for \\(SS_\\text{Effect}\\).\n\nscores &lt;- c(20,11,2,6,2,7,2,11,2)\nconditions &lt;- as.character(rep(c(\"A\",\"B\",\"C\"), each=3))\nsubjects &lt;-rep(1:3,3)\nmeans &lt;-c(11,11,11,5,5,5,5,5,5)\n\ndiff &lt;-means-mean(scores)\ndiff_squared &lt;-diff^2\ndf&lt;-data.frame(subjects,conditions,scores,means,diff, diff_squared)\ndf$conditions&lt;-as.character(df$conditions)\ndf$subjects&lt;-as.character(df$subjects)\n\ndf &lt;- df %&gt;%\n  rbind(c(\"Sums\",\"\", colSums(df[1:9,3:6]))) %&gt;%\n  rbind(c(\"Means\",\"\",colMeans(df[1:9,3:6])))\n\nknitr::kable(df)\n\n\n\n\nsubjects\nconditions\nscores\nmeans\ndiff\ndiff_squared\n\n\n\n\n1\nA\n20\n11\n4\n16\n\n\n2\nA\n11\n11\n4\n16\n\n\n3\nA\n2\n11\n4\n16\n\n\n1\nB\n6\n5\n-2\n4\n\n\n2\nB\n2\n5\n-2\n4\n\n\n3\nB\n7\n5\n-2\n4\n\n\n1\nC\n2\n5\n-2\n4\n\n\n2\nC\n11\n5\n-2\n4\n\n\n3\nC\n2\n5\n-2\n4\n\n\nSums\n\n63\n63\n0\n72\n\n\nMeans\n\n7\n7\n0\n8\n\n\n\n\n\nNotice we created a new column called means, these are the means for each condition, A, B, and C.\n\\(SS_\\text{Effect}\\) represents the amount of variation that is caused by differences between the means. The diff column is the difference between each condition mean and the grand mean, so for the first row, we have 11-7 = 4, and so on.\nWe found that \\(SS_\\text{Effect} = 72\\), this is the same as the ANOVA from the previous chapter\n\n\n8.3.3 SS Error (within-conditions)\nGreat, we made it to SS Error. We already found SS Total, and SS Effect, so now we can solve for SS Error just like this:\n\\(SS_\\text{total} = SS_\\text{Effect} + SS_\\text{Error (within-conditions)}\\)\nswitching around:\n$ SS_ = SS_ - SS_ $\n$ SS_ = 302 - 72 = 230 $\nOr, we could compute \\(SS_\\text{Error (within conditions)}\\) directly from the data as we did last time:\n\nscores &lt;- c(20,11,2,6,2,7,2,11,2)\nconditions &lt;- as.character(rep(c(\"A\",\"B\",\"C\"), each=3))\nsubjects &lt;-rep(1:3,3)\nmeans &lt;-c(11,11,11,5,5,5,5,5,5)\n\ndiff &lt;-means-scores\ndiff_squared &lt;-diff^2\ndf&lt;-data.frame(subjects,conditions,scores,means,diff, diff_squared)\ndf$conditions&lt;-as.character(df$conditions)\ndf$subjects&lt;-as.character(df$subjects)\n\ndf &lt;- df %&gt;%\n  rbind(c(\"Sums\",\"\", colSums(df[1:9,3:6]))) %&gt;%\n  rbind(c(\"Means\",\"\",colMeans(df[1:9,3:6])))\n\nknitr::kable(df)\n\n\n\n\nsubjects\nconditions\nscores\nmeans\ndiff\ndiff_squared\n\n\n\n\n1\nA\n20\n11\n-9\n81\n\n\n2\nA\n11\n11\n0\n0\n\n\n3\nA\n2\n11\n9\n81\n\n\n1\nB\n6\n5\n-1\n1\n\n\n2\nB\n2\n5\n3\n9\n\n\n3\nB\n7\n5\n-2\n4\n\n\n1\nC\n2\n5\n3\n9\n\n\n2\nC\n11\n5\n-6\n36\n\n\n3\nC\n2\n5\n3\n9\n\n\nSums\n\n63\n63\n0\n230\n\n\nMeans\n\n7\n7\n0\n25.5555555555556\n\n\n\n\n\nWhen we compute \\(SS_\\text{Error (within conditions)}\\) directly, we find the difference between each score and the condition mean for that score. This gives us the remaining error variation around the condition mean, that the condition mean does not explain.\n\n\n8.3.4 SS Subjects\nNow we are ready to calculate new partition, called \\(SS_\\text{Subjects}\\). We first find the means for each subject. For subject 1, this is the mean of their scores across Conditions A, B, and C. The mean for subject 1 is 9.33 (repeating). Notice there is going to be some rounding error here, that’s OK for now.\nThe means column now shows all of the subject means. We then find the difference between each subject mean and the grand mean. These deviations are shown in the diff column. Then we square the deviations, and sum them up.\n\nscores &lt;- c(20,11,2,6,2,7,2,11,2)\nconditions &lt;- as.character(rep(c(\"A\",\"B\",\"C\"), each=3))\nsubjects &lt;-rep(1:3,3)\nmeans &lt;-c(9.33,8,3.66,9.33,8,3.66,9.33,8,3.66)\n\ndiff &lt;-means-mean(scores)\ndiff_squared &lt;-diff^2\ndf&lt;-data.frame(subjects,conditions,scores,means,diff, diff_squared)\ndf$conditions&lt;-as.character(df$conditions)\ndf$subjects&lt;-as.character(df$subjects)\n\ndf &lt;- df %&gt;%\n  rbind(c(\"Sums\",\"\", colSums(df[1:9,3:6]))) %&gt;%\n  rbind(c(\"Means\",\"\",colMeans(df[1:9,3:6])))\n\nknitr::kable(df)\n\n\n\n\n\n\n\n\n\n\n\n\nsubjects\nconditions\nscores\nmeans\ndiff\ndiff_squared\n\n\n\n\n1\nA\n20\n9.33\n2.33\n5.4289\n\n\n2\nA\n11\n8\n1\n1\n\n\n3\nA\n2\n3.66\n-3.34\n11.1556\n\n\n1\nB\n6\n9.33\n2.33\n5.4289\n\n\n2\nB\n2\n8\n1\n1\n\n\n3\nB\n7\n3.66\n-3.34\n11.1556\n\n\n1\nC\n2\n9.33\n2.33\n5.4289\n\n\n2\nC\n11\n8\n1\n1\n\n\n3\nC\n2\n3.66\n-3.34\n11.1556\n\n\nSums\n\n63\n62.97\n-0.0299999999999994\n52.7535\n\n\nMeans\n\n7\n6.99666666666667\n-0.00333333333333326\n5.8615\n\n\n\n\n\nWe found that the sum of the squared deviations \\(SS_\\text{Subjects}\\) = 52.75. Note again, this has some small rounding error because some of the subject means had repeating decimal places, and did not divide evenly.\nWe can see the effect of the rounding error if we look at the sum and mean in the diff column. We know these should be both zero, because the Grand mean is the balancing point in the data. The sum and mean are both very close to zero, but they are not zero because of rounding error.\n\n\n8.3.5 SS Error (left-over)\nNow we can do the last thing. Remember we wanted to split up the \\(SS_\\text{Error (within conditions)}\\) into two parts, \\(SS_\\text{Subjects}\\) and \\(SS_\\text{Error (left-over)}\\). Because we have already calculate \\(SS_\\text{Error (within conditions)}\\) and \\(SS_\\text{Subjects}\\), we can solve for \\(SS_\\text{Error (left-over)}\\):\n\\(SS_\\text{Error (left-over)} = SS_\\text{Error (within conditions)} - SS_\\text{Subjects}\\)\n\\(SS_\\text{Error (left-over)} = SS_\\text{Error (within conditions)} - SS_\\text{Subjects} = 230 - 52.75 = 177.25\\)\n\n\n8.3.6 Check our work\nBefore we continue to compute the MSEs and F-value for our data, let’s quickly check our work. For example, we could have R compute the repeated measures ANOVA for us, and then we could look at the ANOVA table and see if we are on the right track so far.\n\nsummary_out &lt;- summary(aov(scores~conditions + Error(subjects/conditions),df[1:9,]))\nknitr::kable(xtable(summary_out))\n\n\n\nTable 8.3: Example ANOVA table table reporting the degrees of freedom, sums of squares, mean squares, \\(F\\) value and associated \\(p\\) value.\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\nResiduals\n2\n52.66667\n26.33333\nNA\nNA\n\n\nconditions\n2\n72.00000\n36.00000\n0.8120301\n0.505848\n\n\nResiduals\n4\n177.33333\n44.33333\nNA\nNA\n\n\n\n\n\n\nTable 8.3 looks good. We found the \\(SS_\\text{Effect}\\) to be 72, and the SS for the conditions (same thing) in the table is also 72. We found the \\(SS_\\text{Subjects}\\) to be 52.75, and the SS for the first residual (same thing) in the table is also 53.66 repeating. That’s close, and our number is off because of rounding error. Finally, we found the \\(SS_\\text{Error (left-over)}\\) to be 177.25, and the SS for the bottom residuals in the table (same thing) in the table is 177.33 repeating, again close but slightly off due to rounding error.\nWe have finished our job of computing the sums of squares that we need in order to do the next steps, which include computing the MSEs for the effect and the error term. Once we do that, we can find the F-value, which is the ratio of the two MSEs.\nBefore we do that, you may have noticed that we solved for \\(SS_\\text{Error (left-over)}\\), rather than directly computing it from the data. In this chapter we are not going to show you the steps for doing this. We are not trying to hide anything from, instead it turns out these steps are related to another important idea in ANOVA. We discuss this idea, which is called an interaction in the next chapter, when we discuss factorial designs (designs with more than one independent variable).\n\n\n8.3.7 Compute the MSEs\nCalculating the MSEs (mean squared error) that we need for the \\(F\\)-value involves the same general steps as last time. We divide each SS by the degrees of freedom for the SS.\nThe degrees of freedom for \\(SS_\\text{Effect}\\) are the same as before, the number of conditions - 1. We have three conditions, so the df is 2. Now we can compute the \\(MSE_\\text{Effect}\\).\n\\(MSE_\\text{Effect} = \\frac{SS_\\text{Effect}}{df} = \\frac{72}{2} = 36\\)\nThe degrees of freedom for \\(SS_\\text{Error (left-over)}\\) are different than before, they are the (number of subjects - 1) multiplied by the (number of conditions -1). We have 3 subjects and three conditions, so \\((3-1) * (3-1) = 2*2 =4\\). You might be wondering why we are multiplying these numbers. Hold that thought for now and wait until the next chapter. Regardless, now we can compute the \\(MSE_\\text{Error (left-over)}\\).\n\\(MSE_\\text{Error (left-over)} = \\frac{SS_\\text{Error (left-over)}}{df} = \\frac{177.33}{4}= 44.33\\)\n\n\n8.3.8 Compute F\nWe just found the two MSEs that we need to compute \\(F\\). We went through all of this to compute \\(F\\) for our data, so let’s do it:\n\\(F = \\frac{MSE_\\text{Effect}}{MSE_\\text{Error (left-over)}} = \\frac{36}{44.33}= 0.812\\)\nAnd, there we have it!\n\n\n8.3.9 p-value\nWe already conducted the repeated-measures ANOVA using R and reported the ANOVA. Here it is again. The table shows the \\(p\\)-value associated with our \\(F\\)-value.\n\nsummary_out &lt;- summary(aov(scores~conditions + Error(subjects/conditions),df[1:9,]))\nknitr::kable(xtable(summary_out))\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\nResiduals\n2\n52.66667\n26.33333\nNA\nNA\n\n\nconditions\n2\n72.00000\n36.00000\n0.8120301\n0.505848\n\n\nResiduals\n4\n177.33333\n44.33333\nNA\nNA\n\n\n\n\n\nWe might write up the results of our experiment and say that the main effect condition was not significant, F(2,4) = 0.812, MSE = 44.33, p = 0.505.\nWhat does this statement mean? Remember, that the \\(p\\)-value represents the probability of getting the \\(F\\) value we observed or larger under the null (assuming that the samples come from the same distribution, the assumption of no differences). So, we know that an \\(F\\)-value of 0.812 or larger happens fairly often by chance (when there are no real differences), in fact it happens 50.5% of the time. As a result, we do not reject the idea that any differences in the means we have observed could have been produced by chance."
  },
  {
    "objectID": "08-RMANOVA.html#things-worth-knowing",
    "href": "08-RMANOVA.html#things-worth-knowing",
    "title": "8  Repeated Measures ANOVA",
    "section": "8.4 Things worth knowing",
    "text": "8.4 Things worth knowing\nRepeated Measures ANOVAs have some special properties that are worth knowing about. The main special property is that the error term used to for the \\(F\\)-value (the MSE in the denominator) will always be smaller than the error term used for the \\(F\\)-value the ANOVA for a between-subjects design. We discussed this earlier. It is smaller, because we subtract out the error associated with the subject means.\nThis can have the consequence of generally making \\(F\\)-values in repeated measures designs larger than \\(F\\)-values in between-subjects designs. When the number in the bottom of the \\(F\\) formula is generally smaller, it will generally make the resulting ratio a larger number. That’s what happens when you make the number in the bottom smaller.\nBecause big \\(F\\) values usually let us reject the idea that differences in our means are due to chance, the repeated-measures ANOVA becomes a more sensitive test of the differences (its \\(F\\)-values are usually larger).\nAt the same time, there is a trade-off here. The repeated measures ANOVA uses different degrees of freedom for the error term, and these are typically a smaller number of degrees of freedom. So, the \\(F\\)-distributions for the repeated measures and between-subjects designs are actually different \\(F\\)-distributions, because they have different degrees of freedom.\n\n8.4.1 Repeated vs between-subjects ANOVA\nLet’s do a couple simulations to see some the differences between the ANOVA for a repeated measures design, and the ANOVA for a between-subjects design.\nWe will do the following.\n\nSimulate a design with three conditions, A, B, and C\nsample 10 scores into each condition from the same normal distribution (mean = 100, SD = 10)\nWe will include a subject factor for the repeated-measures version. Here there are 10 subjects, each contributing three scores, one each condition\nFor the between-subjects design there are 30 different subjects, each contributing one score in the condition they were assigned to (really the group).\n\nWe run 1000 simulated experiments for each design. We calculate the \\(F\\) for each experiment, for both the between and repeated measures designs. Figure 8.4 has the sampling distributions of \\(F\\) for both designs.\n\nb_f &lt;- length(1000)\nw_f &lt;- length(1000)\nfor (i in 1:1000) {\n  scores &lt;- rnorm(30, 100, 10)\n  conditions &lt;- as.factor(rep(c(\"A\", \"B\", \"C\"), each = 10))\n  subjects &lt;- as.factor(rep(1:10, 3))\n  df &lt;- data.frame(scores, conditions, subjects)\n  \n  between_out &lt;- summary(aov(scores ~ conditions, df))\n  b_f[i] &lt;- between_out[[1]]$`F value`[1]\n  \n  within_out &lt;-\n    summary(aov(scores ~ conditions + Error(subjects / conditions), df))\n  w_f[i] &lt;- within_out[[2]][[1]]$`F value`[1]\n  \n}\n\nplot_df &lt;-\n  data.frame(fs = c(b_f, w_f), type = rep(c(\"between\", \"repeated\"), each =\n                                            1000))\n\ncrit_df &lt;- data.frame(type = c(\"between\", \"repeated\"),\n                      crit = c(qf(.95, 2, 27),\n                               qf(.95, 2, 18)))\n\nggplot(plot_df, aes(x = fs)) +\n  geom_histogram(color = \"white\") +\n  geom_vline(data = crit_df, aes(xintercept = crit)) +\n  geom_label(data = crit_df, aes(\n    x = crit,\n    y = 150,\n    label = round(crit, digits = 2)\n  )) +\n  theme_classic() +\n  facet_wrap( ~ type)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 8.4: Comparing critical F values for a between and repeated measures design\n\n\n\n\nThese two \\(F\\) sampling distributions look pretty similar. However, they are subtly different. The between \\(F\\) distribution has degrees of freedom 2, and 27, for the numerator and denominator. There are 3 conditions, so $df$1 = 3-1 = 2. There are 30 subjects, so $df$2 = 30-3 =27. The critical value, assuming an alpha of 0.05 is 3.35. This means \\(F\\) is 3.35 or larger 5% of the time under the null.\nThe repeated-measures \\(F\\) distribution has degrees of freedom 2, and 18, for the numerator and denominator. There are 3 conditions, so $df$1 = 3-1 = 2. There are 10 subjects, so $df$2 = (10-1)(3-1) = 92 = 18. The critical value, assuming an alpha of 0.05 is 3.55. This means \\(F\\) is 3.55 or larger 5% of the time under the null.\nThe critical value for the repeated measures version is slightly higher. This is because when $df$2 (the denominator) is smaller, the \\(F\\)-distribution spreads out to the right a little bit. When it is skewed like this, we get some bigger \\(F\\)s a greater proportion of the time.\nSo, in order to detect a real difference, you need an \\(F\\) of 3.35 or greater in a between-subjects design, or an \\(F\\) of 3.55 or greater for a repeated-measures design. The catch here is that when there is a real difference between the means, you will detect it more often with the repeated-measures design, even though you need a larger \\(F\\) (to pass the higher critical \\(F\\)-value for the repeated measures design).\n\n\n8.4.2 repeated measures designs are more sensitive\nTo illustrate why repeated-measures designs are more sensitive, we will conduct another set of simulations.\nWe will do something slightly different this time. We will make sure that the scores for condition A, are always a little bit higher than the other scores. In other words, we will program in a real true difference. Specifically, the scores for condition will be sampled from a normal distribution with mean = 105, and SD = 10. This mean is 5 larger than the means for the other two conditions (still set to 100).\nWith a real difference in the means, we should now reject the hypothesis of no differences more often. We should find \\(F\\) values larger than the critical value more often. And, we should find \\(p\\)-values for each experiment that are smaller than .05 more often, those should occur more than 5% of the time.\nTo look at this we conduct 1000 experiments for each design, we conduct the ANOVA, then we save the \\(p\\)-value we obtained for each experiment. This is like asking how many times will we find a \\(p\\)-value less than 0.05, when there is a real difference (in this case an average of 5) between some of the means. Figure 8.5 contains histograms of the \\(p\\)-values:\n\nb_p&lt;-length(1000)\nw_p&lt;-length(1000)\nfor(i in 1:1000){\nscores &lt;- c(rnorm(10,110,10),rnorm(20,100,10))\nconditions &lt;- as.factor(rep(c(\"A\",\"B\",\"C\"), each=10))\nsubjects &lt;-as.factor(rep(1:10,3))\ndf&lt;-data.frame(scores,conditions,subjects)\n\nbetween_out&lt;-summary(aov(scores~conditions,df))\nb_p[i] &lt;- between_out[[1]]$`Pr(&gt;F)`[1]\n\nwithin_out&lt;-summary(aov(scores~conditions + Error(subjects/conditions),df))\nw_p[i] &lt;- within_out[[2]][[1]]$`Pr(&gt;F)`[1]\n\n}\n\nplot_df&lt;-data.frame(ps=c(b_p,w_p), type=rep(c(\"between\",\"repeated\"),each=1000))\n\ncrit_df&lt;-data.frame(type=c(\"between\",\"repeated\"),\n           crit=c(qf(.95, 2, 27),\n                  qf(.95, 2, 18)))\n\nggplot(plot_df, aes(x=ps))+\n  geom_histogram(color=\"white\")+\n  geom_vline(xintercept=0.05, color=\"red\")+\n  theme_classic()+\n  facet_wrap(~type)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 8.5: \\(p\\)-value distributions for a between and within-subjects ANOVA\n\n\n\n\nHere we have two distributions of observed p-values for the simulations. The red line shows the location of 0.05. Overall, we can see that for both designs, we got a full range of \\(p\\)-values from 0 to 1. This means that many times we would not have rejected the hypothesis of no differences (even though we know there is a small difference). We would have rejected the null every time the \\(p\\)-value was less than 0.05.\nFor the between subject design, there were 594 experiments with a \\(p\\) less than 0.05, or 0.594 of experiments were “significant”, with alpha=.05.\nFor the within subject design, there were 563 experiments with a \\(p\\) less than 0.05, or 0.563 of experiments were “significant”, with alpha=.05.\nOK, well, you still might not be impressed. In this case, the between-subjects design detected the true effect slightly more often than the repeated measures design. Both them were right around 55% of the time. Based on this, we could say the two designs are pretty comparable in their sensitivity, or ability to detect a true difference when there is one.\nHowever, remember that the between-subjects design uses 30 subjects, and the repeated measures design only uses 10. We had to make a big investment to get our 30 subjects. And, we’re kind of unfairly comparing the between design (which is more sensitive because it has more subjects) with the repeated measures design that has fewer subjects.\nWhat do you think would happen if we ran 30 subjects in the repeated measures design? Let’s find out. Figure 8.6 re-plots the above, but this time only for the repeated measures design. We increase \\(N\\) from 10 to 30.\n\nb_p&lt;-length(1000)\nw_p&lt;-length(1000)\nfor(i in 1:1000){\nscores &lt;- c(rnorm(30,110,10),rnorm(60,100,10))\nconditions &lt;- as.factor(rep(c(\"A\",\"B\",\"C\"), each=30))\nsubjects &lt;-as.factor(rep(1:30,3))\ndf&lt;-data.frame(scores,conditions,subjects)\n\nwithin_out&lt;-summary(aov(scores~conditions + Error(subjects/conditions),df))\nw_p[i] &lt;- within_out[[2]][[1]]$`Pr(&gt;F)`[1]\n\n}\n\nplot_df&lt;-data.frame(ps=w_p, type=rep(\"repeated\",1000))\n\nggplot(plot_df, aes(x=ps))+\n  geom_histogram(color=\"white\")+\n  geom_vline(xintercept=0.05, color=\"red\")+\n  theme_classic()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 8.6: \\(p\\)-value distribution for within-subjects design with \\(n = 30\\)\n\n\n\n\nWowsers! Look at that. When we ran 30 subjects in the repeated measures design almost all of the \\(p\\)-values were less than .05. There were 988 experiments with a \\(p\\) less than 0.05, or 0.988 of experiments were “significant”, with alpha=.05. That’s huge! If we ran the repeated measures design, we would almost always detect the true difference when it is there. This is why the repeated measures design can be more sensitive than the between-subjects design."
  },
  {
    "objectID": "08-RMANOVA.html#real-data",
    "href": "08-RMANOVA.html#real-data",
    "title": "8  Repeated Measures ANOVA",
    "section": "8.5 Real Data",
    "text": "8.5 Real Data\nLet’s look at some real data from a published experiment that uses a repeated measures design. This is the same example that you will be using in the lab for repeated measures ANOVA. The data happen to be taken from a recent study conducted by Lawrence Behmer and myself, at Brooklyn College (Behmer and Crump 2017).\nWe were interested in how people perform sequences of actions. One question is whether people learn individual parts of actions, or the whole larger pattern of a sequence of actions. We looked at these issues in a computer keyboard typing task. One of our questions was whether we would replicate some well known findings about how people type words and letters.\nFrom prior work we knew that people type words way faster than than random letters, but if you made the random letters a little bit more English-like, then people type those letter strings a little bit faster, but not as slow as random string.\nIn the study, 38 participants sat in front of a computer and typed 5 letter strings one at a time. Sometimes the 5 letter made a word (Normal condition, TRUCK), sometimes they were completely random (Random Condition, JWYFG), and sometimes they followed patterns like you find in English (Bigram Condition, QUEND), but were not actual words. So, the independent variable for the typing material had three levels. We measured every single keystroke that participants made. This gave us a few different dependent measures. Let’s take a look a the reaction times. This is how long it took for participants to start typing the first letter in the string.\n\nlibrary(data.table)\n\nexp1_data &lt;- fread(\"data/exp1_BehmerCrumpAPP.csv\")\n\nexp1_data$Block &lt;- as.factor(exp1_data$Block)\nlevels(exp1_data$Block) &lt;- c(\"Visible keyboard\", \"Covered Keyboard\")\n\n## get subject mean RTs\n\nsubject_means &lt;- exp1_data %&gt;%\n  filter(Order == 1, Correct == 1, PureRTs &lt; 5000) %&gt;%\n  dplyr::group_by(Subject, Block, Stimulus) %&gt;%\n  dplyr::summarise(mean_rt = mean(PureRTs))\n\n`summarise()` has grouped output by 'Subject', 'Block'. You can override using\nthe `.groups` argument.\n\nsubject_means$Subject &lt;- as.factor(subject_means$Subject)\nsubject_means$Block &lt;- as.factor(subject_means$Block)\nsubject_means$Stimulus &lt;- as.factor(subject_means$Stimulus)\n\n## get condition mean RTs\n\nplot_means &lt;- subject_means %&gt;%\n  arrange(match(Stimulus, c(\"Normal\", \"Bigrams\", \"Random\"))) %&gt;%\n  dplyr::group_by(Block, Stimulus) %&gt;%\n  dplyr::summarise(means = mean(mean_rt),\n                   SEs = sd(mean_rt) / sqrt(length(mean_rt)))\n\n`summarise()` has grouped output by 'Block'. You can override using the\n`.groups` argument.\n\n## plot the condition means\n\n# re-order stimulus factor for plotting\nplot_means$Stimulus &lt;-\n  factor(plot_means$Stimulus, levels = c(\"Normal\", \"Bigrams\", \"Random\"))\n\nggplot(plot_means, aes(\n  x = Stimulus,\n  y = means,\n  group = Block,\n  color = Block\n)) +\n  geom_point() +\n  geom_line() +\n  geom_errorbar(aes(ymin = means - SEs, ymax = means + SEs), width = .2) +\n  theme_classic() +\n  ylab(\"Mean Reaction Time (ms)\") +\n  xlab(\"Typing Material\")\n\n\n\n\nFigure 8.7: Results from Behmer & Crump (2017)\n\n\n\n\nOK, I made a figure showing the mean reaction times for the different typing material conditions. You will notice that there are two sets of lines. That’s because there was another manipulation I didn’t tell you about. In one block of trials participants got to look at the keyboard while they typed, but in the other condition we covered up the keyboard so people had to type without looking. Finally, the error bars are standard error of the means.\n\n\n\n\n\n\nNote\n\n\n\nNote, the use of error bars for repeated-measures designs is not very straightforward. In fact the standard error of the means that we have added here are not very meaningful for judging whether the differences between the means are likely not due to chance. They would be if this was a between-subjects design. We will update this textbook with a longer discussion of this issue, for now we will just live with these error bars.\n\n\nFor the purpose of this example, we will say, it sure looks like the previous finding replicated. For example, people started typing Normal words faster than Bigram strings (English-like), and they started typing random letters the most slowly of all. Just like prior research had found.\nLet’s focus only on the block of trials where participants were allowed to look at the keyboard while they typed, that’s the red line, for the “visible keyboard” block. We can see the means look different. Let’s next ask, what is the likelihood that chance (random sampling error) could have produced these mean differences. To do that we run a repeated-measures ANOVA in R. Here is the ANOVA table.\n\nvisible_means&lt;- subject_means %&gt;%\n                 filter(Block==\"Visible keyboard\")\n\ns_out &lt;- summary(aov(mean_rt~Stimulus + Error (Subject/Stimulus),visible_means))\n\nknitr::kable(xtable(s_out))\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\nResiduals\n37\n2452611.9\n66286.808\nNA\nNA\n\n\nStimulus\n2\n1424914.0\n712457.010\n235.7342\n0\n\n\nResiduals1\n74\n223649.4\n3022.289\nNA\nNA\n\n\n\n\n\nAlright, we might report the results like this. There was a significant main effect of Stimulus type, F(2, 74) = 235.73, MSE = 3022.289, p &lt; 0.001.\nNotice a couple things. First, this is a huge \\(F\\)-value. It’s 253! Notice also that the p-value is listed as 0. That doesn’t mean there is zero chance of getting an F-value this big under the null. This is a rounding error. The true p-value is 0.00000000000000… The zeros keep going for a while. This means there is only a vanishingly small probability that these differences could have been produced by sampling error. So, we reject the idea that the differences between our means could be explained by chance. Instead, we are pretty confident, based on this evidence and and previous work showing the same thing, that our experimental manipulation caused the difference. In other words, people really do type normal words faster than random letters, and they type English-like strings somewhere in the middle in terms of speed."
  },
  {
    "objectID": "08-RMANOVA.html#summary",
    "href": "08-RMANOVA.html#summary",
    "title": "8  Repeated Measures ANOVA",
    "section": "8.6 Summary",
    "text": "8.6 Summary\nIn this chapter you were introduced to the repeated-measures ANOVA. This analysis is appropriate for within-subjects or repeated measures designs. The main difference between the independent factor ANOVA and the repeated measures ANOVA, is the ability to partial out variance due to the individual subject means. This can often result in the repeated-measures ANOVA being more sensitive to true effects than the between-subjects ANOVA.\n\n\n\n\nBehmer, Lawrence P, and Matthew JC Crump. 2017. “Spatial Knowledge During Skilled Action Sequencing: Hierarchical Versus Nonhierarchical Representations.” Attention, Perception, & Psychophysics 79 (8): 2435–48. https://doi.org/10.3758/s13414-017-1389-3."
  },
  {
    "objectID": "09-FactorialANOVA.html#factorial-basics",
    "href": "09-FactorialANOVA.html#factorial-basics",
    "title": "9  Factorial ANOVA",
    "section": "9.1 Factorial basics",
    "text": "9.1 Factorial basics\n\n9.1.1 2x2 Designs\nWe’ve just started talking about a 2x2 Factorial design. We said this means the IVs are crossed. To illustrate this, take a look at Figure 9.1. We show an abstract version and a concrete version using time of day and caffeine as the two IVs, each with two levels in the design:\n\nknitr::include_graphics(\"imgs/figures/2x2Design.png\")\n\n\n\n\nFigure 9.1: Structure of 2x2 factorial designs\n\n\n\n\nLet’s talk about this crossing business. Here’s what it means for the design. For the first level of Time of Day (morning), we measure test performance when some people drank caffeine and some did not. So, in the morning we manipulate whether or not caffeine is taken. Also, in the second level of the Time of Day (afternoon), we also manipulate caffeine. Some people drink or don’t drink caffeine in the afternoon as well, and we collect measures of test performance in both conditions.\nWe could say the same thing, but talk from the point of view of the second IV. For example, when people drink caffeine, we test those people in the morning, and in the afternoon. So, time of day is manipulated for the people who drank caffeine. Also, when people do not drink caffeine, we test those people in the morning, and in the afternoon, So, time of day is manipulated for the people who did not drink caffeine.\nFinally, each of the four squares representing a DV, is called a condition. So, we have 2 IVs, each with 2 levels, for a total of 4 conditions. This is why we call it a 2x2 design. 2x2 = 4. The notation tells us how to calculate the total number of conditions.\n\n\n9.1.2 Factorial Notation\nAnytime all of the levels of each IV in a design are fully crossed, so that they all occur for each level of every other IV, we can say the design is a fully factorial design.\nWe use a notation system to refer to these designs. The rules for notation are as follows. Each IV get’s it’s own number. The number of levels in the IV is the number we use for the IV. Let’s look at some examples:\n2x2 = There are two IVS, the first IV has two levels, the second IV has 2 levels. There are a total of 4 conditions, 2x2 = 4.\n2x3 = There are two IVs, the first IV has two levels, the second IV has three levels. There are a total of 6 conditions, 2x3 = 6\n3x2 = There are two IVs, the first IV has three levels, the second IV has two levels. There are a total of 6 conditions, 3x2=6.\n4x4 = There are two IVs, the first IV has 4 levels, the second IV has 4 levels. There are a total of 16 condition, 4x4=16\n2x3x2 = There are a total of three IVs. The first IV has 2 levels. The second IV has 3 levels. The third IV has 2 levels. There are a total of 12 condition. 2x3x2 = 12.\n\n\n9.1.3 2 x 3 designs\nFigure 9.2 shows the structure of a 2x3 factorial design.\n\nknitr::include_graphics(\"imgs/figures/2x3Design.png\")\n\n\n\n\nFigure 9.2: Structure of 2x3 factorial designs\n\n\n\n\nAll we did was add another row for the second IV. It’s a 2x3 design, so it should have 6 conditions. As you can see there are now 6 cells to measure the DV."
  },
  {
    "objectID": "09-FactorialANOVA.html#purpose-of-factorial-designs",
    "href": "09-FactorialANOVA.html#purpose-of-factorial-designs",
    "title": "9  Factorial ANOVA",
    "section": "9.2 Purpose of Factorial Designs",
    "text": "9.2 Purpose of Factorial Designs\nFactorial designs let researchers manipulate more than one thing at once. This immediately makes things more complicated, because as you will see, there are many more details to keep track of. Why would researchers want to make things more complicated? Why would they want to manipulate more than one IV at a time.\nBefore we go on, let’s clarify what we mean by manipulating more than one thing at once. When you have one IV in your design, by definition, you are manipulating only one thing. This might seem confusing at first, because the IV has more than one level, so it seems to have more than one manipulation. Consider manipulating the number of coffees that people drink before they do a test. We could have one IV (coffee), with three levels (1, 2, or 3 coffees). You might want to say we have three manipulations here, drinking 1, 2, or 3 coffees. But, the way we define manipulation is terms of the IV. There is only one coffee IV. It does have three levels. Nevertheless, we say you are only doing one coffee manipulation. The only thing you are manipulating is the amount of coffee. That’s just one thing, so it’s called one manipulation. To do another, second manipulation, you need to additionally manipulate something that is not coffee (like time of day in our previous example).\nReturning to our question: why would researchers want to manipulate more than one thing in their experiment. The answer might be kind of obvious. They want to know if more than one thing causes change in the thing they are measuring! For example, if you are measuring people’s happiness, you might assume that more than one thing causes happiness to change. If you wanted to track down how two things caused changes in happiness, then you might want to have two manipulations of two different IVs. This is not a wrong way to think about the reasons why researchers use factorial designs. They are often interested in questions like this. However, we think this is an unhelpful way to first learn about factorial designs.\nWe present a slightly different way of thinking about the usefulness of factorial designs, and we think it is so important, it get’s its own section.\n\n9.2.1 Factorials manipulate an effect of interest\nHere is how researchers often use factorial designs to understand the causal influences behind the effects they are interested in measuring. Notice we didn’t say the dependent variables they are measuring, we are now talking about something called effects. Effects are the change in a measure caused by a manipulation. You get an effect, any time one IV causes a change in a DV.\nHere is an example. We will stick with this one example for a while, so pay attention… In fact, the example is about paying attention. Let’s say you wanted to measure something like paying attention. You could something like this:\n\nPick a task for people to do that you can measure. For example, you can measure how well they perform the task. That will be the dependent measure\nPick a manipulation that you think will cause differences in paying attention. For example, we know that people can get distracted easily when there are distracting things around. You could have two levels for your manipulation: No distraction versus distraction.\nMeasure performance in the task under the two conditions\nIf your distraction manipulation changes how people perform the task, you may have successfully manipulated how well people can pay attention in your task.\n\n\n\n9.2.2 Spot the difference\nLet’s elaborate this with another fake example. First, we pick a task. It’s called spot the difference. You may have played this game before. You look at two pictures side-by-side in Figure 9.3, and then you locate as many differences as you can find.\n\nknitr::include_graphics(\"imgs/figures/spot_dif.png\")\n\n\n\n\nFigure 9.3: Spot the differences between the two images\n\n\n\n\nHow many differences can you spot? When you look for the differences, it feels like you are doing something we would call “paying attention”. If you pay attention to the clock tower, you will see that the hands on the clock are different. Ya! One difference spotted.\nWe could give people 30 seconds to find as many differences as they can. Then we give them another set of pictures and do it again. Every time we will measure how many differences they can spot. So, our measure of performance, our dependent variable, could be the mean number of differences spotted.\n\n\n9.2.3 Distraction manipulation\nNow, let’s think about a manipulation that might cause differences in how people pay attention. If people need to pay attention to spot differences, then presumably if we made it difficult to pay attention, people would spot less differences. What is a good way to distract people? I’m sure there are lots of ways to do this. How about we do the following:\n\nNo distraction condition: Here people do the task with no added distractions. They sit in front of a computer, in a quiet, distraction-free room, and find as many differences as they can for each pair of pictures\nDistraction condition: Here we blast super loud ambulance sounds and fire alarms and heavy metal music while people attempt to spot differences. We also randomly turn the sounds on and off, and make them super-duper annoying and distracting. We make sure that the sounds aren’t loud enough to do any physical damage to anybody’s ear-drums. But, we want to make them loud enough to be super distracting. If you don’t like this, we could also tickle people with a feather, or whisper silly things into their ears, or surround them by clowns, or whatever we want, it just has to be super distracting.\n\n\n\n9.2.4 Distraction effect\nIf our distraction manipulation is super-distracting, then what should we expect to find when we compare spot-the-difference performance between the no-distraction and distraction conditions? We should find a difference!\nIf our manipulation works, then we should find that people find more differences when they are not distracted, and less differences when they are distracted. Figure 9.4 shows how the data might look.\n\ndf &lt;- data.frame(\n  Distraction = c(\"No distraction\", \"Distraction\"),\n  Mean_diffs = c(10, 5)\n)\nggplot(df, aes(x = Distraction, y = Mean_diffs)) +\n  geom_bar(stat = \"identity\") +\n  theme_classic() +\n  ylab(\"Mean differences spotted\") +\n  xlab(\"Distraction Condition\")\n\n\n\n\nFigure 9.4: Example data from pretend experiment showing number of differences spotted in a distraction versus no distraction condition\n\n\n\n\nThe figure shows a big difference in the mean number of difference spotted. People found 5 differences on average when they were distracted, and 10 differences when they were not distracted. We labelled the figure, “The distraction effect”, because it shows a big effect of distraction. The effect of distraction is a mean of 5 spot the differences. It’s the difference between performance in the Distraction and No-Distraction conditions. In general, it is very common to use the word effect to refer to the differences caused by the manipulation. We manipulated distraction, it caused a difference, so we call this the “distraction effect”.\n\n\n9.2.5 Manipulating the Distraction effect\nThis is where factorial designs come in to play. We have done the hard work of finding an effect of interest, in this case the distraction effect. We think this distraction effect actually measures something about your ability to pay attention. For example, if you were the kind of person who had a small distraction effect (maybe you find 10 differences when you are not distracted, and 9 differences when you are distracted), that could mean you are very good at ignoring distracting things while you are paying attention. On the other hand, you could be the kind of person who had a big distraction effect (maybe you found 10 differences under no distraction, and only 1 difference when you were distracted); this could mean you are not very good at ignoring distracting things while you are paying attention.\nOverall now, we are thinking of our distraction effect (the difference in performance between the two conditions) as the important thing we want to measure. We then might want to know how to make people better at ignoring distracting things. Or, we might want to know what makes people worse at ignoring things. In other words we want to find out what manipulations control the size of the distraction effect (make it bigger or smaller, or even flip around!).\nMaybe there is a special drug that helps you ignore distracting things. People taking this drug should be less distracted, and if they took this drug while completing our task, they should have a smaller distraction effect compared to people not taking the drug.\nMaybe rewarding people with money can help you pay attention and ignore distracting things better. People receiving 5 dollars every time they spot a difference might be able to focus more because of the reward, and they would show a smaller distraction effect in our task, compared to people who got no money for finding differences. Let’s see what this would look like.\nWe are going to add a second IV to our task. The second IV will manipulate reward. In one condition, people will get 5 dollars for every difference they find (so they could leave the study with lots of money if they find lots of differences). In the other condition, people will get no money, but they will still have find differences. Remember, this will be a factorial design, so everybody will have to find differences when they are distracted and when they are not distracted.\nThe question we are now asking is: Will manipulating reward cause a change in the size of the distraction effect. We could predict that people receiving rewards will have a smaller distraction effect than people not receiving rewards. If that happened, the data would look something like Figure 9.5.\n\ndf &lt;-\n  data.frame(\n    Distraction = c(\n      \"No distraction\",\n      \"Distraction\",\n      \"No distraction\",\n      \"Distraction\"\n    ),\n    Mean_diffs = c(10, 5, 11, 9),\n    Reward = rep(c(\"No Reward\", \"Reward\"), each = 2)\n  )\nggplot(df,\n       aes(\n         x = Distraction,\n         y = Mean_diffs,\n         group = Reward,\n         fill = Reward\n       )) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  theme_classic() +\n  ylab(\"Mean differences spotted\") +\n  xlab(\"Distraction Condition\")\n\n\n\n\nFigure 9.5: Example data showing how the distraction effect could be modulated by a reward manipulation. Distraction condition plotted on the x-axis, makes it more difficult to compare the changes in the distraction effect between reward conditions\n\n\n\n\nI’ve just shown you a new kind of graph. I apologize right now for showing this to you first. It’s more unhelpful than the next graph. What I did was keep the x-axis the same as before (to be consistent). So, we have distraction vs. no distraction on the x-axis. In the distraction condition, there are means for spot-the-difference performance in the no-reward (red), and reward (aqua) conditions. The same goes for the no-distraction condition, a red and an aqua bar for the no-reward and reward conditions. We can try to interpret this graph, but Figure 9.6 plots the same data in a way that makes it easier to see what we are talking about.\n\nggplot(df, aes(x=Reward, y=Mean_diffs, group=Distraction, fill=Distraction))+\n  geom_bar(stat=\"identity\", position=\"dodge\")+\n  theme_classic()+\n  ylab(\"Mean differences spotted\")+\n  xlab(\"Reward Condition\")+\n  ggtitle(\"The distraction effect as a function of reward\")\n\n\n\n\nFigure 9.6: Example data showing how the distraction effect could be modulated by a reward manipulation. Reward condition plotted on the x-axis, makes it easier to compare the changes in the distraction effect between reward conditions\n\n\n\n\nAll we did was change the x-axis. Now the left side of the x-axis is for the no-reward condition, and the right side is for the reward condition. The red bar is for the distraction condition, and the aqua bar is for the no distraction condition. It is easier to see the distraction effect in this graph. The distraction effect is the difference in size between the red and aqua bars. For each reward condition, the red and aqua bars are right beside each other, so can see if there is a difference between them more easily, compared to the first graph.\nNo-Reward condition: In the no-reward condition people played spot the difference when they were distracted and when they were not distracted. This is a replication of our first fake study. We should expect to find the same pattern of results, and that’s what the graph shows. There was a difference of 5. People found 5 differences when they were distracted and 10 when they were not distracted. So, there was a distraction effect of 5, same as we had last time.\nReward condition: In the reward condition people played spot the difference when they were distracted and when they were not distracted. Except, they got 5 dollars every time they spotted a difference. We predicted this would cause people to pay more attention and do a better job of ignoring distracting things. The graph shows this is what happened. People found 9 differences when they were distracted and 11 when they were not distracted. So, there was a distraction effect of 2.\nIf we had conducted this study, we might have concluded that reward can manipulate the distraction effect. When there was no reward, the size of the distraction effect was 5. When there was reward, the size of the distraction effect was 2. So, the reward manipulation changed the size of the distraction effect by 3 (5-2 =3).\nThis is our description of why factorial designs are so useful. They allow researchers to find out what kinds of manipulations can cause changes in the effects they measure. We measured the distraction effect, then we found that reward causes changes in the distraction effect. If we were trying to understand how paying attention works, we would then need to explain how it is that reward levels could causally change how people pay attention. We would have some evidence that reward does cause change in paying attention, and we would have to come up with some explanations, and then run more experiments to test whether those explanations hold water."
  },
  {
    "objectID": "09-FactorialANOVA.html#graphing-the-means",
    "href": "09-FactorialANOVA.html#graphing-the-means",
    "title": "9  Factorial ANOVA",
    "section": "9.3 Graphing the means",
    "text": "9.3 Graphing the means\nIn our example above we showed you two bar graphs of the very same means for our 2x2 design. Even though the graphs plot identical means, they look different, so they are more or less easy to interpret by looking at them. Results from 2x2 designs are also often plotted with line graphs. Those look different too. There are four different graphs in Figure 9.7, using bars and lines to plot the very same means from before. We are showing you this so that you realize how you graph your data matters because it makes it more or less easy for people to understand the results. Also, how the data is plotted matters for what you need to look at to interpret the results.\n\nb1 &lt;- ggplot(df, aes(x=Reward, y=Mean_diffs, group=Distraction, fill=Distraction))+\n  geom_bar(stat=\"identity\", position=\"dodge\")+\n  theme_classic()+\n  ylab(\"Mean # spotted\")+\n  xlab(\"Reward Condition\")\n\nb2 &lt;- ggplot(df, aes(x=Distraction, y=Mean_diffs, group=Reward, fill=Reward))+\n  geom_bar(stat=\"identity\", position=\"dodge\")+\n  theme_classic()+\n  ylab(\"Mean # spotted\")+\n  xlab(\"Distraction Condition\")\n\nl1 &lt;- ggplot(df, aes(x=Reward, y=Mean_diffs, group=Distraction, color=Distraction))+\n  geom_line()+\n  geom_point()+\n  theme_classic()+\n  ylab(\"Mean # spotted\")+\n  xlab(\"Reward Condition\")\n\nl2 &lt;- ggplot(df, aes(x=Distraction, y=Mean_diffs, group=Reward, color=Reward))+\n geom_line()+\n  geom_point()+\n  theme_classic()+\n  ylab(\"Mean # spotted\")+\n  xlab(\"Distraction Condition\")\n\nggarrange(b1,b2,l1,l2, ncol=2, nrow=2)\n\n\n\n\nFigure 9.7: same example means plotted using bar graphs or line graphs, and with Distraction or Reward on the x-axis"
  },
  {
    "objectID": "09-FactorialANOVA.html#knowing-what-you-want-to-find-out",
    "href": "09-FactorialANOVA.html#knowing-what-you-want-to-find-out",
    "title": "9  Factorial ANOVA",
    "section": "9.4 Knowing what you want to find out",
    "text": "9.4 Knowing what you want to find out\nWhen you conduct a design with more than one IV, you get more means to look at. As a result, there are more kinds of questions that you can ask of the data. Sometimes it turns out that the questions that you can ask, are not the ones that you want to ask, or have an interest in asking. Because you ran the design with more than one IV, you have the opportunity to ask these kinds of extra questions.\nWhat kinds of extra questions? Let’s keep going with our distraction effect experiment. We have the first IV where we manipulated distraction. So, we could find the overall means in spot-the difference for the distraction vs. no-distraction conditions (that’s two means). The second IV was reward. We could find the overall means in spot-the-difference performance for the reward vs. no-reward conditions (that’s two more means). We could do what we already did, and look at the means for each combination, that is the mean for distraction/reward, distraction/no-reward, no-distraction/reward, and no-distraction/no-reward (that’s four more means, if you’re counting).\nThere’s even more. We could look at the mean distraction effect (the difference between distraction and no-distraction) for the reward condition, and the mean distraction effect for the no-reward condition (that’s two more).\nFigure 9.8 shows multiple ways of looking at the means across four panels.\n\ndf1 &lt;- data.frame(Distraction = c(\"No distraction\",\"Distraction\",\"No distraction\",\"Distraction\"),\n                 Mean_diffs = c(10,5,11,9),\n                 Reward = rep(c(\"No Reward\",\"Reward\"),each=2))\n\ndf1gg &lt;- ggplot(df1, aes(x=Reward, y=Mean_diffs, group=Distraction, fill=Distraction))+\n  geom_bar(stat=\"identity\", position=\"dodge\")+\n  theme_classic(base_size=8)+\n  ylab(\"Mean # spotted\")+\n  xlab(\"Reward Condition\")+\n  ggtitle(\"Means for each condition\")+\n  theme(legend.position=\"bottom\")\n\ndf2 &lt;- data.frame(Distraction = c(\"No distraction\",\"Distraction\"),\n                 Mean_diffs = c(10.5,7))\n\ndf2gg &lt;- ggplot(df2, aes(x=Distraction, y=Mean_diffs))+\n  geom_bar(stat=\"identity\", position=\"dodge\")+\n  theme_classic(base_size=8)+\n  ylab(\"Mean # spotted\")+\n  xlab(\"Distraction Condition\")+\n  ggtitle(\"Overall means for Distraction IV1\")+\n  theme(legend.position=\"bottom\")\n\ndf3 &lt;- data.frame(Reward = c(\"No Reward\",\"Reward\"),\n                 Mean_diffs = c(7.5,10))\n\ndf3gg &lt;- ggplot(df3, aes(x=Reward, y=Mean_diffs))+\n  geom_bar(stat=\"identity\", position=\"dodge\")+\n  theme_classic(base_size=8)+\n  ylab(\"Mean # spotted\")+\n  xlab(\"Reward Condition\")+\n  ggtitle(\"Overall means for Reward IV2\")+\n  theme(legend.position=\"bottom\")\n\ndf4 &lt;- data.frame(Distraction = c(\"No-Reward\",\"Reward\"),\n                 Mean_diffs = c(5,2))\n\ndf4gg &lt;- ggplot(df4, aes(x=Distraction, y=Mean_diffs))+\n  geom_bar(stat=\"identity\", position=\"dodge\")+\n  theme_classic(base_size=8)+\n  ylab(\"Distraction Effect\")+\n  xlab(\"Reward Condition\")+\n  ggtitle(\"Overall means for Distraction effect \\n by Reward Condition\")+\n  theme(legend.position=\"bottom\")\n\nggarrange(df1gg,df2gg,df3gg,df4gg, ncol=2, nrow=2)\n\n\n\n\nFigure 9.8: Each panel shows the mean for different effects in the design\n\n\n\n\nThe purpose of showing all of these means is to orient you to your problem. If you conduct a 2x2 design (and this is the most simple factorial that you can conduct), you will get all of these means. You need to know what you want to know from the means. That is, you need to be able to connect the research question to the specific means you are interested in analyzing.\nFor example, in our example, the research question was whether reward would change the size of the distraction effect. The top left panel gives us some info about this question. We can see all of the condition means, and we can visually see that the distraction effect was larger in the No-reward compared to the reward condition. But, to “see” this, we need to do some visual subtraction. You need to look at the difference between the red and aqua bars for each of the reward and no-reward conditions.\nDoes the top right panel tell us about whether reward changed the size of the distraction effect? NO, it just shows that there was an overall distraction effect (this is called the main effect of distraction). Main effects are any differences between the levels of one independent variable.\nDoes the bottom left panel tell us about whether reward changed the size of the distraction effect? NO! it just shows that there was an overall reward effect, called the main effect of reward. People who were rewarded spotted a few more differences than the people who weren’t, but this doesn’t tell us if they were any less distracted.\nFinally, how about the bottom left panel. Does this tell us about whether the reward changed the size of the distraction effect? YES! Notice, the y-axis is different for this panel. The y-axis here is labelled “Distraction Effect”. You are looking at two difference scores. The distraction effect in the no-reward condition (10-5 = 5), and the distraction effect in the Reward condition (11-9 = 2). These two bars are different as a function of reward. So, it looks like reward did produce a difference between the distraction effects! This was the whole point of the fake study. It is these means that were most important for answering the question of the study. As a very last point, this panel contains what we call an interaction. We explain this in the next section.\n\n\n\n\n\n\nPro tip\n\n\n\nMake sure you know what you want to know from your means before you run the study, otherwise you will just have way too many means, and you won’t know what they mean."
  },
  {
    "objectID": "09-FactorialANOVA.html#simple-analysis-of-2x2-repeated-measures-design",
    "href": "09-FactorialANOVA.html#simple-analysis-of-2x2-repeated-measures-design",
    "title": "9  Factorial ANOVA",
    "section": "9.5 Simple analysis of 2x2 repeated measures design",
    "text": "9.5 Simple analysis of 2x2 repeated measures design\nNormally in a chapter about factorial designs we would introduce you to Factorial ANOVAs, which are totally a thing. We will introduce you to them soon. But, before we do that, we are going to show you how to analyze a 2x2 repeated measures ANOVA design with paired-samples t-tests. This is probably something you won’t do very often. However, it turns out the answers you get from this method are the same ones you would get from an ANOVA.\nAdmittedly, if you found the explanation of ANOVA complicated, it will just appear even more complicated for factorial designs. So, our purpose here is to delay the complication, and show you with t-tests what it is that the Factorial ANOVA is doing. More important, when you do the analysis with t-tests, you have to be very careful to make all of the comparisons in the right way. As a result, you will get some experience learning how to know what it is you want to know from factorial designs. Once you know what you want to know, you can use the ANOVA to find out the answers, and then you will also know what answers to look for after you run the ANOVA. Isn’t new knowledge fun!\nThe first thing we need to do is define main effects and interactions. Whenever you conduct a Factorial design, you will also have the opportunity to analyze main effects and interactions. However, the number of main effects and interactions you get to analyse depends on the number of IVs in the design.\n\n9.5.1 Main effects\nFormally, main effects are the mean differences for a single Independent variable. There is always one main effect for each IV. A 2x2 design has 2 IVs, so there are two main effects. In our example, there is one main effect for distraction, and one main effect for reward. We will often ask if the main effect of some IV is significant. This refers to a statistical question: Were the differences between the means for that IV likely or unlikely to be caused by chance (sampling error).\nIf you had a 2x2x2 design, you would measure three main effects, one for each IV. If you had a 3x3x3 design, you would still only have 3 IVs, so you would have three main effects.\n\n\n9.5.2 Interaction\nWe find that the interaction concept is one of the most confusing concepts for factorial designs. Formally, we might say an interaction occurs whenever the effect of one IV has an influence on the size of the effect for another IV. That’s probably not very helpful. In more concrete terms, using our example, we found that the reward IV had an effect on the size of the distraction effect. The distraction effect was larger when there was no-reward, and it was smaller when there was a reward. So, there was an interaction.\nWe might also say an interaction occurs when the difference between the differences are different! Yikes. Let’s explain. There was a difference in spot-the-difference performance between the distraction and no-distraction condition, this is called the distraction effect (it is a difference measure). The reward manipulation changed the size of the distraction effect, that means there was difference in the size of the distraction effect. The distraction effect is itself a measure of differences. So, we did find that the difference (in the distraction effect) between the differences (the two measures of the distraction effect between the reward conditions) were different. When you start to write down explanations of what interactions are, you find out why they come across as complicated. We’ll leave our definition of interaction like this for now. Don’t worry, we’ll go through lots of examples to help firm up this concept for you.\nThe number of interactions in the design also depend on the number of IVs. For a 2x2 design there is only 1 interaction. The interaction between IV1 and IV2. This occurs when the effect of say IV2 (whether there is a difference between the levels of IV2) changes across the levels of IV1. We could write this in reverse, and ask if the effect of IV1 (whether there is a difference between the levels of IV1) changes across the levels of IV2. However, just because we can write this two ways, does not mean there are two interactions. We’ll see in a bit, that no matter how do the calculation to see if the difference scores–measure of effect for one IV– change across the levels of the other IV, we always get the same answer. That is why there is only one interaction for a 2x2. Similarly, there is only one interaction for a 3x3, because there again we only have two IVs (each with three levels). Only when we get up to designs with more than 2 IVs, do we find more possible interactions. A design with three IVS, has four interactions. If the IVs are labelled A, B, and C, then we have three 2-way interactions (AB, AC, and BC), and one three-way interaction (ABC). We hold off on this stuff for much later\n\n\n9.5.3 Looking at the data\nIt is most helpful to see some data in order to understand how we will analyze it. Let’s imagine we ran our fake attention study. We will have five people in the study, and they will participate in all conditions, so it will be a fully repeated-measures design. The data could look like this:\n\nA &lt;- c(10,8,11,9,10)  #nD_nR\nB  &lt;- c(5,4,3,4,2)  #D_nR\nC &lt;- c(12,13,14,11,13)  #nD_R\nD  &lt;- c(9,8,10,11,12)  #D_R\n\nfake_data &lt;- data.frame(subject=1:5, A, B, C, D)\n\nkable(fake_data) %&gt;%\n  add_header_above(c(\" \" = 1, \"No Distraction\" = 1, \"Distraction\" = 1, \"No Distraction\" = 1, \"Distraction\" = 1 )) %&gt;%\n  add_header_above(c(\" \" = 1, \"No Reward\" = 2, \"Reward\" = 2 ))  %&gt;%\n  footnote(general = \"Number of differences spotted for each subject in each condition. \"\n           )\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo Reward\n\n\nReward\n\n\n\n\n\nNo Distraction\n\n\nDistraction\n\n\nNo Distraction\n\n\nDistraction\n\n\n\nsubject\nA\nB\nC\nD\n\n\n\n\n1\n10\n5\n12\n9\n\n\n2\n8\n4\n13\n8\n\n\n3\n11\n3\n14\n10\n\n\n4\n9\n4\n11\n11\n\n\n5\n10\n2\n13\n12\n\n\n\nNote: \n\n\n\n\n\n\n Number of differences spotted for each subject in each condition.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.5.4 Main effect of Distraction\nThe main effect of distraction compares the overall means for all scores in the no-distraction and distraction conditions, collapsing over the reward conditions.\nThe yellow columns show the no-distraction scores for each subject. The blue columns show the distraction scores for each subject.\nThe overall means for for each subject, for the two distraction conditions are shown to the right. For example, subject 1 had a 10 and 12 in the no-distraction condition, so their mean is 11.\nWe are interested in the main effect of distraction. This is the difference between the AC column (average of subject scores in the no-distraction condition) and the BD column (average of the subject scores in the distraction condition). These differences for each subject are shown in the last green column. The overall means, averaging over subjects are in the bottom green row.\n\nfake_data &lt;- data.frame(subject=1:5, A, B, C, D, AC = (A+C)/2, BD =(B+D)/2,\n                        \"AC minus BD\" =  ((A+C)/2 - (B+D)/2) )\n\nfake_data&lt;-fake_data %&gt;%\nrbind(c(\"Means\",\"\",\"\",\"\",\"\",colMeans(fake_data[,6:8])))\n\nkable(fake_data) %&gt;%\n  add_header_above(c(\" \" = 1, \"No Distraction\" = 1, \"Distraction\" = 1, \"No Distraction\" = 1, \"Distraction\" = 1, \"No Distraction\" = 1, \"Distraction\" = 1,\"Difference\"  )) %&gt;%\n  add_header_above(c(\" \" = 1, \"No Reward\" = 2, \"Reward\" = 2 ,\" Distraction Means \"=2,\"Distraction Effect \" =1))  %&gt;%\n  add_header_above(c(\" \" = 1, \" All Conditions\" = 4, \" \" =3)) %&gt;%\n  column_spec(c(2,4), background = \"yellow\")%&gt;%\n  column_spec(c(3,5), background = \"lightgray\")%&gt;%\n  column_spec(8, background = \"lightgray\")%&gt;%\n  row_spec(6, background = \"lightgray\")%&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAll Conditions\n\n\n\n\n\n\nNo Reward\n\n\nReward\n\n\nDistraction Means\n\n\nDistraction Effect\n\n\n\n\n\nNo Distraction\n\n\nDistraction\n\n\nNo Distraction\n\n\nDistraction\n\n\nNo Distraction\n\n\nDistraction\n\n\nDifference\n\n\n\nsubject\nA\nB\nC\nD\nAC\nBD\nAC.minus.BD\n\n\n\n\n1\n10\n5\n12\n9\n11\n7\n4\n\n\n2\n8\n4\n13\n8\n10.5\n6\n4.5\n\n\n3\n11\n3\n14\n10\n12.5\n6.5\n6\n\n\n4\n9\n4\n11\n11\n10\n7.5\n2.5\n\n\n5\n10\n2\n13\n12\n11.5\n7\n4.5\n\n\nMeans\n\n\n\n\n11.1\n6.8\n4.3\n\n\n\n\n\n\n\nJust looking at the means, we can see there was a main effect of Distraction, the mean for the no-distraction condition was 11.1, and the mean for the distraction condition was 6.8. The size of the main effect was 4.3 (the difference between 11.1 and 6.8).\nNow, what if we wanted to know if this main effect of distraction (the difference of 4.3) could have been caused by chance, or sampling error. You could do two things. You could run a paired samples \\(t\\)-test between the mean no-distraction scores for each subject (column AC) and the mean distraction scores for each subject (column BD). Or, you could run a one-sample \\(t\\)-test on the difference scores column, testing against a mean difference of 0. Either way you will get the same answer.\nHere’s the paired samples version:\n\nAC&lt;- (A+C)/2 \nBD&lt;- (B+D)/2\nt.test(AC,BD, paired=TRUE,var.equal=TRUE)\n\n\n    Paired t-test\n\ndata:  AC and BD\nt = 7.6615, df = 4, p-value = 0.00156\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 2.741724 5.858276\nsample estimates:\nmean difference \n            4.3 \n\n\nHere’s the one sample version:\n\nt.test(AC-BD, mu=0)\n\n\n    One Sample t-test\n\ndata:  AC - BD\nt = 7.6615, df = 4, p-value = 0.00156\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 2.741724 5.858276\nsample estimates:\nmean of x \n      4.3 \n\n\nIf we were to write-up our results for the main effect of distraction we could say something like this:\nThe main effect of distraction was significant, \\(t\\)(4) = 7.66, \\(p\\) = 0.001. The mean number of differences spotted was higher in the no-distraction condition (M = 11.1) than the distraction condition (M = 6.8).\n\n\n9.5.5 Main effect of Reward\nThe main effect of reward compares the overall means for all scores in the no-reward and reward conditions, collapsing over the reward conditions.\nThe yellow columns show the no-reward scores for each subject. The blue columns show the reward scores for each subject.\nThe overall means for for each subject, for the two reward conditions are shown to the right. For example, subject 1 had a 10 and 5 in the no-reward condition, so their mean is 7.5.\nWe are interested in the main effect of reward. This is the difference between the AB column (average of subject scores in the no-reward condition) and the CD column (average of the subject scores in the reward condition). These differences for each subject are shown in the last green column. The overall means, averaging over subjects are in the bottom green row.\n\nfake_data &lt;- data.frame(subject=1:5, A, B, C, D, AB = (A+B)/2, CD =(C+D)/2,\n                        \"CD minus AB\" =  ((C+D)/2 - (A+B)/2) )\n\nfake_data&lt;-fake_data %&gt;%\nrbind(c(\"Means\",\"\",\"\",\"\",\"\",colMeans(fake_data[,6:8])))\n\nkable(fake_data) %&gt;%\n  add_header_above(c(\" \" = 1, \"No Distraction\" = 1, \"Distraction\" = 1, \"No Distraction\" = 1, \"Distraction\" = 1, \"No Reward\" = 1, \"Reward\" = 1,\"Difference\"  )) %&gt;%\n  add_header_above(c(\" \" = 1, \"No Reward\" = 2, \"Reward\" = 2 ,\" Reward Means \"=2,\"Reward Effect \" =1))  %&gt;%\n  add_header_above(c(\" \" = 1, \" All Conditions\" = 4, \" \" =3)) %&gt;%\n  column_spec(c(2,3), background = \"yellow\")%&gt;%\n  column_spec(c(4,5), background = \"lightgray\")%&gt;%\n  column_spec(8, background = \"lightgray\")%&gt;%\n  row_spec(6, background = \"lightgray\")%&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAll Conditions\n\n\n\n\n\n\nNo Reward\n\n\nReward\n\n\nReward Means\n\n\nReward Effect\n\n\n\n\n\nNo Distraction\n\n\nDistraction\n\n\nNo Distraction\n\n\nDistraction\n\n\nNo Reward\n\n\nReward\n\n\nDifference\n\n\n\nsubject\nA\nB\nC\nD\nAB\nCD\nCD.minus.AB\n\n\n\n\n1\n10\n5\n12\n9\n7.5\n10.5\n3\n\n\n2\n8\n4\n13\n8\n6\n10.5\n4.5\n\n\n3\n11\n3\n14\n10\n7\n12\n5\n\n\n4\n9\n4\n11\n11\n6.5\n11\n4.5\n\n\n5\n10\n2\n13\n12\n6\n12.5\n6.5\n\n\nMeans\n\n\n\n\n6.6\n11.3\n4.7\n\n\n\n\n\n\n\nJust looking at the means, we can see there was a main effect of reward. The mean number of differences spotted was 11.3 in the reward condition, and 6.6 in the no-reward condition. So, the size of the main effect of reward was 4.7.\nIs a difference of this size likely o unlikely due to chance? We could conduct a paired-samples \\(t\\)-test on the AB vs. CD means, or a one-sample \\(t\\)-test on the difference scores. They both give the same answer:\nHere’s the paired samples version:\n\nAB&lt;- (A+B)/2 \nCD&lt;- (C+D)/2\nt.test(CD,AB, paired=TRUE,var.equal=TRUE)\n\n\n    Paired t-test\n\ndata:  CD and AB\nt = 8.3742, df = 4, p-value = 0.001112\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 3.141724 6.258276\nsample estimates:\nmean difference \n            4.7 \n\n\nHere’s the one sample version:\n\nt.test(CD-AB, mu=0)\n\n\n    One Sample t-test\n\ndata:  CD - AB\nt = 8.3742, df = 4, p-value = 0.001112\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 3.141724 6.258276\nsample estimates:\nmean of x \n      4.7 \n\n\nIf we were to write-up our results for the main effect of reward we could say something like this:\nThe main effect of reward was significant, t(4) = 8.37, p = 0.001. The mean number of differences spotted was higher in the reward condition (M = 11.3) than the no-reward condition (M = 6.6).\n\n\n9.5.6 Interaction between Distraction and Reward\nNow we are ready to look at the interaction. Remember, the whole point of this fake study was what? Can you remember?\nHere’s a reminder. We wanted to know if giving rewards versus not would change the size of the distraction effect.\nNotice, neither the main effect of distraction, or the main effect of reward, which we just went through the process of computing, answers this question.\nIn order to answer the question we need to do two things. First, compute distraction effect for each subject when they were in the no-reward condition. Second, compute the distraction effect for each subject when they were in the reward condition.\nThen, we can compare the two distraction effects and see if they are different. The comparison between the two distraction effects is what we call the interaction effect. Remember, this is a difference between two difference scores. We first get the difference scores for the distraction effects in the no-reward and reward conditions. Then we find the difference scores between the two distraction effects. This difference of differences is the interaction effect (green column in the table)\n\nfake_data &lt;- data.frame(subject=1:5, A, B, C, D, A_B = A-B, C_D =C-D,\n                        \"ABCD\" =  (A-B)-(C-D) )\n\nnames(fake_data)[6]&lt;-\"A-B\"\nnames(fake_data)[7]&lt;-\"C-D\"\nnames(fake_data)[8]&lt;-\"(A-B)-(C-D)\"\n\nfake_data&lt;-fake_data %&gt;%\nrbind(c(\"Means\",\"\",\"\",\"\",\"\",colMeans(fake_data[,6:8])))\n\nkable(fake_data) %&gt;%\n  add_header_above(c(\" \" = 1, \"No Distraction\" = 1, \"Distraction\" = 1, \"No Distraction\" = 1, \"Distraction\" = 1, \"No Reward\" = 1, \"Reward\" = 1,\"Difference\"  )) %&gt;%\n  add_header_above(c(\" \" = 1, \"No Reward\" = 2, \"Reward\" = 2 ,\" Distraction Effects \"=2,\"Interaction Effect \" =1))  %&gt;%\n  add_header_above(c(\" \" = 1, \" All Conditions\" = 4, \" \" =3)) %&gt;%\n  column_spec(c(2,4), background = \"yellow\")%&gt;%\n  column_spec(c(3,5), background = \"lightgray\")%&gt;%\n  column_spec(8, background = \"lightgray\")%&gt;%\n  row_spec(6, background = \"lightgray\")%&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAll Conditions\n\n\n\n\n\n\nNo Reward\n\n\nReward\n\n\nDistraction Effects\n\n\nInteraction Effect\n\n\n\n\n\nNo Distraction\n\n\nDistraction\n\n\nNo Distraction\n\n\nDistraction\n\n\nNo Reward\n\n\nReward\n\n\nDifference\n\n\n\nsubject\nA\nB\nC\nD\nA-B\nC-D\n(A-B)-(C-D)\n\n\n\n\n1\n10\n5\n12\n9\n5\n3\n2\n\n\n2\n8\n4\n13\n8\n4\n5\n-1\n\n\n3\n11\n3\n14\n10\n8\n4\n4\n\n\n4\n9\n4\n11\n11\n5\n0\n5\n\n\n5\n10\n2\n13\n12\n8\n1\n7\n\n\nMeans\n\n\n\n\n6\n2.6\n3.4\n\n\n\n\n\n\n\nThe mean distraction effects in the no-reward (6) and reward (2.6) conditions were different. This difference is the interaction effect. The size of the interaction effect was 3.4.\nHow can we test whether the interaction effect was likely or unlikely due to chance? We could run another paired-sample \\(t\\)-test between the two distraction effect measures for each subject, or a one sample \\(t\\)-test on the green column (representing the difference between the differences). Both of these \\(t\\)-tests will give the same results:\nHere’s the paired samples version:\n\nA_B &lt;- A-B\nC_D &lt;- C-D\nt.test(A_B,C_D, paired=TRUE,var.equal=TRUE)\n\n\n    Paired t-test\n\ndata:  A_B and C_D\nt = 2.493, df = 4, p-value = 0.06727\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -0.3865663  7.1865663\nsample estimates:\nmean difference \n            3.4 \n\n\nHere’s the one sample version:\n\nt.test(A_B-C_D, mu=0)\n\n\n    One Sample t-test\n\ndata:  A_B - C_D\nt = 2.493, df = 4, p-value = 0.06727\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.3865663  7.1865663\nsample estimates:\nmean of x \n      3.4 \n\n\nOh look, the interaction was not significant. At least, if we had set our alpha criterion to 0.05, it would not have met that criteria. We could write up the results like this. The two-way interaction between between distraction and reward was not significant, \\(t\\)(4) = 2.493, \\(p\\) = 0.067.\nOften times when a result is “not significant” according to the alpha criteria, the pattern among the means is not described further. One reason for this practice is that the researcher is treating the means as if they are not different (because there was an above alpha probability that the observed differences were due to chance). If they are not different, then there is no pattern to report.\nThere are differences in opinion among reasonable and expert statisticians on what should or should not be reported. Let’s say we wanted to report the observed mean differences, we would write something like this:\nThe two-way interaction between between distraction and reward was not significant, t(4) = 2.493, p = 0.067. The mean distraction effect in the no-reward condition was 6 and the mean distraction effect in the reward condition was 2.6.\n\n\n9.5.7 Writing it all up\nWe have completed an analysis of a 2x2 repeated measures design using paired-samples \\(t\\)-tests. Here is what a full write-up of the results could look like.\nThe main effect of distraction was significant, \\(t\\)(4) = 7.66, \\(p\\) = 0.001. The mean number of differences spotted was higher in the no-distraction condition (M = 11.1) than the distraction condition (M = 6.8).\nThe main effect of reward was significant, \\(t\\)(4) = 8.37, \\(p\\) = 0.001. The mean number of differences spotted was higher in the reward condition (M = 11.3) than the no-reward condition (M = 6.6).\nThe two-way interaction between between distraction and reward was not significant, \\(t\\)(4) = 2.493, \\(p\\) = 0.067. The mean distraction effect in the no-reward condition was 6 and the mean distraction effect in the reward condition was 2.6.\nInterim Summary. We went through this exercise to show you how to break up the data into individual comparisons of interest. Generally speaking, a 2x2 repeated measures design would not be analyzed with three paired-samples \\(t\\)-test. This is because it is more convenient to use the repeated measures ANOVA for this task. We will do this in a moment to show you that they give the same results. And, by the same results, what we will show is that the \\(p\\)-values for each main effect, and the interaction, are the same. The ANOVA will give us \\(F\\)-values rather than \\(t\\) values. It turns out that in this situation, the \\(F\\)-values are related to the \\(t\\) values. In fact, \\(t^2 = F\\).\n\n\n9.5.8 2x2 Repeated Measures ANOVA\nWe just showed how a 2x2 repeated measures design can be analyzed using paired-sampled \\(t\\)-tests. We broke up the analysis into three parts. The main effect for distraction, the main effect for reward, and the 2-way interaction between distraction and reward. We claimed the results of the paired-samples \\(t\\)-test analysis would mirror what we would find if we conducted the analysis using an ANOVA. Let’s show that the results are the same. Here are the results from the 2x2 repeated-measures ANOVA, using the aov function in R.\n\nNumber_spotted &lt;- c(A, B, C, D)\nDistraction    &lt;- rep(rep(c(\"No Distraction\", \"Distraction\"), each=5),2)\nReward         &lt;- rep(c(\"No Reward\",\"Reward\"),each=10)\nSubjects       &lt;- rep(1:5,4)\n\nDistraction &lt;- as.factor(Distraction)\nReward      &lt;- as.factor(Reward)\nSubjects    &lt;- as.factor(Subjects)\n\nrm_df &lt;- data.frame(Subjects, Distraction, Reward, Number_spotted)\n\naov_summary &lt;- summary(aov(Number_spotted~Distraction*Reward +\n                             Error(Subjects/(Distraction*Reward)), \n                           rm_df))\nknitr::kable(xtable(aov_summary))\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\nResiduals\n4\n3.70\n0.925\nNA\nNA\n\n\nDistraction\n1\n92.45\n92.450\n58.698413\n0.0015600\n\n\nResiduals\n4\n6.30\n1.575\nNA\nNA\n\n\nReward\n1\n110.45\n110.450\n70.126984\n0.0011122\n\n\nResiduals1\n4\n6.30\n1.575\nNA\nNA\n\n\nDistraction:Reward\n1\n14.45\n14.450\n6.215054\n0.0672681\n\n\nResiduals\n4\n9.30\n2.325\nNA\nNA\n\n\n\n\n\nLet’s compare these results with the paired-samples \\(t\\)-tests.\nMain effect of Distraction: Using the paired samples \\(t\\)-test, we found \\(t\\)(4) =7.6615, \\(p\\)=0.00156. Using the ANOVA we found, \\(F\\)(1,4) = 58.69, \\(p\\)=0.00156. See, the \\(p\\)-values are the same, and \\(t^2 = 7.6615^2 = 58.69 = F\\).\nMain effect of Reward: Using the paired samples \\(t\\)-test, we found \\(t\\)(4) =8.3742, \\(p\\)=0.001112. Using the ANOVA we found, \\(F\\)(1,4) = 70.126, \\(p\\)=0.001112. See, the \\(p\\)-values are the same, and \\(t^2 = 8.3742^2 = 70.12 = F\\).\nInteraction effect: Using the paired samples \\(t\\)-test, we found \\(t\\)(4) =2.493, \\(p\\)=0.06727. Using the ANOVA we found, \\(F\\)(1,4) = 6.215, \\(p\\)=0.06727. See, the \\(p\\)-values are the same, and \\(t^2 = 2.493^2 = 6.215 = F\\).\nThere you have it. The results from a 2x2 repeated measures ANOVA are the same as you would get if you used paired-samples \\(t\\)-tests for the main effects and interactions."
  },
  {
    "objectID": "09-FactorialANOVA.html#x2-between-subjects-anova",
    "href": "09-FactorialANOVA.html#x2-between-subjects-anova",
    "title": "9  Factorial ANOVA",
    "section": "9.6 2x2 Between-subjects ANOVA",
    "text": "9.6 2x2 Between-subjects ANOVA\nYou must be wondering how to calculate a 2x2 ANOVA. We haven’t discussed this yet. We’ve only shown you that you don’t have to do it when the design is a 2x2 repeated measures design (note this is a special case).\nWe are now going to work through some examples of calculating the ANOVA table for 2x2 designs. We will start with the between-subjects ANOVA for 2x2 designs. We do essentially the same thing that we did before (in the other ANOVAs), and the only new thing is to show how to compute the interaction effect.\nRemember the logic of the ANOVA is to partition the variance into different parts. The SS formula for the between-subjects 2x2 ANOVA looks like this:\n\\(SS_\\text{Total} = SS_\\text{Effect IV1} + SS_\\text{Effect IV2} + SS_\\text{Effect IV1xIV2} + SS_\\text{Error}\\)\nIn the following sections we use tables to show the calculation of each SS. We use the same example as before with the exception that we are turning this into a between-subjects design. There are now 5 different subjects in each condition, for a total of 20 subjects. As a result, we remove the subjects column.\n\n9.6.1 SS Total\nWe calculate the grand mean (mean of all of the score). Then, we calculate the differences between each score and the grand mean. We square the difference scores, and sum them up. That is \\(SS_\\text{Total}\\), reported in the bottom yellow row.\n\nA &lt;- c(10,8,11,9,10)  #nD_nR\nB  &lt;- c(5,4,3,4,2)  #D_nR\nC &lt;- c(12,13,14,11,13)  #nD_R\nD  &lt;- c(9,8,10,11,12)  #D_R\n\ngrand_mean &lt;- mean(c(A,B,C,D))\n\nNothing &lt;- as.character(c(\"\",\"\",\"\",\"\",\"\"))\n\nfake_data &lt;- data.frame(Nothing,A, B, C, D, \n                        A_d = round(A-grand_mean,digits=4),\n                        B_d = round(B-grand_mean,digits=4),\n                        C_d = round(C-grand_mean,digits=4),\n                        D_d = round(D-grand_mean,digits=4),\n                        A_s = round((A-grand_mean)^2,digits=4),\n                        B_s = round((B-grand_mean)^2,digits=4),\n                        C_s = round((C-grand_mean)^2,digits=4),\n                        D_s = round((D-grand_mean)^2,digits=4))\n\nfake_data$Nothing&lt;-as.character(fake_data$Nothing)\nnames(fake_data)[1]&lt;-c(\" \")\nnames(fake_data)[6:13]&lt;-c(\"A-GrandM\",\n                          \"B-GrandM\",\n                          \"C-GrandM\",\n                          \"D-GrandM\",\n                          \"(A-GrandM)^2\",\n                          \"(B-GrandM)^2\",\n                          \"(C-GrandM)^2\",\n                          \"(D-GrandM)^2\"\n                          )\n\n\n\n\n\nfake_data&lt;-fake_data %&gt;%\nrbind(as.character(c(\"Means\",colMeans(fake_data[1:5,2:5]),rep(c(\"\"),8)))) %&gt;%\nrbind(as.character(c(\"Grand Mean\",mean(colMeans(fake_data[1:5,2:5])),rep(c(\"\"),11)))) %&gt;%\nrbind(as.character(c(\"sums\",rep(c(\" \"),7),\"Sums\",colSums(fake_data[1:5,10:13])))) %&gt;%\nrbind(as.character(c(\"SS Total\",\n                     rep(c(\" \"),7),\n                     \"SS Total\",\n                     sum(colSums(fake_data[1:5,10:13])),\n                     rep(c(\" \"),3)\n                     )))\n\nkable(fake_data) %&gt;%\n  add_header_above(c(\" \",\n                     \"No Distraction\" = 1, \"Distraction\" = 1, \"No Distraction\" = 1, \"Distraction\" = 1, \n                     \"No Distraction\" = 1, \"Distraction\" = 1, \"No Distraction\" = 1, \"Distraction\" = 1,\n                     \"No Distraction\" = 1, \"Distraction\" = 1, \"No Distraction\" = 1, \"Distraction\" = 1)) %&gt;%\n  add_header_above(c(\" \",\n                     \"No Reward\" = 2, \"Reward\" = 2 ,\n                     \"No Reward\" = 2, \"Reward\" = 2 ,\n                     \"No Reward\" = 2, \"Reward\" = 2))  %&gt;%\n  add_header_above(c(\" \",\n                     \"All Conditions\" = 4, \n                     \"Difference from Grand Mean\" = 4, \n                     \"Squared Differences\" = 4 )) %&gt;%\n  row_spec(c(6,7), background = \"lightgray\")%&gt;%\n  row_spec(c(8,9), background = \"yellow\")%&gt;%\n  column_spec(c(10:13), background = \"yellow\")%&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAll Conditions\n\n\nDifference from Grand Mean\n\n\nSquared Differences\n\n\n\n\n\nNo Reward\n\n\nReward\n\n\nNo Reward\n\n\nReward\n\n\nNo Reward\n\n\nReward\n\n\n\n\n\nNo Distraction\n\n\nDistraction\n\n\nNo Distraction\n\n\nDistraction\n\n\nNo Distraction\n\n\nDistraction\n\n\nNo Distraction\n\n\nDistraction\n\n\nNo Distraction\n\n\nDistraction\n\n\nNo Distraction\n\n\nDistraction\n\n\n\n\nA\nB\nC\nD\nA-GrandM\nB-GrandM\nC-GrandM\nD-GrandM\n(A-GrandM)^2\n(B-GrandM)^2\n(C-GrandM)^2\n(D-GrandM)^2\n\n\n\n\n\n10\n5\n12\n9\n1.05\n-3.95\n3.05\n0.05\n1.1025\n15.6025\n9.3025\n0.0025\n\n\n\n8\n4\n13\n8\n-0.95\n-4.95\n4.05\n-0.95\n0.9025\n24.5025\n16.4025\n0.9025\n\n\n\n11\n3\n14\n10\n2.05\n-5.95\n5.05\n1.05\n4.2025\n35.4025\n25.5025\n1.1025\n\n\n\n9\n4\n11\n11\n0.05\n-4.95\n2.05\n2.05\n0.0025\n24.5025\n4.2025\n4.2025\n\n\n\n10\n2\n13\n12\n1.05\n-6.95\n4.05\n3.05\n1.1025\n48.3025\n16.4025\n9.3025\n\n\nMeans\n9.6\n3.6\n12.6\n10\n\n\n\n\n\n\n\n\n\n\nGrand Mean\n8.95\n\n\n\n\n\n\n\n\n\n\n\n\n\nsums\n\n\n\n\n\n\n\nSums\n7.3125\n148.3125\n71.8125\n15.5125\n\n\nSS Total\n\n\n\n\n\n\n\nSS Total\n242.95\n\n\n\n\n\n\n\n\n\n\n\n\n9.6.2 SS Distraction\nWe need to compute the SS for the main effect for distraction. We calculate the grand mean (mean of all of the scores). Then, we calculate the means for the two distraction conditions. Then we treat each score as if it was the mean for it’s respective distraction condition. We find the differences between each distraction condition mean and the grand mean. Then we square the differences and sum them up. That is \\(SS_\\text{Distraction}\\), reported in the bottom yellow row.\n\nA &lt;- c(10,8,11,9,10)  #nD_nR\nB  &lt;- c(5,4,3,4,2)  #D_nR\nC &lt;- c(12,13,14,11,13)  #nD_R\nD  &lt;- c(9,8,10,11,12)  #D_R\n\ngrand_mean &lt;- mean(c(A,B,C,D))\nnd_mean&lt;-mean(c(A,C))\nd_mean&lt;-mean(c(B,D))\n\nNothing &lt;- as.character(c(\"\",\"\",\"\",\"\",\"\"))\n\nfake_data &lt;- data.frame(Nothing,A, B, C, D, \n                        A_d = round(nd_mean-grand_mean,digits=4),\n                        B_d = round(d_mean-grand_mean,digits=4),\n                        C_d = round(nd_mean-grand_mean,digits=4),\n                        D_d = round(d_mean-grand_mean,digits=4),\n                        A_s = round((nd_mean-grand_mean)^2,digits=4),\n                        B_s = round((d_mean-grand_mean)^2,digits=4),\n                        C_s = round((nd_mean-grand_mean)^2,digits=4),\n                        D_s = round((d_mean-grand_mean)^2,digits=4))\n\nfake_data$Nothing&lt;-as.character(fake_data$Nothing)\nnames(fake_data)[1]&lt;-c(\" \")\nnames(fake_data)[6:13]&lt;-c(\"NDM-GM A\",\n                          \"DM-GM B\",\n                          \"NDM-GM C\",\n                          \"DM-GM D\",\n                          \"(NDM-GM )^2 A\",\n                          \"(DM-GM)^2 B\",\n                          \"(NDM-GM)^2 C\",\n                          \"(DM-GM)^2 D\"\n                          )\n\n\n\n\n\nfake_data&lt;-fake_data %&gt;%\nrbind(as.character(c(\"Means\",colMeans(fake_data[1:5,2:5]),rep(c(\"\"),8)))) %&gt;%\nrbind(as.character(c(\"Grand Mean\",mean(colMeans(fake_data[1:5,2:5])),\n                     \"No Distraction\",nd_mean,\n                     \"Distraction\",d_mean,\n                     rep(c(\"\"),8)))) %&gt;%\nrbind(as.character(c(\"sums\",rep(c(\" \"),7),\"Sums\",colSums(fake_data[1:5,10:13])))) %&gt;%\nrbind(as.character(c(\"SS Distraction\",\n                     rep(c(\" \"),7),\n                     \"SS Distraction\",\n                     sum(colSums(fake_data[1:5,10:13])),\n                     rep(c(\" \"),3)\n                     )))\n\nWarning in rbind(deparse.level, ...): El número de columnas del resultado, 13,\nno es un múltiplo de la longitud del vector 14 del argumento 2\n\nkable(fake_data) %&gt;%\n  add_header_above(c(\" \",\n                     \"No Distraction\" = 1, \"Distraction\" = 1, \"No Distraction\" = 1, \"Distraction\" = 1, \n                     \"No Distraction\" = 1, \"Distraction\" = 1, \"No Distraction\" = 1, \"Distraction\" = 1,\n                     \"No Distraction\" = 1, \"Distraction\" = 1, \"No Distraction\" = 1, \"Distraction\" = 1)) %&gt;%\n  add_header_above(c(\" \",\n                     \"No Reward\" = 2, \"Reward\" = 2 ,\n                     \"No Reward\" = 2, \"Reward\" = 2 ,\n                     \"No Reward\" = 2, \"Reward\" = 2))  %&gt;%\n  add_header_above(c(\" \",\n                     \"All Conditions\" = 4, \n                     \"Distraction Mean - GM\" = 4, \n                     \"Squared Differences\" = 4 )) %&gt;%\n  row_spec(c(6,7), background = \"lightgray\")%&gt;%\n  row_spec(c(8,9), background = \"yellow\")%&gt;%\n  column_spec(c(10:13), background = \"yellow\")%&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAll Conditions\n\n\nDistraction Mean - GM\n\n\nSquared Differences\n\n\n\n\n\nNo Reward\n\n\nReward\n\n\nNo Reward\n\n\nReward\n\n\nNo Reward\n\n\nReward\n\n\n\n\n\nNo Distraction\n\n\nDistraction\n\n\nNo Distraction\n\n\nDistraction\n\n\nNo Distraction\n\n\nDistraction\n\n\nNo Distraction\n\n\nDistraction\n\n\nNo Distraction\n\n\nDistraction\n\n\nNo Distraction\n\n\nDistraction\n\n\n\n\nA\nB\nC\nD\nNDM-GM A\nDM-GM B\nNDM-GM C\nDM-GM D\n(NDM-GM )^2 A\n(DM-GM)^2 B\n(NDM-GM)^2 C\n(DM-GM)^2 D\n\n\n\n\n\n10\n5\n12\n9\n2.15\n-2.15\n2.15\n-2.15\n4.6225\n4.6225\n4.6225\n4.6225\n\n\n\n8\n4\n13\n8\n2.15\n-2.15\n2.15\n-2.15\n4.6225\n4.6225\n4.6225\n4.6225\n\n\n\n11\n3\n14\n10\n2.15\n-2.15\n2.15\n-2.15\n4.6225\n4.6225\n4.6225\n4.6225\n\n\n\n9\n4\n11\n11\n2.15\n-2.15\n2.15\n-2.15\n4.6225\n4.6225\n4.6225\n4.6225\n\n\n\n10\n2\n13\n12\n2.15\n-2.15\n2.15\n-2.15\n4.6225\n4.6225\n4.6225\n4.6225\n\n\nMeans\n9.6\n3.6\n12.6\n10\n\n\n\n\n\n\n\n\n\n\nGrand Mean\n8.95\nNo Distraction\n11.1\nDistraction\n6.8\n\n\n\n\n\n\n\n\n\nsums\n\n\n\n\n\n\n\nSums\n23.1125\n23.1125\n23.1125\n23.1125\n\n\nSS Distraction\n\n\n\n\n\n\n\nSS Distraction\n92.45\n\n\n\n\n\n\n\n\n\n\nThese tables are a lot to look at! Notice here, that we first found the grand mean (8.95). Then we found the mean for all the scores in the no-distraction condition (columns A and C), that was 11.1. All of the difference scores for the no-distraction condition are 11.1-8.95 = 2.15. We also found the mean for the scores in the distraction condition (columns B and D), that was 6.8. So, all of the difference scores are 6.8-8.95 = -2.15. Remember, means are the balancing point in the data, this is why the difference scores are +2.15 and -2.15. The grand mean 8.95 is in between the two condition means (11.1 and 6.8), by a difference of 2.15.\n\n\n9.6.3 SS Reward\nWe need to compute the SS for the main effect for reward. We calculate the grand mean (mean of all of the scores). Then, we calculate the means for the two reward conditions. Then we treat each score as if it was the mean for it’s respective reward condition. We find the differences between each reward condition mean and the grand mean. Then we square the differences and sum them up. That is \\(SS_\\text{Reward}\\), reported in the bottom yellow row.\n\nA &lt;- c(10,8,11,9,10)  #nD_nR\nB  &lt;- c(5,4,3,4,2)  #D_nR\nC &lt;- c(12,13,14,11,13)  #nD_R\nD  &lt;- c(9,8,10,11,12)  #D_R\n\ngrand_mean &lt;- mean(c(A,B,C,D))\nnr_mean&lt;-mean(c(A,B))\nr_mean&lt;-mean(c(C,D))\n\nNothing &lt;- as.character(c(\"\",\"\",\"\",\"\",\"\"))\n\nfake_data &lt;- data.frame(Nothing,A, B, C, D, \n                        A_d = round(nr_mean-grand_mean,digits=4),\n                        B_d = round(nr_mean-grand_mean,digits=4),\n                        C_d = round(r_mean-grand_mean,digits=4),\n                        D_d = round(r_mean-grand_mean,digits=4),\n                        A_s = round((nr_mean-grand_mean)^2,digits=4),\n                        B_s = round((nr_mean-grand_mean)^2,digits=4),\n                        C_s = round((r_mean-grand_mean)^2,digits=4),\n                        D_s = round((r_mean-grand_mean)^2,digits=4))\n\nfake_data$Nothing&lt;-as.character(fake_data$Nothing)\nnames(fake_data)[1]&lt;-c(\" \")\nnames(fake_data)[6:13]&lt;-c(\"NRM-GM A\",\n                          \"NRM-GM B\",\n                          \"RM-GM C\",\n                          \"RM-GM D\",\n                          \"(NRM-GM )^2 A\",\n                          \"(NRM-GM)^2 B\",\n                          \"(RM-GM)^2 C\",\n                          \"(RM-GM)^2 D\"\n                          )\n\nfake_data&lt;-fake_data %&gt;%\nrbind(as.character(c(\"Means\",colMeans(fake_data[1:5,2:5]),rep(c(\"\"),8)))) %&gt;%\nrbind(as.character(c(\"Grand Mean\",mean(colMeans(fake_data[1:5,2:5])),\n                     \"No Reward\",nr_mean,\n                     \"Reward\",r_mean,\n                     rep(c(\"\"),8)))) %&gt;%\nrbind(as.character(c(\"sums\",rep(c(\" \"),7),\"Sums\",colSums(fake_data[1:5,10:13])))) %&gt;%\nrbind(as.character(c(\"SS Reward\",\n                     rep(c(\" \"),7),\n                     \"SS Reward\",\n                     sum(colSums(fake_data[1:5,10:13])),\n                     rep(c(\" \"),3)\n                     )))\n\nWarning in rbind(deparse.level, ...): El número de columnas del resultado, 13,\nno es un múltiplo de la longitud del vector 14 del argumento 2\n\nkable(fake_data) %&gt;%\n  add_header_above(c(\" \",\n                     \"No Distraction\" = 1, \"Distraction\" = 1, \"No Distraction\" = 1, \"Distraction\" = 1, \n                     \"No Distraction\" = 1, \"Distraction\" = 1, \"No Distraction\" = 1, \"Distraction\" = 1,\n                     \"No Distraction\" = 1, \"Distraction\" = 1, \"No Distraction\" = 1, \"Distraction\" = 1)) %&gt;%\n  add_header_above(c(\" \",\n                     \"No Reward\" = 2, \"Reward\" = 2 ,\n                     \"No Reward\" = 2, \"Reward\" = 2 ,\n                     \"No Reward\" = 2, \"Reward\" = 2))  %&gt;%\n  add_header_above(c(\" \",\n                     \"All Conditions\" = 4, \n                     \"Reward Mean - GM\" = 4, \n                     \"Squared Differences\" = 4 )) %&gt;%\n  row_spec(c(6,7), background = \"lightgray\")%&gt;%\n  row_spec(c(8,9), background = \"yellow\")%&gt;%\n  column_spec(c(10:13), background = \"yellow\")%&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAll Conditions\n\n\nReward Mean - GM\n\n\nSquared Differences\n\n\n\n\n\nNo Reward\n\n\nReward\n\n\nNo Reward\n\n\nReward\n\n\nNo Reward\n\n\nReward\n\n\n\n\n\nNo Distraction\n\n\nDistraction\n\n\nNo Distraction\n\n\nDistraction\n\n\nNo Distraction\n\n\nDistraction\n\n\nNo Distraction\n\n\nDistraction\n\n\nNo Distraction\n\n\nDistraction\n\n\nNo Distraction\n\n\nDistraction\n\n\n\n\nA\nB\nC\nD\nNRM-GM A\nNRM-GM B\nRM-GM C\nRM-GM D\n(NRM-GM )^2 A\n(NRM-GM)^2 B\n(RM-GM)^2 C\n(RM-GM)^2 D\n\n\n\n\n\n10\n5\n12\n9\n-2.35\n-2.35\n2.35\n2.35\n5.5225\n5.5225\n5.5225\n5.5225\n\n\n\n8\n4\n13\n8\n-2.35\n-2.35\n2.35\n2.35\n5.5225\n5.5225\n5.5225\n5.5225\n\n\n\n11\n3\n14\n10\n-2.35\n-2.35\n2.35\n2.35\n5.5225\n5.5225\n5.5225\n5.5225\n\n\n\n9\n4\n11\n11\n-2.35\n-2.35\n2.35\n2.35\n5.5225\n5.5225\n5.5225\n5.5225\n\n\n\n10\n2\n13\n12\n-2.35\n-2.35\n2.35\n2.35\n5.5225\n5.5225\n5.5225\n5.5225\n\n\nMeans\n9.6\n3.6\n12.6\n10\n\n\n\n\n\n\n\n\n\n\nGrand Mean\n8.95\nNo Reward\n6.6\nReward\n11.3\n\n\n\n\n\n\n\n\n\nsums\n\n\n\n\n\n\n\nSums\n27.6125\n27.6125\n27.6125\n27.6125\n\n\nSS Reward\n\n\n\n\n\n\n\nSS Reward\n110.45\n\n\n\n\n\n\n\n\n\n\nNow we treat each no-reward score as the mean for the no-reward condition (6.6), and subtract it from the grand mean (8.95), to get -2.35. Then, we treat each reward score as the mean for the reward condition (11.3), and subtract it from the grand mean (8.95), to get +2.35. Then we square the differences and sum them up.\n\n\n9.6.4 SS Distraction by Reward\nWe need to compute the SS for the interaction effect between distraction and reward. This is the new thing that we do in an ANOVA with more than one IV. How do we calculate the variation explained by the interaction?\nThe heart of the question is something like this. Do the individual means for each of the four conditions do something a little bit different than the group means for both of the independent variables.\nFor example, consider the overall mean for all of the scores in the no reward group, we found that to be 6.6 Now, was the mean for each no-reward group in the whole design a 6.6? For example, in the no-distraction group, was the mean for column A (the no-reward condition in that group) also 6.6? The answer is no, it was 9.6. How about the distraction group? Was the mean for the reward condition in the distraction group (column B) 6.6? No, it was 3.6. The mean of 9.6 and 3.6 is 6.6. If there was no hint of an interaction, we would expect that the means for the reward condition in both levels of the distraction group would be the same, they would both be 6.6. However, when there is an interaction, the means for the reward group will depend on the levels of the group from another IV. In this case, it looks like there is an interaction because the means are different from 6.6, they are 9.6 and 3.6 for the no-distraction and distraction conditions. This is extra-variance that is not explained by the mean for the reward condition. We want to capture this extra variance and sum it up. Then we will have measure of the portion of the variance that is due to the interaction between the reward and distraction conditions.\nWhat we will do is this. We will find the four condition means. Then we will see how much additional variation they explain beyond the group means for reward and distraction. To do this we treat each score as the condition mean for that score. Then we subtract the mean for the distraction group, and the mean for the reward group, and then we add the grand mean. This gives us the unique variation that is due to the interaction. We could also say that we are subtracting each condition mean from the grand mean, and then adding back in the distraction mean and the reward mean, that would amount to the same thing, and perhaps make more sense.\nHere is a formula to describe the process for each score:\n\\(\\bar{X}_\\text{condition} -\\bar{X}_\\text{IV1} - \\bar{X}_\\text{IV2} + \\bar{X}_\\text{Grand Mean}\\)\nOr we could write it this way:\n\\(\\bar{X}_\\text{condition} - \\bar{X}_\\text{Grand Mean} + \\bar{X}_\\text{IV1} + \\bar{X}_\\text{IV2}\\)\nWhen you look at the following table, we apply this formula to the calculation of each of the differences scores. We then square the difference scores, and sum them up to get \\(SS_\\text{Interaction}\\), which is reported in the bottom yellow row.\n\nA &lt;- c(10,8,11,9,10)  #nD_nR\nB  &lt;- c(5,4,3,4,2)  #D_nR\nC &lt;- c(12,13,14,11,13)  #nD_R\nD  &lt;- c(9,8,10,11,12)  #D_R\n\ngrand_mean &lt;- mean(c(A,B,C,D))\nnr_mean&lt;-mean(c(A,B))\nr_mean&lt;-mean(c(C,D))\nnd_mean&lt;-mean(c(A,C))\nd_mean&lt;-mean(c(B,D))\n\nNothing &lt;- as.character(c(\"\",\"\",\"\",\"\",\"\"))\n\nfake_data &lt;- data.frame(Nothing,A, B, C, D, \n                        A_d = round(mean(A)-nd_mean-nr_mean+grand_mean,digits=4),\n                        B_d = round(mean(B)-d_mean-nr_mean+grand_mean,digits=4),\n                        C_d = round(mean(C)-nd_mean-r_mean+grand_mean,digits=4),\n                        D_d = round(mean(D)-d_mean-r_mean+grand_mean,digits=4),\n                        A_s = round((mean(A)-nd_mean-nr_mean+grand_mean)^2,digits=4),\n                        B_s = round((mean(B)-d_mean-nr_mean+grand_mean)^2,digits=4),\n                        C_s = round((mean(C)-nd_mean-r_mean+grand_mean)^2,digits=4),\n                        D_s = round((mean(D)-d_mean-r_mean+grand_mean)^2,digits=4))\n\nfake_data$Nothing&lt;-as.character(fake_data$Nothing)\nnames(fake_data)[1]&lt;-c(\" \")\nnames(fake_data)[6:13]&lt;-c(\"A-ND-NR+GM\",\n                          \"B-D-NR+GM\",\n                          \"C-ND-R+GM\",\n                          \"D-D-R+GM\",\n                          \"(A-ND-NR+GM)^2 A\",\n                          \"(B-D-NR+GM)^2 B\",\n                          \"(C-ND-R+GM)^2 C\",\n                          \"(D-D-R+GM)^2 D\"\n                          )\n\nfake_data&lt;-fake_data %&gt;%\nrbind(as.character(c(\"Means\",colMeans(fake_data[1:5,2:5]),rep(c(\"\"),8)))) %&gt;%\nrbind(as.character(c(\"Grand Mean\",mean(colMeans(fake_data[1:5,2:5])),\n                     \"\",\"\",\n                     \"\",\"\",\n                     rep(c(\"\"),8)))) %&gt;%\nrbind(as.character(c(\"sums\",rep(c(\" \"),7),\"Sums\",colSums(fake_data[1:5,10:13])))) %&gt;%\nrbind(as.character(c(\"SS Interaction\",\n                     rep(c(\" \"),7),\n                     \"SS Interaction\",\n                     sum(colSums(fake_data[1:5,10:13])),\n                     rep(c(\" \"),3)\n                     )))\n\nWarning in rbind(deparse.level, ...): El número de columnas del resultado, 13,\nno es un múltiplo de la longitud del vector 14 del argumento 2\n\nkable(fake_data) %&gt;%\n  add_header_above(c(\" \",\n                     \"No Distraction\" = 1, \"Distraction\" = 1, \"No Distraction\" = 1, \"Distraction\" = 1, \n                     \"No Distraction\" = 1, \"Distraction\" = 1, \"No Distraction\" = 1, \"Distraction\" = 1,\n                     \"No Distraction\" = 1, \"Distraction\" = 1, \"No Distraction\" = 1, \"Distraction\" = 1)) %&gt;%\n  add_header_above(c(\" \",\n                     \"No Reward\" = 2, \"Reward\" = 2 ,\n                     \"No Reward\" = 2, \"Reward\" = 2 ,\n                     \"No Reward\" = 2, \"Reward\" = 2))  %&gt;%\n  add_header_above(c(\" \",\n                     \"All Conditions\" = 4, \n                     \"Interaction Differences\" = 4, \n                     \"Squared Differences\" = 4 )) %&gt;%\n  row_spec(c(6,7), background = \"lightgray\")%&gt;%\n  row_spec(c(8,9), background = \"yellow\")%&gt;%\n  column_spec(c(10:13), background = \"yellow\")%&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAll Conditions\n\n\nInteraction Differences\n\n\nSquared Differences\n\n\n\n\n\nNo Reward\n\n\nReward\n\n\nNo Reward\n\n\nReward\n\n\nNo Reward\n\n\nReward\n\n\n\n\n\nNo Distraction\n\n\nDistraction\n\n\nNo Distraction\n\n\nDistraction\n\n\nNo Distraction\n\n\nDistraction\n\n\nNo Distraction\n\n\nDistraction\n\n\nNo Distraction\n\n\nDistraction\n\n\nNo Distraction\n\n\nDistraction\n\n\n\n\nA\nB\nC\nD\nA-ND-NR+GM\nB-D-NR+GM\nC-ND-R+GM\nD-D-R+GM\n(A-ND-NR+GM)^2 A\n(B-D-NR+GM)^2 B\n(C-ND-R+GM)^2 C\n(D-D-R+GM)^2 D\n\n\n\n\n\n10\n5\n12\n9\n0.85\n-0.85\n-0.85\n0.85\n0.7225\n0.7225\n0.7225\n0.7225\n\n\n\n8\n4\n13\n8\n0.85\n-0.85\n-0.85\n0.85\n0.7225\n0.7225\n0.7225\n0.7225\n\n\n\n11\n3\n14\n10\n0.85\n-0.85\n-0.85\n0.85\n0.7225\n0.7225\n0.7225\n0.7225\n\n\n\n9\n4\n11\n11\n0.85\n-0.85\n-0.85\n0.85\n0.7225\n0.7225\n0.7225\n0.7225\n\n\n\n10\n2\n13\n12\n0.85\n-0.85\n-0.85\n0.85\n0.7225\n0.7225\n0.7225\n0.7225\n\n\nMeans\n9.6\n3.6\n12.6\n10\n\n\n\n\n\n\n\n\n\n\nGrand Mean\n8.95\n\n\n\n\n\n\n\n\n\n\n\n\n\nsums\n\n\n\n\n\n\n\nSums\n3.6125\n3.6125\n3.6125\n3.6125\n\n\nSS Interaction\n\n\n\n\n\n\n\nSS Interaction\n14.45\n\n\n\n\n\n\n\n\n\n\n\n\n9.6.5 SS Error\nThe last thing we need to find is the SS Error. We can solve for that because we found everything else in this formula:\n\\(SS_\\text{Total} = SS_\\text{Effect IV1} + SS_\\text{Effect IV2} + SS_\\text{Effect IV1xIV2} + SS_\\text{Error}\\)\nEven though this textbook meant to explain things in a step by step way, we guess you are tired from watching us work out the 2x2 ANOVA by hand. You and me both, making these tables was a lot of work. We have already shown you how to compute the SS for error before, so we will not do the full example here. Instead, we solve for SS Error using the numbers we have already obtained.\n$SS_ = SS_- SS_ - SS_ - SS_ $\n$SS_ = 242.95 - 92.45 - 110.45 - 14.45 = 25.6 $\n\n\n9.6.6 Check your work\nWe are going to skip the part where we divide the SSes by their dfs to find the MSEs so that we can compute the three \\(F\\)-values. Instead, if we have done the calculations of the \\(SS\\)es correctly, they should be same as what we would get if we used R to calculate the \\(SS\\)es. Let’s make R do the work, and then compare to check our work.\n\nNumber_spotted &lt;- c(A, B, C, D)\nDistraction    &lt;- rep(rep(c(\"No Distraction\", \"Distraction\"), each=5),2)\nReward         &lt;- rep(c(\"No Reward\",\"Reward\"),each=10)\n\nDistraction &lt;- as.factor(Distraction)\nReward      &lt;- as.factor(Reward)\n\nall_df &lt;- data.frame(Distraction, Reward, Number_spotted)\n\naov_summary &lt;- summary(aov(Number_spotted~Distraction*Reward, all_df))\nknitr::kable(xtable(aov_summary))\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\nDistraction\n1\n92.45\n92.45\n57.78125\n0.0000011\n\n\nReward\n1\n110.45\n110.45\n69.03125\n0.0000003\n\n\nDistraction:Reward\n1\n14.45\n14.45\n9.03125\n0.0083879\n\n\nResiduals\n16\n25.60\n1.60\nNA\nNA\n\n\n\n\n\nA quick look through the column Sum Sq shows that we did our work by hand correctly. Congratulations to us! Note, this is not the same results as we had before with the repeated measures ANOVA. We conducted a between-subjects design, so we did not get to further partition the SS error into a part due to subject variation and a left-over part. We also gained degrees of freedom in the error term. It turns out with this specific set of data, we find p-values of less than 0.05 for all effects (main effects and the interaction, which was not less than 0.05 using the same data, but treating it as a repeated-measures design)"
  },
  {
    "objectID": "09-FactorialANOVA.html#fireside-chat",
    "href": "09-FactorialANOVA.html#fireside-chat",
    "title": "9  Factorial ANOVA",
    "section": "9.7 Fireside chat",
    "text": "9.7 Fireside chat\nSometimes it’s good to get together around a fire and have a chat. Let’s pretend we’re sitting around a fire.\nIt’s been a long day. A long couple of weeks and months since we started this course on statistics. We just went through the most complicated things we have done so far. This is a long chapter. What should we do next?\nHere’s a couple of options. We could work through, by hand, more and more ANOVAs. Do you want to do that? I don’t, making these tables isn’t too bad, but it takes a lot of time. It’s really good to see everything that we do laid bare in the table form a few times. We’ve done that already. It’s really good for you to attempt to calculate an ANOVA by hand at least once in your life. It builds character. It helps you know that you know what you are doing, and what the ANOVA is doing. We can’t make you do this, we can only make the suggestion. If we keep doing these by hand, it is not good for us, and it is not you doing them by hand. So, what are the other options.\nThe other options are to work at a slightly higher level. We will discuss some research designs, and the ANOVAs that are appropriate for their analysis. We will conduct the ANOVAs using R, and print out the ANOVA tables. This is what you do in the lab, and what most researchers do. They use software most of the time to make the computer do the work. Because of this, it is most important that you know what the software is doing. You can make mistakes when telling software what to do, so you need to be able to check the software’s work so you know when the software is giving you wrong answers. All of these skills are built up over time through the process of analyzing different data sets. So, for the remainder of our discussion on ANOVAs we stick to that higher level. No more monster tables of SSes. You are welcome."
  },
  {
    "objectID": "09-FactorialANOVA.html#real-data",
    "href": "09-FactorialANOVA.html#real-data",
    "title": "9  Factorial ANOVA",
    "section": "9.8 Real Data",
    "text": "9.8 Real Data\nLet’s go through the process of looking at a 2x2 factorial design in the wild. This will be the very same data that you will analyze in the lab for factorial designs.\n\n9.8.1 Stand at attention\nDo you pay more attention when you are sitting or standing? This was the kind of research question the researchers were asking in the study we will look at. In fact, the general question and design is very similar to our fake study idea that we used to explain factorial designs in this chapter.\nThe paper we look at is called “Stand by your Stroop: Standing up enhances selective attention and cognitive control” (Rosenbaum, Mama, and Algom 2017). This paper asked whether sitting versus standing would influence a measure of selective attention, the ability to ignore distracting information.\nThey used a classic test of selective attention, called the Stroop effect. You may already know what the Stroop effect is. In a typical Stroop experiment, subjects name the color of words as fast as they can. The trick is that sometimes the color of the word is the same as the name of the word, and sometimes it is not. Figure 9.9 shows some examples:\n\nknitr::include_graphics(\"imgs/figures/Stroop.png\")\n\n\n\n\nFigure 9.9: Examples of congruent and incongruent Stroop stimuli. The task is to name the color, not the word.\n\n\n\n\nCongruent trials occur when the color and word match. So, the correct answers for each of the congruent stimuli shown would be to say, red, green, blue and yellow. Incongruent trials occur when the color and word mismatch. The correct answers for each of the incongruent stimuli would be: blue, yellow, red, green.\nThe Stroop effect is an example of a well-known phenomena. What happens is that people are faster to name the color of the congruent items compared to the color of the incongruent items. This difference (incongruent reaction time - congruent reaction time) is called the Stroop effect.\nMany researchers argue that the Stroop effect measures something about selective attention, the ability to ignore distracting information. In this case, the target information that you need to pay attention to is the color, not the word. For each item, the word is potentially distracting, it is not information that you are supposed to respond to. However, it seems that most people can’t help but notice the word, and their performance in the color-naming task is subsequently influenced by the presence of the distracting word.\nPeople who are good at ignoring the distracting words should have small Stroop effects. They will ignore the word, and it won’t influence them very much for either congruent or incongruent trials. As a result, the difference in performance (the Stroop effect) should be fairly small (if you have “good” selective attention in this task). People who are bad at ignoring the distracting words should have big Stroop effects. They will not ignore the words, causing them to be relatively fast when the word helps, and relatively slow when the word mismatches. As a result, they will show a difference in performance between the incongruent and congruent conditions.\nIf we take the size of the Stroop effect as a measure of selective attention, we can then start wondering what sorts of things improve selective attention (e.g., that make the Stroop effect smaller), and what kinds of things impair selective attention (e.g., make the Stroop effect bigger).\nThe research question of this study was to ask whether standing up improves selective attention compared to sitting down. They predicted smaller Stroop effects when people were standing up and doing the task, compared to when they were sitting down and doing the task.\nThe design of the study was a 2x2 repeated-measures design. The first IV was congruency (congruent vs incongruent). The second IV was posture (sitting vs. standing). The DV was reaction time to name the word.\n\n\n9.8.2 Plot the data\nThey had subjects perform many individual trials responding to single Stroop stimuli, both congruent and incongruent. And they had subjects stand up sometimes and do it, and sit-down sometimes and do it. Figure 9.10 shows what they found:\n\nlibrary(data.table)\nstroop_data &lt;- fread(\"data/stroop_stand.csv\")\n\nRTs &lt;- c(\n  as.numeric(unlist(stroop_data[, 1])),\n  as.numeric(unlist(stroop_data[, 2])),\n  as.numeric(unlist(stroop_data[, 3])),\n  as.numeric(unlist(stroop_data[, 4]))\n)\n\nCongruency &lt;- rep(rep(c(\"Congruent\", \"Incongruent\"), each = 50), 2)\nPosture &lt;- rep(c(\"Stand\", \"Sit\"), each = 100)\nSubject &lt;- rep(1:50, 4)\n\nstroop_df &lt;- data.frame(Subject, Congruency, Posture, RTs)\n\nplot_df &lt;- stroop_df %&gt;%\n  dplyr::group_by(Congruency, Posture) %&gt;%\n  dplyr::summarise(mean_RT = mean(RTs),\n                   SEM = sd(RTs) / sqrt(length(RTs)))\n\n`summarise()` has grouped output by 'Congruency'. You can override using the\n`.groups` argument.\n\nggplot(plot_df,\n       aes(\n         x = Posture,\n         y = mean_RT,\n         group = Congruency,\n         fill = Congruency\n       )) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  theme_classic() +\n  coord_cartesian(ylim = c(700, 1000))\n\n\n\n\nFigure 9.10: Means from Rosenbaum et al (2017)\n\n\n\n\nThe figure shows the means. We can see that Stroop effects were observed in both the sitting position and the standing position. In the sitting position, mean congruent RTs were shorter than mean incongruent RTs (the red bar is lower than the aqua bar). The same general pattern is observed for the standing position. However, it does look as if the Stroop effect is slightly smaller in the stand condition: the difference between the red and aqua bars is slightly smaller compared to the difference when people were sitting.\n\n\n9.8.3 Conduct the ANOVA\nLet’s conduct a 2x2 repeated measures ANOVA on the data to evaluate whether the differences in the means are likely or unlikely to be due to chance. The ANOVA will give us main effects for congruency and posture (the two IVs), as well as one interaction effect to evaluate (congruency X posture). Remember, the interaction effect tells us whether the congruency effect changes across the levels of the posture manipulation.\n\nlibrary(xtable)\nstroop_df$Subject &lt;- as.factor(stroop_df$Subject)\naov_summary &lt;- summary(aov(RTs~Congruency*Posture +\n                             Error(Subject/(Congruency*Posture)), \n                           stroop_df))\nknitr::kable(xtable(aov_summary))\n\n\n\n\n\n\n\n\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\nResiduals\n49\n2250738.636\n45933.4416\nNA\nNA\n\n\nCongruency\n1\n576821.635\n576821.6349\n342.452244\n0.0000000\n\n\nResiduals\n49\n82534.895\n1684.3856\nNA\nNA\n\n\nPosture\n1\n32303.453\n32303.4534\n7.329876\n0.0093104\n\n\nResiduals1\n49\n215947.614\n4407.0942\nNA\nNA\n\n\nCongruency:Posture\n1\n6560.339\n6560.3389\n8.964444\n0.0043060\n\n\nResiduals\n49\n35859.069\n731.8177\nNA\nNA\n\n\n\n\n\n\n\n9.8.4 Main effect of Congruency\nLet’s talk about each aspect of the ANOVA table, one step at a time. First, we see that there was a significant main effect of congruency, \\(F\\)(1, 49) = 342.45, \\(p\\) &lt; 0.001. The \\(F\\) value is extremely large, and the \\(p\\)-value is so small it reads as a zero. This \\(F\\)-value basically never happens by sampling error. We can be very confident that the overall mean difference between congruent and incongruent RTs was not caused by sampling error.\nWhat were the overall mean differences between mean RTs in the congruent and incongruent conditions? We would have to look at those means to find out. Here’s a table:\n\ncongruency_means &lt;- stroop_df %&gt;%\n                      group_by(Congruency) %&gt;%\n                      summarise(mean_rt = mean(RTs),\n                                sd = sd(RTs),\n                                SEM = sd(RTs)/sqrt(length(RTs)))\nknitr::kable(congruency_means)\n\n\n\n\nCongruency\nmean_rt\nsd\nSEM\n\n\n\n\nCongruent\n814.9415\n111.3193\n11.13193\n\n\nIncongruent\n922.3493\n118.7960\n11.87960\n\n\n\n\n\nThe table shows the mean RTs, standard deviation (sd), and standard error of the mean for each condition. These means show that there was a Stroop effect. Mean incongruent RTs were slower (larger number in milliseconds) than mean congruent RTs. The main effect of congruency is important for establishing that the researchers were able to measure the Stroop effect. However, the main effect of congruency does not say whether the size of the Stroop effect changed between the levels of the posture variable. So, this main effect was not particularly important for answering the specific question posed by the study.\n\n\n9.8.5 Main effect of Posture\nThere was also a main effect of posture, \\(F\\)(1,49) = 7.329, \\(p\\) =0.009.\nLet’s look at the overall means for the sitting and standing conditions and see what this is all about:\n\nposture_means &lt;- stroop_df %&gt;%\n                      group_by(Posture) %&gt;%\n                      summarise(mean_rt = mean(RTs),\n                                sd = sd(RTs),\n                                SEM = sd(RTs)/sqrt(length(RTs)))\nknitr::kable(posture_means)\n\n\n\n\nPosture\nmean_rt\nsd\nSEM\n\n\n\n\nSit\n881.3544\n135.3842\n13.53842\n\n\nStand\n855.9365\n116.9436\n11.69436\n\n\n\n\n\nRemember, the posture main effect collapses over the means in the congruency condition. We are not measuring a Stroop effect here. We are measuring a general effect of sitting vs standing on overall reaction time. The table shows that people were a little faster overall when they were standing, compared to when they were sitting.\nAgain, the main effect of posture was not the primary effect of interest. The authors weren’t interested if people are in general faster when they stand. They wanted to know if their selective attention would improve when they stand vs when they sit. They were most interested in whether the size of the Stroop effect (difference between incongruent and congruent performance) would be smaller when people stand, compared to when they sit. To answer this question, we need to look at the interaction effect.\n\n\n9.8.6 Congruency X Posture Interaction\nLast, there was a significant congruency X posture interaction, \\(F\\)(1,49) = 8.96, \\(p\\) = 0.004.\nWith this information, and by looking at the figure, we can get a pretty good idea of what this means. We know the size of the Stroop effect must have been different between the standing and sitting conditions, otherwise we would have gotten a smaller \\(F\\)-value and a much larger \\(p\\)-value.\nWe can see from the figure the direction of this difference, but let’s look at the table to see the numbers more clearly.\n\nint_means &lt;- stroop_df %&gt;%\n                      group_by(Posture, Congruency) %&gt;%\n                      summarise(mean_rt = mean(RTs),\n                                sd = sd(RTs),\n                                SEM = sd(RTs)/sqrt(length(RTs)))\n\n`summarise()` has grouped output by 'Posture'. You can override using the\n`.groups` argument.\n\nknitr::kable(int_means)\n\n\n\n\nPosture\nCongruency\nmean_rt\nsd\nSEM\n\n\n\n\nSit\nCongruent\n821.9232\n117.4069\n16.60384\n\n\nSit\nIncongruent\n940.7855\n126.6457\n17.91041\n\n\nStand\nCongruent\n807.9599\n105.6079\n14.93521\n\n\nStand\nIncongruent\n903.9131\n108.5366\n15.34939\n\n\n\n\n\nIn the sitting condition the Stroop effect was roughly 941-822 = 119 ms.\nIn the standing condition the Stroop effect was roughly 904-808 = 96 ms.\nSo, the Stroop effect was 119-96 = 23 ms smaller when people were standing. This is a pretty small effect in terms of the amount of time reduced, but even though it is small, a difference even this big was not very likely to be due to chance.\n\n\n9.8.7 What does it all mean?\nBased on this research there appears to be some support for the following logic chain. First, the researchers can say that standing up reduces the size of a person’s Stroop effect. Fine, what could that mean? Well, if the Stroop effect is an index of selective attention, then it could mean that standing up is one way to improve your ability to selectively focus and ignore distracting information. The actual size of the benefit is fairly small, so the real-world implications are not that clear. Nevertheless, maybe the next time you lose your keys, you should stand up and look for them, rather than sitting down and not look for them."
  },
  {
    "objectID": "09-FactorialANOVA.html#factorial-summary",
    "href": "09-FactorialANOVA.html#factorial-summary",
    "title": "9  Factorial ANOVA",
    "section": "9.9 Factorial summary",
    "text": "9.9 Factorial summary\nWe have introduced you to factorial designs, which are simply designs with more than one IV. The special property of factorial designs is that all of the levels of each IV need to be crossed with the other IVs.\nWe showed you how to analyse a repeated measures 2x2 design with paired samples-tests, and what an ANOVA table would look like if you did this in R. We also went through, by hand, the task of calculating an ANOVA table for a 2x2 between subjects design.\nThe main point we want you take away is that factorial designs are extremely useful for determining things that cause effects to change. Generally a researcher measures an effect of interest (their IV 1). Then, they want to know what makes that effect get bigger or smaller. They want to exert experimental control over their effect. For example, they might have a theory that says doing X should make the effect bigger, but doing Y should make it smaller. They can test these theories using factorial designs, and manipulating X or Y as a second independent variable.\nIn a factorial design each IV will have it’s own main effect. Sometimes the main effect themselves are what the researcher is interested in measures. But more often, it is the interaction effect that is most relevant. The interaction can test whether the effect of IV1 changes between the levels of IV2. When it does, researchers can infer that their second manipulation (IV2) causes change in their effect of interest. These changes are then documented and used to test underlying causal theories about the effects of interest.\n\n\n\n\n\nRosenbaum, David, Yaniv Mama, and Daniel Algom. 2017. “Stand by Your Stroop: Standing up Enhances Selective Attention and Cognitive Control.” Psychological Science 28 (12): 1864–67. https://doi.org/10.1177/0956797617721270."
  },
  {
    "objectID": "10-MixedANOVA.html#looking-at-main-effects-and-interactions",
    "href": "10-MixedANOVA.html#looking-at-main-effects-and-interactions",
    "title": "10  More On Factorial Designs",
    "section": "10.1 Looking at main effects and interactions",
    "text": "10.1 Looking at main effects and interactions\nDesigns with multiple factors are very common. When you read a research article you will often see graphs that show the results from designs with multiple factors. It would be good for you if you were comfortable interpreting the meaning of those results. The skill here is to be able to look at a graph and see the pattern of main effects and interactions. This skill is important, because the patterns in the data can quickly become very complicated looking, especially when there are more than two independent variables, with more than two levels.\n\n10.1.1 2x2 designs\nLet’s take the case of 2x2 designs. There will always be the possibility of two main effects and one interaction. You will always be able to compare the means for each main effect and interaction. If the appropriate means are different then there is a main effect or interaction. Here’s the thing, there a bunch of ways all of this can turn out. Check out the ways, there are 8 of them:\n\nno IV1 main effect, no IV2 main effect, no interaction\nIV1 main effect, no IV2 main effect, no interaction\nIV1 main effect, no IV2 main effect, interaction\nIV1 main effect, IV2 main effect, no interaction\nIV1 main effect, IV2 main effect, interaction\nno IV1 main effect, IV2 main effect, no interaction\nno IV1 main effect, IV2 main effect, interaction\nno IV1 main effect, no IV2 main effect, interaction\n\nOK, so if you run a 2x2, any of these 8 general patterns could occur in your data. That’s a lot to keep track of isn’t. As you develop your skills in examining graphs that plot means, you should be able to look at the graph and visually guesstimate if there is, or is not, a main effect or interaction. You will need you inferential statistics to tell you for sure, but it is worth knowing how to know see the patterns.\nFigure 10.1 shows the possible patterns of main effects and interactions in bar graph form. Here is a legend for the labels in the panels.\n\n1 = there was a main effect for IV1.\n~1 = there was not a main effect for IV1\n2 = there was a main effect for IV2\n~2 = there was not a main effect of IV2\n1x2 = there was an interaction\n~1x2 = there was not an interaction\n\n\np1 &lt;- data.frame(\n  IV1 = c(\"A\", \"A\", \"B\", \"B\"),\n  IV2 = c(\"1\", \"2\", \"1\", \"2\"),\n  means = c(5, 5, 5, 5)\n)\n\np2 &lt;- data.frame(\n  IV1 = c(\"A\", \"A\", \"B\", \"B\"),\n  IV2 = c(\"1\", \"2\", \"1\", \"2\"),\n  means = c(10, 10, 5, 5)\n)\n\np3 &lt;- data.frame(\n  IV1 = c(\"A\", \"A\", \"B\", \"B\"),\n  IV2 = c(\"1\", \"2\", \"1\", \"2\"),\n  means = c(10, 13, 5, 2)\n)\n\np4 &lt;- data.frame(\n  IV1 = c(\"A\", \"A\", \"B\", \"B\"),\n  IV2 = c(\"1\", \"2\", \"1\", \"2\"),\n  means = c(5, 10, 10, 15)\n)\n\np5 &lt;- data.frame(\n  IV1 = c(\"A\", \"A\", \"B\", \"B\"),\n  IV2 = c(\"1\", \"2\", \"1\", \"2\"),\n  means = c(10, 18, 5, 7)\n)\n\np6 &lt;- data.frame(\n  IV1 = c(\"A\", \"A\", \"B\", \"B\"),\n  IV2 = c(\"1\", \"2\", \"1\", \"2\"),\n  means = c(5, 10, 5, 10)\n)\n\np7 &lt;- data.frame(\n  IV1 = c(\"A\", \"A\", \"B\", \"B\"),\n  IV2 = c(\"1\", \"2\", \"1\", \"2\"),\n  means = c(2, 12, 5, 9)\n)\n\np8 &lt;- data.frame(\n  IV1 = c(\"A\", \"A\", \"B\", \"B\"),\n  IV2 = c(\"1\", \"2\", \"1\", \"2\"),\n  means = c(5, 10, 10, 5)\n)\n\nall_22s &lt;- rbind(p1, p2, p3, p4, p5, p6, p7, p8)\n\ntype &lt;- c(\n  rep(\"~1, ~2, ~1x2\", 4),\n  rep(\"1, ~2, ~1x2\", 4),\n  rep(\"1, ~2, 1x2\", 4),\n  rep(\"1, 2, ~1x2\", 4),\n  rep(\"1, 2, 1x2\", 4),\n  rep(\"~1, 2, ~1x2\", 4),\n  rep(\"~1, 2, 1x2\", 4),\n  rep(\"~1, ~2, 1x2\", 4)\n)\n\ntype &lt;- as.factor(type)\n\nall_22s &lt;- cbind(all_22s, type)\n\nggplot(all_22s, aes(\n  x = IV1,\n  y = means,\n  group = IV2,\n  fill = IV2\n)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  theme_classic() +\n  facet_wrap( ~ type, nrow = 2) +\n  theme(legend.position = \"top\")\n\n\n\n\nFigure 10.1: 8 Example patterns for means for each of the possible kinds of general outcomes in a 2x2 design.\n\n\n\n\nFigure 10.2 shows the same eight patterns in line graph form:\n\nggplot(all_22s, aes(x=IV1, y=means, group=IV2, color=IV2))+\n  geom_point()+\n  geom_line()+\n  theme_classic()+\n  facet_wrap(~type, nrow=2)+\n  theme(legend.position = \"top\")\n\n\n\n\nFigure 10.2: Line graphs showing 8 possible general outcomes for a 2x2 design.\n\n\n\n\nThe line graphs accentuates the presence of interaction effects. Whenever the lines cross, or would cross if they kept going, you have a possibility of an interaction. Whenever the lines are parallel, there can’t be an interaction. When both of the points on the A side are higher or lower than both of the points on the B side, then you have a main effect for IV1 (A vs B). Whenever the green line is above or below the red line, then you have a main effect for IV2 (1 vs. 2). We know this is complicated. You should see what all the possibilities look like when we start adding more levels or more IVs. It gets nuts. Because of this nuttiness, it is often good practice to make your research designs simple (as few IVs and levels as possible to test your question). That way it will be easier to interpret your data. Whenever you see that someone ran a 4x3x7x2 design, your head should spin. It’s just too complicated."
  },
  {
    "objectID": "10-MixedANOVA.html#interpreting-main-effects-and-interactions",
    "href": "10-MixedANOVA.html#interpreting-main-effects-and-interactions",
    "title": "10  More On Factorial Designs",
    "section": "10.2 Interpreting main effects and interactions",
    "text": "10.2 Interpreting main effects and interactions\nThe interpretation of main effects and interactions can get tricky. Consider the concept of a main effect. This is the idea that a particular IV has a consistent effect. For example, drinking 5 cups of coffee makes you more awake compared to not drinking 5 cups of coffee. The main effect of drinking 5 cups of coffee vs not drinking coffee will generally be true across the levels of other IVs in our life. For example, let’s say you conducted an experiment testing whether the effect of drinking 5 cups of coffee vs not, changes depending on whether you are in your house or in a car. Perhaps the situation matters? No, probably not so much. You will probably still be more awake in your house, or your car, after having 5 cups of coffee, compared to if you hadn’t.\nThe coffee example is a reasonably good example of a consistent main effect. Another silly kind of example might be the main effect of shoes on your height. For example, if your IV was wearing shoes or not, and your DV was height, then we could expect to find a main effect of wearing shoes on your measurement of height. When you wear shoes, you will become taller compared to when you don’t wear shoes. Wearing shoes adds to your total height. In fact, it’s hard to imagine how the effect of wearing shoes on your total height would ever interact with other kinds of variables. You will be always be that extra bit taller wearing shoes. Indeed, if there was another manipulation that could cause an interaction that would truly be strange. For example, imagine if the effect of being inside a bodega or outside a bodega interacted with the effect of wearing shoes on your height. That could mean that shoes make you taller when you are outside a bodega, but when you step inside, your shoes make you shorter…but, obviously this is just totally ridiculous. That’s correct, it is often ridiculous to expect that one IV will have an influence on the effect of another, especially when there is no good reason.\nThe summary here is that it is convenient to think of main effects as a consistent influence of one manipulation. However, when an interaction is observed, this messes up the consistency of the main effect. That is the very definition of an interaction. It means that some main effect is not behaving consistently across different situations. Indeed, whenever we find an interaction, sometimes we can question whether or not there really is a general consistent effect of some manipulation, or instead whether that effect only happens in specific situations.\nFor this reason, you will often see that researchers report their findings this way:\n“We found a main effect of X, BUT, this main effect was qualified by an interaction between X and Y”.\nNotice the big BUT. Why is it there? The sentence points out that before they talk about the main effect, they need to first talk about the interaction, which is making the main effect behave inconsistently. In other words, the interpretation of the main effect depends on the interaction, the two things have to be thought of together to make sense of them.\nHere are two examples to help you make sense of these issues:\n\n10.2.1 A consistent main effect and an interaction\n\np7 &lt;- data.frame(\n  IV1 = c(\"A\", \"A\", \"B\", \"B\"),\n  IV2 = c(\"1\", \"2\", \"1\", \"2\"),\n  means = c(2, 12, 5, 9)\n)\n\nggplot(p7, aes(\n  x = IV1,\n  y = means,\n  group = IV2,\n  color = IV2\n)) +\n  geom_point() +\n  geom_line() +\n  theme_classic() +\n  theme(legend.position = \"top\")\n\n\n\n\nFigure 10.3: Example means showing a generally consistent main effect along with an interaction\n\n\n\n\nFigure 10.3 shows a main effect and interaction. There is a main effect of IV2: the level 1 means (red points and line) are both lower than the level 2 means (aqua points and line). There is also an interaction. The size of the difference between the red and aqua points in the A condition (left) is bigger than the size of the difference in the B condition.\nHow would we interpret this? We could say there WAS a main effect of IV2, BUT it was qualified by an IV1 x IV2 interaction.\nWhat’s the qualification? The size of the IV2 effect changed as a function of the levels of IV1. It was big for level A, and small for level B of IV1.\nWhat does the qualification mean for the main effect? Well, first it means the main effect can be changed by the other IV. That’s important to know. Does it also mean that the main effect is not a real main effect because there was an interaction? Not really, there is a generally consistent effect of IV2. The green points are above the red points in all cases. Whatever IV2 is doing, it seems to work in at least a couple situations, even if the other IV also causes some change to the influence.\n\n\n10.2.2 An inconsistent main effect and an interaction\n\np7 &lt;- data.frame(\n  IV1 = c(\"A\", \"A\", \"B\", \"B\"),\n  IV2 = c(\"1\", \"2\", \"1\", \"2\"),\n  means = c(5, 10, 5, 5)\n)\n\nggplot(p7, aes(\n  x = IV1,\n  y = means,\n  group = IV2,\n  color = IV2\n)) +\n  geom_point() +\n  geom_line() +\n  theme_classic() +\n  theme(legend.position = \"top\")\n\n\n\n\nFigure 10.4: Example data showing how an interaction exists, and a main effect does not, even though the means for the main effect may show a difference\n\n\n\n\nFigure 10.4 shows another 2x2 design. You should see an interaction here straight away. The difference between the aqua and red points in condition A (left two dots) is huge, and there is 0 difference between them in condition B. Is there an interaction? Yes!\nAre there any main effects here? With data like this, sometimes an ANOVA will suggest that you do have significant main effects. For example, what is the mean difference between level 1 and 2 of IV2? That is the average of the green points ( (10+5)/2 = 15/2= 7.5 ) compared to the average of the red points (5). There will be a difference of 2.5 for the main effect (7.5 vs. 5).\nStarting to see the issue here? From the perspective of the main effect (which collapses over everything and ignores the interaction), there is an overall effect of 2.5. In other words, level 2 adds 2.5 in general compared to level 1. However, we can see from the graph that IV2 does not do anything in general. It does not add 2.5s everywhere. It adds 5 in condition A, and nothing in condition B. It only does one thing in one condition.\nWhat is happening here is that a “main effect” is produced by the process of averaging over a clear interaction.\nHow would we interpret this? We might have to say there was a main effect of IV2, BUT we would definitely say it was qualified by an IV1 x IV2 interaction.\nWhat’s the qualification? The size of the IV2 effect completely changes as a function of the levels of IV1. It was big for level A, and nonexistent for level B of IV1.\nWhat does the qualification mean for the main effect? In this case, we might doubt whether there is a main effect of IV2 at all. It could turn out that IV2 does not have a general influence over the DV all of the time, it may only do something in very specific circumstances, in combination with the presence of other factors."
  },
  {
    "objectID": "10-MixedANOVA.html#mixed-designs",
    "href": "10-MixedANOVA.html#mixed-designs",
    "title": "10  More On Factorial Designs",
    "section": "10.3 Mixed Designs",
    "text": "10.3 Mixed Designs\nThroughout this book we keep reminding you that research designs can take different forms. The manipulations can be between-subjects (different subjects in each group), or within-subjects (everybody contributes data in all conditions). If you have more than one manipulation, you can have a mixed design when one of your IVs is between-subjects and one of the other ones is within-subjects.\nThe only “trick” to these designs is to use the appropriate error terms to construct the F-values for each effect. Effects that have a within-subjects repeated measure (IV) use different error terms than effects that only have a between-subject IV. In principle, you could run an ANOVA with any number of IVs, and any of them good be between or within-subjects variables.\nBecause this is an introductory textbook, we leave out a full discussion on mixed designs. What we are leaving out are the formulas to construct ANOVA tables that show how to use the correct error terms for each effect. There are many good more advanced textbooks that discuss these issues in much more depth. And, these things can all be Googled. This is a bit of a cop-out on our part, and we may return to fill in this section at some point in the future (or perhaps someone else will add a chapter about this).\nIn the lab manual, you will learn how to conduct a mixed design ANOVA using software. Generally speaking, the software takes care of the problem of using the correct error terms to construct the ANOVA table."
  },
  {
    "objectID": "10-MixedANOVA.html#more-complicated-designs",
    "href": "10-MixedANOVA.html#more-complicated-designs",
    "title": "10  More On Factorial Designs",
    "section": "10.4 More complicated designs",
    "text": "10.4 More complicated designs\nUp until now we have focused on the simplest case for factorial designs, the 2x2 design, with two IVs, each with 2 levels. It is worth spending some time looking at a few more complicated designs and how to interpret them.\n\n10.4.1 2x3 design\nIn a 2x3 design there are two IVs. IV1 has two levels, and IV2 has three levels. Typically, there would be one DV. Let’s talk about the main effects and interaction for this design.\nFirst, let’s make the design concrete. Let’s imagine we are running a memory experiment. We give people some words to remember, and then test them to see how many they can correctly remember. Our DV is proportion correct. We know that people forget things over time. Our first IV will be time of test, immediate vs. 1 week. The time of test IV will produce a forgetting effect. Generally, people will have a higher proportion correct on an immediate test of their memory for things they just saw, compared to testing a week later.\nWe might be interested in manipulations that reduce the amount of forgetting that happens over the week. The second IV could be many things. Let’s make it the number of time people got to study the items before the memory test, once, twice or three times. We call IV2 the repetition manipulation.\nWe might expect data like shown in Figure 10.5:\n\nproportion_correct &lt;- c(.9, .6, .95, .7, .99, .8)\ndelay &lt;- rep(c(\"Immediate\", \"One week\"), 3)\nrepetition &lt;- as.factor(rep(c(1, 2, 3), each = 2))\ndf &lt;- data.frame(proportion_correct, delay, repetition)\n\nggplot(df,\n       aes(\n         x = repetition,\n         y = proportion_correct,\n         color = delay,\n         group = delay\n       )) +\n  geom_point() +\n  geom_line() +\n  theme_classic()\n\n\n\n\nFigure 10.5: Example means for a 2x3 factorial design\n\n\n\n\nThe figure shows some pretend means in all conditions. Let’s talk about the main effects and interaction.\nFirst, the main effect of delay (time of test) is very obvious, the red line is way above the aqua line. Proportion correct on the memory test is always higher when the memory test is taken immediately compared to after one week.\nSecond, the main effect of repetition seems to be clearly present. The more times people saw the items in the memory test (once, twice, or three times), the more they remembered, as measured by increasingly higher proportion correct as a function of number of repetitions.\nIs there an interaction? Yes, there is. Remember, an interaction occurs when the effect of one IV depends on the levels of an another. The delay IV measures the forgetting effect. Does the size of the forgetting effect change across the levels of the repetition variable? Yes it does. With one repetition the forgetting effect is .9-.6 =.4. With two repetitions, the forgetting effect is a little bit smaller, and with three, the repetition is even smaller still. So, the size of the forgetting effect changes as a function of the levels of the repetition IV. There is evidence in the means for an interaction. You would have to conduct an inferential test on the interaction term to see if these differences were likely or unlikely to be due to sampling error.\nIf there was no interaction and no main effect of repetition, we would see something like the pattern in Figure 10.6.\n\nproportion_correct &lt;- c(.9, .6, .9, .6, .9, .6)\ndelay &lt;- rep(c(\"Immediate\", \"One week\"), 3)\nrepetition &lt;- as.factor(rep(c(1, 2, 3), each = 2))\ndf &lt;- data.frame(proportion_correct, delay, repetition)\n\nggplot(df,\n       aes(\n         x = repetition,\n         y = proportion_correct,\n         color = delay,\n         group = delay\n       )) +\n  geom_point() +\n  geom_line() +\n  theme_classic()\n\n\n\n\nFigure 10.6: Example means for a 2x3 design when there is only one main effect\n\n\n\n\nWhat would you say about the interaction if you saw the pattern in Figure 10.7?\n\nproportion_correct &lt;- c(.9, .6, .9, .6, .9, .8)\ndelay &lt;- rep(c(\"Immediate\", \"One week\"), 3)\nrepetition &lt;- as.factor(rep(c(1, 2, 3), each = 2))\ndf &lt;- data.frame(proportion_correct, delay, repetition)\n\nggplot(df,\n       aes(\n         x = repetition,\n         y = proportion_correct,\n         color = delay,\n         group = delay\n       )) +\n  geom_point() +\n  geom_line() +\n  theme_classic()\n\n\n\n\nFigure 10.7: Example means for a 2x3 design showing another pattern that produces an interaction\n\n\n\n\nThe correct answer is that there is evidence in the means for an interaction. Remember, we are measuring the forgetting effect (effect of delay) three times. The forgetting effect is the same for repetition condition 1 and 2, but it is much smaller for repetition condition 3. The size of the forgetting effect depends on the levels of the repetition IV, so here again there is an interaction.\n\n\n10.4.2 2x2x2 designs\nLet’s take it up a notch and look at a 2x2x2 design. Here, there are three IVs with 2 levels each. There are three main effects, three two-way (2x2) interactions, and one 3-way (2x2x2) interaction.\nWe will use the same example as before but add an additional manipulation of the kind of material that is to be remembered. For example, we could present words during an encoding phase either visually or spoken (auditory) over headphones.\n\nproportion_correct &lt;- c(.9, .6, .9, .8,\n                        .9, .6, .9, .8)\ndelay &lt;- as.factor(rep(c(\"Immediate\", \"One week\"), 4))\nrepetition &lt;- as.factor(rep(rep(c(1, 2), each = 2), 2))\nmodality &lt;- as.factor(rep(c(\"visual\", \"auditory\"), each = 4))\ndf &lt;- data.frame(proportion_correct, delay, repetition, modality)\n\nggplot(df,\n       aes(\n         x = repetition,\n         y = proportion_correct,\n         color = delay,\n         group = delay\n       )) +\n  geom_point() +\n  geom_line() +\n  theme_classic() +\n  facet_wrap( ~ modality)\n\n\n\n\nFigure 10.8: Example means from a 2x2x2 design with no three-way interaction.\n\n\n\n\nFigure 10.8 has two panels one for auditory and one for visual. You can think of the 2x2x2, as two 2x2s, one for auditory and one for visual. What’s the take home from this example data? We can see that the graphs for auditory and visual are the same. They both show a 2x2 interaction between delay and repetition. People forgot more things across the week when they studied the material once, compared to when they studied the material twice. There is a main effect of delay, there is a main effect of repetition, there is no main effect of modality, and there is no three-way interaction.\nWhat is a three-way interaction anyway? That would occur if there was a difference between the 2x2 interactions. For example, consider the pattern of results in Figure 10.9.\n\nproportion_correct &lt;- c(.9, .6, .9, .8,\n                        .9, .8, .9, .5)\ndelay &lt;- as.factor(rep(c(\"Immediate\", \"One week\"), 4))\nrepetition &lt;- as.factor(rep(rep(c(1, 2), each = 2), 2))\nmodality &lt;- as.factor(rep(c(\"visual\", \"auditory\"), each = 4))\ndf &lt;- data.frame(proportion_correct, delay, repetition, modality)\n\nggplot(df,\n       aes(\n         x = repetition,\n         y = proportion_correct,\n         color = delay,\n         group = delay\n       )) +\n  geom_point() +\n  geom_line() +\n  theme_classic() +\n  facet_wrap( ~ modality)\n\n\n\n\nFigure 10.9: Example means from a 2x2x2 design with a three-way interaction.\n\n\n\n\nWe are looking at a 3-way interaction between modality, repetition and delay. What is going on here? These results would be very strange, here is an interpretation.\nFor auditory stimuli, we see that there is a small forgetting effect when people studied things once, but the forgetting effect gets bigger if they studies things twice. A pattern like this would generally be very strange, usually people would do better if they got to review the material twice.\nThe visual stimuli show a different pattern. Here, the forgetting effect is large when studying visual things once, and it get’s smaller when studying visual things twice.\nWe see that there is an interaction between delay (the forgetting effect) and repetition for the auditory stimuli; BUT, this interaction effect is different from the interaction effect we see for the visual stimuli. The 2x2 interaction for the auditory stimuli is different from the 2x2 interaction for the visual stimuli. In other words, there is an interaction between the two interactions, as a result there is a three-way interaction, called a 2x2x2 interaction.\nWe will note a general pattern here. Imagine you had a 2x2x2x2 design. That would have a 4-way interaction. What would that mean? It would mean that the pattern of the 2x2x2 interaction changes across the levels of the 4th IV. If two three-way interactions are different, then there is a four-way interaction."
  },
  {
    "objectID": "11-Simulation.html#reasons-to-simulate",
    "href": "11-Simulation.html#reasons-to-simulate",
    "title": "11  Simulating Data",
    "section": "11.1 Reasons to simulate",
    "text": "11.1 Reasons to simulate\nThere are many good reasons to learn simulation techniques, here are some:\n\nYou force yourself to consider the details of your design, how many subjects, how many conditions, how many observations per condition per subject, and how you will store and represent the data to describe all of these details when you run the experiment\nYou force yourself to consider the kinds of numbers you will be collecting. Specifically, the distributional properties of those numbers. You will have to make decisions about the distributions that you sample from in your simulation, and thinking about this issue helps you better understand your own data when you get it.\nYou learn a bit of computer programming, and this is a very useful general skill that you can build upon to do many things.\nYou can make reasonable and informed assumptions about how your experiment might turn out, and then use the results of your simulation to choose parameters for your design (such as number of subjects, number of observations per condition and subject) that will improve the sensitivity of your design to detect the effects you are interested in measuring.\nYou can even run simulations on the data that you collect to learn more about how it behaves, and to do other kinds of advanced statistics that we don’t discuss in this book.\nYou get to improve your intuitions about how data behaves when you measure it. You can test your intuitions by running simulations, and you can learn things you didn’t know to begin with. Simulations can be highly informative.\nWhen you simulate data in advance of collecting real data, you can work out exactly what kinds of tests you are planning to perform, and you will have already written your analysis code, so it will be ready and waiting for you as soon as you collect the data\n\nOK, so that’s just a few reasons why simulations are useful."
  },
  {
    "objectID": "11-Simulation.html#simulation-overview",
    "href": "11-Simulation.html#simulation-overview",
    "title": "11  Simulating Data",
    "section": "11.2 Simulation overview",
    "text": "11.2 Simulation overview\nThe basic idea here is actually pretty simple. You make some assumptions about how many subjects will be in your design (set N), you make some assumptions about the distributions that you will be sampling your scores from, then you use R to fabricate fake data according to the parameters you set. Once you build some simulated data, you can conduct a statistical analysis that you would be planning to run on the real data. Then you can see what happens. More importantly, you can repeat the above process many times. This is similar to conducting a replication of your experiment to see if you find the same thing, only you make the computer replicate your simulation 1000s of times. This way you can see how your simulated experiment would turn out over the long run. For example, you might find that the experiment you are planning to run will only produce a “signficant” result 25% of the time, that’s not very good. Your simulation might also tell you that if you increase your N by say 25, that could really help, and your new experiment with N=25 might succeed 90% of the time. That’s information worth knowing.\nBefore we go into more simulation details, let’s just run a quick one. We’ll do an independent samples \\(t\\)-test. Imagine we have a study with N=10 in each group. There are two groups. We are measuring heart rate. Let’s say we know that heart rate is on average 100 beats per minute with a standard deviation of 7. We are going to measure heart rate in condition A where nothing happens, and we are going to measure heart rate in condition B while they watch a scary movie. We think the scary movie might increase heart rate by 5 beats per minute. Let’s run a simulation of this:\n\ngroup_A &lt;- rnorm(10,100,7)\ngroup_B &lt;- rnorm(10,105, 7)\nt.test(group_A,group_B,var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  group_A and group_B\nt = -3.0362, df = 18, p-value = 0.007103\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -15.754445  -2.868281\nsample estimates:\nmean of x mean of y \n 97.10206 106.41342 \n\n\nWe sampled 10 scores from a normal distribution for each group. We changed the mean for group_b to 105, because we were thinking their heart rate would be 5 more than group A. We ran one \\(t\\)-test, and we got a result. This result tells us what happens for this one simulation.\nWe could learn more by repeating the simulation 1000 times, saving the \\(p\\)-values from each replication, and then finding out how many of our 1000 simulated experiments give us a significant result:\n\nsave_ps&lt;-length(1000)\nfor(i in 1:1000){\n  group_A &lt;- rnorm(10,100,7)\n  group_B &lt;- rnorm(10,105, 7)\n  t_results &lt;- t.test(group_A,group_B,var.equal = TRUE)\n  save_ps[i] &lt;- t_results$p.value\n}\n\nprop_p&lt;-length(save_ps[save_ps&lt;0.05])/1000\nprint(prop_p)\n\n[1] 0.333\n\n\nNow this is more interesting. We found that 33.3% of simulated experiments had a \\(p\\)-value less than 0.05. That’s not very good. If you were going to collect data in this kind of experiment, and you made the correct assumptions about the mean and standard deviation of the distribution, and you made the correct assumption about the size of difference between the groups, you would be planning to run an experiment that would not work-out most of the time.\nWhat happens if we increase the number of subject to 50 in each group?\n\nsave_ps&lt;-length(1000)\nfor(i in 1:1000){\n  group_A &lt;- rnorm(50,100,7)\n  group_B &lt;- rnorm(50,105, 7)\n  t_results &lt;- t.test(group_A,group_B,var.equal = TRUE)\n  save_ps[i] &lt;- t_results$p.value\n}\n\nprop_p&lt;-length(save_ps[save_ps&lt;0.05])/1000\nprint(prop_p)\n\n[1] 0.955\n\n\nOoh, look, almost all of the experiments are significant now. So, it would be better to use 50 subjects per group than 10 per group according to this simulation.\nOf course, you might already be wondering so many different kinds of things. How can we plausibly know the parameters for the distribution we are sampling from? Isn’t this all just guess work? We’ll discuss some of these issues as we move forward in this chapter."
  },
  {
    "objectID": "11-Simulation.html#simulating-t-tests",
    "href": "11-Simulation.html#simulating-t-tests",
    "title": "11  Simulating Data",
    "section": "11.3 Simulating t-tests",
    "text": "11.3 Simulating t-tests\nWe’ve already seen some code for simulating a \\(t\\)-test 1000 times, saving the \\(p\\)-values, and then calculating the proportion of simulations that are significant (p&lt;0.05). It looked like this:\n\nsave_ps&lt;-length(1000)\nfor(i in 1:1000){\n  group_A &lt;- rnorm(50,100,7)\n  group_B &lt;- rnorm(50,105, 7)\n  t_results &lt;- t.test(group_A,group_B,var.equal = TRUE)\n  save_ps[i] &lt;- t_results$p.value\n}\n\nprop_p&lt;-length(save_ps[save_ps&lt;0.05])/1000\nprint(prop_p)\n\n[1] 0.947\n\n\nYou could play around with that, and it would be very useful I think. Is there anything else that we can do that would be more useful? Sure there is. With the above simulation, you have to change N or the mean difference each time to see how proportion of significant experiments turns out. It would be nice to look at a graph where we could vary the number of subjects, and the size of the mean difference. That’s what the next simulation does. This kind of simulation can make your computer do some hard work depening on how many simulations you run. To make my computer do less work, we will only run 100 simulations for each parameter. But, what we will do is vary the number of subjects from 10 to 50 (steps of 10), and vary the size of the effect from 0 to 20 in steps of 4.\n\nnum_sims        &lt;-500\nN               &lt;-c(10,20,30,40,50)\nmean_difference &lt;-c(0,4,8,12,16,20)\nsave_ps&lt;-length(num_sims)\n\nall_df&lt;-data.frame()\nfor(diff in mean_difference){\n  for (j in N){\n    for(i in 1:num_sims){\n      group_A &lt;- rnorm(j,100,7)\n      group_B &lt;- rnorm(j,100+diff, 7)\n      t_results &lt;- t.test(group_A,group_B,var.equal = TRUE)\n      save_ps[i] &lt;- t_results$p.value\n    }\n    sim_df &lt;-data.frame(save_ps,\n                        num_subjects=as.factor(rep(j,num_sims)),\n                        mean_diff =rep(diff,num_sims))\n    all_df &lt;- rbind(all_df,sim_df)\n  }\n}  \n\nplot_df &lt;- all_df %&gt;%\n            dplyr::group_by(num_subjects,mean_diff) %&gt;%\n            dplyr::summarise(proportion_sig = length(save_ps[save_ps&lt;0.05])/num_sims)\n\n`summarise()` has grouped output by 'num_subjects'. You can override using the\n`.groups` argument.\n\nggplot(plot_df, aes(x=mean_diff, \n                    y=proportion_sig, \n                    group=num_subjects, \n                    color=num_subjects))+\n  geom_point()+\n  geom_line()+\n  theme_classic()\n\n\n\n\nFigure 11.1: A simulation showing the proportion of significant \\(t\\) tests as a function of the programmed mean difference and number of subjects.\n\n\n\n\nA graph like Figure 11.1 is very helpful to look at. Generally, before we run an experiment we might not have a very good idea of the size of the effect that our manipulation might cause. Will it be a mean difference of 0 (no effect), or 5, or 10, or 20? If you are doing something new it’s hard to know. You would know in general that bigger effects are easier to detect. You would be able to detect smaller and smaller effects if you ran more and more subjects. When you run this kind of simulation, you can vary the possible mean differences and the number of the subjects at the same time and then see what happens.\nWhen the mean difference is 0, we should get an average of 5%, or (0.05 proportion) experiments being significant. This is what we expect by chance, and it doesn’t matter how many subjects we run. When there is no difference, we will reject the null 5% of the time (these would all be type 1 errors).\nHow about when there is a difference of 4? This a pretty small effect. If we only run 10 subjects in each group, we can see that less than 25% of simulated experiments would show significant results. If we wanted a higher chance of success to measure an effect of this size, then we should go up to 40-50 subjects, that would get us around 75% success rates. If that’s not good enough for you (25% failures remember, that’s still alot), then re-run the simulation with even more subjects.\nAnother thing worth pointing out is that if the mean difference is bigger than about 12.5, you can see that all of the designs produce significant outcomes nearly 100% of the time. If you knew this, perhaps you would simply run 10-20 subjects in your experiment, rather than 50. After all, 10-20 is just fine for detecting the effect, and 50 subjects might be a waste of resources (both yours and your participants)."
  },
  {
    "objectID": "11-Simulation.html#simulating-one-factor-anovas",
    "href": "11-Simulation.html#simulating-one-factor-anovas",
    "title": "11  Simulating Data",
    "section": "11.4 Simulating one-factor ANOVAs",
    "text": "11.4 Simulating one-factor ANOVAs\nThe following builds simulated data for a one-factor ANOVA, appropriate for a between subjects design. We build the data frame containg a column for the group factor levels, and a column for the DV. Then, we run the ANOVA an print it out.\n\nN &lt;- 10\ngroups &lt;- rep(c(\"A\",\"B\",\"C\"), each=10)\nDV &lt;- c(rnorm(100,10,15),   # means for group A\n        rnorm(100,10,15),   # means for group B\n        rnorm(100,20,15)    # means for group C\n        )\nsim_df&lt;-data.frame(groups,DV)\n\naov_results &lt;- summary(aov(DV~groups, sim_df))\n\nlibrary(xtable)\nknitr::kable(xtable(aov_results))\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\ngroups\n2\n1266.487\n633.2434\n2.372723\n0.0949916\n\n\nResiduals\n297\n79264.755\n266.8847\nNA\nNA\n\n\n\n\n\nIn this next example, we simulate the same design 100 times, save the \\(p\\)-values, and the determine the proportion of significant simulations.\n\nN &lt;- 10\n\nsave_p&lt;-length(100)\nfor(i in 1:100){\n  groups &lt;- rep(c(\"A\",\"B\",\"C\"), each=10)\n  DV &lt;- c(rnorm(100,10,15),   # means for group A\n          rnorm(100,10,15),   # means for group B\n          rnorm(100,20,15)    # means for group C\n          )\n  sim_df&lt;-data.frame(groups,DV)\n  \n  aov_results &lt;- summary(aov(DV~groups, sim_df))\n  save_p[i]&lt;-aov_results[[1]]$`Pr(&gt;F)`[1]\n}\n\nlength(save_p[save_p&lt;0.05])/100\n\n[1] 0.04"
  },
  {
    "objectID": "11-Simulation.html#other-resources",
    "href": "11-Simulation.html#other-resources",
    "title": "11  Simulating Data",
    "section": "11.5 Other resources",
    "text": "11.5 Other resources\nOK, It’s a tuesday, the summer is almost over. I’ve spent most of this summer (2018) writing this textbook, because we are using it this Fall 2018. Because I am running out of time, I need to finish this and make sure everything is in place for the course to work. As a result, I am not going to finish this chapter right now. The nice thing about this book, is that I (and other people) can fill things in over time. We have shown a few examples of data-simulation, so that’s at least something.\nIf you want to see more examples, I suggest you check out this chapter:\nhttps://crumplab.github.io/programmingforpsych/simulating-and-analyzing-data-in-r.html#simulating-data-for-multi-factor-designs\nThis section will get longer as I find more resources to add, and hopefully the entire chapter will get longer as I add in more examples over time."
  },
  {
    "objectID": "12-Thinking.html#effect-size-and-power",
    "href": "12-Thinking.html#effect-size-and-power",
    "title": "12  Thinking about answering questions with data",
    "section": "12.1 Effect-size and power",
    "text": "12.1 Effect-size and power\nIf you already know something about statistics while you were reading this book, you might have noticed that we neglected to discuss the topic of effect-size, and we barely talked about statistical power. We will talk a little bit about these things here.\nFirst, it is worth pointing out that over the years, at least in Psychology, many societies and journals have made recommendations about how researchers should report their statistical analyses. Among the recommendations is that measures of “effect size” should be reported. Similarly, many journals now require that researchers report an “a priori” power-analysis (the recommendation is this should be done before the data is collected). Because these recommendations are so prevalent, it is worth discussing what these ideas refer to. At the same time, the meaning of effect-size and power somewhat depend on your “philosophical” bent, and these two ideas can become completely meaningless depending on how you think of statistics. For these complicating reasons we have suspended our discussion of the topic until now.\nThe question or practice of using measures of effect size and conducting power-analyses are also good examples of the more general need to think about about what you are doing. If you are going to report effect size, and conduct power analyses, these activities should not be done blindly because someone else recommends that you do them, these activities and other suitable ones should be done as a part of justifying what you are doing. It is a part of thinking about how to make your data answer questions for you.\n\n12.1.1 Chance vs. real effects\nLet’s rehash something we’ve said over and over again. First, researchers are interested in whether their manipulation causes a change in their measurement. If it does, they can become confident that they have uncovered a causal force (the manipulation). However, we know that differences in the measure between experimental conditions can arise by chance alone, just by sampling error. In fact, we can create pictures that show us the window of chance for a given statistic, these tells us roughly the range and likelihoods of getting various differences just by chance. With these windows in hand, we can then determine whether the differences we found in some data that we collected were likely or unlikely to be due to chance. We also learned that sample-size plays a big role in the shape of the chance window. Small samples give chance a large opportunity make big differences. Large samples give chance a small opportunity to make big differences. The general lesson up to this point has been, design an experiment with a large enough sample to detect the effect of interest. If your design isn’t well formed, you could easily be measuring noise, and your differences could be caused by sampling error. Generally speaking, this is still a very good lesson: better designs produce better data; and you can’t fix a broken design with statistics.\nThere is clearly another thing that can determine whether or not your differences are due to chance. That is the effect itself. If the manipulation does cause a change, then there is an effect, and that effect is a real one. Effects refer to differences in the measurement between experimental conditions. The thing about effects is that they can be big or small, they have a size.\nFor example, you can think of a manipulation in terms of the size of its hammer. A strong manipulation is like a jack-hammer: it is loud, it produces a big effect, it creates huge differences. A medium manipulation is like regular hammer: it works, you can hear it, it drives a nail into wood, but it doesn’t destroy concrete like a jack-hammer, it produces a reliable effect. A small manipulation is like tapping something with a pencil: it does something, you can barely hear it, and only in a quiet room, it doesn’t do a good job of driving a nail into wood, and it does nothing to concrete, it produces tiny, unreliable effects. Finally, a really small effect would be hammering something with a feather, it leaves almost no mark and does nothing that is obviously perceptiple to nails or pavement. The lesson is, if you want to break up concrete, use a jack-hammer; or, if you want to measure your effect, make your manipulation stronger (like a jack-hammer) so it produces a bigger difference.\n\n\n12.1.2 Effect size: concrete vs. abstract notions\nGenerally speaking, the big concept of effect size, is simply how big the differences are, that’s it. However, the biggness or smallness of effects quickly becomes a little bit complicated. On the one hand, the raw difference in the means can be very meaningful. Let’s saw we are measuring performance on a final exam, and we are testing whether or not a miracle drug can make you do better on the test. Let’s say taking the drug makes you do 5% better on the test, compared to not taking the drug. You know what 5% means, that’s basically a whole letter grade. Pretty good. An effect-size of 25% would be even better right! Lot’s of measures have a concrete quality to them, and we often want to the size of the effect expressed in terms of the original measure.\nLet’s talk about concrete measures some more. How about learning a musical instrument. Let’s say it takes 10,000 hours to become an expert piano, violin, or guitar player. And, let’s say you found something online that says that using their method, you will learn the instrument in less time than normal. That is a claim about the effect size of their method. You would want to know how big the effect is right? For example, the effect-size could be 10 hours. That would mean it would take you 9,980 hours to become an expert (that’s a whole 10 hours less). If I knew the effect-size was so tiny, I wouldn’t bother with their new method. But, if the effect size was say 1,000 hours, that’s a pretty big deal, that’s 10% less (still doesn’t seem like much, but saving 1,000 hours seems like a lot).\nJust as often as we have concrete measures that are readily interpretable, Psychology often produces measures that are extremely difficult to interpret. For example, questionnaire measures often have no concrete meaning, and only an abstract statistical meaning. If you wanted to know whether a manipulation caused people to more or less happy, and you used to questionnaire to measure happiness, you might find that people were 50 happy in condition 1, and 60 happy in condition 2, that’s a difference of 10 happy units. But how much is 10? Is that a big or small difference? It’s not immediately obvious. What is the solution here? A common solution is to provide a standardized measure of the difference, like a z-score. For example, if a difference of 10 reflected a shift of one standard deviation that would be useful to know, and that would be a sizeable shift. If the difference was only a .1 shift in terms of standard deviation, then the difference of 10 wouldn’t be very large. We elaborate on this idea next in describing cohen’s d.\n\n\n12.1.3 Cohen’s d\nLet’s look a few distributions to firm up some ideas about effect-size. Figure 12.1 has four panels. The first panel (0) represents the null distribution of no differences. This is the idea that your manipulation (A vs. B) doesn’t do anything at all, as a result when you measure scores in conditions A and B, you are effectively sampling scores from the very same overall distribution. The panel shows the distribution as green for condition B, but the red one for condition A is identical and drawn underneath (it’s invisible). There is 0 difference between these distributions, so it represent a null effect.\n\nX &lt;- c(\n  seq(-5, 5, .1),\n  seq(-5, 5, .1),\n  seq(-5, 5, .1),\n  seq(-5, 5, .1),\n  seq(-5, 5, .1),\n  seq(-5, 5, .1),\n  seq(-5, 5, .1),\n  seq(-5, 5, .1)\n)\nY &lt;- c(\n  dnorm(seq(-5, 5, .1), 0, 1),\n  dnorm(seq(-5, 5, .1), 0, 1),\n  dnorm(seq(-5, 5, .1), 0, 1),\n  dnorm(seq(-5, 5, .1), .5, 1),\n  dnorm(seq(-5, 5, .1), 0, 1),\n  dnorm(seq(-5, 5, .1), 1, 1),\n  dnorm(seq(-5, 5, .1), 0, 1),\n  dnorm(seq(-5, 5, .1), 2, 1)\n)\neffect_size &lt;- rep(c(0, .5, 1, 2), each = 101 * 2)\ncondition &lt;- rep(rep(c(\"A\", \"B\"), each = 101), 2)\ndf &lt;- data.frame(effect_size,\n                 condition,\n                 X, Y)\n\nggplot(df, aes(\n  x = X,\n  y = Y,\n  group = condition,\n  color = condition\n)) +\n  geom_line() +\n  theme_classic(base_size = 15) +\n  facet_wrap( ~ effect_size) +\n  xlab(\"values\") +\n  ylab(\"density\")\n\n\n\n\nFigure 12.1: Each panel shows hypothetical distributions for two conditions. As the effect-size increases, the difference between the distributions become larger.\n\n\n\n\nThe remaining panels are hypothetical examples of what a true effect could look like, when your manipulation actually causes a difference. For example, if condition A is a control group, and condition B is a treatment group, we are looking at three cases where the treatment manipulation causes a positive shift in the mean of distribution. We are using normal curves with mean =0 and sd =1 for this demonstration, so a shift of .5 is a shift of half of a standard deviation. A shift of 1 is a shift of 1 standard deviation, and a shift of 2 is a shift of 2 standard deviations. We could draw many more examples showing even bigger shifts, or shifts that go in the other direction.\nLet’s look at another example, but this time we’ll use some concrete measurements. Let’s say we are looking at final exam performance, so our numbers are grade percentages. Let’s also say that we know the mean on the test is 65%, with a standard deviation of 5%. Group A could be a control that just takes the test, Group B could receive some “educational” manipulation designed to improve the test score. These graphs then show us some hypotheses about what the manipulation may or may not be doing.\n\nX &lt;- c(\n  seq(25, 100, 1),\n  seq(25, 100, 1),\n  seq(25, 100, 1),\n  seq(25, 100, 1),\n  seq(25, 100, 1),\n  seq(25, 100, 1),\n  seq(25, 100, 1),\n  seq(25, 100, 1)\n)\nY &lt;- c(\n  dnorm(seq(25, 100, 1), 65, 5),\n  dnorm(seq(25, 100, 1), 65, 5),\n  dnorm(seq(25, 100, 1), 65, 5),\n  dnorm(seq(25, 100, 1), 67.5, 5),\n  dnorm(seq(25, 100, 1), 65, 5),\n  dnorm(seq(25, 100, 1), 70, 5),\n  dnorm(seq(25, 100, 1), 65, 5),\n  dnorm(seq(25, 100, 1), 75, 5)\n)\neffect_size &lt;-\n  rep(c(\"65, d=0\", \"67.5,d=.5\", \"70, d=1\", \"75, d=2\"), each = 76 * 2)\ncondition &lt;- rep(rep(c(\"A\", \"B\"), each = 76), 2)\ndf &lt;- data.frame(effect_size,\n                 condition,\n                 X, Y)\n\nggplot(df, aes(\n  x = X,\n  y = Y,\n  group = condition,\n  color = condition\n)) +\n  geom_line() +\n  theme_classic(base_size = 15) +\n  facet_wrap( ~ effect_size) +\n  xlab(\"values\") +\n  ylab(\"density\")\n\n\n\n\nFigure 12.2: Each panel shows hypothetical distributions for two conditions. As the effect-size increases, the difference between the distributions become larger.\n\n\n\n\nThe first panel shows that both condition A and B will sample test scores from the same distribution (mean =65, with 0 effect). The other panels show shifted mean for condition B (the treatment that is supposed to increase test performance). So, the treatment could increase the test performance by 2.5% (mean 67.5, .5 sd shift), or by 5% (mean 70, 1 sd shift), or by 10% (mean 75%, 2 sd shift), or by any other amount. In terms of our previous metaphor, a shift of 2 standard deviations is more like jack-hammer in terms of size, and a shift of .5 standard deviations is more like using a pencil. The thing about research, is we often have no clue about whether our manipulation will produce a big or small effect, that’s why we are conducting the research.\nYou might have noticed that the letter \\(d\\) appears in the above figure. Why is that? Jacob Cohen (cohen1988?) used the letter \\(d\\) in defining the effect-size for this situation, and now everyone calls it Cohen’s \\(d\\). The formula for Cohen’s \\(d\\) is:\n\\(d = \\frac{\\text{mean for condition 1} - \\text{mean for condition 2}}{\\text{population standard deviation}}\\)\nIf you notice, this is just a kind of z-score. It is a way to standardize the mean difference in terms of the population standard deviation.\nIt is also worth noting again that this measure of effect-size is entirely hypothetical for most purposes. In general, researchers do not know the population standard deviation, they can only guess at it, or estimate it from the sample. The same goes for means, in the formula these are hypothetical mean differences in two population distributions. In practice, researchers do not know these values, they guess at them from their samples.\nBefore discussing why the concept of effect-size can be useful, we note that Cohen’s \\(d\\) is useful for understanding abstract measures. For example, when you don’t know what a difference of 10 or 20 means as a raw score, you can standardize the difference by the sample standard deviation, then you know roughly how big the effect is in terms of standard units. If you thought a 20 was big, but it turned out to be only 1/10th of a standard deviation, then you would know the effect is actually quite small with respect to the overall variability in the data."
  },
  {
    "objectID": "12-Thinking.html#power",
    "href": "12-Thinking.html#power",
    "title": "12  Thinking about answering questions with data",
    "section": "12.2 Power",
    "text": "12.2 Power\nWhen there is a true effect out there to measure, you want to make sure your design is sensitive enough to detect the effect, otherwise what’s the point. We’ve already talked about the idea that an effect can have different sizes. The next idea is that your design can be more less sensitive in its ability to reliabily measure the effect. We have discussed this general idea many times already in the textbook, for example we know that we will be more likely to detect “significant” effects (when there are real differences) when we increase our sample-size. Here, we will talk about the idea of design sensitivity in terms of the concept of power. Interestingly, the concept of power is a somewhat limited concept, in that it only exists as a concept within some philosophies of statistics.\n\n12.2.1 A digresssion about hypothesis testing\nIn particular, the concept of power falls out of the Neyman-Pearson concept of null vs. alternative hypothesis testing. Up to this point, we have largely avoided this terminology. This is perhaps a disservice in that the Neyman-Pearson ideas are by now the most common and widespread, and in the opinion of some of us, they are also the most widely misunderstood and abused idea, which is why we have avoided these ideas until now.\nWhat we have been mainly doing is talking about hypothesis testing from the Fisherian (Sir Ronald Fisher, the ANOVA guy) perspective. This is a basic perspective that we think can’t be easily ignored. It is also quite limited. The basic idea is this:\n\nWe know that chance can cause some differences when we measure something between experimental conditions.\nWe want to rule out the possibility that the difference that we observed can not be due to chance\nWe construct large N designs that permit us to do this when a real effect is observed, such that we can confidently say that big differences that we find are so big (well outside the chance window) that it is highly implausible that chance alone could have produced.\nThe final conclusion is that chance was extremely unlikely to have produced the differences. We then infer that something else, like the manipulation, must have caused the difference.\nWe don’t say anything else about the something else.\nWe either reject the null distribution as an explanation (that chance couldn’t have done it), or retain the null (admit that chance could have done it, and if it did we couldn’t tell the difference between what we found and what chance could do)\n\nNeyman and Pearson introduced one more idea to this mix, the idea of an alternative hypothesis. The alternative hypothesis is the idea that if there is a true effect, then the data sampled into each condition of the experiment must have come from two different distributions. Remember, when there is no effect we assume all of the data cam from the same distribution (which by definition can’t produce true differences in the long run, because all of the numbers are coming from the same distribution). The graphs of effect-sizes from before show examples of these alternative distributions, with samples for condition A coming from one distribution, and samples from condition B coming from a shifted distribution with a different mean.\nSo, under the Neyman-Pearson tradition, when a researcher find a signifcant effect they do more than one things. First, they reject the null-hypothesis of no differences, and they accept the alternative hypothesis that there was differences. This seems like a sensible thing to do. And, because the researcher is actually interested in the properties of the real effect, they might be interested in learning more about the actual alternative hypothesis, that is they might want to know if their data come from two different distributions that were separated by some amount…in other words, they would want to know the size of the effect that they were measuring.\n\n\n12.2.2 Back to power\nWe have now discussed enough ideas to formalize the concept of statistical power. For this concept to exist we need to do a couple things.\n\nAgree to set an alpha criterion. When the p-value for our test-statistic is below this value we will call our finding statistically significant, and agree to reject the null hypothesis and accept the “alternative” hypothesis (sidenote, usually it isn’t very clear which specific alternative hypothesis was accepted)\nIn advance of conducting the study, figure out what kinds of effect-sizes our design is capable of detecting with particular probabilites.\n\nThe power of a study is determined by the relationship between\n\nThe sample-size of the study\nThe effect-size of the manipulation\nThe alpha value set by the researcher.\n\nTo see this in practice let’s do a simulation. We will do a t-test on a between-groups design 10 subjects in each group. Group A will be a control group with scores sampled from a normal distribution with mean of 10, and standard deviation of 5. Group B will be a treatment group, we will say the treatment has an effect-size of Cohen’s \\(d\\) = .5, that’s a standard deviation shift of .5, so the scores with come from a normal distribution with mean =12.5 and standard deivation of 5. Remember 1 standard deviation here is 5, so half of a standard deviation is 2.5.\nThe following R script runs this simulated experiment 1000 times. We set the alpha criterion to .05, this means we will reject the null whenever the \\(p\\)-value is less than .05. With this specific design, how many times out of of 1000 do we reject the null, and accept the alternative hypothesis?\n\np&lt;-length(1000)\nfor(i in 1:1000){\n  A&lt;-rnorm(10,10,5)\n  B&lt;-rnorm(10,12.5,5)\n  p[i]&lt;-t.test(A,B,var.equal = TRUE)$p.value\n}\n\nlength(p[p&lt;.05])\n\n[1] 181\n\n\nThe answer is that we reject the null, and accept the alternative 181 times out of 1000. In other words our experiment succesfully accepts the alternative hypothesis 18.1 percent of the time, this is known as the power of the study. Power is the probability that a design will succesfully detect an effect of a specific size.\nImportantly, power is completely abstract idea that is completely determined by many assumptions including N, effect-size, and alpha. As a result, it is best not to think of power as a single number, but instead as a family of numbers.\nFor example, power is different when we change N. If we increase N, our samples will more precisely estimate the true distributions that they came from. Increasing N reduces sampling error, and shrinks the range of differences that can be produced by chance. Lets’ increase our N in this simulation from 10 to 20 in each group and see what happens.\n\np&lt;-length(1000)\nfor(i in 1:1000){\n  A&lt;-rnorm(20,10,5)\n  B&lt;-rnorm(20,12.5,5)\n  p[i]&lt;-t.test(A,B,var.equal = TRUE)$p.value\n}\n\nlength(p[p&lt;.05])\n\n[1] 376\n\n\nNow the number of significant experiments i 376 out of 1000, or a power of 37.6 percent. That’s roughly doubled from before. We have made the design more sensitive to the effect by increasing N.\nWe can change the power of the design by changing the alpha-value, which tells us how much evidence we need to reject the null. For example, if we set the alpha criterion to 0.01, then we will be more conservative, only rejecting the null when chance can produce the observed difference 1% of the time. In our example, this will have the effect of reducing power. Let’s keep N at 20, but reduce the alpha to 0.01 and see what happens:\n\np&lt;-length(1000)\nfor(i in 1:1000){\n  A&lt;-rnorm(20,10,5)\n  B&lt;-rnorm(20,12.5,5)\n  p[i]&lt;-t.test(A,B,var.equal = TRUE)$p.value\n}\n\nlength(p[p&lt;.01])\n\n[1] 162\n\n\nNow only 162 out of 1000 experiments are significant, that’s 16.2 power.\nFinally, the power of the design depends on the actual size of the effect caused by the manipulation. In our example, we hypothesized that the effect caused a shift of .5 standard deviations. What if the effect causes a bigger shift? Say, a shift of 2 standard deviations. Let’s keep N= 20, and alpha &lt; .01, but change the effect-size to two standard deviations. When the effect in the real-world is bigger, it should be easier to measure, so our power will increase.\n\np&lt;-length(1000)\nfor(i in 1:1000){\n  A&lt;-rnorm(20,10,5)\n  B&lt;-rnorm(20,30,5)\n  p[i]&lt;-t.test(A,B,var.equal = TRUE)$p.value\n}\n\nlength(p[p&lt;.01])\n\n[1] 1000\n\n\nNeat, if the effect-size is actually huge (2 standard deviation shift), then we have power 100 percent to detect the true effect.\n\n\n12.2.3 Power curves\nWe mentioned that it is best to think of power as a family of numbers, rather than as a single number. To elaborate on this consider the power curve below. This is the power curve for a specific design: a between groups experiments with two levels, that uses an independent samples t-test to test whether an observed difference is due to chance. Critically, N is set to 10 in each group, and alpha is set to .05\nIn Figure 12.3 power (as a proportion, not a percentage) is plotted on the y-axis, and effect-size (Cohen’s d) in standard deviation units is plotted on the x-axis.\n\npower&lt;-c()\nfor(i in seq(0,2,.1)){\nsd_AB &lt;- 1\nn&lt;-10\nC &lt;- qnorm(0.975)\nse &lt;- sqrt( sd_AB/n + sd_AB/n )\ndelta&lt;-i\npower &lt;- c(power,1-pnorm(C-delta/se) + pnorm(-C-delta/se))\n}\n\nplot_df&lt;-data.frame(power,\n                    effect_size = seq(0,2,.1))\n\nggplot(plot_df, aes(x=effect_size, y=power))+\n  geom_line()+\n  theme_classic()\n\n\n\n\nFigure 12.3: This figure shows power as a function of effect-size (Cohen’s d) for a between-subjects independent samples t-test, with N=10, and alpha criterion 0.05.\n\n\n\n\nA power curve like this one is very helpful to understand the sensitivity of a particular design. For example, we can see that a between subjects design with N=10 in both groups, will detect an effect of d=.5 (half a standard deviation shift) about 20% of the time, will detect an effect of d=.8 about 50% of the time, and will detect an effect of d=2 about 100% of the time. All of the percentages reflect the power of the design, which is the percentage of times the design would be expected to find a \\(p\\) &lt; 0.05.\nLet’s imagine that based on prior research, the effect you are interested in measuring is fairly small, d=0.2. If you want to run an experiment that will detect an effect of this size a large percentage of the time, how many subjects do you need to have in each group? We know from the above graph that with N=10, power is very low to detect an effect of d=0.2. Let’s make Figure 12.4 and vary the number of subjects rather than the size of the effect.\n\npower&lt;-c()\nfor(i in seq(10,800,10)){\nsd_AB &lt;- 1\nn&lt;-i\nC &lt;- qnorm(0.975)\nse &lt;- sqrt( sd_AB/n + sd_AB/n )\ndelta&lt;-0.2\npower &lt;- c(power,1-pnorm(C-delta/se) + pnorm(-C-delta/se))\n}\n\nplot_df&lt;-data.frame(power,\n                    N = seq(10,800,10))\n\nggplot(plot_df, aes(x=N, y=power))+\n  geom_line()+\n  theme_classic()+\n  geom_hline(yintercept=.8, color=\"green\")\n\n\n\n\nFigure 12.4: This figure shows power as a function of N for a between-subjects independent samples t-test, with d=0.2, and alpha criterion 0.05.\n\n\n\n\nThe figure plots power to detect an effect of d=0.2, as a function of N. The green line shows where power = .8, or 80%. It looks like we would nee about 380 subjects in each group to measure an effect of d=0.2, with power = .8. This means that 80% of our experiments would succesfully show p &lt; 0.05. Often times power of 80% is recommended as a reasonable level of power, however even when your design has power = 80%, your experiment will still fail to find an effect (associated with that level of power) 20% of the time!"
  },
  {
    "objectID": "12-Thinking.html#planning-your-design",
    "href": "12-Thinking.html#planning-your-design",
    "title": "12  Thinking about answering questions with data",
    "section": "12.3 Planning your design",
    "text": "12.3 Planning your design\nOur discussion of effect size and power highlight the importance of the understanding the statistical limitations of an experimental design. In particular, we have seen the relationship between:\n\nSample-size\nEffect-size\nAlpha criterion\nPower\n\nAs a general rule of thumb, small N designs can only reliably detect very large effects, whereas large N designs can reliably detect much smaller effects. As a researcher, it is your responsibility to plan your design accordingly so that it is capable of reliably detecting the kinds of effects it is intended to measure."
  },
  {
    "objectID": "12-Thinking.html#some-considerations",
    "href": "12-Thinking.html#some-considerations",
    "title": "12  Thinking about answering questions with data",
    "section": "12.4 Some considerations",
    "text": "12.4 Some considerations\n\n12.4.1 Low powered studies\nConsider the following case. A researcher runs a study to detect an effect of interest. There is good reason, from prior research, to believe the effect-size is d=0.5. The researcher uses a design that has 30% power to detect the effect. They run the experiment and find a significant p-value, (p&lt;.05). They conclude their manipulation worked, because it was unlikely that their result could have been caused by chance. How would you interpret the results of a study like this? Would you agree with thte researchers that the manipulation likely caused the difference? Would you be skeptical of the result?\nThe situation above requires thinking about two kinds of probabilities. On the one hand we know that the result observed by the researchers does not occur often by chance (p is less than 0.05). At the same time, we know that the design was underpowered, it only detects results of the expected size 30% of the time. We are face with wondering what kind of luck was driving the difference. The researchers could have gotten unlucky, and the difference really could be due to chance. In this case, they would be making a type I error (saying the result is real when it isn’t). If the result was not due to chance, then they would also be lucky, as their design only detects this effect 30% of the time.\nPerhaps another way to look at this situation is in terms of the replicability of the result. Replicability refers to whether or not the findings of the study would be the same if the experiment was repeated. Because we know that power is low here (only 30%), we would expect that most replications of this experiment would not find a significant effect. Instead, the experiment would be expected to replicate only 30% of the time.\n\n\n12.4.2 Large N and small effects\nPerhaps you have noticed that there is an intriguiing relationship between N (sample-size) and power and effect-size. As N increases, so does power to detect an effect of a particular size. Additionally, as N increases, a design is capable of detecting smaller and smaller effects with greater and greater power. For example, if N was large enough, we would have high power to detect very small effects, say d= 0.01, or even d=0.001. Let’s think about what this means.\nImagine a drug company told you that they ran an experiment with 1 billion people to test whether their drug causes a significant change in headache pain. Let’s say they found a significant effect (with power =100%), but the effect was very small, it turns out the drug reduces headache pain by less than 1%, let’s say 0.01%. For our imaginary study we will also assume that this effect is very real, and not caused by chance.\nClearly the design had enough power to detect the effect, and the effect was there, so the design did detect the effect. However, the issue is that there is little practical value to this effect. Nobody is going to by a drug to reduce their headache pain by 0.01%, even if it was “scientifcally proven” to work. This example brings up two issues. First, increasing N to very large levels will allow designs to detect almost any effect (even very tiny ones) with very high power. Second, sometimes effects are meaningless when they are very small, especially in applied research such as drug studies.\nThese two issues can lead to interesting suggestions. For example, someone might claim that large N studies aren’t very useful, because they can always detect really tiny effects that are practically meaningless. On the other hand, large N studies will also detect larger effects too, and they will give a better estimate of the “true” effect in the population (because we know that larger samples do a better job of estimating population parameters). Additionally, although really small effects are often not interesting in the context of applied research, they can be very important in theoretical research. For example, one theory might predict that manipulating X should have no effect, but another theory might predict that X does have an effect, even if it is a small one. So, detecting a small effect can have theoretical implication that can help rule out false theories. Generally speaking, researchers asking both theoretical and applied questions should think about and establish guidelines for “meaningful” effect-sizes so that they can run designs of appropriate size to detect effects of “meaningful size”.\n\n\n12.4.3 Small N and Large effects\nAll other things being equal would you trust the results from a study with small N or large N? This isn’t a trick question, but sometimes people tie themselves into a knot trying to answer it. We already know that large sample-sizes provide better estimates of the distributions the samples come from. As a result, we can safely conclude that we should trust the data from large N studies more than small N studies.\nAt the same time, you might try to convince yourself otherwise. For example, you know that large N studies can detect very small effects that are practically and possibly even theoretically meaningless. You also know that that small N studies are only capable of reliably detecting very large effects. So, you might reason that a small N study is better than a large N study because if a small N study detects an effect, that effect must be big and meaningful; whereas, a large N study could easily detect an effect that is tiny and meaningless.\nThis line of thinking needs some improvement. First, just because a large N study can detect small effects, doesn’t mean that it only detects small effects. If the effect is large, a large N study will easily detect it. Large N studies have the power to detect a much wider range of effects, from small to large. Second, just because a small N study detected an effect, does not mean that the effect is real, or that the effect is large. For example, small N studies have more variability, so the estimate of the effect size will have more error. Also, there is 5% (or alpha rate) chance that the effect was spurious. Interestingly, there is a pernicious relationship between effect-size and type I error rate\n\n\n12.4.4 Type I errors are convincing when N is small\nSo what is this pernicious relationship between Type I errors and effect-size? Mainly, this relationship is pernicious for small N studies. For example, the following figure illustrates the results of 1000s of simulated experiments, all assuming the null distribution. In other words, for all of these simulations there is no true effect, as the numbers are all sampled from an identical distribution (normal distribution with mean =0, and standard deviation =1). The true effect-size is 0 in all cases.\nWe know that under the null, researchers will find p values that are less 5% about 5% of the time, remember that is the definition. So, if a researcher happened to be in this situation (where there manipulation did absolutely nothing), they would make a type I error 5% of the time, or if they conducted 100 experiments, they would expect to find a significant result for 5 of them.\nFigure 12.5 reports the findings from only the type I errors, where the simulated study did produce p &lt; 0.05. For each type I error, we calculated the exact p-value, as well as the effect-size (cohen’s D) (mean difference divided by standard deviation). We already know that the true effect-size is zero, however take a look at this graph, and pay close attention to the smaller sample-sizes.\n\nall_df&lt;-data.frame()\nfor(i in 1:1000){\n  for(n in c(10,20,50,100,1000)){\n    some_data&lt;-rnorm(n,0,1)\n    p_value&lt;-t.test(some_data)$p.value\n    effect_size&lt;-mean(some_data)/sd(some_data)\n    mean_scores&lt;-mean(some_data)\n    standard_error&lt;-sd(some_data)/sqrt(length(some_data))\n    t_df&lt;-data.frame(sim=i,sample_size=n,p_value,effect_size,mean_scores,standard_error)\n    all_df&lt;-rbind(all_df,t_df)\n  }\n}\n\ntype_I_error &lt;-all_df[all_df$p_value&lt;.05,]\ntype_I_error$sample_size&lt;-as.factor(type_I_error$sample_size)\n\nggplot(type_I_error,aes(x=p_value,y=effect_size, group=sample_size,color=sample_size))+\n  geom_point()+\n  theme_classic()+\n  ggtitle(\"Effect sizes for type I errors\")\n\n\n\n\nFigure 12.5: Effect size as a function of p-values for type 1 Errors under the null, for a paired samples t-test.\n\n\n\n\nFor example, look at the red dots, when sample size is 10. Here we see that the effect-sizes are quite large. When p is near 0.05 the effect-size is around .8, and it goes up and up as when p gets smaller and smaller. What does this mean? It means that when you get unlucky with a small N design, and your manipulation does not work, but you by chance find a “significant” effect, the effect-size measurement will show you a “big effect”. This is the pernicious aspect. When you make a type I error for small N, your data will make you think there is no way it could be a type I error because the effect is just so big!. Notice that when N is very large, like 1000, the measure of effect-size approaches 0 (which is the true effect-size in the simulation shown in Figure 12.6).\n\nall_df&lt;-data.frame()\nfor(i in 1:10000){\n  sample      &lt;- rnorm(50,100,20)\n  sample_mean &lt;- mean(sample[1:25]-sample[26:50])\n  sample_sem  &lt;- sd(sample)/sqrt(length(sample))\n  sample_t    &lt;- t.test(sample, mu=100)$statistic\n  sample_d    &lt;- (mean(sample)-100)/sd(sample)\n  t_df&lt;-data.frame(i,sample_mean,\n                   sample_sem,\n                   sample_t,\n                   sample_d)\n  all_df&lt;-rbind(all_df,t_df)\n}\n\nlibrary(ggpubr)\na&lt;-ggplot(all_df,aes(x=sample_mean))+\n  geom_histogram(color=\"white\")+\n  theme_classic()\nb&lt;-ggplot(all_df,aes(x=sample_sem))+\n  geom_histogram(color=\"white\")+\n  theme_classic()\nc&lt;-ggplot(all_df,aes(x=sample_t))+\n  geom_histogram(color=\"white\")+\n  theme_classic()\nd&lt;-ggplot(all_df,aes(x=sample_d))+\n  geom_histogram(color=\"white\")+\n  theme_classic()\n\nggarrange(a,b,c,d,\n          ncol = 2, nrow = 2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 12.6: Each panel shows a histogram of a different sampling statistic."
  },
  {
    "objectID": "13-Gifs.html#correlation-gifs",
    "href": "13-Gifs.html#correlation-gifs",
    "title": "13  GIFs",
    "section": "13.1 Correlation GIFs",
    "text": "13.1 Correlation GIFs\nNote regression lines and confidence bands can be added using geom_smooth(method=lm, se=T)\n\n13.1.1 N=10, both variables drawn from a uniform distribution\n\n\n\n\n\n\nall_df&lt;-data.frame()\nfor(sim in 1:10){\n  North_pole &lt;- runif(10,1,10)\n  South_pole &lt;- runif(10,1,10)\n  t_df&lt;-data.frame(simulation=rep(sim,10),\n                                  North_pole,\n                                  South_pole)\n  all_df&lt;-rbind(all_df,t_df)\n}\n\n\nggplot(all_df,aes(x=North_pole,y=South_pole))+\n  geom_point()+\n  geom_smooth(method=lm, se=FALSE)+\n  theme_classic()+\n  transition_states(\n    simulation,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n\n\n\n13.1.2 Correlation between random deviates from uniform distribution across four sample sizes\nN= 10,50,100,1000 All values sampled from a uniform distribution\n\n\n\n\n\n\nall_df&lt;-data.frame()\nfor(sim in 1:10){\n  for(n in c(10,50,100,1000)){\n  North_pole &lt;- runif(n,1,10)\n  South_pole &lt;- runif(n,1,10)\n  t_df&lt;-data.frame(nsize=rep(n,n),\n                   simulation=rep(sim,n),\n                                  North_pole,\n                                  South_pole)\n  all_df&lt;-rbind(all_df,t_df)\n  }\n}\n\n\nggplot(all_df,aes(x=North_pole,y=South_pole))+\n  geom_point()+\n  geom_smooth(method=lm, se=FALSE)+\n  theme_classic()+\n  facet_wrap(~nsize)+\n  transition_states(\n    simulation,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n\n\n\n13.1.3 Correlation between random deviates from normal distribution across four sample sizes\nN= 10,50,100,1000 All values sampled from the same normal distribution (mean=0, sd=1)\n\n\n\n\n\n\nall_df&lt;-data.frame()\nfor(sim in 1:10){\n  for(n in c(10,50,100,1000)){\n  North_pole &lt;- rnorm(n,0,1)\n  South_pole &lt;- rnorm(n,0,1)\n  t_df&lt;-data.frame(nsize=rep(n,n),\n                   simulation=rep(sim,n),\n                                  North_pole,\n                                  South_pole)\n  all_df&lt;-rbind(all_df,t_df)\n  }\n}\n\n\nggplot(all_df,aes(x=North_pole,y=South_pole))+\n  geom_point()+\n  geom_smooth(method=lm, se=FALSE)+\n  theme_classic()+\n  facet_wrap(~nsize)+\n  transition_states(\n    simulation,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n\n\n\n13.1.4 Correlation between X and Y variables that have a true correlation as a function of sample-size\n\n\n\n\n\n\nlibrary(MASS)\nr&lt;-.7\n\nproportional_permute&lt;-function(x,prop){\n  indices&lt;-seq(1:length(x))\n  s_indices&lt;-sample(indices)\n  n_shuffle&lt;-round(length(x)*prop)\n  switch&lt;-sample(indices)\n  x[s_indices[1:n_shuffle]]&lt;-x[switch[1:n_shuffle]]\n  return(x)\n}\n\nall_df&lt;-data.frame()\nfor(sim in 1:10){\n  for(samples in c(10,50,100,1000)){\n    #data &lt;- mvrnorm(n=samples, mu=c(0, 0), Sigma=matrix(c(1, r, r, 1), nrow=2), empirical=TRUE)\n    #North_pole &lt;- data[, 1]  # standard normal (mu=0, sd=1)\n    #South_pole &lt;- data[, 2] \n    \n    North_pole &lt;- runif(samples,1,10)\n    South_pole &lt;- proportional_permute(North_pole,.5)+runif(samples,-5,5)\n\n    t_df&lt;-data.frame(nsize=rep(samples,samples),\n                   simulation=rep(sim,samples),\n                                  North_pole,\n                                  South_pole)\n  all_df&lt;-rbind(all_df,t_df)\n  }\n}\n\nggplot(all_df,aes(x=North_pole,y=South_pole))+\n  geom_point()+\n  geom_smooth(method=lm, se=FALSE)+\n  theme_classic()+\n  facet_wrap(~nsize)+\n  transition_states(\n    simulation,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n\n\n\n13.1.5 Type I errors, sampling random deviates from normal distribution with regression lines\nThese scatter plots only show what would be type I errors (assuming alpha=.05). The X and Y values were both sampled from the same normal distribution (mean = 0, sd=1). 1000 simulations were conducted for each sample size (10,50,100,1000). For each, the animation shows 10 scatter plots where the observed “correlation” would have passed a significance test. According to definition, these correlations only arise from random normal deviates 5% of the time, but when they do arise for small sample sizes, they look fairly convincing.\n\n\n\n\n\n\nall_df&lt;-data.frame()\nfor(n in c(10,50,100,1000)){\n  count_sims&lt;-0\n  for(sim in 1:1000){\n    North_pole &lt;- rnorm(n,0,1)\n    South_pole &lt;- rnorm(n,0,1)\n    if(cor.test(North_pole,South_pole)$p.value&lt;.05){\n      count_sims&lt;-count_sims+1\n    t_df&lt;-data.frame(nsize=rep(n,n),\n                     simulation=rep(count_sims,n),\n                     North_pole,\n                     South_pole)\n    all_df&lt;-rbind(all_df,t_df)\n    \n    if(count_sims==10){\n      break\n    }\n    }\n  }\n}\n\n\nggplot(all_df,aes(x=North_pole,y=South_pole))+\n  geom_point()+\n  geom_smooth(method=lm, se=TRUE)+\n  theme_classic()+\n  facet_wrap(~nsize)+\n  transition_states(\n    simulation,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n\n\n\n13.1.6 Cell-size and correlation\nThis simulation illustrates how the behavior of correlating two random normal samples as a function of cell-size. The sample-size is always set at N=10. For each panel, the simulation uses an increasing cell-size to estimate the mean for X and Y. When cell-size is 1, 10 X and Y values are drawn from the same normal (u=0, sd=1). When cell-size is 5, for each X,Y score in the plot, 5 samples were drawn from the same normal, and then the mean of the samples is plotted. The effect of cell-size shrinks the dot cloud, as both X and Y scores provide better estimates of the population mean = 0. Cell-size has no effect on the behavior of r, which swings around because sample-size N is small. These are all random, so there is always a 5% type I error rate (alpha =.05).\n\n\n\n\n\n\nget_sampling_means&lt;-function(m,sd,cell_size,s_size){\n  save_means&lt;-length(s_size)\n  for(i in 1:s_size){\n    save_means[i]&lt;-mean(rnorm(cell_size,m,sd))\n  }\n  return(save_means)\n}\n\nall_df&lt;-data.frame()\nfor(n in c(1,5,10,100)){\n  count_sims&lt;-0\n  for(sim in 1:10){\n    North_pole &lt;- get_sampling_means(0,1,n,10)\n    South_pole &lt;- get_sampling_means(0,1,n,10)\n      count_sims&lt;-count_sims+1\n      t_df&lt;-data.frame(nsize=rep(n,10),\n                       simulation=rep(count_sims,10),\n                       North_pole,\n                       South_pole)\n      all_df&lt;-rbind(all_df,t_df)\n  }\n}\n\n\nggplot(all_df,aes(x=North_pole,y=South_pole))+\n  geom_point()+\n  geom_smooth(method=lm, se=TRUE)+\n  theme_classic()+\n  facet_wrap(~nsize)+\n  ggtitle(\"Random scatterplots, N=10, Cell-size = 1,5,10,100\")+\n  transition_states(\n    simulation,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n\n\n\n13.1.7 Regression\nWe look at how the residuals (error from points to line) behave as the regression lines moves above and below it’s true value. The total error associated with all of the red lines is represents by the grey area. This total error is smallest (minimized) when the black line overlaps with the blue regression line (the best fit line). The total error expands as the black line moves away from the regression. That’s why the regression line is the least wrong (best fit) line to skewer the data (according to least squares definition)\n\n\n\n\n\n\nd &lt;- mtcars\nfit &lt;- lm(mpg ~ hp, data = d)\nd$predicted &lt;- predict(fit)   # Save the predicted values\nd$residuals &lt;- residuals(fit) # Save the residual values\n\ncoefs&lt;-coef(lm(mpg ~ hp, data = mtcars))\ncoefs[1]\ncoefs[2]\n\nx&lt;-d$hp\nmove_line&lt;-c(seq(-6,6,.5),seq(6,-6,-.5))\ntotal_error&lt;-length(length(move_line))\ncnt&lt;-0\nfor(i in move_line){\n  cnt&lt;-cnt+1\n  predicted_y &lt;- coefs[2]*x + coefs[1]+i\n  error_y &lt;- (predicted_y-d$mpg)^2\n  total_error[cnt]&lt;-sqrt(sum(error_y)/32)\n}\n\nmove_line_sims&lt;-rep(move_line,each=32)\ntotal_error_sims&lt;-rep(total_error,each=32)\nsims&lt;-rep(1:50,each=32)\n\nd&lt;-d %&gt;% slice(rep(row_number(), 50))\n\nd&lt;-cbind(d,sims,move_line_sims,total_error_sims)\n\n\nanim&lt;-ggplot(d, aes(x = hp, y = mpg, frame=sims)) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"lightblue\") +  \n  geom_abline(intercept = 30.09886+move_line_sims, slope = -0.06822828)+\n  lims(x = c(0,400), y = c(-10,40))+\n  geom_segment(aes(xend = hp, yend = predicted+move_line_sims, color=\"red\"), alpha = .5) + \n  geom_point() +\n  geom_ribbon(aes(ymin = predicted+move_line_sims - total_error_sims, ymax = predicted+move_line_sims + total_error_sims), fill = \"lightgrey\", alpha=.2)+ \n  theme_classic()+\n  theme(legend.position=\"none\")+\n  xlab(\"X\")+ylab(\"Y\")+\n  transition_manual(frames=sims)+\n  enter_fade() + \n  exit_fade()+\n  ease_aes('sine-in-out')\n\nanimate(anim,fps=5)"
  },
  {
    "objectID": "13-Gifs.html#sampling-distributions",
    "href": "13-Gifs.html#sampling-distributions",
    "title": "13  GIFs",
    "section": "13.2 Sampling distributions",
    "text": "13.2 Sampling distributions\n\n13.2.1 Sampling from a uniform distribution\nAnimation shows histograms for N=20, sampled from a uniform distribution, along with mean (red line). Uniform distribution in this case is integer values from 1 to 10.\n\n\n\n\n\n\na&lt;-round(runif(20*10,1,10))\ndf&lt;-data.frame(a,sample=rep(1:10,each=20))\ndf2&lt;-aggregate(a~sample,df,mean)\ndf&lt;-cbind(df,mean_loc=rep(df2$a,each=20))\n\nlibrary(gganimate)\n\nggplot(df,aes(x=a, group=sample,frame=sample)) +\n  geom_histogram() +\n  geom_vline(aes(xintercept=mean_loc,frame = sample),color=\"red\")+\n  scale_x_continuous(breaks=seq(1,10,1))+\n  theme_classic()+\n  transition_states(\n    sample,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n\n\n\n13.2.2 Sampling from uniform with line showing expected value for each number\n\n\n\n\n\n\na&lt;-round(runif(20*10,1,10))\ndf&lt;-data.frame(a,sample=rep(1:10,each=20))\n\n\nlibrary(gganimate)\nggplot(df,aes(x=a))+\n  geom_histogram(bins=10, color=\"white\")+\n  theme_classic()+\n  scale_x_continuous(breaks=seq(1,10,1))+\n  geom_hline(yintercept=2)+\n  ggtitle(\"Small N=20 samples from a uniform distribution\")+\n  transition_states(\n    sample,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n\n\n\n13.2.3 Sampling distribution of the mean, Normal population distribution and sample histograms\nThis animation illustrates the relationship between a distribution (population), samples from the distribution, and the sampling distribution of the sample means, all as a function of n\nNormal distribution in red. Individual sample histograms in grey. Vertical red line is mean of individual sample. Histograms for sampling distribution of the sample mean in blue. Vertical blue line is mean of the sampling distribution of the sample mean.\nNote: for purposes of the animation (and because it was easier to do this way), the histograms for the sampling distribution of the sample means have different sizes. When sample-size = 10, the histogram shows 10 sample means. When sample size=100, the histogram shows 100 sample means. I could have simulated many more sample means (say 10000) for each, but then the histograms for the sample means would be static.\nThe y-axis is very rough. The heights of the histograms and distributions were scaled to be in the same range for the animation.\n\n\n\n\n\n\nget_sampling_means&lt;-function(m,sd,s_size){\n  save_means&lt;-length(s_size)\n  for(i in 1:s_size){\n    save_means[i]&lt;-mean(rnorm(s_size,m,sd))\n  }\n  return(save_means)\n}\n\nall_df&lt;-data.frame()\nfor(sims in 1:10){\n  for(n in c(10,50,100,1000)){\n    sample&lt;-rnorm(n,0,1)\n    sample_means&lt;-get_sampling_means(0,1,n)\n    t_df&lt;-data.frame(sims=rep(sims,n),\n                     sample,\n                     sample_means,\n                     sample_size=rep(n,n),\n                     sample_mean=rep(mean(sample),n),\n                     sampling_mean=rep(mean(sample_means),n)\n                     )\n    all_df&lt;-rbind(all_df,t_df)\n  }\n}\n\n\nggplot(all_df, aes(x=sample))+\n  geom_histogram(aes(y=(..density..)/max(..density..)^.8),color=\"white\",fill=\"grey\")+\n  geom_histogram(aes(x=sample_means,y=(..density..)/max(..density..)),fill=\"blue\",color=\"white\",alpha=.5)+\n  stat_function(fun = dnorm, \n                args = list(mean = 0, sd = 1), \n                lwd = .75, \n                col = 'red')+\n  geom_vline(aes(xintercept=sample_mean,frame=sims),color=\"red\")+\n  geom_vline(aes(xintercept=sampling_mean,frame=sims),color=\"blue\")+\n  facet_wrap(~sample_size)+xlim(-3,3)+\n  theme_classic()+ggtitle(\"Population (red), Samples (grey), \\n and Sampling distribution of the mean (blue)\")+ylab(\"Rough likelihoods\")+\n  xlab(\"value\")+\n  transition_states(\n    sims,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n\n\n\n13.2.4 Null and True effect samples and sampling means\nThe null dots show 50 different samples, with the red dot as the mean for each sample. Null dots are all sampled from normal (u=0, sd=1). The true dots show 50 more samples, with red dots for their means. However, the mean of the true shifts between -1.5 and +1.5 standard deviations of 0. This illustrates how a true effect moves in and out of the null range.\n\n\n\n\n\n\nall_df&lt;-data.frame()\nall_df_means&lt;-data.frame()\ndif_sim&lt;-seq(-1.5,1.5,.25)\nfor(sim in 1:13){\n  values&lt;-c(rnorm(25*25,0,1),rnorm(25*25,dif_sim[sim],1))\n  samples&lt;-c(rep(seq(1:25),each=25),rep(seq(1:25),each=25))\n  df&lt;-data.frame(samples,values,sims=rep(sim,50*25),type=rep(c(\"null\",\"true\"),each=625))\n  df_means&lt;-aggregate(values~samples*type,df,mean, sims=rep(sim,50))\n  all_df&lt;-rbind(all_df,df)\n  all_df_means&lt;-rbind(all_df_means,df_means)\n}\n\nall_df&lt;-cbind(all_df,means=rep(all_df_means$values,each=25))\n\nggplot(all_df,aes(y=values,x=samples))+\n  geom_point(aes(color=abs(values)), alpha=.25)+\n  geom_point(aes(y=means,x=samples),color=\"red\")+\n  theme_classic()+\n  geom_vline(xintercept=25.5)+\n  facet_wrap(~type)+\n  geom_hline(yintercept=0)+\n  theme(legend.position=\"none\") +\n  ggtitle(\"null=0, True effect moves from -1.5 sd to 1.5 sd\")+\n  transition_states(\n    sims,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')"
  },
  {
    "objectID": "13-Gifs.html#statistical-inference",
    "href": "13-Gifs.html#statistical-inference",
    "title": "13  GIFs",
    "section": "13.3 Statistical Inference",
    "text": "13.3 Statistical Inference\n\n13.3.1 Randomization Test\nThis is an attempt at visualizing a randomization test. Samples are taken under two conditions of the IV (A and B). At the beginning of the animation, the original scores in the first condition are shown as green dots on the left, and the original scores in the second condition are the red dots on the right. The means for each group are the purple dots. During the randomization, the original scores are shuffled randomly between the two conditions. After each shuffle, two new means are computed and displayed as the yellow dots. This occurs either for all permutations, or for a large random sample of them. The animation shows the original scores being shuffled around across the randomizations (the colored dots switch their original condition, appearing from side to side).\nFor intuitive inference, one might look at the range of motion of the yellow dots. This is how the mean difference between group 1 and group 2 behaves under randomization. It’s what chance can do. If the difference between the purple dots is well outside the range of motion of the yellow dots, then the mean difference observed in the beginning is not likely produced by chance.\n\n\n\n\n\n\nstudy&lt;-round(runif(10,80,100))\nno_study&lt;-round(runif(10,40,90))\n\nstudy_df&lt;-data.frame(student=seq(1:10),study,no_study)\nmean_original&lt;-data.frame(IV=c(\"studied\",\"didnt_study\"),\n                          means=c(mean(study),mean(no_study)))\nt_df&lt;-data.frame(sims=rep(1,20),\n                 IV=rep(c(\"studied\",\"didnt_study\"),each=10),\n                 values=c(study,no_study),\n                 rand_order=rep(c(0,1),each=10))\n\nraw_df&lt;-t_df\nfor(i in 2:10){\n  new_index&lt;-sample(1:20)\n  t_df$values&lt;-t_df$values[new_index]\n  t_df$rand_order&lt;-t_df$rand_order[new_index]\n  t_df$sims&lt;-rep(i,20)\n  raw_df&lt;-rbind(raw_df,t_df)\n}\n\nraw_df$rand_order&lt;-as.factor(raw_df$rand_order)\nrand_df&lt;-aggregate(values~sims*IV,raw_df,mean)\nnames(rand_df)&lt;-c(\"sims\",\"IV\",\"means\")\n\n\na&lt;-ggplot(raw_df,aes(x=IV,y=values,color=rand_order,size=3))+\n  geom_point(stat=\"identity\",alpha=.5)+\n  geom_point(data=mean_original,aes(x=IV,y=means),stat=\"identity\",shape=21,size=6,color=\"black\",fill=\"mediumorchid2\")+\n  geom_point(data=rand_df,aes(x=IV,y=means),stat=\"identity\",shape=21,size=6,color=\"black\",fill=\"gold\")+\n  theme_classic(base_size = 15)+\n  coord_cartesian(ylim=c(40, 100))+\n  theme(legend.position=\"none\") +\n  ggtitle(\"Randomization test: Original Means (purple), \n          \\n Randomized means (yellow)\n          \\n Original scores (red,greenish)\")+\n  transition_states(\n    sims,\n    transition_length = 1,\n    state_length = 2\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n\nanimate(a,nframes=100,fps=5)\n\n\n\n13.3.2 Independent t-test Null\nThis is a simulation of the null distribution for an independent samples t-test, two groups, 10 observations per group.\nThis animation has two panels. The left panel shows means for group A and B, sampled from the same normal distribution (mu=50, sd =10). The dots represent individual scores for each of 10 observations per group.\nThe right panel shows a t-distribution (df=18) along with the observed t-statistic for each simulation.\ngganimate does not yet directly support multiple panels as shown in this gif. I hacked together these two gifs using the magick package. Apologies for the hackiness.\n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(magick)\nlibrary(gganimate)\n\nA&lt;-rnorm(100,50,10)\nB&lt;-rnorm(100,50,10)\nDV &lt;- c(A,B)\nIV &lt;- rep(c(\"A\",\"B\"),each=100)\nsims &lt;- rep(rep(1:10,each=10),2)\ndf&lt;-data.frame(sims,IV,DV)\n\nmeans_df &lt;- df %&gt;%\n               group_by(sims,IV) %&gt;%\n               summarize(means=mean(DV),\n                         sem = sd(DV)/sqrt(length(DV)))\n\nstats_df &lt;- df %&gt;%\n              group_by(sims) %&gt;%\n              summarize(ts = t.test(DV~IV,var.equal=TRUE)$statistic)\n\na&lt;-ggplot(means_df, aes(x=IV,y=means, fill=IV))+\n  geom_bar(stat=\"identity\")+\n  geom_point(data=df,aes(x=IV, y=DV), alpha=.25)+\n  geom_errorbar(aes(ymin=means-sem, ymax=means+sem),width=.2)+\n  theme_classic()+\n  transition_states(\n    states=sims,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n  \na_gif&lt;-animate(a, width = 240, height = 240)\n\nb&lt;-ggplot(stats_df,aes(x=ts))+\n  geom_vline(aes(xintercept=ts, frame=sims))+\n  geom_line(data=data.frame(x=seq(-5,5,.1),\n                            y=dt(seq(-5,5,.1),df=18)),\n            aes(x=x,y=y))+\n  theme_classic()+\n  ylab(\"density\")+\n  xlab(\"t value\")+\n  transition_states(\n    states=sims,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n\nb_gif&lt;-animate(b, width = 240, height = 240)\n\n\nd&lt;-image_blank(240*2,240)\n\nthe_frame&lt;-d\nfor(i in 2:100){\n  the_frame&lt;-c(the_frame,d)\n}\n\na_mgif&lt;-image_read(a_gif)\nb_mgif&lt;-image_read(b_gif)\n\nnew_gif&lt;-image_append(c(a_mgif[1], b_mgif[1]))\nfor(i in 2:100){\n  combined &lt;- image_append(c(a_mgif[i], b_mgif[i]))\n  new_gif&lt;-c(new_gif,combined)\n}\n\nnew_gif\n\n\n\n13.3.3 Independent t-test True\nThis is a simulation of an independent samples t-test, two groups, 10 observations per group, assuming a true difference of 2 standard deviations between groups\nThis animation has two panels. The left panel shows means for group A (normal, mu=50, sd=10) and B (normal, mu=70, sd=10). The dots represent individual scores for each of 10 observations per group.\nThe right panel shows a t-distribution (df=18) along with the observed t-statistic for each simulation.\n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(magick)\nlibrary(gganimate)\n\nA&lt;-rnorm(100,70,10)\nB&lt;-rnorm(100,50,10)\nDV &lt;- c(A,B)\nIV &lt;- rep(c(\"A\",\"B\"),each=100)\nsims &lt;- rep(rep(1:10,each=10),2)\ndf&lt;-data.frame(sims,IV,DV)\n\nmeans_df &lt;- df %&gt;%\n               group_by(sims,IV) %&gt;%\n               summarize(means=mean(DV),\n                         sem = sd(DV)/sqrt(length(DV)))\n\nstats_df &lt;- df %&gt;%\n              group_by(sims) %&gt;%\n              summarize(ts = t.test(DV~IV,var.equal=TRUE)$statistic)\n\na&lt;-ggplot(means_df, aes(x=IV,y=means, fill=IV))+\n  geom_bar(stat=\"identity\")+\n  geom_point(data=df,aes(x=IV, y=DV), alpha=.25)+\n  geom_errorbar(aes(ymin=means-sem, ymax=means+sem),width=.2)+\n  theme_classic()+\n  transition_states(\n    states=sims,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n  \na_gif&lt;-animate(a, width = 240, height = 240)\n\nb&lt;-ggplot(stats_df,aes(x=ts))+\n  geom_vline(aes(xintercept=ts, frame=sims))+\n  geom_vline(xintercept=qt(c(.025, .975), df=18),color=\"green\")+\n  geom_line(data=data.frame(x=seq(-5,5,.1),\n                            y=dt(seq(-5,5,.1),df=18)),\n            aes(x=x,y=y))+\n  theme_classic()+\n  ylab(\"density\")+\n  xlab(\"t value\")+\n  transition_states(\n    states=sims,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n\nb_gif&lt;-animate(b, width = 240, height = 240)\n\n\nd&lt;-image_blank(240*2,240)\n\nthe_frame&lt;-d\nfor(i in 2:100){\n  the_frame&lt;-c(the_frame,d)\n}\n\na_mgif&lt;-image_read(a_gif)\nb_mgif&lt;-image_read(b_gif)\n\nnew_gif&lt;-image_append(c(a_mgif[1], b_mgif[1]))\nfor(i in 2:100){\n  combined &lt;- image_append(c(a_mgif[i], b_mgif[i]))\n  new_gif&lt;-c(new_gif,combined)\n}\n\nnew_gif\n\n\n\n13.3.4 T-test True sample-size\nThe top row shows 10 simulations of an independent sample t-test, with N=10, and true difference of 1 sd.\nThe bottom row shows 10 simulations with N=50.\nThe observed t-value occurs past the critical value (green) line much more reliably and often when sample size is larger than smaller.\n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(magick)\nlibrary(gganimate)\n\nA&lt;-rnorm(100,60,10)\nB&lt;-rnorm(100,50,10)\nDV &lt;- c(A,B)\nIV &lt;- rep(c(\"A\",\"B\"),each=100)\nsims &lt;- rep(rep(1:10,each=10),2)\ndf&lt;-data.frame(sims,IV,DV)\n\nmeans_df &lt;- df %&gt;%\n               group_by(sims,IV) %&gt;%\n               summarize(means=mean(DV),\n                         sem = sd(DV)/sqrt(length(DV)))\n\nstats_df &lt;- df %&gt;%\n              group_by(sims) %&gt;%\n              summarize(ts = t.test(DV~IV,var.equal=TRUE)$statistic)\n\na&lt;-ggplot(means_df, aes(x=IV,y=means, fill=IV))+\n  geom_bar(stat=\"identity\")+\n  geom_point(data=df,aes(x=IV, y=DV), alpha=.25)+\n  geom_errorbar(aes(ymin=means-sem, ymax=means+sem),width=.2)+\n  theme_classic()+\n  transition_states(\n    states=sims,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n  \na_gif&lt;-animate(a, width = 240, height = 240)\n\nb&lt;-ggplot(stats_df,aes(x=ts))+\n  geom_vline(aes(xintercept=ts, frame=sims))+\n  geom_vline(xintercept=qt(c(.025, .975), df=18),color=\"green\")+\n  geom_line(data=data.frame(x=seq(-5,5,.1),\n                            y=dt(seq(-5,5,.1),df=18)),\n            aes(x=x,y=y))+\n  theme_classic()+\n  ylab(\"density\")+\n  xlab(\"t value\")+\n  transition_states(\n    states=sims,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n\nb_gif&lt;-animate(b, width = 240, height = 240)\n\n\nd&lt;-image_blank(240*2,240)\n\nthe_frame&lt;-d\nfor(i in 2:100){\n  the_frame&lt;-c(the_frame,d)\n}\n\na_mgif&lt;-image_read(a_gif)\nb_mgif&lt;-image_read(b_gif)\n\nnew_gif&lt;-image_append(c(a_mgif[1], b_mgif[1]))\nfor(i in 2:100){\n  combined &lt;- image_append(c(a_mgif[i], b_mgif[i]))\n  new_gif&lt;-c(new_gif,combined)\n}\n\nnew_gif\n\n## increase sample-size\n\nA&lt;-rnorm(50*10,60,10)\nB&lt;-rnorm(50*10,50,10)\nDV &lt;- c(A,B)\nIV &lt;- rep(c(\"A\",\"B\"),each=50*10)\nsims &lt;- rep(rep(1:10,each=50),2)\ndf&lt;-data.frame(sims,IV,DV)\n\nmeans_df &lt;- df %&gt;%\n               group_by(sims,IV) %&gt;%\n               summarize(means=mean(DV),\n                         sem = sd(DV)/sqrt(length(DV)))\n\nstats_df &lt;- df %&gt;%\n              group_by(sims) %&gt;%\n              summarize(ts = t.test(DV~IV,var.equal=TRUE)$statistic)\n\na&lt;-ggplot(means_df, aes(x=IV,y=means, fill=IV))+\n  geom_bar(stat=\"identity\")+\n  geom_point(data=df,aes(x=IV, y=DV), alpha=.25)+\n  geom_errorbar(aes(ymin=means-sem, ymax=means+sem),width=.2)+\n  theme_classic()+\n  transition_states(\n    states=sims,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n  \na_gif&lt;-animate(a, width = 240, height = 240)\n\nb&lt;-ggplot(stats_df,aes(x=ts))+\n  geom_vline(aes(xintercept=ts, frame=sims))+\n  geom_vline(xintercept=qt(c(.025, .975), df=98),color=\"green\")+\n  geom_line(data=data.frame(x=seq(-5,5,.1),\n                            y=dt(seq(-5,5,.1),df=98)),\n            aes(x=x,y=y))+\n  theme_classic()+\n  ylab(\"density\")+\n  xlab(\"t value\")+\n  transition_states(\n    states=sims,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n\nb_gif&lt;-animate(b, width = 240, height = 240)\n\n\nd&lt;-image_blank(240*2,240)\n\nthe_frame&lt;-d\nfor(i in 2:100){\n  the_frame&lt;-c(the_frame,d)\n}\n\na_mgif&lt;-image_read(a_gif)\nb_mgif&lt;-image_read(b_gif)\n\nnew_gif2&lt;-image_append(c(a_mgif[1], b_mgif[1]))\nfor(i in 2:100){\n  combined &lt;- image_append(c(a_mgif[i], b_mgif[i]))\n  new_gif2&lt;-c(new_gif2,combined)\n}\n\n## add new row\n\nfinal_gif &lt;- image_append(c(new_gif[1], new_gif2[1]),stack=TRUE)\nfor(i in 2:100){\n  combined &lt;- image_append(c(new_gif[i], new_gif2[i]),stack=TRUE)\n  final_gif&lt;-c(final_gif,combined)\n}\n\nfinal_gif\n\n\n\n13.3.5 one-factor ANOVA Null\nThree groups, N=10, all observations sampled from same normal distribution (mu=50, sd = 10)\n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(magick)\nlibrary(gganimate)\n\n\nA&lt;-rnorm(100,50,10)\nB&lt;-rnorm(100,50,10)\nC&lt;-rnorm(100,50,10)\nDV &lt;- c(A,B,C)\nIV &lt;- rep(rep(c(\"A\",\"B\",\"C\"),each=10),10)\nsims &lt;- rep(1:10,each=30)\ndf&lt;-data.frame(sims,IV,DV)\n\nmeans_df &lt;- df %&gt;%\n  group_by(sims,IV) %&gt;%\n  summarize(means=mean(DV),\n            sem = sd(DV)/sqrt(length(DV)))\n\nstats_df &lt;- df %&gt;%\n  group_by(sims) %&gt;%\n  summarize(Fs = summary(aov(DV~IV))[[1]][[4]][1])\n\na&lt;-ggplot(means_df, aes(x=IV,y=means, fill=IV))+\n  geom_bar(stat=\"identity\")+\n  geom_point(data=df,aes(x=IV, y=DV), alpha=.25)+\n  geom_errorbar(aes(ymin=means-sem, ymax=means+sem),width=.2)+\n  theme_classic(base_size = 20)+\n  transition_states(\n    states=sims,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n\nb&lt;-ggplot(stats_df,aes(x=Fs))+\n  geom_vline(aes(xintercept=Fs))+\n  geom_vline(xintercept=qf(.95, df1=2,df2=27),color=\"green\")+\n  geom_line(data=data.frame(x=seq(0,6,.1),\n                            y=df(seq(0,6,.1),df1=2,df2=27)),\n            aes(x=x,y=y))+\n  theme_classic(base_size = 20)+\n  ylab(\"density\")+\n  xlab(\"F value\")+\n  transition_states(\n    states=sims,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n\na_gif&lt;-animate(a,width=480,height=480)\nb_gif&lt;-animate(b,width=480,height=480)\n\na_mgif&lt;-image_read(a_gif)\nb_mgif&lt;-image_read(b_gif)\n\nnew_gif&lt;-image_append(c(a_mgif[1], b_mgif[1]))\nfor(i in 2:100){\n  combined &lt;- image_append(c(a_mgif[i], b_mgif[i]))\n  new_gif&lt;-c(new_gif,combined)\n}\n\nnew_gif\n\n\n\n13.3.6 Factorial Null\n10 simulations, N=10 in each of 4 conditions in a 2x2 (between-subjects). All observations taken from the same normal distribution (mu=50, sd =10).\n\n\n\n\n\n\nA&lt;-rnorm(100,50,10)\nB&lt;-rnorm(100,50,10)\nC&lt;-rnorm(100,50,10)\nD&lt;-rnorm(100,50,10)\nDV &lt;- c(A,B,C,D)\nIV1 &lt;- rep(c(\"A\",\"B\"),each=200)\nIV2&lt;-rep(rep(c(\"1\",\"2\"),each=100),2)\nsims &lt;- rep(1:10,40)\ndf&lt;-data.frame(sims,IV1,IV2,DV)\n\nmeans_df &lt;- df %&gt;%\n  group_by(sims,IV1,IV2) %&gt;%\n  summarize(means=mean(DV),\n            sem = sd(DV)/sqrt(length(DV)))\n\nstats_df &lt;- df %&gt;%\n  group_by(sims) %&gt;%\n  summarize(FIV1 = summary(aov(DV~IV1*IV2))[[1]][[4]][1],\n            FIV2 = summary(aov(DV~IV1*IV2))[[1]][[4]][2],\n            F1x2 = summary(aov(DV~IV1*IV2))[[1]][[4]][3]\n            )\n\na&lt;-ggplot(means_df, aes(x=IV1,y=means, \n                                           group=IV2,\n                                           color=IV2))+\n  geom_point(data=df,aes(x=IV1, y=DV,group=IV2), \n             position=position_dodge(width=.2),\n             size=2,\n             alpha=.25)+\n  geom_point(size=4)+\n  geom_line(size=1.3)+\n  geom_errorbar(aes(ymin=means-sem, ymax=means+sem),width=.2,\n                color=\"black\")+\n  theme_classic(base_size = 20)+\n  transition_states(\n    states=sims,\n    transition_length = 2,\n    state_length = 1\n  )+enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')\n\nb&lt;-ggplot(stats_df,aes(x=FIV1))+\n  geom_vline(aes(xintercept=FIV1),color=\"red\",size=1.2)+\n  geom_vline(aes(xintercept=FIV2),color=\"blue\",size=1.2)+\n  geom_vline(aes(xintercept=F1x2),color=\"purple\",size=1.2)+\n  geom_vline(xintercept=qf(.95, df1=1,df2=36),color=\"green\",size=1.2)+\n  geom_line(data=data.frame(x=seq(0,20,.1),\n                            y=df(seq(0,20,.1),df1=1,df2=36)),\n            aes(x=x,y=y))+\n  theme_classic(base_size = 20)+\n  ylab(\"density\")+\n  xlab(\"F value\")+\n  ggtitle(label=\"\",subtitle=\"red=IV1, blue=IV2, \\n purple=Interaction\")+\n  transition_states(\n    states=sims,\n    transition_length = 2,\n    state_length = 1\n  )\n\na_gif&lt;-animate(a,width=480,height=480)\nb_gif&lt;-animate(b,width=480,height=480)\n\na_mgif&lt;-image_read(a_gif)\nb_mgif&lt;-image_read(b_gif)\n\nnew_gif&lt;-image_append(c(a_mgif[1], b_mgif[1]))\nfor(i in 2:100){\n  combined &lt;- image_append(c(a_mgif[i], b_mgif[i]))\n  new_gif&lt;-c(new_gif,combined)\n}\n\nimage_animate(new_gif, fps = 10,dispose=\"none\")"
  },
  {
    "objectID": "13-Gifs.html#distributions",
    "href": "13-Gifs.html#distributions",
    "title": "13  GIFs",
    "section": "13.4 Distributions",
    "text": "13.4 Distributions\n\n13.4.1 Normal changing mean\n\n\n\n\n\n\nsome_means&lt;-c(0,1,2,3,4,5,4,3,2,1)\nall_df&lt;-data.frame()\nfor(i in 1:10){\n  dnorm_vec &lt;- dnorm(seq(-10,10,.1),mean=some_means[i],sd=1)\n  x_range   &lt;- seq(-10,10,.1)\n  means &lt;- rep(some_means[i], length(x_range))\n  sims &lt;- rep(i, length(x_range))\n  t_df&lt;-data.frame(sims,means,x_range,dnorm_vec)\n  all_df&lt;-rbind(all_df,t_df)\n}\n\nggplot(all_df, aes(x=x_range,y=dnorm_vec))+\n  geom_line()+\n  theme_classic()+\n  ylab(\"probability density\")+\n  xlab(\"value\")+\n  ggtitle(\"Normal Distribution with changing Mean\")+\n   transition_states(\n    sims,\n    transition_length = 1,\n    state_length = 1\n  )\n  #enter_fade() + \n  #exit_shrink() +\n  #ease_aes('sine-in-out')\n\n\n\n13.4.2 Normal changing sd\n\n\n\n\n\n\nsome_sds&lt;-seq(0.5,5,.5)\nall_df&lt;-data.frame()\nfor(i in 1:10){\n  dnorm_vec &lt;- dnorm(seq(-10,10,.1),mean=0,sd=some_sds[i])\n  x_range   &lt;- seq(-10,10,.1)\n  sds &lt;- rep(some_sds[i], length(x_range))\n  sims &lt;- rep(i, length(x_range))\n  t_df&lt;-data.frame(sims,sds,x_range,dnorm_vec)\n  all_df&lt;-rbind(all_df,t_df)\n}\n\nlabs_df&lt;-data.frame(sims=1:10,\n                    sds=as.character(seq(0.5,5,.5)))\n\nggplot(all_df, aes(x=x_range,y=dnorm_vec, frame=sims))+\n  geom_line()+\n  theme_classic()+\n  ylab(\"probability density\")+\n  xlab(\"value\")+\n  ggtitle(\"Normal Distribution with changing sd\")+\n  geom_label(data = labs_df, aes(x = 5, y = .5, label = sds))+\n   transition_states(\n    sims,\n    transition_length = 2,\n    state_length = 1\n  )+\n  enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out')"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Adair, G. 1984. “The Hawthorne Effect: A\nReconsideration of the Methodological Artifact.” Journal of\nApplied Psychology 69: 334–45. https://doi.org/10.1037/0021-9010.69.2.334.\n\n\nBehmer, Lawrence P, and Matthew JC Crump. 2017. “Spatial Knowledge\nDuring Skilled Action Sequencing: Hierarchical Versus\nNonhierarchical Representations.” Attention, Perception,\n& Psychophysics 79 (8): 2435–48. https://doi.org/10.3758/s13414-017-1389-3.\n\n\nBickel, P. J., E. A. Hammel, and J. W. O’Connell. 1975. “Sex Bias\nin Graduate Admissions: Data from\nBerkeley.” Science (New York, N.Y.) 187:\n398–404. https://doi.org/10.1126/science.187.4175.398.\n\n\nCampbell, D. T., and J. C. Stanley. 1963. Experimental and\nQuasi-Experimental Designs for Research. Boston, MA:\nHoughton Mifflin.\n\n\nEvans, J. St. B. T., J. L. Barston, and P. Pollard. 1983. “On the\nConflict Between Logic and Belief in Syllogistic Reasoning.”\nMemory and Cognition 11: 295–306. https://doi.org/10.3758/BF03196976.\n\n\nFisher, R. A. 1922. “On the Mathematical Foundation of Theoretical\nStatistics.” Philosophical Transactions of the Royal Society\nA 222: 309–68.\n\n\nHothersall, D. 2004. History of Psychology.\nMcGraw-Hill.\n\n\nIoannidis, John P. A. 2005. “Why Most Published Research Findings\nAre False.” PLoS Medicine 2 (8): 697–701. https://doi.org/10.1371/journal.pmed.0020124.\n\n\nJames, Ella L, Michael B Bonsall, Laura Hoppitt, Elizabeth M Tunbridge,\nJohn R Geddes, Amy L Milton, and Emily A Holmes. 2015. “Computer\nGame Play Reduces Intrusive Memories of Experimental Trauma via\nReconsolidation-Update Mechanisms.” Psychological\nScience 26 (8): 1201–15. https://doi.org/10.1177/0956797615583071.\n\n\nKahneman, D., and A. Tversky. 1973. “On the Psychology of\nPrediction.” Psychological Review 80: 237–51. https://doi.org/10.1037/h0034747.\n\n\nKeynes, John Maynard. 1923. A Tract on Monetary Reform.\nLondon: Macmillan and Company.\n\n\nKühberger, A, A Fritz, and T. Scherndl. 2014. “Publication Bias in\nPsychology: A Diagnosis Based on the Correlation Between\nEffect Size and Sample Size.” Public Library of Science\nOne 9: 1–8.\n\n\nMatejka, Justin, and George Fitzmaurice. 2017. “Same Stats,\nDifferent Graphs: Generating Datasets with Varied Appearance and\nIdentical Statistics Through Simulated Annealing.” In\nProceedings of the 2017 CHI Conference on Human Factors\nin Computing Systems, 1290–94. ACM. https://doi.org/10.1145/3025453.3025912.\n\n\nMeehl, P. H. 1967. “Theory Testing in Psychology and Physics:\nA Methodological Paradox.” Philosophy of\nScience 34: 103–15. https://doi.org/10.1086/288135.\n\n\nMehr, Samuel A, Lee Ann Song, and Elizabeth S Spelke. 2016. “For\n5-Month-Old Infants, Melodies Are Social.” Psychological\nScience 27 (4): 486–501. https://doi.org/10.1177/0956797615626691.\n\n\nPfungst, O. 1911. Clever Hans (The Horse\nof Mr. Von Osten): A Contribution\nto Experimental Animal and Human Psychology. Translated by C. L.\nRahn. New York: Henry Holt.\n\n\nRosenbaum, David, Yaniv Mama, and Daniel Algom. 2017. “Stand by\nYour Stroop: Standing up Enhances Selective Attention and\nCognitive Control.” Psychological Science 28 (12):\n1864–67. https://doi.org/10.1177/0956797617721270.\n\n\nRosenthal, R. 1966. Experimenter Effects in Behavioral\nResearch. New York: Appleton.\n\n\nSalsburg, David. 2001. The Lady Tasting Tea: How\nStatistics Revolutionized Science in the Twentieth Century.\nMacmillan.\n\n\nStevens, S. S. 1946. “On the Theory of Scales of\nMeasurement.” Science (New York, N.Y.) 103: 677–80. https://doi.org/10.1126/science.103.2684.677.\n\n\nStudent, A. 1908. “The Probable Error of a Mean.”\nBiometrika 6: 1–2.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Respondiendo preguntas con datos",
    "section": "",
    "text": "Prefacio\nPrimera traducción al español (versión 0.1 = 7 Abril 2025) Compilado: r Sys.Date()\nTraducción de: https://crumplab.github.io/statistics/\nCita original: Crump, M. J. C., Navarro, D. J., & Suzuki, J. (2019, June 5). Answering Questions with Data (Textbook): Introductory Statistics for Psychology Students. https://doi.org/10.17605/OSF.IO/JZE52"
  },
  {
    "objectID": "index.html#notas-importantes",
    "href": "index.html#notas-importantes",
    "title": "Respondiendo preguntas con datos",
    "section": "Notas importantes",
    "text": "Notas importantes\nThis is a free textbook teaching introductory statistics for undergraduates in Psychology. This textbook is part of a larger OER course package for teaching undergraduate statistics in Psychology, including this textbook, a lab manual, and a course website. All of the materials are free and copiable, with source code maintained in Github repositories. The links below connect to various components of the project. The main OSF project link is https://osf.io/3s68c/.\n\nTextbook\n\nwebsite: https://crumplab.github.io/statistics/\nOSF: https://osf.io/jze52/\nGithub: https://github.com/CrumpLab/statistics\nDOI: 10.17605/OSF.IO/JZE52\n\n\n\nLab Manual\n\nwebsite: https://crumplab.github.io/statisticsLab/\nOSF: https://osf.io/m2npj/\nGithub: https://github.com/CrumpLab/statisticsLab\nDOI: 10.17605/OSF.IO/M2NPJ\n\n\n\nCourse website\n\nwebsite: https://crumplab.github.io/psyc3400/\nOSF: https://osf.io/awxu9/\nGithub: https://github.com/CrumpLab/psyc3400\nDOI: 10.17605/OSF.IO/AWXU9\n\nAll resources are released under a creative commons licence CC BY-SA 4.0. Click the link to read more about the license, or read more below in the license section.\n\n\nContributors\nTraducción: ChatGPT-4o. Supervisión: Álvaro Cabana\nTeam members contributing new content include, Matthew Crump, Alla Chavarga, Anjali Krishnan, Jeffrey Suzuki, and Stephen Volz. This textbook was primarily written by Matthew J. C. Crump. Jeff contributed the YouTube videos peppered throughout the textbook. All of Jeff’s statistics videos are available on his Youtube channel: Statistics Video playlist.\nAlla, Anjali, and Stephen wrote the lab manual exercises for SPSS, JAMOVI, and Excel. Matt Crump wrote the lab manual exercises for R.\nMatt Crump wrote a free and copiable course website, in R Markdown. The course website also contains slide decks for the lectures.\n\n\nAttributions\nTwo of the chapters were adapted from Danielle Navarro’s wonderful (and bigger) free textbook, also licensed under the same creative commons license. The citation for that textbook is: Navarro, D. (2018). Learning statistics with R: A tutorial for psychology students and other beginners (version 0.6). The website is https://compcogscisydney.org/learning-statistics-with-r/\nChapter notes within the book are provided to indicate sections where material from Navarro was included. A short summary is here\nChapter 1: Why statistics, Adapted nearly verbatim with some editorial changes from Chapters 1 and 2, Navarro, D.\nChapter 4: Probability, Sampling, and Estimation, Adapted and expanded from Chapters 9 and 10, Navarro D.\n\n\nCC BY-SA 4.0 license\nThis license means that you are free to:\n\nShare: copy and redistribute the material in any medium or format\nAdapt: remix, transform, and build upon the material for any purpose, even commercially.\n\nThe licensor cannot revoke these freedoms as long as you follow the license terms.\nUnder the following terms:\n\nAttribution: You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.\nShareAlike: If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.\nNo additional restrictions: You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits."
  },
  {
    "objectID": "index.html#copying-the-textbook",
    "href": "index.html#copying-the-textbook",
    "title": "Respondiendo preguntas con datos",
    "section": "Copying the textbook",
    "text": "Copying the textbook\nThis textbook was written in R-Studio, using R Markdown, and compiled into a web-book format using the bookdown package. In general, I thank the larger R community for all of the amazing tools they made, and for making those tools open, so that I could use them to make this thing.\nAll of the source code for compiling the book is available in the GitHub repository for this book:\nhttps://github.com/CrumpLab/statistics\nIn principle, anybody could fork or otherwise download this repository. Load the Rproj file in R-studio, and then compile the entire book. Then, the individual .rmd files for each chapter could be edited for content and style to better suit your needs.\nIf you want to contribute to this version of the textbook, you could make pull requests on GitHub, or discuss issues and request on the issues tab.\n\nAcknowledgments\nThanks to the librarians at Brooklyn College of CUNY, especially Miriam Deutch, and Emily Fairey, for their support throughout the process. Thanks to CUNY for supporting OER development, and for the grant we received to develop this work. Thanks to Jenn Richler for letting me talk about statistics all summer long.\n\n\nWhy we did this\nWhy write another statistics textbook, aren’t there already loads of those? Yes, there are. We had a couple reasons. First, we would like to make R more accessible for the undergraduate population, and we wrote this textbook around the capabilities of R. The textbook was written entirely in R-Studio, and most of the examples have associated R-code. R is not much of a focus in the textbook, but there is an introduction to using R to solve data-analysis problems in the lab manual. Many instructors still use SPSS, Excel, or newer free GUIs like JAMOVI, so we also made lab exercises for each of those as well.\nThis is a mildly opinionated, non-traditional introduction to statistics. It acknowledges some of the major ideas from traditional frequentist approaches, and some Bayesian approaches. Much of the conceptual foundation is rooted in simulations that can be conducted in R. We use some formulas, but mostly explain things without formulas. The textbook was written with math-phobia in mind, and attempts to reduce the phobia associated with arithmetic computations. There are many things missing that should probably be added. We will do our best to add necessary things as we update the textbook."
  },
  {
    "objectID": "02-Describing_Data.html",
    "href": "02-Describing_Data.html",
    "title": "2  Describir los datos",
    "section": "",
    "text": "2.1 Así se ve un exceso de números\nSupongamos que querés saber cuán feliz se siente la gente. Entonces salís a la calle y le preguntás a miles de personas cuán felices están. Les permitís elegir cualquier número, desde menos infinito hasta más infinito. Registrás todos esos números. ¿Y ahora?\nBueno, podrías mirar esos números y ver si eso te ayuda a entender algo sobre la felicidad de las personas. ¿Cómo podrían verse esos números? Quizás algo así:\n492\n165\n-150\n364\n-332\n935\n-228\n-116\n-449\n1422\n\n\n78\n9\n195\n958\n380\n328\n503\n-549\n353\n174\n\n\n-290\n173\n1167\n-809\n-379\n-221\n174\n1071\n134\n229\n\n\n623\n538\n803\n165\n509\n188\n296\n1178\n-114\n66\n\n\n114\n484\n453\n71\n-1100\n-268\n-768\n-13\n-580\n265\n\n\n440\n367\n1016\n323\n538\n116\n-138\n299\n-792\n768\n\n\n68\n-681\n133\n136\n92\n801\n339\n-732\n-402\n-131\n\n\n-141\n-216\n-1108\n915\n303\n531\n-1128\n-65\n598\n-215\n\n\n-332\n96\n560\n-355\n-523\n426\n-79\n-90\n289\n426\n\n\n-647\n-1219\n-1088\n690\n-360\n231\n863\n447\n-309\n-248\n\n\n1826\n1035\n243\n580\n362\n292\n505\n-40\n-82\n833\n\n\n-81\n-759\n861\n-456\n58\n571\n-225\n-769\n-199\n166\n\n\n36\n281\n-339\n250\n403\n582\n652\n33\n495\n-80\n\n\n39\n-89\n511\n97\n-44\n-94\n345\n-6\n20\n-292\n\n\n665\n241\n-423\n171\n424\n181\n-739\n348\n581\n33\n\n\n258\n-312\n227\n702\n455\n-21\n-672\n1083\n-724\n392\n\n\n-125\n534\n-424\n-230\n-178\n173\n1026\n398\n528\n-710\n\n\n-896\n-426\n-514\n45\n185\n100\n1102\n-46\n185\n802\n\n\n-405\n-25\n-316\n258\n-202\n734\n1301\n-464\n-1020\n-176\n\n\n307\n-612\n1004\n309\n-14\n-195\n661\n-416\n1386\n398\n\n\n-303\n551\n113\n-66\n-117\n234\n445\n-566\n130\n52\n\n\n-652\n63\n732\n-219\n68\n242\n-828\n-573\n-380\n157\n\n\n403\n917\n-548\n-684\n-366\n1119\n-22\n-299\n305\n-537\n\n\n-261\n446\n142\n385\n552\n-498\n-570\n-212\n-90\n-280\n\n\n864\n-908\n367\n181\n-236\n387\n543\n234\n-287\n416\n\n\n671\n429\n-361\n236\n239\n-205\n-231\n207\n-429\n-248\n\n\n662\n-535\n32\n-410\n-938\n-577\n-170\n647\n-967\n-1385\n\n\n-178\n511\n428\n-102\n851\n294\n-93\n470\n-494\n637\n\n\n736\n506\n44\n93\n-609\n45\n578\n-699\n-504\n407\n\n\n-332\n-242\n-190\n-674\n514\n552\n-143\n-355\n315\n430\n\n\n295\n719\n1084\n372\n307\n-20\n631\n-147\n810\n351\n\n\n778\n204\n633\n-392\n689\n-34\n339\n-610\n155\n145\n\n\n308\n-215\n-455\n456\n687\n39\n151\n940\n-116\n265\n\n\n493\n136\n-1\n252\n-313\n454\n309\n-1191\n606\n-729\n\n\n38\n740\n208\n726\n-376\n1177\n-402\n655\n-141\n512\n\n\n979\n625\n230\n637\n-326\n395\n-40\n345\n-472\n96\n\n\n-693\n38\n887\n441\n-1037\n-212\n-19\n-462\n55\n-40\n\n\n-208\n-394\n267\n653\n-612\n-214\n592\n317\n559\n6\n\n\n0\n243\n52\n518\n76\n-71\n361\n62\n-271\n-226\n\n\n-462\n634\n359\n-345\n733\n-414\n-310\n247\n269\n-165\n\n\n90\n34\n413\n-53\n346\n709\n624\n-70\n-20\n311\n\n\n-464\n199\n68\n-61\n369\n-44\n341\n174\n0\n276\n\n\n187\n538\n-108\n379\n-144\n-332\n58\n275\n676\n117\n\n\n425\n275\n-126\n667\n376\n377\n-118\n-661\n-622\n1666\n\n\n-268\n1425\n86\n432\n94\n300\n231\n371\n-450\n303\n\n\n-206\n655\n-647\n-116\n576\n205\n-82\n541\n118\n670\n\n\n-265\n29\n25\n-237\n-243\n9\n875\n-896\n1599\n594\n\n\n180\n65\n584\n-215\n881\n804\n-303\n204\n196\n29\n\n\n317\n762\n659\n930\n248\n-157\n112\n692\n-261\n-743\n\n\n-273\n-672\n-478\n1387\n-383\n1189\n929\n-203\n43\n141\n¿Y qué vas a hacer con esa montaña de números? ¿Mirarlos todo el día? Cuando trabajás con datos, te vas a encontrar con tantos números que te van a abrumar. Por eso necesitamos formas de describirlos de una manera más manejable.\nLa descripción completa de los datos siempre son los datos mismos. Pero las estadísticas descriptivas y otras herramientas de resumen nos permiten ir un paso más allá: comprimir y simplificar la información para entenderla mejor. Resumir los datos es como comprimir las partes importantes de algo en una versión útil y manejable. Es como contarle a una amiga por qué debería ver una película: no le contás toda la película, sino que le marcás los momentos clave. Resumir los datos es como el adelanto de una película, pero aplicado a los datos.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Describir los datos</span>"
    ]
  },
  {
    "objectID": "02-Describing_Data.html#mirar-los-datos",
    "href": "02-Describing_Data.html#mirar-los-datos",
    "title": "2  Describir los datos",
    "section": "2.2 Mirar los datos",
    "text": "2.2 Mirar los datos\nYa intentamos una forma de mirar los números, y no fue muy útil. Probemos ahora con otras maneras de ver los datos, usando gráficos.\n\n2.2.1 ¡Pará! Es hora de graficar!\nConvirtamos todos los números en puntos, y mostremos esos puntos en un gráfico. Ojo: cuando hacemos esto, todavía no hemos resumido nada de los datos. Simplemente los estamos observando en formato visual, en vez de mirar los números directamente.\n\n\n\n\n\n\n\n\nFigura 2.1: Supuestas puntuaciones de felicidad de 500 personas\n\n\n\n\n\nLa Figura 2.1 muestra 500 mediciones de felicidad. El gráfico tiene dos ejes. El eje horizontal x, que va de izquierda a derecha, está etiquetado como “Índice”. El eje vertical y, que va de abajo hacia arriba, está etiquetado como “felicidad”. Cada punto representa una medición de felicidad de cada persona en nuestro estudio ficticio.\nAntes de hablar sobre lo que podemos y no podemos ver en los datos, vale la pena mencionar que la forma en que los graficás puede hacer que algunas cosas sean más fáciles de ver y otras más difíciles. Entonces, ¿qué podemos ver ahora en los datos?\nHay muchos puntos por todos lados. Parece que hay 500, porque el índice llega hasta 500. Algunos puntos parecen llegar tan alto como 1000–1500, y tan bajo como -1500. Da la impresión de que hay más puntos en la zona central del gráfico, más o menos alrededor del 0.\n\nConclusión: podemos ver todos los números a la vez al ponerlos en un gráfico, y eso es mucho más fácil y útil que mirar los números crudos.\n\nBien, si estos puntos representan cuán felices están 500 personas, ¿qué podemos decir sobre esa gente? Primero, los puntos están bastante dispersos, así que distintas personas tienen niveles de felicidad diferentes.¿Hay más personas felices que infelices, o al revés? Eso no se ve claramente en el gráfico, así que hagamos otro tipo de gráfico, llamado histograma.\n\n\n2.2.2 Histogramas\nHacer un histograma será nuestro primer acto de resumir oficialmente algo sobre los datos. Ya no vamos a mirar cada punto individualmente, sino que vamos a ver cómo se agrupan los números. Mirá Figura 2.2, un histograma de los datos de felicidad, y después lo explicamos.\n\n\n\n\n\n\n\n\nFigura 2.2: Un histograma de las puntuaciones de felicidad\n\n\n\n\n\nLos puntos desaparecieron, y ahora vemos unas barras. Cada barra resume varios puntos: representa cuántos puntos (frecuencia) caen dentro de un determinado rango de felicidad, también llamado bin o intervalo. Por ejemplo, ¿cuántas personas dieron una puntuación de felicidad entre 0 y 500? La quinta barra —la que va de 0 a 500 en el eje x— te lo dice. Mirá qué alta es esa barra. ¿Cuánto mide? La altura se muestra en el eje y, que indica la frecuencia (el número de puntos o valores). Parece que unas 150 personas dijeron que su felicidad estaba entre 0 y 500.\nEn general, vemos que hay muchos bins en el eje x. Dividimos los datos en intervalos de 500. El bin #1 va de -2000 a -1500, el bin #2 de -1500 a -1000, y así sucesivamente hasta el último intervalo. Para hacer el histograma, simplemente contamos cuántos valores caen en cada intervalo, y graficamos esas frecuencias como función de los bins. Voilà, un histograma.\n¿Qué nos ayuda a ver el histograma sobre los datos? Primero, podemos ver la forma de los datos. La forma del histograma se refiere a cómo sube y baja. Nos dice dónde están concentrados los datos. Por ejemplo, cuando las barras son bajas, sabemos que no hay muchos datos ahí. Cuando son altas, sabemos que hay más. Entonces, ¿dónde están la mayoría de los datos? Parece que están sobre todo en los dos bins del medio, entre -500 y 500. También podemos ver el rango de los datos, o sea los mínimos y máximos. La mayoría de los datos están entre -1500 y +1500, así que no hay felicidad ni tristeza infinitas en este conjunto de datos.\nCuando hacés un histograma, podés elegir qué tan ancho será cada intervalo. Por ejemplo, abajo hay cuatro histogramas diferentes del mismo conjunto de datos de felicidad. Lo que cambia es el ancho de los intervalos.\n\n\n\n\n\n\n\n\nFigura 2.3: Cuatro histogramas del mismo conjunto de datos usando diferentes anchos de bin\n\n\n\n\n\nTodos los histogramas tienen aproximadamente la misma forma general: de izquierda a derecha, las barras arrancan bajas, suben, y después bajan otra vez. En otras palabras, a medida que los números se acercan al cero, tienden a aparecer con más frecuencia. Este patrón general se ve en todos los histogramas. Pero algunos detalles del patrón se pierden cuando las barras son demasiado angostas. Por ejemplo, aunque en general las barras se hacen más altas al ir de -1000 a 0, hay algunas excepciones y las alturas fluctúan un poco. Cuando las barras son más anchas, hay menos excepciones al patrón general. ¿Qué tan ancho o angosto debe ser tu histograma? Es como la historia de Ricitos de Oro: tiene que ser justo el indicado para tus datos.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Describir los datos</span>"
    ]
  },
  {
    "objectID": "02-Describing_Data.html#ideas-importantes-distribución-tendencia-central-y-varianza",
    "href": "02-Describing_Data.html#ideas-importantes-distribución-tendencia-central-y-varianza",
    "title": "2  Describir los datos",
    "section": "2.3 Ideas importantes: distribución, tendencia central y varianza",
    "text": "2.3 Ideas importantes: distribución, tendencia central y varianza\nVamos a presentar tres términos importantes que vamos a usar mucho: distribución, tendencia central y varianza. Estos términos son parecidos a su uso cotidiano (aunque sospecho que la mayoría no dice “tendencia central” muy seguido).\nDistribución. Cuando pedís algo por internet, ¿de dónde viene y cómo llega a tu casa? Eso viene de uno de los centros de distribución de Amazon. Ellos distribuyen todo tipo de cosas repartiéndolas por todos lados. “Distribuir” significa eso: esparcir algo. Notá que los datos en el histograma están distribuidos, o sea, están repartidos en distintos intervalos. También podemos hablar de una distribución como sustantivo. El histograma es una distribución de las frecuencias a lo largo de los bins. Las distribuciones son muy, muy, muy, muy, muy importantes. Pueden tener muchas formas distintas. Pueden describir datos, como en el histograma que vimos. Y, como vamos a ver más adelante, también pueden generar datos. Muchas veces nos vamos a preguntar de dónde vienen nuestros datos, y eso suele significar preguntarse qué tipo de distribución podría haber generado esos datos (más sobre eso más adelante).\nTendencia central tiene que ver con lo común: ¿Qué tienen en común ciertos números? Por ejemplo, ¿hay algo en común en los números del histograma? Sí, podemos decir que la mayoría están cerca del 0. Hay una tendencia de muchos de esos números a estar centrados cerca del 0. Fijate que estamos siendo cautelosos: no estamos diciendo que todos sean 0, sino que hay una tendencia a estar cerca de 0. Hay muchas formas de hablar de la tendencia central de un conjunto de números. Incluso puede haber más de una. Por ejemplo, si muchos de los números estuvieran alrededor de -1000, y otro grupo similar estuviera alrededor de 1000, podríamos decir que hay dos tendencias.\nVarianza tiene que ver con las diferencias: ¿Qué tan distintos son los números? Por ejemplo, ¿hay algo diferente entre los números del histograma? ¡SÍ! ¡No son todos iguales! Si los números no son todos iguales, es porque varían. Entonces, la varianza se refiere a cuánto difieren entre sí los números. Hay muchas formas de resumir la cantidad de varianza en un conjunto, y ya vamos a ver eso dentro de poco.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Describir los datos</span>"
    ]
  },
  {
    "objectID": "02-Describing_Data.html#medidas-de-tendencia-central-lo-común",
    "href": "02-Describing_Data.html#medidas-de-tendencia-central-lo-común",
    "title": "2  Describir los datos",
    "section": "2.4 Medidas de tendencia central (lo común)",
    "text": "2.4 Medidas de tendencia central (lo común)\nYa vimos que podemos hacernos una idea de los datos graficando puntos o haciendo un histograma. Estas herramientas nos muestran cómo se ven los números, cuán grandes o pequeños son, y qué tan parecidos o diferentes son entre sí. Está bueno tener esa intuición. Pero esas percepciones visuales no son muy precisas. Además de resumir los datos con gráficos, podemos resumirlos con números (¡NO, por favor, más números no! Bueno, prometemos que pueden ser tus amigos).\n\n2.4.1 De muchos números a uno solo\nLas medidas de tendencia central tienen un objetivo importante: reducir un montón de números a un solo número que podamos mirar. Ya sabemos que mirar miles de números no sirve de mucho. ¿No estaría bueno poder mirar uno solo? Nosotros creemos que sí. Resulta que hay muchas formas de hacerlo. Así, si alguien alguna vez te hace la temida pregunta: “che, ¿cómo son todos estos números?”, podés responder: “son más o menos como este número que ves acá”.\nPero, al igual que en Indiana Jones y la Última Cruzada (gran película, recomendada), tenés que elegir sabiamente tu medida de tendencia central.\n\n\n2.4.2 Moda\nLa moda es el número que aparece con más frecuencia en tu conjunto de datos. Eso es todo. ¿Cómo se calcula? Tenés que contar cuántas veces aparece cada número, y el que más veces aparece, esa es la moda.\n\nEjemplo: 1 1 1 2 3 4 5 6\nLa moda de este conjunto es 1, que aparece tres veces. Los demás números aparecen solo una vez.\n\nBueno, perfecto. ¿Y qué pasa acá?\n\nEjemplo: 1 1 1 2 2 2 3 4 5 6\nAhora tanto 1 como 2 aparecen tres veces. ¿Qué hacemos? Decimos que hay dos modas: 1 y 2.\n\n¿Por qué la moda es una medida de tendencia central? Bueno, cuando preguntamos “¿cómo son mis números?”, podemos decir: “la mayoría son como un 1” (o lo que sea que sea la moda).\n¿Es una buena medida de tendencia central? Depende de los números. Por ejemplo, considerá estos:\n\n1 1 2 3 4 5 6 7 8\n\nAcá, la moda vuelve a ser 1, porque aparece dos veces y los demás números una sola vez. Pero, ¿la mayoría de los números son “como un 1”? No, la mayoría no lo son.\n“¡Argh! Entonces, ¿debería o no debería usar la moda? Pensé que esta clase me iba a decir qué hacer…”. No, nadie te va a decir qué hacer. Cada vez que uses una herramienta estadística vas a tener que pensar qué estás haciendo y justificar por qué tiene sentido. Lo siento.\n\n\n2.4.3 Mediana\nLa mediana es el punto exacto del medio en el conjunto de datos. Después de todo, estamos preguntándonos sobre la tendencia central, así que, ¿por qué no ir al centro del conjunto de datos a ver qué hay ahí? ¿Qué significa “el medio”? Miremos estos números:\n\n1 5 4 3 6 7 9\n\nMmm, ok. ¿Entonces el 3 está en el medio? Parece un poco arbitrario. Y sí. Antes de calcular la mediana, hay que ordenar los números de menor a mayor:\n\n1 3 4 5 6 7 9\n\nAhora el 5 está en el medio. Y por “medio” nos referimos literalmente al medio. Hay tres números a la izquierda del 5, y tres a la derecha. Así que sí, el 5 está justo en el centro.\nBueno, pero ¿qué pasa si hay una cantidad par de números? Entonces el medio no va a estar en el conjunto, ¿no? Veamos:\n\n1 2 3 4 5 6\n\nAcá no hay ningún número entre el 3 y el 4; el centro está vacío. En este caso, calculamos la mediana sacando el número que está exactamente entre 3 y 4. Así que la mediana es 3.5.\n¿Es la mediana una buena medida de tendencia central? Sí, muchas veces es muy útil. Una propiedad interesante de la mediana es que se mantiene en el medio incluso cuando algunos números se desvían mucho. Por ejemplo:\n\n1 2 3 4 4 4 5 6 6 6 7 7 1000\n\nLa mayoría de estos números son más o menos pequeños, pero 1000 es un número grande y raro, muy distinto del resto. La mediana sigue siendo 5, porque está en el medio de los números ordenados. Y además, el 5 es bastante parecido a la mayoría de los valores (salvo 1000). Así que la mediana representa bastante bien al conjunto, incluso cuando hay uno o dos valores muy distintos.\nPor último, el término valor atípico (o outlier) lo usamos para describir esos números que son muy diferentes al resto. En este ejemplo, 1000 es un outlier, porque está muy alejado en la línea numérica comparado con los otros. Qué hacer con los outliers es otro tema del que hablaremos varias veces en este curso.\n\n\n2.4.4 Media\n¿Notaste que este libro de estadística todavía no usó una fórmula? Bueno, eso está por cambiar. Pero si te dan ansiedad las fórmulas, tranqui: vamos a explicarlas bien.\nLa media también se llama promedio. Y probablemente ya sepas qué es: se suman todos los números y se divide por la cantidad total, ¿no? ¿Cómo se escribe eso como fórmula? Así:\n\\(\\text{Media} = \\bar{X} = \\frac{\\sum_{i=1}^{n} x_{i}}{N}\\)\n“Eso me suena a griego”. Y sí. El símbolo \\(\\sum\\) se llama sigma y representa la operación de sumar. La “i” abajo y la “n” arriba indican que hay que sumar todos los números del conjunto, desde el primero (i) hasta el último (n). Las letras son etiquetas arbitrarias, llamadas variables, que usamos para describir. El \\(x_i\\) se refiere a cada número del conjunto. Sumamos todo, y dividimos por \\(N\\), que es la cantidad total de valores. A veces usamos \\(\\bar{X}\\) para referirnos a la media.\nEn lenguaje común, la fórmula sería algo así:\n\\(\\text{Media} = \\frac{\\text{Suma de mis números}}{\\text{Cantidad de números}}\\)\n“¿Y por qué no dijiste eso desde el principio?”. Bueno… lo acabamos de decir.\nCalculemos la media de estos cinco números:\n\n3 7 9 2 6\n\nSumamos:\n\n3 + 7 + 9 + 2 + 6 = 27\n\nContamos:\n\n\\(i_1 = 3\\), \\(i_2 = 7\\), \\(i_3 = 9\\), \\(i_4 = 2\\), \\(i_5 = 6\\)\n\\(N = 5\\), porque \\(i\\) va de 1 a 5\n\nDividimos:\n\nmedia = 27 / 5 = 5.4\n\nSi ponemos los números en la fórmula:\n\\(\\text{Media} = \\bar{X} = \\frac{\\sum_{i=1}^{n} x_{i}}{N} = \\frac{3+7+9+2+6}{5} = \\frac{27}{5} = 5.4\\)\nBueno, así se calcula la media. Pero, como imaginábamos, probablemente ya sabías eso, y si no, ahora lo sabés. ¿Y ahora qué?\n¿Es la media una buena medida de tendencia central? A esta altura ya deberías saber: depende.\n\n\n2.4.5 ¿Qué significa realmente la media?\nNo alcanza con saber la fórmula de la media, ni con poder calcularla para un conjunto de números. Confiamos en tu capacidad de sumar y dividir. Lo que realmente importa es entender qué significa la media. Tenés que saber qué hace la media, no solo cómo se hace. ¿Te desconcertó? Te lo explicamos.\n¿Podés responder esta pregunta?: ¿Qué pasa cuando dividís una suma de números por la cantidad de números? ¿Cuáles son las consecuencias de hacer eso? ¿Qué hace exactamente esa fórmula? ¿Qué propiedades tiene el resultado? Ojo: la respuesta no es simplemente “da la media”.\nOk, ¿qué pasa cuando dividís cualquier número por otro? La palabra clave es “dividir”. Literalmente, estamos cortando el número de arriba (el numerador) en partes. ¿En cuántas partes? Depende del número de abajo (el denominador). Mirá:\n\\(\\frac{12}{3} = 4\\)\nYa sabés que el resultado es 4. Pero, ¿qué está pasando en realidad? Estamos cortando el 12 en tres partes. Y resulta que esas partes miden 4. Entonces, podemos pensar el 12 como 3 pedazos iguales: (12 = 4 + 4 + 4). ¿Y qué propiedad tienen esos pedazos? Son todos iguales. Sí. La división iguala el numerador en partes iguales.\n“Ehh… creo que esto lo aprendí en la escuela. ¿Qué tiene que ver con la media?”. Bueno, el número de arriba en la fórmula de la media es justamente otro numerador dividido por un denominador. En este caso, el numerador es la suma de todos los valores. ¿Y si fuera la suma de las 500 puntuaciones de felicidad? Esa suma sería un solo número. Si dividimos esa suma en partes iguales, una para cada persona, ¿qué obtenemos? Obtendríamos 500 valores iguales, uno por persona. Sería como tomar toda la felicidad del mundo, dividirla en partes iguales, y después devolverle a cada persona exactamente la misma cantidad. A algunas personas les tocaría más de lo que tenían antes, a otras menos. Porque estaríamos igualando la distribución de felicidad. Ese proceso de dividir en partes iguales es lo que hace la media. ¿Ves? Es más que una fórmula. Es una idea. Y esta es solo la primera de muchas ideas que vamos a ir explorando. Vamos a volver a hablar de esto más adelante.\n\nConsejo útil: La media es el único número que puede reemplazar a todos los valores de un conjunto, de manera que si sumás esos valores iguales, obtenés la misma suma original de los datos.\n\n\n\n2.4.6 Recapitulando\nPara repasar la moda, la mediana y la media, mirá el siguiente histograma en Figura 2.4. Indicamos la ubicación de la media (rojo), la mediana (verde) y la moda (azul). Para este conjunto de datos, las tres medidas de tendencia central dan valores diferentes. La media es la más alta porque se ve influida por valores grandes, incluso si aparecen pocas veces. La moda y la mediana no se ven afectadas por esos valores extremos, por eso tienen valores más bajos.\n\n\n\n\n\n\n\n\nFigura 2.4: Un histograma con la media (rojo), la mediana (verde) y la moda (azul)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Describir los datos</span>"
    ]
  },
  {
    "objectID": "02-Describing_Data.html#medidas-de-variación-lo-diferente",
    "href": "02-Describing_Data.html#medidas-de-variación-lo-diferente",
    "title": "2  Describir los datos",
    "section": "2.5 Medidas de variación (lo diferente)",
    "text": "2.5 Medidas de variación (lo diferente)\n¿Qué hacías cuando tenías que escribir ensayos en el liceo sobre un libro que habías leído? Probablemente comparabas y contrastabas cosas, ¿no? Bueno, cuando resumís datos, hacés lo mismo. Las medidas de tendencia central nos dan algo parecido a comparar: nos dicen qué tienen en común los datos. Las medidas de variación nos dan algo parecido a contrastar: nos dicen en qué se diferencian.\nPrimero, notamos que cuando ves un montón de números distintos, ya sabés que hay diferencias. Eso quiere decir que los números varían, y hay variación en su magnitud.\n\n2.5.1 El rango\nConsiderá estos 10 números, que ya están ordenados de menor a mayor:\n\n1 3 4 5 5 6 7 8 9 24\n\nEstos números varían, porque no son todos iguales. Podemos usar el rango para describir la amplitud de esa variación. El rango se refiere al mínimo (el valor más bajo) y al máximo (el valor más alto) del conjunto. Así que el rango sería 1 y 24.\nEl rango es una forma rápida de resumir los límites de los datos con solo dos números. Al calcular el rango, sabemos que ningún valor es más chico ni más grande que esos extremos. Además, puede alertarte sobre valores atípicos. Por ejemplo, si esperás que tus números estén entre 1 y 7, pero el rango es 1–340.500, entonces sabés que hay algunos valores enormes que no deberían estar ahí. Eso te permite investigar por qué aparecieron (y posiblemente eliminarlos si hubo algún error).\n\n\n2.5.2 Las diferencias\nEstaría bueno poder resumir cuánto difieren los datos entre sí. ¿Por qué? Bueno, si pensás que los datos crudos (muchos números) ya son difíciles de mirar, vas a entrar en pánico al ver cuántas diferencias hay entre todos esos valores. Por ejemplo, estos 10 números son fáciles de ver:\n\n1 3 4 5 5 6 7 8 9 24\n\nPero, ¿qué pasa con las diferencias entre ellos? ¿Cómo se ven esas diferencias? Podemos calcular todas las diferencias entre cada par de números y ponerlas en una matriz como esta:\n\n\n\n\n\n\n1\n3\n4\n5\n5\n6\n7\n8\n9\n24\n\n\n\n\n1\n0\n2\n3\n4\n4\n5\n6\n7\n8\n23\n\n\n3\n-2\n0\n1\n2\n2\n3\n4\n5\n6\n21\n\n\n4\n-3\n-1\n0\n1\n1\n2\n3\n4\n5\n20\n\n\n5\n-4\n-2\n-1\n0\n0\n1\n2\n3\n4\n19\n\n\n5\n-4\n-2\n-1\n0\n0\n1\n2\n3\n4\n19\n\n\n6\n-5\n-3\n-2\n-1\n-1\n0\n1\n2\n3\n18\n\n\n7\n-6\n-4\n-3\n-2\n-2\n-1\n0\n1\n2\n17\n\n\n8\n-7\n-5\n-4\n-3\n-3\n-2\n-1\n0\n1\n16\n\n\n9\n-8\n-6\n-5\n-4\n-4\n-3\n-2\n-1\n0\n15\n\n\n24\n-23\n-21\n-20\n-19\n-19\n-18\n-17\n-16\n-15\n0\n\n\n\n\n\nEstamos viendo todas las diferencias posibles entre cada número y los demás. Por ejemplo, en la esquina superior izquierda, la diferencia entre 1 y sí mismo es 0. Una columna a la derecha, la diferencia entre 3 y 1 (3–1) es 2, etc. Como ves, es una matriz de 10x10, o sea, 100 diferencias. No está mal… pero si tuviéramos 500 números, serían 500×500 = 250.000 diferencias para mirar (dale, si te divierte ese tipo de cosas).\nPausa para una pregunta simple: ¿cómo se vería esta matriz si los 10 números fueran todos iguales? Debería estar llena de ceros, ¿no? Exacto. En ese caso, podríamos ver fácilmente que no hay variación.\nPero cuando los números son distintos, terminamos con una matriz gigante de diferencias. ¿Cómo la resumimos? Bueno, podríamos aplicar lo que aprendimos antes sobre tendencia central. Tenemos muchas diferencias, así que podríamos preguntarnos: ¿cuál es la diferencia promedio? ¿Y si simplemente calculamos la media de todas esas diferencias? ¿Qué pensás que pasaría?\nProbemos con estos tres números:\n\n1 2 3\n\n\n\n\n\n\n\n1\n2\n3\n\n\n\n\n1\n0\n1\n2\n\n\n2\n-1\n0\n1\n\n\n3\n-2\n-1\n0\n\n\n\n\n\nProbablemente ya te imaginás lo que va a pasar. Calculemos la media:\n\\(\\text{media de las diferencias} = \\frac{0+1+2-1+0+1-2-1+0}{9} = \\frac{0}{9} = 0\\)\nUps. La media de las diferencias da cero. Esto siempre va a pasar cuando calculás la media de todas las diferencias. Pero claramente hay diferencias entre los números, así que usar 0 como resumen de la variación no tiene mucho sentido.\nAdemás, capaz ya te diste cuenta de que estas matrices de diferencias son redundantes. La diagonal siempre tiene ceros, y los valores a un lado de la diagonal son iguales a los del otro lado, pero con el signo cambiado. Esa es una de las razones por las que la suma de las diferencias da cero.\nEstos pequeños problemas se pueden resolver con el uso de la varianza y la desviación estándar. Por ahora, podés pensar que la desviación estándar es un truco para evitar obtener cero. Más adelante vas a ver que tiene propiedades importantes por otras razones.\n\n\n2.5.3 La varianza\nVariabilidad, variación, varianza, variar, variable, variado, variedad. ¿Confundido? Antes de describir la varianza, queremos que te sientas cómodo con cómo se usa esta palabra. Primero, no te olvides de lo importante: sabemos que variabilidad y variación se refieren a la gran idea de que los números pueden ser diferentes. Podemos usar “varianza” en ese mismo sentido. Cuando los números son distintos, tienen varianza.\n\n\n\n\n\n\nNota\n\n\n\nLas fórmulas para la varianza y la desviación estándar dependen de si creés que tus datos representan toda una población, o si son solo una muestra. Vamos a discutir esto más adelante. Por ahora, dividimos por (N); más adelante veremos por qué a veces conviene dividir por (N-1).\n\n\nLa palabra varianza también se refiere a una estadística específica: la suma de las desviaciones al cuadrado respecto de la media. ¿Qué? Tranquilo. En lenguaje claro: la varianza es la suma de los cuadrados de las diferencias entre cada valor y la media. ¿Cuáles son esos valores? Los del conjunto de datos. Primero veamos la fórmula en lenguaje común:\n\\(\\text{varianza} = \\frac{\\text{suma de las diferencias al cuadrado}}{\\text{cantidad de valores}}\\)\n\n2.5.3.1 Desviaciones respecto de la media\nAntes nos complicamos un poco calculando todas las diferencias entre los valores del conjunto. Ahora hagámoslo de una manera más simple. Esta vez, calculamos la diferencia entre cada valor y la media. La idea es:\n\nPodemos saber qué tan similares son nuestros valores calculando la media.\n\nLuego podemos ver qué tan diferentes son respecto de esa media.\n\nEsto nos dice: 1) si nuestros datos están más o menos cerca de la media (lo que ayuda a saber si la media representa bien al conjunto), y 2) cuánta variación hay entre los números.\nMirá esta tabla:\n\n\n\n\n\nmedidas\nvalores\nmedia\nDiferencia_a_la_media\n\n\n\n\n1\n1\n4.5\n-3.5\n\n\n2\n6\n4.5\n1.5\n\n\n3\n4\n4.5\n-0.5\n\n\n4\n2\n4.5\n-2.5\n\n\n5\n6\n4.5\n1.5\n\n\n6\n8\n4.5\n3.5\n\n\nSumas\n27\n27\n0\n\n\nMedias\n4.5\n4.5\n0\n\n\n\n\n\nLa primera columna muestra que hay 6 valores. La columna valores muestra cada uno. La suma y la media aparecen en las dos últimas filas. Se calcularon así:\n\\(\\frac{1+6+4+2+6+8}{6} = \\frac{27}{6} = 4,5\\).\nLa tercera columna, medias, parece medio ridícula: estamos repitiendo la media una y otra vez. Pero si recordás lo que dijimos antes sobre el significado de la media, vas a ver que esto tiene sentido: distribuye la suma total por igual entre todos los valores. Si tratamos cada valor como si fuera la media, todos serían 4,5. Y si sumamos todos esos 4,5 obtenemos 27, que es la suma original de los valores. Y si sacamos la media de esas medias, también nos da 4,5 otra vez.\nToda la acción está en la cuarta columna, Diferencia_a_la_Media. Ahí estamos mostrando las diferencias respecto de la media, usando \\(X_{i}-\\bar{X}\\). Es decir, a cada valor le restamos la media. Por ejemplo, el primer valor, 1, está a -3,5 de la media; el segundo valor, 6, está a +1,5 de la media, y así sucesivamente.\nAhora podemos mirar nuestros valores originales y también sus diferencias con respecto a la media. Fijate que ya no tenemos una matriz completa de diferencias, así que es mucho más fácil de ver. Pero seguimos con un problema:\nPodemos ver que hay valores distintos de cero en las diferencias, así que sabemos que hay variación en los datos. Pero cuando las sumamos, obtenemos cero, lo que haría pensar que no hay ninguna diferencia en los datos… ¿por qué pasa esto? ¿Y qué hacemos al respecto?\n\n\n\n2.5.4 La media como punto de equilibrio de los datos\nHacemos una pausa para destacar otra propiedad genial de la media: es el punto de equilibrio del conjunto de datos. Si agarrás una birome o un lápiz e intentás equilibrarlo en un dedo, ¿qué estás haciendo? Buscás el centro de masa, el punto donde hay la misma cantidad de peso de un lado y del otro. Así funciona el equilibrio: un lado = el otro.\nPodemos pensar los datos como si tuvieran peso. Si los pusiéramos sobre una balanza, podríamos saber cuánto “pesan” sumándolos. Si quisiéramos dividir ese peso por la mitad, para que un lado tenga lo mismo que el otro, podríamos poner un alfiler para hacer de eje. La media nos dice exactamente dónde colocar ese alfiler. Es el punto donde los números de un lado suman lo mismo que los del otro.\nSi lo pensás, eso significa que la suma de las diferencias con respecto a la media siempre va a dar cero. Porque los números a un lado de la media suman -x, y los del otro lado suman +x. Y:\n\\(-x + x = 0\\)\n¿No?\nExacto.\n\n\n2.5.5 Las desviaciones al cuadrado\nAlguien muy ingenioso ideó una solución al hecho de que las diferencias con respecto a la media siempre suman cero. ¿Se te ocurre alguna? Por ejemplo, ¿qué podrías hacer con esas diferencias para que, al sumarlas, te den algo útil (o sea, que no sea cero)?\nLa solución ingeniosa es elevarlas al cuadrado. Elevar al cuadrado convierte todos los valores negativos en positivos. Por ejemplo,\\(2^2 = 4\\), y \\(-2^2 = 4\\). Recordá cómo funciona: \\(2^2 = 2 \\times 2 = 4\\), y \\((-2)^2 = -2 \\times -2 = 4\\). Usamos el término desviaciones al cuadrado para referirnos a las diferencias que han sido elevadas al cuadrado. Las desviaciones son justamente eso: apartamientos. Las diferencias con respecto a la media son desviaciones.\nVolvamos a mirar la tabla anterior, pero ahora con una columna adicional de desviaciones al cuadrado:\n\n\n\n\n\nmedidas\nvalores\nmedia\nDiferencia_a_la_media\nDesvíos_cuadráticos\n\n\n\n\n1\n1\n4.5\n-3.5\n12.25\n\n\n2\n6\n4.5\n1.5\n2.25\n\n\n3\n4\n4.5\n-0.5\n0.25\n\n\n4\n2\n4.5\n-2.5\n6.25\n\n\n5\n6\n4.5\n1.5\n2.25\n\n\n6\n8\n4.5\n3.5\n12.25\n\n\nSumas\n27\n27\n0\n35.5\n\n\nMedias\n4.5\n4.5\n0\n5.91666666666667\n\n\n\n\n\nBien, ahora tenemos una nueva columna llamada Desvíos_cuadráticos. Son simplemente las diferencias elevadas al cuadrado. Por ejemplo, \\((-3,5)^2 = 12,25\\), etc. Lo podés comprobar con la calculadora del celular.\nAhora que todas las desviaciones al cuadrado son positivas, podemos sumarlos. Y al hacerlo obtenemos algo muy importante: la suma de los cuadrados (SS, por “sum of squares” en ingles), o sea, la suma de las desviaciones al cuadrado respecto de la media. Vamos a hablar mucho de esta SS más adelante, en el capítulo de ANOVA. Pero cuando llegues ahí, ya vas a saber de qué se trata: solo una suma de cuadrados, nada raro.\n\n\n2.5.6 Finalmente, la varianza\nSorpresa: ya calculamos la varianza. Sí, ya pasó… capaz no te diste cuenta.\n“¿Eh? ¿Qué me perdí?”\nBueno, antes que nada: ¿te acordás qué problema estábamos tratando de resolver? Hacé una pausa y tratá de decírtelo a vos mismo:\n\npausa\n\nExacto: queremos resumir cuán diferentes son los valores. Como hay tantas diferencias como valores, nos gustaría tener un solo número que resuma todo eso, algo como una media que represente el promedio de esas diferencias.\nSi mirás la tabla, vas a ver que ya calculamos la media de las desviaciones al cuadrado. Primero obtuvimos la suma (SS), y luego, debajo, la media: 5,916… Esa es la varianza. Es la media de las desviaciones al cuadrado:\n\\(\\text{Varianza} = \\frac{SS}{N}\\)\ndonde \\(SS\\) es la suma de los cuadrados y \\(N\\) es el número de observaciones.\nY ahora, ¿qué hago con la varianza? ¿Qué significa ese número? Buena pregunta. Muchas veces, la varianza no es un número muy útil para mirar. ¿Por qué? Porque no está en la misma escala que los datos originales. Esto pasa porque al elevar al cuadrado se obtienen números más grandes. Por ejemplo, tenemos un 12,25 por ahí. Ese número es más grande que cualquier diferencia entre dos valores originales. Entonces, ¿cómo hacemos para “bajar” ese número a la escala original?\nSi pensaste en sacar la raíz cuadrada, ¡ding ding ding! Correcto! Siempre podés deshacer un cuadrado sacando la raíz cuadrada. Entonces, hagámoslo con 5,916:\n\\(\\sqrt{5,916} = \\text{desviación estándar}\\)\n\n\n2.5.7 La desviación estándar\nSí, otra vez lo hicimos sin avisarte. Ya calculamos la desviación estándar (o desvío estándar, o desviación típica, o desvío típico, tiene varios nombres en español). Es simplemente la raíz cuadrada de la varianza. Al menos, por ahora… en el próximo capítulo se va a complicar un poquito más.\nAcá va la fórmula:\n\\(\\text{Desviación estándar} = \\sqrt{\\text{Varianza}} = \\sqrt{\\frac{SS}{N}}\\)\nTambién se puede expandir así:\n\\(\\text{Desviación estándar} = \\sqrt{\\frac{\\sum_{i}^{n}({x_{i}-\\bar{x})^2}}{N}}\\)\nNo dejes que esas raíces gigantes te asusten. Ahora sabés que están ahí para devolvernos una medida que tenga la misma escala que los datos originales.\nSi miramos de nuevo la tabla:\n\n\n\n\n\nmedidas\nvalores\nmedia\nDiferencia_a_la_media\nDesvíos_cuadráticos\n\n\n\n\n1\n1\n4.5\n-3.5\n12.25\n\n\n2\n6\n4.5\n1.5\n2.25\n\n\n3\n4\n4.5\n-0.5\n0.25\n\n\n4\n2\n4.5\n-2.5\n6.25\n\n\n5\n6\n4.5\n1.5\n2.25\n\n\n6\n8\n4.5\n3.5\n12.25\n\n\nSumas\n27\n27\n0\n35.5\n\n\nMedias\n4.5\n4.5\n0\n5.91666666666667\n\n\n\n\n\nLa desviación estándar que obtuvimos fue aproximadamente √5,916. Notá cómo ese número encaja con las diferencias respecto de la media: la mayoría están más o menos a +/– √5,916 de distancia. En cambio, la varianza (5,916) es demasiado grande para dar una buena idea de la dispersión real.\n¿Qué significa todo esto? Bueno, si alguien te dice que tiene un conjunto de datos con media 4,5 (como los de nuestra tabla) y desviación estándar √5,916, ya tenés un muy buen resumen de esos valores: sabés que giran alrededor del 4,5, pero no todos son iguales, y que esa dispersión no es muy grande.\nSi tuvieras un montón de datos, podrías resumirlos con solo dos números: la media y la desviación estándar. Y eso ya te daría una idea bastante buena del conjunto.\n\n\n2.5.8 Desviaciones absolutas\n¿Y si simplemente tomamos el valor absoluto de las diferencias respecto de la media? Recordá que el valor absoluto convierte cualquier número en un valor positivo. Fijate en la siguiente tabla:\n\n\n\n\n\n\n\n\n\n\n\n\nmedidas\nvalores\nmedia\nDiferencia_a_la_media\nDesviaciones_absolutas\n\n\n\n\n1\n1\n4.5\n-3.5\n3.5\n\n\n2\n6\n4.5\n1.5\n1.5\n\n\n3\n4\n4.5\n-0.5\n0.5\n\n\n4\n2\n4.5\n-2.5\n2.5\n\n\n5\n6\n4.5\n1.5\n1.5\n\n\n6\n8\n4.5\n3.5\n3.5\n\n\nSumas\n27\n27\n0\n13\n\n\nMedias\n4.5\n4.5\n0\n2.16666666666667\n\n\n\n\n\nEsto también funciona bastante bien. Al convertir las diferencias respecto de la media en valores positivos, ahora podemos sumarlas y obtener un valor distinto de cero (si es que hay diferencias). Después, podemos calcular el promedio de la suma de las desviaciones absolutas. Si quisiéramos hacer un paralelismo entre los términos suma de cuadrados (SS), varianza y desviación estándar, y estas nuevas medidas basadas en la desviación absoluta, ¿cómo sería ese mapeo? Por ejemplo, ¿qué valor en la tabla correspondería a la SS? Eso sería la suma de desviaciones absolutas en la última columna. ¿Y la varianza y la desviación estándar, a qué corresponderían? Recordá que la varianza es el promedio (\\(SS/N\\)), y que la desviación estándar es la raíz cuadrada de ese promedio (\\(\\sqrt{SS/N}\\)).\nEn la tabla de arriba solo tenemos un promedio: el promedio de la suma de desviaciones absolutas. Entonces, tenemos una medida de varianza que no necesita extraer raíz cuadrada. Podemos decir que la desviación media absoluta cumple un doble rol: es tanto una medida de varianza como de desviación estándar. Gol.\n\n\n2.5.9 Otras operaciones que invierten el signo\nEn principio, podríamos crear muchas estadísticas de resumen distintas para la variación que resuelvan el problema de que la suma de las diferencias da cero. Por ejemplo, podríamos elevar cada diferencia a cualquier potencia par mayor que 2 (además del cuadrado). Podríamos usar 4, 6, 8, 10, etc. Hay infinitos números pares, así que hay infinitas estadísticas posibles de varianza. También podríamos usar potencias impares y luego tomar el valor absoluto. Muchas cosas son posibles. Lo importante es tener una razón clara para lo que estás haciendo, y elegir un método que funcione bien con el problema que estás analizando. También te mostramos esto porque queremos que entiendas que la estadística es un ejercicio creativo. Inventamos cosas cuando las necesitamos, y usamos cosas que ya existen cuando resuelven bien el problema.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Describir los datos</span>"
    ]
  },
  {
    "objectID": "02-Describing_Data.html#acordate-de-mirar-los-datos",
    "href": "02-Describing_Data.html#acordate-de-mirar-los-datos",
    "title": "2  Describir los datos",
    "section": "2.6 Acordate de mirar los datos",
    "text": "2.6 Acordate de mirar los datos\nLas estadísticas descriptivas son geniales y las vamos a usar mucho en el curso para describir datos. Pero tal vez sospechás que también tienen algunas limitaciones. Y tenés razón. Son resúmenes comprimidos de montones de números. Casi nunca van a poder representar fielmente todos los valores. Además, hay distintos tipos de estadísticas descriptivas, y a veces no es claro cuál conviene usar.\nTal vez lo más importante que podés hacer cuando usás estadísticas descriptivas es complementarlas con una visualización gráfica de los datos. Esto puede ayudarte a ver si esos resúmenes realmente están capturando lo que hay en los datos.\n\n2.6.1 El cuarteto de Anscombe\nPara ilustrar este punto —y prepararte para los temas del próximo capítulo— mirá este ejemplo. Se llama el Cuarteto de Anscombe, porque estos gráficos y valores tan curiosos fueron producidos por [Anscombe, 1973]. En la Figura 2.5 estás viendo pares de mediciones. Cada gráfico tiene un eje X y un eje Y, y cada punto representa dos valores. A simple vista, cada gráfico se ve muy distinto, ¿no?\n\n\n\n\n\n\n\n\nFigura 2.5: El Cuarteto de Anscombe\n\n\n\n\n\nBueno, ¿te sorprendería si te dijera que las estadísticas descriptivas de los valores en estos gráficos son exactamente las mismas? Resulta que sí: tienen las mismas estadísticas descriptivas. En la tabla de abajo se presentan la media y la varianza para los valores de \\(x\\) en cada gráfico, y la media y varianza de los valores de \\(y\\) en cada gráfico:\n\n\n\n\n\ncuarteto\nmedia x\nvarianza x\nmedia y\nvarianza y\n\n\n\n\n1\n9\n11\n7.500909\n4.127269\n\n\n2\n9\n11\n7.500909\n4.127629\n\n\n3\n9\n11\n7.500000\n4.122620\n\n\n4\n9\n11\n7.500909\n4.123249\n\n\n\n\n\n¡Las estadísticas descriptivas son todas iguales! Anscombe armó estos conjuntos especiales justamente para mostrar por qué es tan importante graficar los datos. Si solo mirás los resúmenes numéricos, no tenés idea de qué patrones están ocultando. Pero si mirás el gráfico, podés comprender mucho más.\n\n\n2.6.2 El “Datasaurus Dozen”\nSi te pareció genial el Cuarteto de Anscombe, no te pierdas el Datasaurus Dozen (Matejka y Fitzmaurice 2017). Bajá un poco en la página para ver los ejemplos. Vas a ver diagramas de puntos con muchísimos patrones distintos —¡incluso dinosaurios!— y lo más increíble es que todos tienen estadísticas descriptivas casi idénticas. Otro recordatorio más: mirá tus datos, ¡quizás tengan forma de dinosaurio!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Describir los datos</span>"
    ]
  },
  {
    "objectID": "02-Describing_Data.html#videos-en-inglés",
    "href": "02-Describing_Data.html#videos-en-inglés",
    "title": "2  Describir los datos",
    "section": "2.7 Videos (en inglés)",
    "text": "2.7 Videos (en inglés)\n\n2.7.1 Medidas de tendencia central: Moda\n\n\n\n\n2.7.2 Medidas de tendencia central: Mediana y Media\n\n\n\n\n2.7.3 Desviación estándar – Parte I\n\n\n\n\n2.7.4 Desviación estándar – Parte II\n\n\n\n\n\n\nMatejka, Justin, y George Fitzmaurice. 2017. «Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing». En Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, 1290-94. ACM. https://doi.org/10.1145/3025453.3025912.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Describir los datos</span>"
    ]
  },
  {
    "objectID": "03-Correlation.html",
    "href": "03-Correlation.html",
    "title": "4  Correlación",
    "section": "",
    "text": "4.1 Si algo causara que otra cosa cambie, ¿cómo se vería eso?\nEn el capítulo anterior teníamos un conjunto de datos. Era demasiado para mirar y no tenía mucho sentido. Así que hablamos de cómo visualizar los datos usando gráficos e histogramas, y de cómo resumir un montón de números para determinar su tendencia central (lo común) y su variabilidad (lo diferente). Y todo estaba bien en el mundo.\nPero no olvidemos la gran razón por la que aprendimos sobre estadísticas descriptivas. La gran razón es que estamos interesados en responder preguntas usando datos.\nSi estás buscando un tema central para tener en mente durante este curso, es este: ¿cómo hacemos para formular y responder preguntas usando datos?\nEn cada sección del libro, deberías conectar ese diálogo interno con esta pregunta, y pensar: ¿cómo lo que estoy aprendiendo me ayuda a responder preguntas con datos?\nAdvertencia anticipada: sabemos que es fácil olvidarse de esto cuando nos metemos en los detalles, así que vamos a intentar tirarte una cuerda cada tanto… recordá: estamos tratando de responder preguntas con datos.\nEn el capítulo dos empezamos con unos datos ficticios sobre la felicidad humana, ¿te acordás? Imaginamos que le pedíamos a muchas personas que nos dijeran cuán felices se sentían, y luego mirábamos los números que nos daban. Sigamos con ese experimento mental imaginario.\n¿Qué obtenés cuando le pedís a la gente que use un número para describir cuán feliz está? Un montón de números. ¿Qué tipo de preguntas podés hacer con esos números? Bueno, podés mirar los valores y estimar sus propiedades generales, como ya hicimos. Esperamos que esos números nos digan cosas que más o menos ya sabemos. Que hay personas diferentes, y que cada persona tiene un nivel diferente de felicidad. Probablemente conociste a algunas personas realmente felices, y otras realmente infelices, y vos también seguramente tengas algún nivel de felicidad. “Buenísimo, ¡chocolate por la noticia!”.\nAntes de seguir, también deberías tener una actitud crítica respecto a qué significan esos números. Por ejemplo, si forzás a las personas a calificar su felicidad con un número entre 0 y 100, ¿ese número refleja realmente cuán felices son? ¿Puede una persona saber cuán feliz está? ¿El formato de la pregunta influye en la respuesta? ¿La felicidad es siquiera algo real?. Estas son todas preguntas válidas sobre la validez del constructo (la felicidad misma) y del instrumento de medida (los números) que estás usando para cuantificarlo. Pero por ahora vamos a dejar esas preguntas importantes de lado, y vamos a asumir que la felicidad existe, y que nuestra medida de felicidad refleja algo sobre cuán feliz está la gente.\nBien. Una vez que medimos un poco de felicidad, seguro se te ocurren preguntas más importantes. Por ejemplo, ¿qué cosas hacen que la felicidad suba o baje? Si supieras qué la causa, ¿qué podrías hacer? ¿Qué tal aumentar tu propia felicidad?; o ayudar a personas que están infelices; o entender mejor por qué Ígor, el burrito de Winnie the Pooh, está siempre bajoneado; o presentar argumentos científicos válidos contra afirmaciones erróneas sobre las causas de la felicidad. Una teoría causal y un entendimiento de la felicidad servirían para todo eso. Entonces, ¿cómo hacemos para llegamos ahí?\nImaginá que sos una observadora alienígena. Llegás a la Tierra y escuchás hablar de esta cosa llamada “felicidad” que tienen las personas. Querés saber qué la causa. También descubrís que en el planeta Tierra hay muchas otras cosas. ¿Cuáles de todas esas cosas —te preguntás— causan la felicidad? ¿Cómo haría, tu yo-alienígena, para empezar a responder esa pregunta gigante?\nComo persona que tiene felicidad, quizás ya tengas algunas corazonadas sobre qué la hace cambiar. Por ejemplo: el clima, las amistades, la música, el dinero, la educación, las drogas, los libros, las películas, las creencias, la personalidad, el color de tus zapatos, el largo de tus cejas, cuántos gatos ves por día, cuántas veces se demora el subte, una provisión de por vida de chocolate, etcétera, etcétera (como diría Willy Wonka), podrían contribuir de alguna forma a la felicidad. Podría haber muchas causas distintas de la felicidad.\nAntes de salir a buscar las causas de la felicidad, deberíamos prepararnos con algunas herramientas analíticas que nos permitan identificar cómo se ve una relación causal. Si no nos preparamos para lo que podríamos encontrar, no vamos a saber cómo interpretar nuestros propios datos. En cambio, necesitamos anticipar cómo podrían verse esos datos. En concreto, necesitamos saber cómo lucen los datos cuando una cosa no causa a otra, y cómo lucen cuando sí la causa. Este capítulo trata justamente de esa preparación. Advertencia: vamos a descubrir cosas complejas. Por ejemplo, hay patrones que parecen mostrar que una cosa causa a otra, incluso cuando en realidad no la causa. Aguantá, ya vamos a verlo.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Correlación</span>"
    ]
  },
  {
    "objectID": "03-Correlation.html#si-algo-causara-que-otra-cosa-cambie-cómo-se-vería-eso",
    "href": "03-Correlation.html#si-algo-causara-que-otra-cosa-cambie-cómo-se-vería-eso",
    "title": "4  Correlación",
    "section": "",
    "text": "4.1.1 Charlie y la fábrica de chocolate\nImaginemos que la cantidad de chocolate que tiene una persona tiene una influencia causal sobre su nivel de felicidad. Sigamos imaginando que, como Charlie, cuanto más chocolate tenés, más feliz sos; y cuanto menos chocolate, menos feliz. Por último, como sospechamos que la felicidad depende también de muchas otras cosas en la vida de una persona, anticipamos que la relación entre el suministro de chocolate y la felicidad no va a ser perfecta. ¿Qué significan estas suposiciones para cómo deberían verse los datos?\nEl primer paso es recolectar unos datos imaginarios de 100 personas. Caminamos por ahí y le pedimos a las primeras 100 personas que respondan dos preguntas:\n\n¿Cuánto chocolate tenés?\n\n¿Cuán feliz estás?\n\nPara hacerlo más sencillo, ambas escalas van de 0 a 100. En la escala de chocolate, 0 significa “sin chocolate” y 100 significa “suministro vitalicio de chocolate”. Cualquier otro número está en el medio. En la escala de felicidad, 0 es “nada de felicidad”, 100 es “toda la felicidad”, y los valores intermedios reflejan distintos grados de felicidad en el medio.\nAcá tenés una muestra de datos de los primeros 10 sujetos imaginarios.\n\n\n\n\n\nsujeto\nchocolate\nfelicidad\n\n\n\n\n1\n1\n1\n\n\n2\n2\n2\n\n\n3\n3\n2\n\n\n4\n4\n3\n\n\n5\n4\n3\n\n\n6\n5\n5\n\n\n7\n5\n4\n\n\n8\n7\n4\n\n\n9\n8\n8\n\n\n10\n6\n6\n\n\n\n\n\nLe hicimos dos preguntas a cada persona, así que hay dos valores por persona: uno para su suministro de chocolate, y otro para su nivel de felicidad. Capaz ya notás cierta relación entre la cantidad de chocolate y el nivel de felicidad en la tabla. Para que esas relaciones se vean aún más claras, vamos a graficar todos los datos.\n\n\n4.1.2 Diagramas de dispersión\nCuando tenés dos variables medidas, siempre podés convertirlas en puntos y mostrarlas en un diagrama de dispersión (scatter plot). Un gráfico de dispersión tiene un eje horizontal (x) y un eje vertical (y). Vos elegís qué variable va en qué eje. Vamos a poner el suministro de chocolate en el eje x, y el nivel de felicidad en el eje y. La Figura 4.1 muestra 100 puntos, uno por cada persona.\n\n\n\n\n\n\n\n\nFigura 4.1: Datos imaginarios que muestran una correlación positiva entre la cantidad de chocolate y el nivel de felicidad\n\n\n\n\n\nCapaz te estés preguntando: ¿por qué hay solo 100 puntos en el gráfico? ¿No recolectamos 100 medidas de chocolate y 100 de felicidad? ¿No debería haber 200 puntos?\nNo. Cada punto representa a una persona. Hay 100 personas, por lo tanto hay 100 puntos.\n¿Qué significa cada punto? Cada punto tiene dos coordenadas: una en x (chocolate) y una en y (felicidad). El primer punto, abajo a la izquierda, representa a la primera persona de la tabla, que tenía casi 0 de chocolate y casi 0 de felicidad. Podés mirar cualquier punto y trazar una línea recta hacia abajo hasta el eje x: eso te dice cuán “chocolatada” está esa persona. Si trazás una línea recta hacia la izquierda hasta el eje y, eso te dice cuán feliz está.\nAhora que estamos mirando el gráfico de dispersión, podemos ver muchas cosas. Los puntos están algo esparcidos, ¿no? De ahí viene el nombre “diagrama de dispersión”. Incluso cuando no están tan dispersos, se les sigue llamando así. Capaz porque en la vida real los puntos siempre se dispersan un poco.\nMás importante todavía: los puntos muestran una relación entre el suministro de chocolate y la felicidad. Las personas con menos chocolate tienden a tener menos felicidad, y las que tienen más chocolate tienden a ser más felices. Parece que cuanto más chocolate tenés, más feliz sos —y viceversa. Este tipo de relación se llama correlación positiva.\n\n\n4.1.3 Correlación positiva, negativa y nula\nYa que estamos en el negocio de imaginar datos, sigamos imaginando un poco más. Ya imaginamos cómo se verían los datos si tener más chocolate aumentara la felicidad. Vamos a mostrar eso de nuevo en un momento. Pero, ¿cómo te imaginás que se vería un gráfico de dispersión si la relación fuera al revés, y tener más chocolate disminuyera la felicidad? ¿O cómo se vería si no hubiera ninguna relación entre la cantidad de chocolate y la felicidad?\nTe invitamos a imaginarlo mirando la Figura 4.2:\n\n\n\n\n\n\n\n\nFigura 4.2: Tres diagramas de dispersión mostrando una correlación negativa, positiva, y ausencia de correlación\n\n\n\n\n\nEl primer panel muestra una correlación negativa. La felicidad disminuye a medida que aumenta el suministro de chocolate. La correlación negativa ocurre cuando una variable sube y la otra baja; o sea, cuando tener más de X implica tener menos de Y, y viceversa. El segundo panel muestra una correlación positiva. La felicidad aumenta a medida que aumenta el suministro de chocolate. En una correlación positiva, ambas cosas suben juntas, o bajan juntas: más de X implica más de Y, y viceversa. El tercer panel muestra ausencia de correlación. En este caso, no parece haber ninguna relación clara entre el suministro de chocolate y la felicidad. Los puntos están totalmente dispersos —el más disperso de todos los gráficos de dispersión.\n\n\n\n\n\n\nNota\n\n\n\nNos estamos metiendo en la idea de que las mediciones de dos cosas pueden estar relacionadas, o correlacionadas entre sí. Las relaciones pueden ser más complejas que simplemente “sube” o “baja”. Por ejemplo, podríamos tener una relación en la que los puntos suben en la primera mitad del eje X, y bajan en la segunda.\n\n\nUna correlación nula ocurre cuando una cosa no está relacionada de ninguna forma con la otra: los cambios en X no tienen ninguna relación con los cambios en Y, y viceversa.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Correlación</span>"
    ]
  },
  {
    "objectID": "03-Correlation.html#el-r-de-pearson",
    "href": "03-Correlation.html#el-r-de-pearson",
    "title": "4  Correlación",
    "section": "4.2 El r de Pearson",
    "text": "4.2 El r de Pearson\nYa aprendimos a calcular estadísticos descriptivos para una sola variable, como el chocolate o la felicidad (medias, varianzas, etc.). Pero, ¿es posible crear un estadístico descriptivo que resuma la relación entre dos variables, todo en un solo número? ¿Se puede? Karl Pearson al rescate.\n\n\n\n\n\n\nNota\n\n\n\nLas historias sobre la invención de distintas estadísticas son muy interesantes. Podés leer más sobre eso en el libro The Lady Tasting Tea (Salsburg 2001).\n\n\nExiste una estadística para eso, y Karl Pearson la inventó. Hoy en día todxs la llaman el r de Pearson. Más adelante veremos que Pearson fue editor de Biometrika en los años 30. Le tenía bastante bronca a otro estadístico famoso, Sir Ronald Fisher (del que también vamos a hablar), y se tiraban con estadísticas… ¿Por qué no podemos simplemente llevarnos bien en estadística?\n¿Cómo funciona el r de Pearson? Volvamos a mirar los datos de los primeros 10 sujetos de nuestro experimento ficticio:\n\n\n\n\n\nsujeto\nchocolate\nfelicidad\n\n\n\n\n1\n1\n1\n\n\n2\n2\n2\n\n\n3\n3\n2\n\n\n4\n4\n3\n\n\n5\n4\n3\n\n\n6\n5\n5\n\n\n7\n5\n4\n\n\n8\n7\n4\n\n\n9\n8\n8\n\n\n10\n6\n6\n\n\nSumas\n45\n38\n\n\nMedias\n4.5\n3.8\n\n\n\n\n\n¿Qué podríamos hacer con estos números para obtener un único valor que resuma la relación entre el suministro de chocolate y la felicidad?\n\n4.2.1 La idea de la covarianza\n“Por favor no… otra vez no uses la palabra varianza”. Sí, lo vamos a hacer. Vamos a usar la palabra varianza una y otra vez, hasta que tenga sentido.\nRecordá lo que significa la varianza respecto a un conjunto de números: significa que los números tienen cierta variación, que no son todos iguales, que algunos son más grandes y otros más chicos. Podemos ver que hay varianza en el suministro de chocolate entre los 10 sujetos. También hay varianza en los niveles de felicidad. Y también vimos, en el gráfico de dispersión, que la felicidad aumenta cuando aumenta el chocolate —una relación positiva, una correlación positiva. ¿Qué tiene que ver eso con la varianza? Bueno, quiere decir que hay una relación entre la varianza en el suministro de chocolate y la varianza en los niveles de felicidad. ¿No te parece que ambas medidas varían juntas? Cuando tenemos dos variables que varían juntas, son como una pareja feliz que comparte su varianza. Eso es lo que significa covarianza: la idea de que el patrón de variación en una medida se comparte con el patrón de variación en otra.\nLa covarianza es muy, muy, muy, muy importante. Sabemos que al principio puede ser una palabra confusa, especialmente si todavía no te sentís del todo cómodo con lo que significa varianza en una sola variable. Sin embargo, tenemos que seguir adelante y usar la idea de covarianza una y otra vez, para que quede bien grabada en tu mente estadística (sí, ya lo dijimos, pero repetir ayuda, es una hecho).\n\n🧠 Consejo pro: La carrera de tres piernas es una metáfora de la covarianza. Dos personas se atan una pierna entre sí, y luego intentan caminar. Funciona cuando sus piernas se mueven juntas (relación positiva). También pueden moverse juntas de forma torpe, por ejemplo cuando una intenta avanzar justo cuando la otra intenta retroceder. Eso también es covarianza (pero negativa). Caminan de forma aleatoria cuando no hay covarianza: cada persona hace lo suyo, sin coordinación. Hay mucha varianza, pero la varianza se comparte al azar. Es solo un montón de piernas moviéndose sin lograr nada.\n\n\n🧠 Consejo pro #2: Jugar bien a las palmas requiere que dos personas coordinen sus acciones. Eso es una covarianza positiva bien compartida.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Correlación</span>"
    ]
  },
  {
    "objectID": "03-Correlation.html#convertir-los-números-en-una-medida-de-covarianza",
    "href": "03-Correlation.html#convertir-los-números-en-una-medida-de-covarianza",
    "title": "4  Correlación",
    "section": "4.3 Convertir los números en una medida de covarianza",
    "text": "4.3 Convertir los números en una medida de covarianza\n“OK, si me decís que covarianza es otra forma de decir ‘correlación’ o ‘relación’ entre dos variables, me sirve. Supongo que deberíamos tener alguna forma de medir eso.” Correcto. Volvamos a nuestra tabla… ¿notás algo nuevo?\n\n\n\n\n\nsujeto\nchocolate\nfelicidad\nChocolate_X_Felicidad\n\n\n\n\n1\n1\n1\n1\n\n\n2\n2\n2\n4\n\n\n3\n3\n2\n6\n\n\n4\n4\n3\n12\n\n\n5\n4\n3\n12\n\n\n6\n5\n5\n25\n\n\n7\n5\n4\n20\n\n\n8\n7\n4\n28\n\n\n9\n8\n8\n64\n\n\n10\n6\n6\n36\n\n\nSumas\n45\n38\n208\n\n\nMedias\n4.5\n3.8\n20.8\n\n\n\n\n\nAgregamos una columna nueva, que representa las puntuaciones de Chocolate multiplicadas por las de Felicidad. Cada fila en esa nueva columna es el producto —la multiplicación entre los puntajes de chocolate y felicidad para esa persona. Sí, pero ¿por qué haríamos esto?\nEn el capítulo anterior te llevamos de vuelta a la escuela primaria para pensar en la división. Ahora vamos a hacer lo mismo con la multiplicación. Suponemos que ya sabés cómo funciona. Un número multiplicado por otro quiere decir que sumás el primero tantas veces como indica el segundo:\n\\(2*2= 2+2=4\\)\n\\(2*6= 2+2+2+2+2+2 = 12\\), o \\(6+6=12\\), que es lo mismo.\nTodo eso ya lo sabés. Pero, ¿podés usar la multiplicación a tu favor y hacer que haga lo que necesitás cuando se trata de resumir una covarianza? La multiplicación es el droide que estás buscando.\nSabemos cómo multiplicar números, y lo que tenemos que hacer ahora es pensar en las consecuencias de multiplicar dos conjuntos de números. Por ejemplo: ¿qué pasa cuando multiplicás dos números chicos, en comparación con dos números grandes? El primer producto debería ser más chico, ¿no? ¿Y qué pasa si multiplicás un número chico con uno grande? Los productos deberían quedar en un punto intermedio, ¿cierto?\nEl siguiente paso es pensar qué pasa cuando sumamos los productos de dos variables, dependiendo de cómo se alinean. Veamos otra tabla:\n\n\n\n\n\npuntajes\nX\nY\nA\nB\nXY\nAB\n\n\n\n\n1\n1\n1\n1\n10\n1\n10\n\n\n2\n2\n2\n2\n9\n4\n18\n\n\n3\n3\n3\n3\n8\n9\n24\n\n\n4\n4\n4\n4\n7\n16\n28\n\n\n5\n5\n5\n5\n6\n25\n30\n\n\n6\n6\n6\n6\n5\n36\n30\n\n\n7\n7\n7\n7\n4\n49\n28\n\n\n8\n8\n8\n8\n3\n64\n24\n\n\n9\n9\n9\n9\n2\n81\n18\n\n\n10\n10\n10\n10\n1\n100\n10\n\n\nSumas\n55\n55\n55\n55\n385\n220\n\n\nMedias\n5.5\n5.5\n5.5\n5.5\n38.5\n22\n\n\n\n\n\nObservá las columnas \\(X\\) y \\(Y\\). Los valores de \\(X\\) y \\(Y\\) covarían perfectamente. Cuando \\(X\\) es 1, \\(Y\\) también es 1; cuando \\(X\\) es 2, \\(Y\\) es 2, y así. Están perfectamente alineados.\nLos valores de \\(A\\) y \\(B\\) también covarían perfectamente, pero en sentido opuesto. Cuando \\(A\\) es 1, \\(B\\) es 10; cuando \\(A\\) es 2, \\(B\\) es 9, y así. \\(B\\) es una copia invertida de \\(A\\).\nAhora mirá la columna \\(XY\\). Son los productos que obtenemos al multiplicar cada valor de \\(X\\) con el correspondiente valor de \\(Y\\). Y la columna \\(AB\\) son los productos de \\(A\\) por \\(B\\) en cada fila. Hasta acá todo bien.\nAhora observá las sumas de las columnas \\(XY\\) y \\(AB\\). No son iguales. La suma de los productos \\(XY\\) es 385, y la suma de los productos \\(AB\\) es 220. En este conjunto de datos en particular los números 385 y 220 son muy importantes. Representan la mayor suma posible de productos (385) y la menor suma posible de productos (220). No hay forma de reordenar los números del 1 al 10, digamos para \\(X\\) y para \\(Y\\), que produzca una suma mayor o menor. ¿No me creés? Mirá esto:\n\n\n\n\n\n\n\n\nFigura 4.3: Sumas de productos simuladas que muestran los tipos de valores que pueden producirse al ordenar aleatoriamente los números en X e Y.\n\n\n\n\n\nLa Figura 4.3 muestra 1000 simulaciones por computadora. Convencí a mi computadora de que ordenara al azar los números del 1 al 10 para X, y también del 1 al 10 para Y. Luego, multiplicó X por Y, y sumó los productos. Hizo esto 1000 veces. Cada punto en el gráfico muestra la suma de los productos para una de esas simulaciones. Las dos líneas negras marcan la suma máxima posible (385) y la mínima posible (220) para este conjunto de números. Notá cómo todos los puntos están entre esos dos valores. Te lo dije.\n“OK, está bien, me lo advertiste… ¿y qué? ¿a quién le importa?”.\nEstábamos buscando una forma de resumir la covarianza entre dos variables, ¿no? Bueno, para estos números, encontramos una: la suma de los productos. Sabemos que cuando esa suma da 385, hay una correlación perfecta y positiva. Sabemos que si da 220, hay una correlación perfecta y negativa. ¿Y qué pasa con los números que están en el medio? ¿Qué podemos concluir sobre la correlación si la suma de productos da, por ejemplo, 350?\nBueno, va a ser positiva, porque está cerca de 385, que es correlación perfectamente positiva.\nSi la suma de productos fuera 240, va a ser negativa, porque está cerca de 220, que es la correlación perfectamente negativa.\n¿Y si no hay correlación? Entonces la suma va a estar justo en el medio entre 220 y 385, ¿no?\nLo que acabamos de hacer es inventar una medida de resumen específica para estos datos: una forma de cuantificar la correlación entre los números del 1 al 10 en X, y del 1 al 10 en Y, usando la suma de productos. Y como sabemos cuál es el máximo (385) y el mínimo (220), ahora podemos interpretar cualquier suma de productos de este tipo de datos respecto a esa escala.\n\n🧠 Consejo pro: Cuando la correlación entre dos variables aumenta en dirección positiva, la suma de sus productos se acerca al valor máximo posible. Esto ocurre porque los números grandes en X tienden a alinearse con los números grandes en Y, creando la mayor suma de productos. Cuando la correlación es negativa, la suma de los productos se acerca al valor mínimo posible, porque los números grandes en X se alinean con los números pequeños en Y. Si no hay correlación, los números grandes de X se alinean aleatoriamente con los grandes y pequeños de Y, haciendo que la suma quede en un valor intermedio.\n\n\n4.3.1 La covarianza como medida\nNos tomamos un tiempo para ver qué pasa cuando multiplicamos dos conjuntos de números. Descubrimos que:\n\\[\n\\text{grande} \\times \\text{grande} = \\text{más grande}\n\\]\n\\[\n\\text{chico} \\times \\text{chico} = \\text{sigue siendo chico}\n\\]\n\\[\n\\text{grande} \\times \\text{chico} = \\text{algo intermedio}\n\\]\nLa idea era darte una intuición conceptual sobre cómo se refleja la covarianza entre dos variables en la suma de sus productos. Hicimos algo bien simple: multiplicamos X por Y, y observamos cómo varía la suma de los productos cuando X e Y covarían de diferentes maneras. Ahora podemos ponernos un poco más formales. En estadística, la covarianza no es simplemente multiplicar los valores de X y Y. En realidad, se trata de multiplicar las desviaciones de X respecto de su media, por las desviaciones de Y respecto de su media. ¿Te acordás de esas diferencias con respecto a la media que vimos en el capítulo anterior? Bueno, ahora vuelven… pero tranquilos, vuelven en son de paz, como Gasparín, el fantasma amigable.\nVeamos cómo se ve esto en una tabla:\n\n\n\n\n\nsujeto\nchocolate\nfelicidad\nC_d\nF_d\nCd_x_Fd\n\n\n\n\n1\n1\n1\n-3.5\n-2.8\n9.8\n\n\n2\n2\n2\n-2.5\n-1.8\n4.5\n\n\n3\n3\n2\n-1.5\n-1.8\n2.7\n\n\n4\n4\n3\n-0.5\n-0.8\n0.4\n\n\n5\n4\n3\n-0.5\n-0.8\n0.4\n\n\n6\n5\n5\n0.5\n1.2\n0.6\n\n\n7\n5\n4\n0.5\n0.2\n0.1\n\n\n8\n7\n4\n2.5\n0.2\n0.5\n\n\n9\n8\n8\n3.5\n4.2\n14.7\n\n\n10\n6\n6\n1.5\n2.2\n3.3\n\n\nSumas\n45\n38\n0\n0\n37\n\n\nMedias\n4.5\n3.8\n0\n0\n3.7\n\n\n\n\n\nCalculamos las desviaciones respecto de la media para las puntuaciones de chocolate (columna C_d), y también para las de felicidad (columna F_d). Después las multiplicamos (última columna). Finalmente, podés ver el promedio de los productos listado en la esquina inferior derecha de la tabla, que muestra la covarianza oficial.\nLa fórmula de la covarianza es:\n\\(cov(X,Y) = \\frac{\\sum_{i}^{n}(x_{i}-\\bar{X})(y_{i}-\\bar{Y})}{N}\\)\nOK, ahora tenemos un número formal que resume la relación entre dos variables. Esto está buenísimo, era lo que veníamos buscando.\nSin embargo, hay un problema. ¿Te acordás cuando aprendimos a calcular la varianza? Obteníamos un número y no sabíamos bien qué hacer con él. Estaba al cuadrado, no estaba en la misma escala que los datos originales. Entonces sacábamos la raíz cuadrada de la varianza para obtener la desviación estándar, que era un número más interpretable, en la misma escala que los datos.\nBueno, con la covarianza pasa algo parecido. Cuando la calculás como acabamos de hacer, no sabés inmediatamente en qué escala está. ¿Un valor de 3 es grande? ¿Y 6? ¿100? ¿Qué tan grande o chico es?\nEn la discusión previa sobre covarianza, aprendimos que la suma de productos entre dos variables puede ir de un valor mínimo a un valor máximo. Lo mismo vale para la covarianza. Para un conjunto de datos determinado, hay un valor máximo posible de covarianza positiva (cuando la correlación es perfectamente positiva), y un mínimo posible para la negativa (cuando es perfectamente negativa). Y cuando no hay covariación, adiviná qué pasa: da cero.\nAsí que al menos, cuando miramos una covarianza, podemos ver si apunta en dirección positiva o negativa. Pero no sabemos cuán grande o chica es en relación con los valores máximos o mínimos posibles. Eso significa que no podemos decir qué tan fuerte es la correlación.\nEntonces… ¿qué hacemos?\n\n\n4.3.2 ¿Ya llegamos al r de Pearson?\nSí, ya llegamos. ¿No estaría bueno si pudiéramos forzar nuestra medida de covarianza a estar siempre entre –1 y +1?\n–1 sería el valor mínimo para una correlación negativa perfecta.\n+1 sería el máximo posible para una correlación positiva perfecta. 0 indicaría que no hay correlación.\nTodo lo que esté entre 0 y –1 serían correlaciones negativas cada vez más fuertes. Todo lo que esté entre 0 y +1 serían correlaciones positivas cada vez más fuertes.\nSería un sistema fantástico, coherente, fácil de interpretar. Si tan solo pudiéramos forzar el número de covarianza a estar entre –1 y 1…\nPor suerte, este episodio está auspiciado por el r de Pearson, que hace justamente eso tan maravilloso.\nVeamos la fórmula del r de Pearson:\n\\[\nr = \\frac{\\mathrm{cov}(X,Y)}{\\sigma_{X}\\sigma_{Y}} = \\frac{\\mathrm{cov}(X,Y)}{\\mathrm{SD}_{X}\\mathrm{SD}_{Y}}\n\\]\nAparece el símbolo \\(\\sigma\\) (sí, más griego para vos). \\(\\sigma\\) es el símbolo que se usa frecuentemente para la desviación estándar.\nLeído en voz alta: r es la covarianza de X e Y dividida por el producto de las desviaciones estándar de X y Y. ¿Y por qué hacemos esa división? Porque esta operación tiene el efecto de normalizar la covarianza, forzándola a caer dentro del rango –1 a 1.\n\n\n\n\n\n\nNota\n\n\n\nMás adelante vamos a explicar esta magia matemática… prometido. Pero te adelantamos que no es magia. La explicación rápida es que dividir cada variable por su desviación estándar garantiza que todas las mediciones estén en el mismo rango de escala.\n\n\nPor ahora, podés pensar que es magia matemática. Funciona. Ya después veremos por qué.\nVale decir que existen muchas fórmulas diferentes para calcular el r de Pearson. Podés buscarlas en Google y vas a ver varias versiones. Algunas aparecerán más adelante en este libro también. Pero todas dan el mismo resultado. Algunas son más elegantes que otras. Y algunas pueden parecer un poco intimidantes. En otros libros de estadística vas a encontrar fórmulas pensadas para facilitar los cálculos a mano, por ejemplo si solo tenés papel y lápiz. Para ser honestos, no estamos muy interesados en enseñarte cómo meter números en fórmulas. Damos una sola lección sobre eso aquí: pon los números donde están las letras y luego calcula el resultado. Perdón por ser sarcásticos. Hoy en día tenés una computadora que deberías usar para estas cosas. Así que nuestro interés está más en enseñarte qué significan los cálculos, más que en cómo hacerlos. Claro que igual, cada semana en el laboratorio te mostramos cómo hacer los cálculos en la compu, porque eso también es importante.\n¿El r de Pearson siempre queda entre –1 y 1, sin importar qué datos uses? Sí. Mirá la siguiente simulación. Reordenamos al azar los números del 1 al 10 para una variable X, y lo mismo para Y. Calculamos el r de Pearson y repetimos eso 1000 veces. Como podés ver en la Figura 4.4, todos los puntos caen entre –1 y 1. Tremendo, ¿no?\n\n\n\n\n\n\n\n\nFigura 4.4: Una simulación de correlaciones. Cada punto representa el valor de r correspondiente a la correlación entre una variable X y una variable Y, donde ambas contienen los números del 1 al 10 en órdenes aleatorios. La figura ilustra que este proceso aleatorio puede producir muchos valores distintos de r.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Correlación</span>"
    ]
  },
  {
    "objectID": "03-Correlation.html#ejemplos-con-datos",
    "href": "03-Correlation.html#ejemplos-con-datos",
    "title": "4  Correlación",
    "section": "4.4 Ejemplos con datos",
    "text": "4.4 Ejemplos con datos\nEn el laboratorio sobre correlación, vas a aprender a calcular correlaciones con datos reales usando software. Para darte una vista previa, miremos algunos datos del Informe Mundial de la Felicidad (2018).\nEste informe mide diversas actitudes en personas de distintos países. Por ejemplo, una de las preguntas era sobre cuánta libertad sentían que tenían para tomar decisiones en la vida. Otra preguntaba cuánta confianza tenían en su gobierno nacional. La Figura 4.5 muestra un gráfico de dispersión con la relación entre estas dos variables. Cada punto representa promedios para un país diferente.\n\n\n\n\n\n\n\n\nFigura 4.5: Relación entre la libertad para tomar decisiones en la vida y la confianza en el gobierno nacional. Datos del Informe Mundial de la Felicidad de 2018\n\n\n\n\n\nAgregamos una línea azul al gráfico para resumir la relación positiva. Parece que, a medida que aumenta la “libertad para tomar decisiones en la vida”, también aumenta la confianza en el gobierno nacional. Es una correlación positiva.\nLa correlación real, medida por el r de Pearson, es:\n\n#&gt; [1] 0.4080963\n\nEn el laboratorio vas a hacer mucho análisis como este. Al mirar el gráfico, podrías empezar a preguntarte: ¿acaso la libertad para tomar decisiones causa un cambio en la confianza en el gobierno nacional? ¿O será al revés? ¿Tener confianza en el gobierno hace que uno se sienta más libre para tomar decisiones? ¿O tal vez esta relación es solo una coincidencia sin sentido?\nTodas son buenas preguntas. Pero estos datos no nos dan la respuesta. Solo sugieren que podría haber una relación.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Correlación</span>"
    ]
  },
  {
    "objectID": "03-Correlation.html#regresión-una-mini-introducción",
    "href": "03-Correlation.html#regresión-una-mini-introducción",
    "title": "4  Correlación",
    "section": "4.5 Regresión: una mini introducción",
    "text": "4.5 Regresión: una mini introducción\nAhora vamos a agregar una cosita más a lo que venimos aprendiendo sobre correlación. Se llama regresión lineal. Suena intimidante, y… en parte lo es. Mucho más adelante, vas a descubrir que todo lo que estamos por aprender es en realidad un caso especial de la regresión.\nPero no queremos que salgas corriendo todavía, así que ahora solo te presentamos los conceptos básicos.\nPrimero, miremos una regresión lineal. Así vas a ver de qué estamos hablando. La Figura 4.6 muestra los mismos diagramas de dispersión que antes, pero con algo nuevo: ¡líneas!\n\n\n\n\n\n\n\n\nFigura 4.6: Tres diagramas de dispersión que muestran una correlación negativa, una positiva y una aleatoria (donde se espera que el valor de r sea 0), junto con la línea de regresión de mejor ajuste\n\n\n\n\n\n\n4.5.1 La línea de mejor ajuste\n¿Notás algo particular en estas líneas azules? Esperamos que, al menos en los dos primeros paneles, puedas ver que pasan directamente por el medio de los datos, como si fueran un palito de brochette. A estas líneas las llamamos líneas de mejor ajuste, porque según la definición que vamos a ver (te prometemos que ya llega), no hay ninguna otra línea recta que puedas dibujar que haga un mejor trabajo atravesando los datos.\nUna idea importante acá es que estamos usando la línea como una especie de “media” para describir la relación entre las dos variables. Cuando solo tenemos una variable, esa variable existe en una dimensión: es un dato 1D. En ese caso, tiene sentido usar un único número —como la media— para describir su tendencia central. Pero cuando tenemos dos variables, y las graficamos juntas, estamos en un espacio bidimensional. Entonces, para ese espacio 2D, podemos usar algo más grande —como una línea— para resumir la tendencia central en la relación entre ambas variables.\n¿Qué esperamos de esa línea? Bueno, si tuvieras un lápiz y un gráfico impreso, podrías dibujar un montón de líneas rectas como se te ocurriera. Tus líneas ni siquiera tendrían que pasar por los datos: podrían tener cualquier inclinación o dirección. ¿Serían todas esas líneas buenas para describir el patrón general de los puntos? La mayoría, no. Las mejores líneas serían aquellas que siguen el patrón general de los puntos. Pero de entre todas las líneas buenas, ¿cuál es la mejor? ¿Cómo podemos saberlo? ¿Y qué significa “mejor”? En resumen: la línea de mejor ajuste es aquella que tiene el menor error.\n\n\n\n\n\n\nNota\n\n\n\nCódigo R para graficar los residuos, cortesía del blog de Simon Jackson: https://drsimonj.svbtle.com/visualising-residuals\n\n\nMirá este próximo gráfico. Muestra una línea que atraviesa algunos puntos. Pero también muestra unas líneas pequeñas que bajan desde cada punto hasta la línea. Cada una de esas líneas chiquitas se llama residuo. Los residuos te muestran cuán lejos está cada punto de la línea. Es una medida de error, y nos dice qué tan equivocada está la línea. Después de todo, no todos los puntos están sobre la línea. Eso significa que la línea no representa perfectamente a los datos. Pero la línea de mejor ajuste es la menos equivocada de todas las posibles.\n\n\n\n\n\n\n\n\nFigura 4.7: Los puntos negros representan los datos reales. La línea azul es la de mejor ajuste. Los puntos blancos indican los valores predichos por la línea azul. Las líneas rojas muestran el error entre cada punto real y su predicción. La línea azul es la mejor porque minimiza ese error.\n\n\n\n\n\nPasan muchas cosas en la Figura 4.7. Primero, estamos mirando un diagrama de dispersión de dos variables, una variable X y una variable Y. Cada punto negro representa los valores reales de esas variables. Podés ver que hay una correlación negativa: a medida que X aumenta, Y tiende a disminuir. Trazamos una línea de regresión sobre los datos —esa es la línea azul. También aparecen unos puntitos blancos. Esos muestran dónde la línea “cree” que deberían estar los puntos negros. Y están también las líneas rojas, que son los residuos de los que veníamos hablando. Cada punto negro tiene una línea roja que baja (o sube) en línea recta desde su ubicación hasta tocar la línea. Ya podemos ver que muchos de los puntos no están sobre la línea, así que sabemos que la línea se equivoca un poco en cada punto. La línea roja simplemente hace que sea más fácil ver exactamente cuánto se equivoca.\nLo importante que está ocurriendo acá es que la línea azul fue trazada de manera tal que minimiza la suma total de las líneas rojas. Por ejemplo, si quisiéramos saber cuán equivocada está esta línea, podríamos juntar todas las líneas rojas, medir cuánto miden, y sumar todas esas equivocaciones. Eso nos daría el error total. De hecho, ya hablamos de esta idea cuando discutimos la desviación estándar. Lo que realmente vamos a hacer con estas líneas rojas es elevar al cuadrado su longitud (para que los valores negativos no se cancelen), y luego sumarlas. Esa suma representa el total de error. Y esta línea azul minimiza esa suma. Cualquier otra línea tendría un error total mayor.\nLa Figura 4.8 es una animación que muestra esto en acción. La animación compara la línea azul (la de mejor ajuste) con otras líneas posibles (en negro). La línea negra se mueve hacia arriba y abajo. Las líneas rojas muestran el error entre la línea negra y los puntos de datos. A medida que la línea negra se acerca a la línea azul, el error total —representado visualmente como un área gris— se achica hasta su valor mínimo. Ese error total se agranda a medida que la línea negra se aleja de la línea azul.\n\n\n\n\n\n\n\n\nFigura 4.8: La línea azul es la línea de regresión que mejor explica la covariación entre los puntos negros. La línea negra se mueve hacia arriba y abajo, mostrando líneas alternativas que podrían trazarse. Las líneas rojas indican el error entre cada punto de datos y la línea negra. La cantidad total de error está representada por el área gris sombreada. Esta área se agranda a medida que la línea negra se aleja de la línea azul, y se achica al mínimo cuando la línea negra se acerca a la línea de mejor ajuste.”\n\n\n\n\n\nSiempre que la línea negra no coincida con la línea azul, es peor que la línea de mejor ajuste. La línea de regresión azul es como Ricitos de Oro: está justo en el medio, y es la adecuada.\nLa Figura 4.9 muestra cómo se comporta la suma de las desviaciones al cuadrado (la suma de las longitudes al cuadrado de las líneas rojas) a medida que movemos la línea hacia arriba y hacia abajo. Lo que está ocurriendo aquí es que estamos calculando una medida del error total a medida que la línea negra pasa por la línea de mejor ajuste. Esto representa la suma de las desviaciones al cuadrado. Es decir, elevamos al cuadrado la longitud de cada línea roja de la animación anterior, luego sumamos todas esas líneas rojas al cuadrado y obtenemos el error total (la suma total de las desviaciones al cuadrado).\nEl gráfico de abajo muestra cómo se ve el error total a medida que la línea negra se acerca y luego se aleja de la línea de mejor ajuste. Fijate que los puntos en este gráfico comienzan altos en el lado izquierdo, luego descienden hasta alcanzar un mínimo en la parte inferior central del gráfico. Cuando alcanzan ese punto mínimo, hemos encontrado una línea que minimiza el error total. Esa es la línea de regresión de mejor ajuste.\n\n\n\n\n\n\n\n\nFigura 4.9: Suma de los errores cuadrados para diferentes líneas con distintas intersecciones. La suma mínima ocurre con la línea de mejor ajuste.\n\n\n\n\n\nTodavía no hablamos de la intersección con el eje y. Pero este gráfico nos muestra cómo se comporta el error total al mover la línea hacia arriba o hacia abajo. La intersección con el eje y es lo que estamos cambiando cuando movemos la línea verticalmente. Como podés ver, el error aumenta cuando bajamos la línea desde 0 hasta –5, y también aumenta cuando la subimos desde 0 hasta +5. La mejor línea —la que minimiza el error— ocurre justo en el medio, cuando no movemos en absoluto la línea azul de regresión.\n\n\n4.5.2 Líneas\nBueno, decís. Así que hay una línea mágica que atraviesa el centro del diagrama de dispersión y minimiza la suma de los errores al cuadrado. ¿Cómo encuentro esa línea mágica? Te lo vamos a mostrar. Pero, para ser completamente honestos, casi nunca vas a hacerlo de la manera que vamos a explicar acá. En cambio, es mucho más fácil usar software y hacer que la computadora lo calcule por vos. Vas a aprender a hacerlo así en los laboratorios.\nAntes de mostrarte cómo encontrar la línea de regresión, vale la pena refrescar la memoria sobre cómo funcionan las líneas, especialmente en 2 dimensiones. ¿Te acordás de esto?\n\\(y = ax + b\\), o también \\(y = mx + b\\) (A veces se usa \\(a\\) o \\(m\\) para representar la pendiente)\nEsta es la fórmula de una línea recta. Otra forma de escribirla sería:\n\\(y = \\text{pendiente} * x + \\text{intercepto}\\)\nLa pendiente indica la inclinación de la línea, y el intercepto o la intersección con el eje y es el punto donde la línea cruza el eje vertical. Miremos las líneas de la Figura 4.10.\n\n\n\n\n\n\n\n\nFigura 4.10: Dos líneas diferentes con distintas intersecciones en el eje y (donde la línea cruza el eje vertical) y distintas pendientes. Una pendiente positiva hace que la línea suba de izquierda a derecha. Una pendiente negativa hace que la línea baje de izquierda a derecha.\n\n\n\n\n\nLa fórmula para la línea azul es: \\(y = 1*x + 5\\). Hablemos de eso. Cuando \\(x=0\\) , ¿dónde está la línea azul en el eje y? Está en 5.\nEso pasa porque 1 multiplicado por 0 es 0, y entonces nos queda solo el 5. ¿Y qué pasa cuando \\(x = 5\\)? En ese caso:\n\\(y = 1*x + 5y = 1*5 + 5 = 5 + 5 = 10\\)\nLa idea de la fórmula es decirte dónde está \\(y\\), para cualquier valor de \\(x\\). La pendiente de la línea te indica si la línea sube o baja cuando te movés de izquierda a derecha. La línea azul tiene una pendiente positiva de uno, así que sube a medida que \\(x\\) aumenta. ¿Cuánto sube? Sube uno por cada unidad de \\(x\\). Si hiciéramos que la pendiente fuera 2, la línea sería mucho más empinada y subiría más rápido. La línea roja tiene pendiente negativa, así que se inclina hacia abajo. Eso significa que \\(y\\) disminuye a medida que \\(x\\) aumenta. Y si no hay pendiente —es decir, queremos una línea completamente horizontal—, simplemente ponemos la pendiente en 0. Eso significa que \\(y\\) no cambia nada, aunque \\(x\\) aumente o disminuya.\nEso son las líneas.\n\n\n4.5.3 Calcular la línea de mejor ajuste\nSi tenés un diagrama de dispersión que muestra la ubicación de puntajes de dos variables, la verdadera pregunta es: ¿cómo encontrás la pendiente y la intersección con el eje y de la mejor línea de ajuste? ¿Qué vas a hacer? ¿Dibujar millones de líneas, sumar los residuos y ver cuál es la mejor? Eso llevaría una eternidad. Por suerte, existen las computadoras. Y si no tenés una a mano, también hay algunas fórmulas útiles.\n\n\n\n\n\n\nNota\n\n\n\nVale la pena mencionar cuánto cambiaron las cosas desde que existen las computadoras. Antes, todo el mundo tenía que hacer estos cálculos a mano —¡un embole total! Más allá de las ideas matemáticas profundas detrás de las fórmulas, muchas fueron diseñadas por conveniencia, para facilitar los cálculos a mano, porque no había computadoras. Ahora que las tenemos, hacer las cuentas a mano suele ser solo un ejercicio de álgebra… Tal vez sirve para forjar el carácter. Vos decidís.\n\n\nTe vamos a mostrar las fórmulas. Y vamos a resolver un ejemplo a mano. Sí, lo sabemos: es un horror. Por cierto, deberías sentir un poco de lástima por mí mientras hago todo esto a mano por vos.\nAcá están las dos fórmulas que podés usar para calcular la pendiente y la intersección directamente desde los datos. No vamos a explicar por qué hacen lo que hacen. Estas son solo fórmulas “prácticas” para facilitar los cálculos:\n\\(\\text{intercepto} = b = \\frac{\\sum{y}\\sum{x^2}-\\sum{x}\\sum{xy}}{n\\sum{x^2}-(\\sum{x})^2}\\)\n\\(\\text{pendiente}= m = \\frac{n\\sum{xy}-\\sum{x}\\sum{y}}{n\\sum{x^2}-(\\sum{x})^2}\\)\nEn estas fórmulas, \\(x\\) e \\(y\\) se refieren a los puntajes individuales. Acá tenés una tabla que muestra cómo encajan todas las partes.\n\n\n\n\n\nscores\nx\ny\nx_squared\ny_squared\nxy\n\n\n\n\n1\n1\n2\n1\n4\n2\n\n\n2\n4\n5\n16\n25\n20\n\n\n3\n3\n1\n9\n1\n3\n\n\n4\n6\n8\n36\n64\n48\n\n\n5\n5\n6\n25\n36\n30\n\n\n6\n7\n8\n49\n64\n56\n\n\n7\n8\n9\n64\n81\n72\n\n\nSumas\n34\n39\n200\n275\n231\n\n\n\n\n\nVemos 7 pares de valores para las variables \\(x\\) e \\(y\\). Calculamos \\(x^2\\) elevando al cuadrado cada valor de \\(x\\), y lo pusimos en una columna. Hicimos lo mismo con \\(y^2\\). Después calculamos \\(xy\\), multiplicando cada valor de \\(x\\) con su correspondiente valor de \\(y\\), y también lo pusimos en una columna. Finalmente, sumamos todas las columnas y pusimos esos totales al final. Esos son los números que necesitamos para usar las fórmulas y encontrar la mejor línea de ajuste.\nAsí se ven las fórmulas con los números incluidos:\n\\(\\text{intercepto} = b = \\frac{\\sum{y}\\sum{x^2}-\\sum{x}\\sum{xy}}{n\\sum{x^2}-(\\sum{x})^2} = \\frac{39 * 200 - 34*231}{7*200-34^2} = -.221\\)\n\\(\\text{pendiente} = m = \\frac{n\\sum{xy}-\\sum{x}\\sum{y}}{n\\sum{x^2}-(\\sum{x})^2} = \\frac{7*231-34*39}{7*275-34^2} = 1.19\\)\nGenial. Ahora podemos verificar si lo hicimos bien. Vamos a graficar los datos en un diagrama de dispersión y trazar una línea con pendiente = 1.19 y una intersección en el eje y de –0.221. Como se muestra en la Figura 4.11, la línea debería atravesar el centro de los puntos.\n\n\n\n\n\n\n\n\nFigura 4.11: Ejemplo de una línea de regresión con bandas de confianza que atraviesa algunos puntos en un diagrama de dispersión",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Correlación</span>"
    ]
  },
  {
    "objectID": "03-Correlation.html#interpretar-correlaciones",
    "href": "03-Correlation.html#interpretar-correlaciones",
    "title": "4  Correlación",
    "section": "4.6 Interpretar correlaciones",
    "text": "4.6 Interpretar correlaciones\n¿Qué significa que haya —o no haya— una correlación entre dos medidas? ¿Cómo deberían interpretarse las correlaciones? ¿Qué tipo de inferencias se pueden hacer a partir de una correlación? Todas estas son preguntas excelentes. El primer consejo es: tené cuidado al interpretar correlaciones. Y ahora te contamos por qué.\n\n4.6.1 Correlación no implica causalidad\nProbablemente ya escuchaste que “correlación no implica causalidad”. ¿Pero por qué no? Hay muchas razones. Sin embargo, antes de meternos con las razones, empecemos con un caso en el que sí esperaríamos una conexión causal entre dos medidas. Supongamos que comprás una planta serpiente (Sansevieria) para tu casa. Estas plantas son conocidas por ser fáciles de cuidar, porque casi no requieren atención. Como la mayoría de las plantas, necesitan algo de agua para sobrevivir. Pero también necesitan la cantidad justa de agua. Imaginemos un experimento donde cultivás 1000 plantas serpiente en tu casa. Cada planta recibe una cantidad diferente de agua por día, desde 0 cucharaditas hasta 1000 cucharaditas por día. Vamos a asumir que el agua forma parte del proceso causal que permite que las plantas crezcan. Entonces, la cantidad de agua por día sería una de nuestras medidas. Y cada semana, medís cuánto crecieron las plantas. Esa sería la segunda medida. Ahora, ¿te podés imaginar cómo se vería un gráfico de dispersión con estos datos? Eje X: cucharaditas de agua por día. Eje Y: crecimiento semanal de la planta.\n\n4.6.1.1 Incluso cuando hay causalidad, puede que no haya correlación clara\nLa primera planta, que no recibe nada de agua, seguramente lo va a pasar mal y eventualmente morir. Debería mostrar el menor crecimiento semanal. ¿Y las plantas que reciben solo unas pocas cucharaditas al día? Quizás reciban justo lo necesario para mantenerse vivas, pero no para crecer mucho. Si estás imaginando un gráfico de dispersión, cada punto sería una planta. Algunos puntos deberían estar en la esquina inferior izquierda (nada de agua = nada de crecimiento). A medida que aumenta el agua, los puntos deberían ir subiendo hacia la derecha: más agua, más crecimiento. ¿Pero qué pasa cuando le das demasiada agua a una planta serpiente? Por experiencia personal: se muere. Así que en algún momento, los puntos del gráfico deberían empezar a bajar otra vez. Las plantas que reciben demasiada agua no crecen bien.\nEse gráfico imaginario tendría una forma de U invertida. De izquierda a derecha, los puntos suben, llegan a un máximo, y después vuelven a bajar. ¿Y qué pasa si calculás el r de Pearson con esos datos? Te puede dar un valor cercano a 0. El gráfico de dispersión se vería más o menos como en la Figura 4.12.\n\n\n\n\n\n\n\n\nFigura 4.12: Ilustración de una posible relación entre la cantidad de agua y el crecimiento de la planta serpiente. El crecimiento aumenta con el agua, pero eventualmente vuelve a disminuir, porque demasiada agua hace que las plantas mueran.\n\n\n\n\n\nEs cierto que parece más una V invertida que una U invertida, pero se entiende la idea, ¿no? Hay claramente una relación entre el riego y el crecimiento de la planta serpiente. Pero esa relación no es en una sola dirección. Como resultado, cuando calculamos el r de Pearson, obtenemos un valor que sugiere que no hay relación.\n\n#&gt; [1] 0.002927243\n\nLo que esto significa, en realidad, es que no hay una relación lineal que se pueda describir con una sola línea recta. Cuando necesitamos líneas o curvas que cambien de dirección, estamos ante una relación no lineal.\nEste ejemplo muestra algunas complicaciones al interpretar correlaciones. Ya sabemos que las plantas necesitan agua para crecer, así que esperamos que haya alguna relación entre la cantidad de agua y el crecimiento. Si miramos solo la primera mitad de los datos, vemos una correlación positiva. Si miramos solo la segunda mitad, vemos una correlación negativa. Y si miramos todos los datos juntos, no vemos ninguna correlación. Uf. Entonces, incluso cuando hay una conexión causal entre dos variables, eso no garantiza que obtengamos evidencia clara al calcular un coeficiente de correlación.\n\n🧠 Consejo pro: Esta es una de las razones por las que es tan importante graficar tus datos. Si ves un patrón en forma de U invertida, entonces un análisis de correlación probablemente no sea el mejor para esos datos.\n\n\n\n4.6.1.2 Variable confundida, o problema de la tercera variable\nCualquiera puede correlacionar dos cosas que se puedan medir. Por ejemplo, podríamos tomar a 100 personas y preguntarles todo tipo de cosas, como:\n\n¿Qué tan feliz sos?\n\n¿Qué edad tenés?\n\n¿Cuánto medís?\n\n¿Cuánto ganás por año?\n\n¿Qué tan largas son tus pestañas?\n\n¿Cuántos libros leíste en tu vida?\n\n¿Qué tan fuerte es tu voz interior?\n\nSupongamos que encontramos una correlación positiva entre el salario anual y la felicidad. Notá que podríamos haber hecho exactamente lo mismo correlacionando felicidad con salario. Si encontramos una correlación, ¿te animarías a decir que el salario causa felicidad? Tal vez tiene algo que ver. Pero algo como la felicidad seguramente tiene muchas causas. La plata podría hacer feliz a algunas personas directamente. Pero, más probablemente, la plata les permite acceder a otras cosas… y quizás algunas de esas cosas sí generen felicidad. A esas “otras cosas” les decimos variables de tercer orden o terceras variables. Por ejemplo: quizás las personas que viven en lugares más lindos, con casas más caras, son más felices que quienes viven en lugares peores, con casas más baratas. En ese escenario, no sería la plata la que causa felicidad, sino los lugares y casas que la plata puede comprar.\nY aunque eso fuera cierto, las personas pueden ser más o menos felices en un montón de situaciones diferentes.\nLa lección acá es que una correlación puede aparecer entre dos medidas debido a una tercera variable no medida directamente. Entonces, aunque encontremos una correlación, no significa que podamos concluir nada firme sobre causalidad.\n\n\n\n4.6.2 Correlación y el azar\nOtro aspecto muy importante de las correlaciones es el hecho de que pueden producirse por puro azar. Esto significa que podés encontrar una correlación positiva o negativa entre dos medidas aunque no tengan absolutamente nada que ver una con la otra. Tal vez esperarías que la correlación fuera cero cuando dos medidas están completamente desligadas. Aunque eso puede pasar, también puede suceder que dos variables no relacionadas produzcan correlaciones espurias simplemente por azar.\nVamos a demostrar cómo pueden aparecer correlaciones por azar incluso cuando no hay ninguna conexión causal entre las medidas. Imaginá dos personas participantes. Una está en el Polo Norte con una máquina de lotería llena de bolillas numeradas del 1 al 10. La otra está en el Polo Sur, con otra máquina diferente pero también con bolillas del 1 al 10. Hay un suministro infinito de bolillas en cada máquina, así que cualquier número puede salir en cualquier momento. Cada participante elige 10 bolillas al azar y anota los números. En este caso vamos a asumir que no hay forma de que las bolillas elegidas por un participante puedan influir causalmente en las del otro. Están en extremos opuestos del planeta. Así que debemos asumir que los números son elegidos por azar puro.\nEsto es lo que podrían mostrar los números elegidos por cada participante:\n\n\n\n\n\nBolilla\nPolo_norte\nPolo_sur\n\n\n\n\n1\n9\n5\n\n\n2\n2\n2\n\n\n3\n5\n3\n\n\n4\n2\n3\n\n\n5\n6\n9\n\n\n6\n8\n2\n\n\n7\n8\n7\n\n\n8\n9\n4\n\n\n9\n8\n1\n\n\n10\n1\n5\n\n\n\n\n\nEn este caso puntual, si calculamos el r de Pearson, podríamos obtener, por ejemplo, un valor como ( r = 0.51 ). Pero ya sabemos que ese valor no nos dice nada sobre la relación entre las bolillas del Polo Norte y las del Polo Sur. Sabemos que esa relación debería ser completamente aleatoria, porque así diseñamos el experimento.\nLa mejor pregunta en este caso no es “¿cuál fue la correlación?”, sino ¿qué puede producir el azar? Por ejemplo: si repitiéramos este juego una y otra vez miles de veces —cada vez eligiendo bolillas nuevas y cada vez calculando la correlación—, ¿qué obtendríamos? Primero, notaríamos variación. El valor de r a veces sería positivo, a veces negativo, a veces grande y a veces pequeño. Segundo, podríamos ver cómo se ve esa variación. Esto nos daría una ventana hacia los tipos de correlaciones que el azar, por sí solo, puede producir. Vamos a ver qué pasa.\n\n4.6.2.1 Simulación Monte Carlo de correlaciones aleatorias\nEs posible usar una computadora para simular nuestro juego tantas veces como queramos. Este proceso se conoce como simulación Monte Carlo.\nAbajo hay un script escrito en el lenguaje de programación R. No vamos a entrar en los detalles del código ahora. Pero vamos a explicar brevemente qué está haciendo. Fijate en la parte que dice que se crea un bucle que repite nuestro juego 1000 veces. Dentro del bucle hay variables llamadas, por ejemplo, x e y. Durante cada simulación, se generan al azar 10 números (entre 1 y 10) para cada variable. Esos números representan las bolillas que habrían salido de la máquina de lotería. Una vez que tenemos los 10 números para cada una, calculamos la correlación usando cor(x, y). Luego, guardamos ese valor de correlación y pasamos a la próxima simulación. Al final, vamos a tener 1000 valores de correlación de Pearson.\n\ncorrelaciones_simuladas &lt;- length(0)\nfor(sim in 1:1000){\n  Polo_norte &lt;- runif(10,1,10)\n  Polo_sur &lt;- runif(10,1,10)\n  correlaciones_simuladas[sim] &lt;- cor(Polo_norte,Polo_sur)\n}\n\nsim_df &lt;- data.frame(simulaciones=1:1000,correlaciones_simuladas)\n\nggplot(sim_df, aes(x = simulaciones, y = correlaciones_simuladas))+\n  geom_point()+\n  theme_classic()+\n  geom_hline(yintercept = -1)+\n  geom_hline(yintercept = 1)+\n  ggtitle(\"Simulación de 1000 valores de r\")+\n  labs(x=\"Correlación simulada\",y=\"Frecuencia\")\n\n\n\n\n\n\n\nFigura 4.13: Otra figura que muestra el rango de valores de r que pueden obtenerse por azar.\n\n\n\n\n\nLa Figura 4.13 muestra esos 1000 valores de \\(r\\) obtenidos en la simulación. ¿Te resulta familiar el gráfico? Ya hicimos una simulación parecida antes. Cada punto en el gráfico representa el valor de ( r ) de una de las 1000 simulaciones. Como ves, los puntos están por todos lados, entre –1 y 1. La lección importante acá es que el azar produjo todas estas correlaciones. Eso quiere decir que podemos encontrar “correlaciones” en los datos que son completamente sin sentido, y que no reflejan ninguna relación causal entre una medida y otra.\nVamos a ilustrar esta idea de encontrar correlaciones “al azar” una vez más, con una pequeña animación. Esta vez, te vamos a mostrar un gráfico de dispersión con los valores aleatorios de las bolillas elegidas desde el Polo Norte y el Polo Sur. Si no hay relación, deberíamos ver puntos por todos lados. Si llega a aparecer una relación positiva (por pura casualidad), los puntos irán de abajo a la izquierda hacia arriba a la derecha. Si aparece una relación negativa (también por azar), los puntos irán de arriba a la izquierda hacia abajo a la derecha.\nUna cosa más para prepararte para la animación: abajo, en la Figura 4.14, hay tres diagramas de dispersión que muestran correlaciones negativa, positiva y nula entre dos variables. Ya viste ese gráfico antes. Solo te estamos recordando que las líneas azules ayudan a ver la dirección de la correlación. Las correlaciones negativas ocurren cuando la línea baja de izquierda a derecha. Las positivas, cuando la línea sube de izquierda a derecha. Y las correlaciones nulas, cuando la línea es plana (ni sube ni baja).\n\n\n\n\n\n\n\n\nFigura 4.14: Un recordatorio de cómo se ven las correlaciones positiva, negativa y nula.\n\n\n\n\n\nOk, ahora estamos listos para la animación. La Figura 4.15 muestra el proceso de muestreo aleatorio de dos conjuntos de números: uno para la variable X y otro para la variable Y. Cada vez tomamos 10 números para cada variable, los graficamos, y trazamos una línea a través de los puntos. Recordá: estos números son completamente aleatorios, así que deberíamos esperar, en promedio, que no haya correlación entre ellos. Sin embargo, eso no es lo que pasa. Podés ver cómo la línea cambia todo el tiempo. A veces encontramos una correlación negativa (la línea baja), otras veces una correlación positiva (la línea sube), y otras veces parece que no hay ninguna correlación (la línea es más plana).\n\n\n\n\n\n\n\n\nFigura 4.15: Puntos de datos completamente aleatorios tomados de una distribución uniforme, con un tamaño muestral pequeño de 10. La línea azul gira a veces mostrando correlaciones grandes que se producen por azar\n\n\n\n\n\nCapaz estás pensando que esto es un poco inquietante. Si sabemos que no debería haber ninguna correlación entre dos variables aleatorias, ¿cómo puede ser que estemos encontrando correlaciones? ¿No es un gran problema? O sea, si alguien me muestra una correlación entre dos cosas y me dice que están relacionadas, ¿cómo puedo saber si eso es verdad? Después de todo, ¡podría ser pura suerte! El azar también puede producir eso.\nPor suerte, no todo está perdido. Podemos mirar nuestros datos simulados de otra manera, usando un histograma. ¿Te acordás que antes de la animación hicimos 1000 simulaciones con números aleatorios y calculamos el valor de (r) para cada una? Si ponemos todos esos valores de (r) en un histograma, vamos a tener una mejor idea de cómo se comporta el azar. Vamos a ver qué tipo de correlaciones es más o menos probable que el azar produzca. La Figura 4.16 muestra ese histograma de los valores simulados de \\(r\\).\n\n\n\n\n\n\n\n\nFigura 4.16: Histograma que muestra la distribución de frecuencia de los valores de r obtenidos entre dos variables X e Y completamente aleatorias (tamaño muestral = 10). Todo el rango de valores de r puede surgir solo por azar. Los valores grandes de r son menos comunes que los pequeños.\n\n\n\n\n\nNotá que este histograma no es plano. La mayoría de los valores de (r) simulados están cerca de cero. Y también podés ver que las barras se achican a medida que te alejás de cero, ya sea en dirección positiva o negativa. El mensaje principal acá es que el azar puede producir un rango amplio de correlaciones. Sin embargo, no todas ocurren con la misma frecuencia. Por ejemplo, las barras cercanas a –1 y 1 son muy bajas. El azar casi nunca produce correlaciones perfectas. Las barras alrededor de –0.5 y 0.5 también son más chicas que las del centro: eso significa que correlaciones medianas no son tan comunes cuando los datos son puramente aleatorios.\nPodés pensar este histograma como una “ventana del azar”. Muestra lo que el azar suele hacer, y lo que no suele hacer. Si encontraste una correlación en estas mismas condiciones (por ejemplo, mediste la correlación entre dos conjuntos de 10 números al azar), entonces podrías consultar esta ventana. ¿Qué le preguntarías a la ventana? Algo como: ¿mi correlación observada (la que encontraste en tus datos) podría haber salido de esta ventana? Ponele que encontraste una correlación de r=0.1. ¿Podría haber salido un r=0.1 de este histograma? Bueno, mirá el histograma alrededor del punto 0.1 en el eje x. ¿Hay una barra grande ahí? Si la hay, eso significa que el azar genera ese valor bastante seguido. Podrías sentirte cómodx sacando la siguiente conclusión: sí, este 0.1 podría haber sido producto del azar, porque está bien dentro de la ventana del azar. ¿Y qué pasa con un r=0.5? La barra es mucho más chica ahí. Podrías pensar: “bueno, veo que el azar genera 0.5 algunas veces, así que podría haber sido producto del azar. ¿Lo fue? Tal vez sí, tal vez no, no estoy segurx”. En este punto, tu confianza para sacar una conclusión fuerte sobre el rol del azar quizás empieza a tambalear un poco. ¿Y un r=0.95? Capaz ves que la barra para 0.95 es muy muy chiquita, quizás tan chiquita que ni se ve. ¿Qué te dice eso? Que el azar no produce 0.95 muy seguido, casi nunca. Entonces, si encontraste un 0.95 en tus datos, ¿qué podrías inferir? Quizás te sentirías cómodx diciendo que el azar no generó ese 0.95, porque el 0.95 está casi completamente fuera de la ventana del azar.\nLa Figura 4.17 muestra cuatro histogramas diferentes con los valores de correlación de Pearson ( r ) para cada uno de los escenarios. Cada escenario involucra un tamaño muestral distinto: 10, 50, 100 y 1000.\n\n\n\n\n\n\n\n\nFigura 4.17: Histograma de 1000 valores simulados de r, generados al azar, con diferentes tamaños muestrales. Cada panel muestra el histograma para N = 10, 50, 100 y 1000. Cuanto más grande el tamaño muestral, más cerca de 0 tienden los valores de r generados por azar.\n\n\n\n\n\nSi observás los cuatro histogramas, deberías notar un patrón claro. El ancho o rango de cada histograma se achica a medida que aumenta el tamaño muestral. ¿Qué está pasando acá? Bueno, ya sabemos que podemos pensar estos histogramas como ventanas del azar. Nos muestran qué valores de ( r ) ocurren con más frecuencia, y cuáles no.\nCuando el tamaño muestral es 10, aparecen muchos valores diferentes de ( r ). Ese histograma es bastante plano y extendido. Pero a medida que el tamaño muestral aumenta, vemos que la ventana del azar se vuelve más angosta. Por ejemplo, cuando llegamos a N = 1000, casi todos los valores de ( r ) están muy cerca de 0.\nUna conclusión importante es que aumentar el tamaño muestral estrecha la ventana del azar. Entonces, si hacés un estudio con 1000 observaciones por variable y encontrás una correlación de 0.5, podés mirar el histograma de abajo a la derecha y ver que ese valor no ocurre muy seguido solo por azar. De hecho, puede que no aparezca ni una sola vez en 1000 simulaciones.\nComo resultado, si tenés una muestra grande como N = 1000 y encontrás una correlación observada de 0.5, podés estar más seguro de que no es espuria. Si no fue el azar el que la produjo, entonces algo más sí lo hizo.\nPor último, notá cómo tu confianza respecto a si el azar está afectando tus resultados depende del tamaño de tu muestra. Si solo tenés 10 observaciones por variable y encontrás ( r = 0.5 ), no deberías estar tan seguro de que refleja una relación real. Como ves, valores de ( r = 0.5 ) ocurren bastante seguido solo por azar cuando la muestra es pequeña.\n\n🧠 Consejo pro: cuando diseñás un experimento, vos decidís cuántas muestras vas a recolectar. Eso significa que podés elegir estrechar la ventana del azar. Así, si encontrás una relación en los datos, podés tener más confianza en que es real, y no un simple capricho del azar.\n\n\n\n\n4.6.3 Algunas películas más\nVamos a reforzar estas ideas con algunas animaciones más. Cuando el tamaño muestral es pequeño (N es chico), el error muestral puede causar todo tipo de “patrones” en los datos. Esto hace posible —y de hecho común— que aparezcan “correlaciones” entre dos conjuntos de números. Cuando aumentamos el tamaño muestral, el error muestral se reduce, lo que hace menos probable que aparezcan correlaciones solo por azar. Cuando N es grande, el azar tiene menos oportunidades de operar.\n\n4.6.3.1 Ver cómo se comporta la correlación cuando no hay correlación\nAbajo se muestran números generados aleatoriamente para dos variables, los graficamos, y mostramos la correlación usando una línea. Hay cuatro paneles, y cada uno muestra un tamaño muestral distinto: 10, 50, 100 y 1000 observaciones en cada muestra.\nRecordá: como estamos muestreando números aleatoriamente, no debería haber ninguna relación entre las variables X e Y. Pero, como venimos discutiendo, a veces podemos observar una correlación —producto del azar. Lo importante es mirar cómo se comporta la línea en los cuatro paneles de la Figura 4.18.\nCon tamaño muestral de 10, la línea gira para cualquier lado. También se mueve bastante con tamaños de 50 o 100. Incluso con 1000 se mueve un poco, pero mucho menos. En todos los casos esperamos que la línea sea plana, pero cada vez que tomamos nuevas muestras, a veces la línea nos muestra patrones falsos.\n\n\n\n\n\n\n\n\nFigura 4.18: Animación de cómo se comporta la correlación entre variables X e Y completamente aleatorias en función del tamaño de la muestra. La línea de mejor ajuste no es muy estable con tamaños de muestra pequeños, pero se vuelve más consistentemente plana a medida que aumenta el tamaño de la muestra.\n\n\n\n\n\n¿En cuál de estas líneas deberías creer? Bueno, esperamos que veas que la línea correspondiente a N = 1000 es la más estable. Tiende a ser muy plana cada vez, y no depende tanto de la muestra en particular. En cambio, la línea con solo 10 observaciones va para cualquier lado. La conclusión acá es que si alguien te dice que encontró una correlación, deberías preguntar cuántas observaciones tenía en su muestra. Si solo tenía 10 observaciones, ¿cómo podrías confiar en esa afirmación? ¡No podés! Ahora que sabés que muestras tan chicas pueden producir cualquier cosa solo por azar, deberías desconfiar. En cambio, si descubrís que la muestra era grande, podés confiar un poco más en el hallazgo.\nEn el ejemplo anterior, muestreamos números de una distribución uniforme. Muchos datos reales provienen de una distribución normal (o aproximadamente normal). Podemos repetir lo anterior, pero muestreando de una misma distribución normal. Igual habrá cero correlación real entre X e Y, porque todo fue generado al azar. La Figura 4.19 muestra el mismo comportamiento. Con tamaños muestrales pequeños, la correlación calculada fluctúa mucho. Con muestras grandes, no.\n\n\n\n\n\n\n\n\nFigura 4.19: Animación de la correlación para valores aleatorios muestreados de una distribución normal, en lugar de una distribución uniforme.\n\n\n\n\n\nOK, ¿y cómo se ven las cosas cuando realmente hay una correlación entre las variables?\n\n\n\n4.6.4 Ver cómo se comporta la correlación cuando realmente hay una correlación\nA veces sí existen correlaciones entre dos variables que no son producto del azar. La Figura 4.20 muestra una animación con cuatro gráficos de dispersión. Cada uno muestra la correlación entre dos variables. Nuevamente, variamos el tamaño muestral: 10, 50, 100 y 1000 observaciones. Los datos fueron generados de forma que contengan una correlación positiva real. Así que deberíamos esperar que la línea suba desde la esquina inferior izquierda hacia la esquina superior derecha. Sin embargo, los datos todavía tienen variabilidad. Así que esta vez, el error muestral debido al azar va a “difuminar” un poco la correlación. Sabemos que está ahí, pero a veces el azar puede hacer que desaparezca.\nNotá que en el panel superior izquierdo (tamaño muestral = 10), la línea se mueve mucho más que en los otros paneles. Cada nuevo conjunto de muestras produce correlaciones distintas. A veces, la línea incluso se aplana o baja. Pero a medida que aumentamos el tamaño muestral, vemos que la línea no cambia tanto: siempre va hacia arriba, mostrando una correlación positiva.\n\n\n\n\n\n\n\n\nFigura 4.20: Cómo se comporta la correlación en función del tamaño muestral cuando existe una correlación real entre las variables X e Y.\n\n\n\n\n\nLa lección principal acá es que incluso cuando hay una correlación positiva real entre dos variables, podrías no verla si tu muestra es muy pequeña. Por ejemplo, podrías tener mala suerte con la muestra que recolectaste. Esa muestra podría mostrar una correlación negativa, ¡aunque la verdadera correlación sea positiva!\nLamentablemente, en el mundo real casi siempre tenemos solo una muestra, la que logramos recolectar. Así que siempre tenemos que preguntarnos si tuvimos suerte… o no. Por suerte, si querés eliminar el factor suerte, todo lo que necesitás es recolectar una muestra más grande. Así vas a tener muchas más chances de observar el patrón real, y no uno que apareció por azar.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Correlación</span>"
    ]
  },
  {
    "objectID": "03-Correlation.html#resumen",
    "href": "03-Correlation.html#resumen",
    "title": "4  Correlación",
    "section": "4.7 Resumen",
    "text": "4.7 Resumen\nEn esta sección hablamos sobre la correlación, y empezamos a construir algunas intuiciones sobre estadística inferencial, que es el gran tema de los próximos capítulos. Por ahora, las ideas clave son:\n\nPodemos medir relaciones en los datos usando herramientas como la correlación.\n\nLas correlaciones que encontramos pueden deberse a muchas causas distintas, por eso son difíciles de interpretar.\n\nEl azar puede producir correlaciones, así que existe la posibilidad de que sean completamente sin sentido.\n\nSin embargo, podemos crear un modelo que represente exactamente lo que el azar puede hacer. Ese modelo nos dice si es más o menos probable que el azar genere correlaciones de cierto tamaño.\n\nPodemos usar ese modelo del azar para ayudarnos a decidir sobre nuestros propios datos. Podemos comparar la correlación que encontramos con ese modelo, y preguntarnos si es probable que haya sido generada por azar.\n\n\n\n\n\nSalsburg, David. 2001. The Lady Tasting Tea: How Statistics Revolutionized Science in the Twentieth Century. Macmillan.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Correlación</span>"
    ]
  },
  {
    "objectID": "13-Gifs.html",
    "href": "13-Gifs.html",
    "title": "13  GIFs",
    "section": "",
    "text": "13.1 Correlation GIFs\nNote regression lines and confidence bands can be added using geom_smooth(method=lm, se=T)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>GIFs</span>"
    ]
  },
  {
    "objectID": "04-SamplesPopulations.html",
    "href": "04-SamplesPopulations.html",
    "title": "5  Probabilidad, muestreo y estimación",
    "section": "",
    "text": "5.1 ¿En qué se diferencian la probabilidad y la estadística?\nHasta ahora en el libro, hablamos sobre algunas de las ideas clave en el diseño experimental, y comentamos un poco cómo podés resumir un conjunto de datos. Para muchas personas, eso es todo lo que hay en estadística: se trata de calcular promedios, juntar números, hacer gráficos y poner todo eso en un informe. Algo así como coleccionar estampillas, pero con números. Sin embargo, la estadística abarca mucho más que eso. De hecho, la estadística descriptiva es una de las partes más pequeñas —y menos potentes— de la estadística. La parte más grande y útil de la estadística es que nos da herramientas para hacer inferencias a partir de los datos.\nCuando empezás a pensar en la estadística en estos términos —que está ahí para ayudarnos a sacar conclusiones a partir de los datos—, empezás a ver ejemplos por todas partes. Por ejemplo, acá hay un pequeño extracto de un artículo del Sydney Morning Herald (30 de octubre de 2010):\nEste tipo de afirmación es totalmente común en los diarios y en la vida cotidiana, pero pensemos por un momento qué implica. Una encuestadora hizo un sondeo —probablemente bastante grande porque pueden pagarlo—. No tengo ganas de buscar el informe original, así que imaginemos que llamaron a 1000 votantes de New South Wales al azar, y que 230 (23%) dijeron que pensaban votar al Partido Laborista Australiano. Para las elecciones federales de 2010, la Comisión Electoral Australiana informó que había 4.610.795 votantes registrados en New South Wales; así que las opiniones de los 4.609.795 votantes restantes (aproximadamente el 99,98%) no las conocemos. Incluso suponiendo que nadie le mintió a la encuestadora, lo único que podemos decir con 100% de certeza es que el voto real está entre 230/4.610.795 (aprox. 0,005%) y 4.610.025/4.610.795 (aprox. 99,83%). Entonces, ¿en qué se basa la encuestadora, el diario y la opinión pública para concluir que el voto al partido es aproximadamente del 23%?\nLa respuesta es bastante obvia: si llamo a 1000 personas al azar y 230 dicen que votarían a ese partido, parece muy poco probable que sean las únicas 230 personas en toda la población que lo harían. En otras palabras, asumimos que los datos recolectados por la encuestadora representan bastante bien a toda la población. Pero, ¿cuán representativos son? ¿Nos sorprendería descubrir que el verdadero apoyo al partido es en realidad del 24%? ¿Del 29%? ¿Del 37%? Acá es donde la intuición cotidiana empieza a fallar. A nadie le sorprendería un 24%, y a todo el mundo le sorprendería un 37%, pero no es tan fácil decidir si un 29% sería plausible. Necesitamos herramientas más potentes que simplemente mirar los números y adivinar.\nLa estadística inferencial proporciona las herramientas necesarias para responder ese tipo de preguntas. Y como ese tipo de preguntas están en el corazón del trabajo científico, ocupan la mayor parte de cualquier curso introductorio de estadística o métodos de investigación. Y como ese tipo de preguntas están en el corazón del trabajo científico, ocupan la mayor parte de cualquier curso introductorio de estadística o métodos de investigación. Sin embargo, nuestras herramientas para hacer inferencias estadísticas están: 1. construidas sobre la base de la teoría de la probabilidad, y 2. requieren entender cómo se comportan las muestras cuando se extraen de distribuciones (definidas por la teoría de la probabilidad…). Así que este capítulo tiene dos partes principales: una breve introducción a la teoría de la probabilidad, y una introducción al muestreo a partir de distribuciones.\nAntes de empezar a hablar de teoría de la probabilidad, conviene dedicar un momento a pensar en la relación entre probabilidad y estadística. Las dos disciplinas están muy relacionadas, pero no son lo mismo. La teoría de la probabilidad es “la doctrina del azar”. Es una rama de las matemáticas que te dice con qué frecuencia ocurren distintos tipos de eventos. Por ejemplo, todas estas preguntas pueden responderse usando teoría de la probabilidad:\nFijate que todas estas preguntas tienen algo en común. En cada caso la “verdad del mundo” es conocida, y mi pregunta se relaciona con “qué clase de eventos” van a ocurrir. En la primera pregunta, sabemos que la moneda es justa, así que hay un 50% de probabilidad de que cualquier lanzamiento individual salga cara. En la segunda, sabemos que la chance de sacar un 6 con un dado es 1 en 6. En la tercera, sabemos que el mazo está bien mezclado. Y en la cuarta, sabemos que la lotería sigue reglas específicas. Ya te hacés una idea. Lo importante es que las preguntas de probabilidad parten de un modelo conocido del mundo, y usamos ese modelo para hacer algunos cálculos.\nEl modelo subyacente puede ser muy simple. Por ejemplo, en el caso de la moneda, podemos escribir el modelo así: \\(P(\\text{cara}) = 0.5\\) que se lee: “la probabilidad de que salga cara es 0.5”.\nComo veremos más adelante, así como los porcentajes van de 0% a 100%, las probabilidades son simplemente números que van de 0 a 1. Cuando usamos este modelo para responder la primera pregunta, en realidad no sabemos exactamente qué va a pasar. Tal vez salgan 10 caras, como plantea la pregunta. Pero también podrían salir 3 caras. Eso es lo importante: en la teoría de la probabilidad, el modelo es conocido, pero los datos no lo son.\nEso es la probabilidad. ¿Y la estadística? Las preguntas estadísticas funcionan al revés. En estadística, no sabemos cómo es el mundo en realidad. Lo único que tenemos son datos, y a partir de ellos queremos aprender la verdad sobre el mundo. Las preguntas estadísticas suelen ser más del tipo:\nEsta vez, lo único que tenemos son los datos. Lo que sé es que vi a mi amiga lanzar la moneda 10 veces y salió cara todas las veces. Y lo que quiero inferir es si debería concluir que lo que acabo de ver fue realmente una moneda justa siendo lanzada 10 veces seguidas, o si debería sospechar que mi amiga me está haciendo una trampa. Los datos que tengo se ven así:\ny lo que estoy intentando hacer es decidir en cuál “modelo del mundo” debería confiar. Si la moneda es justa, entonces el modelo que debería adoptar es uno en el que la probabilidad de que salga cara es 0.5; es decir, \\(P(\\text{cara}) = 0.5\\) Si la moneda no es justa, entonces debería concluir que la probabilidad de que salga cara no es 0.5, lo que escribiríamos como \\(P(\\text{cara}) \\neq 0.5\\). En otras palabras, el problema de la inferencia estadística consiste en darme cuenta cuál de estos modelos de probabilidad es el correcto. Está claro que la pregunta estadística no es la misma que la pregunta de probabilidad, pero están profundamente relacionadas. Por eso, una buena introducción a la teoría estadística comienza con una discusión sobre qué es la probabilidad y cómo funciona.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probabilidad, muestreo y estimación</span>"
    ]
  },
  {
    "objectID": "04-SamplesPopulations.html#en-qué-se-diferencian-la-probabilidad-y-la-estadística",
    "href": "04-SamplesPopulations.html#en-qué-se-diferencian-la-probabilidad-y-la-estadística",
    "title": "5  Probabilidad, muestreo y estimación",
    "section": "5.1 ¿En qué se diferencian la probabilidad y la estadística?",
    "text": "5.1 ¿En qué se diferencian la probabilidad y la estadística?\nAntes de empezar a hablar de teoría de la probabilidad, conviene dedicar un momento a pensar en la relación entre probabilidad y estadística. Las dos disciplinas están muy relacionadas, pero no son lo mismo. La teoría de la probabilidad es “la doctrina del azar”. Es una rama de las matemáticas que te dice con qué frecuencia ocurren distintos tipos de eventos. Por ejemplo, todas estas preguntas pueden responderse usando teoría de la probabilidad:\n\n¿Cuál es la probabilidad de que una moneda salga cara 10 veces seguidas?\nSi tiro dos dados de seis caras, ¿qué tan probable es que salgan dos seises?\n¿Qué probabilidad hay de que cinco cartas extraídas al azar de un mazo bien mezclado sean todas corazones?\n¿Qué chance tengo de ganar la lotería?\n\nFijate que todas estas preguntas tienen algo en común. En cada caso la “verdad del mundo” es conocida, y mi pregunta se relaciona con “qué clase de eventos” van a ocurrir. En la primera pregunta, sabemos que la moneda es justa, así que hay un 50% de probabilidad de que cualquier lanzamiento individual salga cara. En la segunda, sabemos que la chance de sacar un 6 con un dado es 1 en 6. En la tercera, sabemos que el mazo está bien mezclado. Y en la cuarta, sabemos que la lotería sigue reglas específicas. Ya te hacés una idea. Lo importante es que las preguntas de probabilidad parten de un modelo conocido del mundo, y usamos ese modelo para hacer algunos cálculos.\nEl modelo subyacente puede ser muy simple. Por ejemplo, en el caso de la moneda, podemos escribir el modelo así:$P() = 0.5 $que se lee: “la probabilidad de que salga cara es 0.5”.\nComo veremos más adelante, así como los porcentajes van de 0% a 100%, las probabilidades son simplemente números que van de 0 a 1. Cuando usamos este modelo para responder la primera pregunta, en realidad no sabemos exactamente qué va a pasar. Tal vez salgan 10 caras, como plantea la pregunta. Pero también podrían salir 3 caras. Eso es lo importante: en la teoría de la probabilidad, el modelo es conocido, pero los datos no lo son.\nEso es la probabilidad. ¿Y la estadística? Las preguntas estadísticas funcionan al revés. En estadística, no sabemos cómo es el mundo en realidad. Lo único que tenemos son datos, y a partir de ellos queremos aprender la verdad sobre el mundo. Las preguntas estadísticas suelen ser más del tipo:\n\nSi mi amiga lanza una moneda 10 veces y sale cara todas las veces, ¿me está haciendo trampa?\nSi saco cinco cartas y todas son corazones, ¿qué tan probable es que el mazo estuviera mezclado?\nSi la pareja del comisario de la lotería gana la lotería, ¿qué tan probable es que esté arreglada?\n\nEsta vez, lo único que tenemos son los datos. Lo que sé es que vi a mi amiga lanzar la moneda 10 veces y salió cara todas las veces. Y lo que quiero inferir es si debería concluir que lo que acabo de ver fue realmente una moneda justa siendo lanzada 10 veces seguidas, o si debería sospechar que mi amiga me está haciendo una trampa. Los datos que tengo se ven así:\nH H H H H H H H H H\ny lo que estoy intentando hacer es decidir en cuál “modelo del mundo” debería confiar. Si la moneda es justa, entonces el modelo que debería adoptar es uno en el que la probabilidad de que salga cara es 0.5; es decir,$P() = 0.5 $Si la moneda no es justa, entonces debería concluir que la probabilidad de que salga cara no es 0.5, lo que escribiríamos como \\(P(\\text{cara}) \\neq 0.5\\). En otras palabras, el problema de la inferencia estadística consiste en darme cuenta cuál de estos modelos de probabilidad es el correcto. Está claro que la pregunta estadística no es la misma que la pregunta de probabilidad, pero están profundamente relacionadas. Por eso, una buena introducción a la teoría estadística comienza con una discusión sobre qué es la probabilidad y cómo funciona."
  },
  {
    "objectID": "04-SamplesPopulations.html#qué-significa-probabilidad",
    "href": "04-SamplesPopulations.html#qué-significa-probabilidad",
    "title": "5  Probabilidad, muestreo y estimación",
    "section": "5.2 ¿Qué significa probabilidad?",
    "text": "5.2 ¿Qué significa probabilidad?\nEmpecemos con la primera de estas preguntas: ¿qué es la “probabilidad”? Puede parecerte sorprendente, pero aunque los estadísticos y matemáticos (en general) están de acuerdo sobre cuáles son las reglas de la probabilidad, hay mucho menos consenso sobre qué significa realmente la palabra. Suena raro, porque todos usamos palabras como “chance”, “posibilidad” y “probable” sin problema, y no parece una pregunta difícil de responder. Si tuvieras que explicarle a una persona de cinco años qué es la probabilidad, probablemente te las arreglarías bastante bien. Pero si alguna vez viviste esa situación en la vida real, tal vez te fuiste con la sensación de que no lo explicaste tan bien, y que —como pasa con muchos conceptos cotidianos— no sabés realmente de qué se trata.\nAsí que voy a intentarlo. Supongamos que quiero apostar en un partido de fútbol entre dos equipos de robots, Arduino Arsenal y C Milan. Después de pensarlo, decido que hay una probabilidad del 80% de que Arduino Arsenal gane. ¿Qué quiero decir con eso? Acá hay tres posibles interpretaciones:\n\nSon equipos robot, así que puedo hacerlos jugar muchas veces. Si lo hiciera, Arduino Arsenal ganaría 8 de cada 10 partidos, en promedio.\nPara cualquier partido en particular, solo estaría de acuerdo en que apostar es “justo” si una apuesta de $1 a favor de C Milan paga $5 (es decir, recupero mi $1 más $4 de ganancia), al igual que lo sería una apuesta de $4 a favor de Arduino Arsenal (recupero mis $4 más $1 de ganancia).\nMi “creencia” o “confianza” subjetiva en que gana Arduino Arsenal es cuatro veces más fuerte que la que tengo en que gane C Milan.\n\nTodas estas opciones parecen razonables. Sin embargo, no son idénticas, y no todos los estadísticos estarían de acuerdo con todas. La razón es que existen diferentes ideologías estadísticas (¡sí, en serio!), y dependiendo de cuál suscribas, podrías decir que algunas de esas afirmaciones no tienen sentido o son irrelevantes. En esta sección, voy a introducir brevemente los dos enfoques principales que existen en la literatura. No son los únicos, pero sí son los más importantes.\n\n5.2.1 La visión frecuentista\nEl primero de los dos enfoques principales, y el más dominante en estadística, se conoce como la visión frecuentista, que define la probabilidad como una frecuencia a largo plazo. Supongamos que empezamos a lanzar una moneda justa, una y otra vez. Por definición, es una moneda en la que \\(P(H) = 0.5\\). ¿Qué podríamos observar? Una posibilidad es que los primeros 20 lanzamientos se vean así:\nT, H, H, H, H, T, T, H, H, H, H, T, H, H, T, T, T, T, T, H\nEn este caso, 11 de los 20 lanzamientos (55%) salieron cara. Ahora supongamos que fui llevando la cuenta del número de caras (que voy a llamar \\(N_H\\)) en los primeros \\(N\\)lanzamientos, y que cada vez calculo la proporción de caras \\(N_H/N\\). Esto es lo que obtendría (¡sí, de verdad lancé monedas para obtener esto!):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnúmero de lanzamientos\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nnúmero de caras\n0\n1\n2\n3\n4\n4\n4\n5\n6\n7\n\n\nproporción\n.00\n.50\n.67\n.75\n.80\n.67\n.57\n.63\n.67\n.70\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnúmero de lanzamientos\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\n\n\nnúmero de caras\n8\n8\n9\n10\n10\n10\n10\n10\n10\n11\n\n\nproporción\n.73\n.67\n.69\n.71\n.67\n.63\n.59\n.56\n.53\n.55\n\n\n\nFijate que al principio de la secuencia, la proporción de caras fluctúa bastante, empezando en .00 y subiendo hasta .80. Más adelante, da la impresión de que se estabiliza, con valores que se acercan cada vez más al valor “correcto” de 0.50. Esta es la definición frecuentista de probabilidad en pocas palabras: lanzá una moneda justa muchas veces, y a medida que \\(N\\)crece (se acerca al infinito, denotado \\(N \\rightarrow \\infty\\)), la proporción de caras converge a 50%. Hay algunas sutilezas técnicas que le importan a los matemáticos, pero en términos cualitativos, así es como los frecuentistas definen la probabilidad. Por supuesto, no tengo un número infinito de monedas, ni la paciencia infinita para lanzarlas eternamente. Pero sí tengo una computadora, y las computadoras son excelentes para tareas repetitivas. Así que le pedí a la computadora que simulara lanzar una moneda 1000 veces, y luego grafiqué lo que pasa con la proporción \\(N_H / N\\) a medida que \\(N\\) aumenta. En realidad, lo hice cuatro veces, solo para asegurarme de que no fuera casualidad. Los resultados se muestran en la Figura 5.1. Como podés ver, la proporción de caras observadas eventualmente deja de fluctuar, y se estabiliza. Cuando eso ocurre, el número al que se estabiliza es la verdadera probabilidad de cara.\n\n\n\n\n\nFigura 5.1: Ilustración de cómo funciona la probabilidad frecuentista. Si lanzás una moneda justa muchas veces, la proporción de caras observadas se estabiliza y converge hacia la probabilidad verdadera de 0.5. Cada panel muestra un experimento simulado distinto: en cada caso, se simula lanzar una moneda 1000 veces y se lleva un registro de la proporción de caras acumuladas. Aunque ninguna secuencia termina exactamente en 0.5, si el experimento se extendiera indefinidamente, lo haría.\n\n\n\n\nLa definición frecuentista de probabilidad tiene algunas características deseables. Primero, es objetiva: la probabilidad de un evento está necesariamente anclada en el mundo. La única forma en que las afirmaciones probabilísticas tienen sentido es si se refieren a (una secuencia de) eventos que ocurren en el universo físico. Segundo, no es ambigua: dos personas que observan la misma secuencia de eventos y tratan de calcular la probabilidad de un evento, necesariamente deben llegar a la misma respuesta.\nSin embargo, también tiene características indeseables. Las secuencias infinitas no existen en el mundo físico. Supongamos que agarrás una moneda de tu bolsillo y empezás a lanzarla. Cada vez que cae, impacta contra el suelo. Cada impacto desgasta un poco la moneda; eventualmente, la moneda se va a destruir. Entonces, uno podría preguntarse si realmente tiene sentido fingir que una “secuencia infinita” de lanzamientos de moneda es un concepto significativo, o incluso objetivo. No podemos decir que una “secuencia infinita” de eventos sea algo real en el universo físico, porque el universo físico no permite nada infinito.\nMás seriamente, la definición frecuentista tiene un alcance muy limitado. Hay un montón de cosas a las que les asignamos probabilidades en el lenguaje cotidiano, pero que no pueden (ni siquiera en teoría) ser representadas como una secuencia hipotética de eventos. Por ejemplo, si una meteoróloga aparece en la tele y dice: “la probabilidad de que llueva en Adelaida el 2 de noviembre de 2048 es del 60%”, las personas aceptamos esa afirmación sin problemas. Pero no está claro cómo definir eso en términos frecuentistas. Solo hay una ciudad de Adelaida, y solo hay un 2 de noviembre de 2048. No hay ninguna secuencia infinita de eventos acá: es algo que ocurre una sola vez. Desde la perspectiva frecuentista, está prohibido hacer afirmaciones de probabilidad sobre un evento único. Para un frecuentista, mañana o va a llover o no va a llover; no hay ninguna “probabilidad” asociada a un evento no repetible. Ahora bien, hay que decir que los frecuentistas tienen algunos trucos muy ingeniosos para sortear esta limitación. Una posibilidad es que lo que quiso decir la meteoróloga sea algo así como: “Hay una categoría de días para los que yo predigo un 60% de chance de lluvia; si miramos sólo esos días en los que hice esa predicción, entonces en el 60% de ellos efectivamente llueve”. Es una forma rara de pensarlo y contraria a la intuición, pero sí: los frecuentistas hacen esto a veces.\n\n\n5.2.2 La visión bayesiana\nLa visión bayesiana de la probabilidad también se llama la visión subjetivista, y es una posición minoritaria entre los estadísticos, pero que ha ido ganando terreno de forma sostenida en las últimas décadas. Hay muchas variantes del bayesianismo, lo que dificulta definir exactamente cuál es “la” visión bayesiana. La forma más común de pensar en la probabilidad subjetiva es definir la probabilidad de un evento como el grado de creencia que una persona (o agente racional e inteligente) asigna a la verdad de ese evento. Desde esta perspectiva, las probabilidades no existen en el mundo, sino que viven en los pensamientos y suposiciones de las personas (o seres inteligentes). Sin embargo, para que este enfoque funcione, necesitamos alguna forma de operacionalizar ese “grado de creencia”. Una manera de hacerlo es formalizarlo en términos de “apuestas racionales”, aunque existen otras formas. Supongamos que creo que hay un 60% de probabilidad de que mañana llueva. Si alguien me ofrece una apuesta que dice: si llueve, ganás $5; si no llueve, perdés $5, desde mi perspectiva es una buena apuesta. En cambio, si creo que hay solo un 40% de probabilidad de lluvia, entonces esa misma apuesta sería una mala idea. Así, podemos operacionalizar el concepto de “probabilidad subjetiva” en función de qué apuestas estoy dispuesto a aceptar.\n¿Cuáles son las ventajas y desventajas de este enfoque bayesiano? La principal ventaja es que permite asignar probabilidades a cualquier evento que queramos. No es necesario que el evento sea repetible. La principal desventaja (para muchas personas) es que no podemos ser completamente objetivos: al asignar una probabilidad, es necesario especificar una entidad que posea el grado de creencia pertinente. Esa entidad puede ser una persona, un extraterrestre, un robot, o incluso un estadístico, pero tiene que haber un agente inteligente que tenga creencias. Para muchos esto resulta incómodo: hace que la probabilidad parezca arbitraria. Mientras que el enfoque bayesiano exige que el agente en cuestión sea racional (es decir, que respete las reglas de la probabilidad), permite que cada quien tenga sus propias creencias; yo puedo creer que la moneda está equilibrada y vos no, y aun así ambos ser racionales. La visión frecuentista no permite que dos observadores asignen probabilidades distintas al mismo evento: si eso ocurre, al menos uno de los dos está equivocado. La perspectiva bayesiana no impone esa restricción. Dos personas con conocimientos previos diferentes pueden, de forma legítima, tener creencias distintas sobre un mismo evento. En resumen, mientras que la visión frecuentista a veces se considera demasiado restrictiva (prohíbe asignar probabilidades a muchas cosas que nos interesa evaluar), la visión bayesiana a veces se percibe como demasiado amplia (permite demasiadas diferencias entre observadores).\n\n\n5.2.3 ¿Cuál es la diferencia? ¿Y quién tiene razón?\nAhora que viste cada una de estas dos perspectivas por separado, conviene asegurarse de que podés compararlas. Volvé al partido hipotético de fútbol robot al comienzo de la sección. ¿Qué pensás que dirían un frecuentista y un bayesiano sobre estas tres afirmaciones? ¿Cuál de ellas consideraría un frecuentista como la definición correcta de probabilidad? ¿Y cuál aceptaría un bayesiano? ¿Algunas de estas afirmaciones carecerían de sentido para un frecuentista o para un bayesiano? Si entendiste bien las dos perspectivas, deberías tener una idea de cómo responder esas preguntas.\nBien, suponiendo que entendiste la diferencia, tal vez te estés preguntando cuál de ellas es la correcta. Sinceramente, no sé si hay una respuesta correcta. Hasta donde puedo decir, no hay nada matemáticamente incorrecto en la forma en que los frecuentistas piensan las secuencias de eventos, y tampoco hay nada matemáticamente incorrecto en la forma en que los bayesianos definen las creencias de un agente racional. De hecho, cuando uno se mete en los detalles, los bayesianos y los frecuentistas en realidad están de acuerdo en muchas cosas. Muchos métodos frecuentistas llevan a decisiones que los bayesianos aceptarían como consistentes con un agente racional. Y muchos métodos bayesianos tienen muy buenas propiedades frecuentistas.\nEn general, yo soy pragmática, así que uso cualquier método estadístico en el que confíe. Resulta que eso me hace preferir los métodos bayesianos, por razones que voy a explicar hacia el final del libro, pero no estoy fundamentalmente en contra de los métodos frecuentistas. No todos son tan relajados. Por ejemplo, pensemos en Sir Ronald Fisher, una de las figuras más importantes de la estadística del siglo XX y un ferviente opositor de todo lo bayesiano, cuyo artículo sobre los fundamentos matemáticos de la estadística se refiere a la probabilidad bayesiana como “una selva impenetrable [que] detiene el progreso hacia la precisión de los conceptos estadísticos” Fisher (1922, 311). O el psicólogo Paul Meehl, quien sugiere que confiar en métodos frecuentistas podría convertirte en “un libertino intelectual potente pero estéril, que deja a su paso alegre una larga estela de doncellas seducidas, pero ninguna descendencia científica viable” Meehl (1967, 114). La historia de la estadística, como podés ver, no carece de entretenimiento."
  },
  {
    "objectID": "04-SamplesPopulations.html#teoría-básica-de-la-probabilidad",
    "href": "04-SamplesPopulations.html#teoría-básica-de-la-probabilidad",
    "title": "5  Probabilidad, muestreo y estimación",
    "section": "5.3 Teoría básica de la probabilidad",
    "text": "5.3 Teoría básica de la probabilidad\nMás allá de las discusiones ideológicas entre bayesianos y frecuentistas, resulta que en general hay bastante consenso sobre las reglas que deben seguir las probabilidades. Hay muchas maneras de llegar a estas reglas. El enfoque más común se basa en el trabajo de Andrey Kolmogorov, uno de los grandes matemáticos soviéticos del siglo XX. No voy a entrar en demasiados detalles, pero voy a intentar darte una idea general de cómo funciona. Y para hacerlo, voy a tener que hablar de mis pantalones.\n\n5.3.1 Introducción a las distribuciones de probabilidad\nUna de las verdades perturbadoras de mi vida es que solo tengo cinco pares de pantalones: tres jeans, la parte de abajo de un traje, y un pantalón deportivo. Más triste aún: les puse nombre. Los llamo \\(X_1\\), \\(X_2\\), \\(X_3\\), \\(X_4\\) y \\(X_5\\). De verdad lo hago: por eso me dicen “Señor Imaginativo”. Cada día, elijo exactamente un par de pantalones para usar. Ni yo soy tan tonto como para tratar de usar dos al mismo tiempo, y gracias a años de entrenamiento ya no salgo de casa sin pantalones. Si quisiera describir esta situación usando el lenguaje de la teoría de la probabilidad, diría que cada par de pantalones (es decir, cada \\(X\\)) es un evento elemental. La característica clave de los eventos elementales es que cada vez que hacemos una observación (por ejemplo, cada vez que me pongo un par de pantalones), el resultado será uno y solo uno de esos eventos. Como dije, hoy en día siempre uso exactamente un par, así que mis pantalones cumplen con esa condición. De forma similar, al conjunto de todos los posibles eventos se lo llama espacio muestral. Es verdad que algunas personas lo llamarían “guardarropa”, pero eso es porque se niegan a pensar en mis pantalones desde una perspectiva probabilística. Triste.\nBien, ahora que tenemos un espacio muestral (un guardarropa), formado por varios eventos elementales (pantalones), lo que queremos hacer es asignar una probabilidad a cada uno de esos eventos. Para un evento \\(X\\), la probabilidad de ese evento \\(P(X)\\)es un número entre 0 y 1. Cuanto mayor es el valor de \\(P(X)\\), más probable es que ocurra ese evento. Por ejemplo: - Si \\(P(X) = 0\\), significa que el evento \\(X\\) es imposible (nunca uso ese pantalón). - Si \\(P(X) = 1\\), significa que el evento \\(X\\) es seguro (siempre uso ese pantalón). - Valores de probabilidad intermedios significan que a veces uso ese pantalón. Por ejemplo, si \\(P(X) = 0.5\\), significa que uso ese pantalón la mitad del tiempo.\nA esta altura, ya casi terminamos. Lo último que necesitamos reconocer es que “algo siempre ocurre”. Cada vez que me pongo pantalones, efectivamente termino con un par puesto (una locura, ¿no?). Esta afirmación algo trillada quiere decir, en términos probabilísticos, que las probabilidades de todos los eventos elementales deben sumar 1. Esto se conoce como la ley de probabilidad total, aunque a nadie le importe demasiado. Más importante, si se cumplen estos requisitos, lo que tenemos es una distribución de probabilidad. Por ejemplo, esta es una distribución de probabilidad\n\n\n\n¿Qué pantalón?\nEtiqueta\nProbabilidad\n\n\n\n\nJean azul\n\\(X_1\\)\n\\(P(X_1) = 0.5\\)\n\n\nJean gris\n\\(X_2\\)\n\\(P(X_2) = 0.3\\)\n\n\nJean negro\n\\(X_3\\)\n\\(P(X_3) = 0.1\\)\n\n\nPantalón de traje\n\\(X_4\\)\n\\(P(X_4) = 0\\)\n\n\nPantalón deportivo\n\\(X_5\\)\n\\(P(X_5) = 0.1\\)\n\n\n\nCada uno de estos eventos tiene una probabilidad que está entre 0 y 1, y si sumamos todas las probabilidades, el total es 1. Genial. Incluso podemos hacer un lindo gráfico de barras para visualizar esta distribución, como se muestra en la Figura 5.2. Y en este punto, todos logramos algo. Vos aprendiste qué es una distribución de probabilidad, y yo por fin encontré la forma de hacer un gráfico que trata exclusivamente sobre mis pantalones. ¡Ganamos todos!\n\n\n\n\n\nFigura 5.2: Representación visual de la distribución de probabilidad de los pantalones. Hay cinco eventos elementales, que corresponden a los cinco pares de pantalones que tengo. Cada evento tiene alguna probabilidad de ocurrir: ese valor es un número entre 0 y 1. La suma de estas probabilidades es igual a 1.\n\n\n\n\nLa única otra cosa que necesito señalar es que la teoría de la probabilidad también te permite hablar de eventos no elementales, además de los elementales. La forma más fácil de ilustrar esto es con un ejemplo. En el caso de los pantalones, es perfectamente válido referirse a la probabilidad de que use jeans. En este escenario, decimos que el evento “Dan usa jeans” ocurre si el evento elemental que ocurrió fue uno de los apropiados; en este caso, “jean azul”, “jean negro” o “jean gris”. En términos matemáticos, definimos el evento “jeans” \\(E\\) como el conjunto de eventos elementales \\((X_1, X_2, X_3)\\). Si ocurre cualquiera de esos eventos, entonces se dice que ocurrió \\(E\\). Una vez definida así la \\(E\\), es muy fácil calcular la probabilidad \\(P(E)\\): simplemente sumamos todo. En este caso:\n\\[\nP(E) = P(X_1) + P(X_2) + P(X_3)\n\\]\nY como las probabilidades de jean azul, gris y negro son 0.5, 0.3 y 0.1 respectivamente, la probabilidad de que use jeans es igual a 0.9.\nA esta altura tal vez estés pensando que todo esto es increíblemente obvio y simple, y tenés razón. Lo único que hicimos fue envolver algunas intuiciones de sentido común con un poco de matemática básica. Sin embargo, a partir de estos comienzos simples se pueden construir herramientas matemáticas extremadamente poderosas. Definitivamente no voy a entrar en esos detalles en este libro, pero lo que sí voy a hacer es listar algunas otras reglas que las probabilidades deben cumplir. Estas reglas se pueden derivar de los supuestos básicos que acabamos de ver, pero como no las vamos a usar directamente en este libro, no me voy a detener en eso.\n\nAlgunas reglas básicas que deben cumplir las probabilidades. No necesitás conocerlas para entender los análisis que vamos a ver más adelante en el libro, pero son importantes si querés entender la teoría de la probabilidad con más profundidad.\n\n\nEvento\nNotación\n=\nRegla\n\n\n\n\nno \\(A\\)\n\\(P(\\neg A)\\)\n=\n\\(1 - P(A)\\)\n\n\n\\(A\\) o \\(B\\)\n\\(P(A \\cup B)\\)\n=\n\\(P(A) + P(B) - P(A \\cap B)\\)\n\n\n\\(A\\) y \\(B\\)\n\\(P(A \\cap B)\\)\n=\n\\(P(A|B) \\cdot P(B)\\)\n\n\n\nAhora que tenemos la capacidad de “definir” eventos no elementales en términos de eventos elementales, en realidad podemos usar eso para construir (o, si querés sonar matemáticamente refinado, “derivar”) algunas de las otras reglas de probabilidad. Estas reglas están listadas arriba, y aunque estoy bastante seguro de que muy pocos lectores realmente están interesados en cómo se construyen, te las voy a mostrar igual: aunque sea aburrido y probablemente nunca les encuentres mucho uso práctico, si lo leés un par de veces y tratás de entender cómo funciona, vas a ver que la probabilidad deja de parecer tan misteriosa, y con suerte te va a intimidar menos. Así que ahí vamos. Primero, para construir las reglas voy a necesitar un espacio muestral \\(X\\) compuesto por un conjunto de eventos elementales \\(x\\), y dos eventos no elementales que voy a llamar \\(A\\) y \\(B\\). Supongamos que:\n\\[\n\\begin{array}{rcl}\nX &=& (x_1, x_2, x_3, x_4, x_5) \\\\\nA &=& (x_1, x_2, x_3) \\\\\nB &=& (x_3, x_4)\n\\end{array}\n\\]\nPara hacerlo más concreto, supongamos que todavía estamos hablando de la distribución de pantalones. Si es así, \\(A\\) correspondería al evento “jeans”, y \\(B\\) al evento “negro”:\n\\[\n\\begin{array}{rcl}\n\\text{“jeans”} &=& (\\text{“jean azul”}, \\text{“jean gris”}, \\text{“jean negro”}) \\\\\n\\text{“negro”} &=& (\\text{“jean negro”}, \\text{“traje negro”})\n\\end{array}\n\\]\nAhora podemos empezar a comprobar las reglas que figuran en la tabla.\nEn la primera línea, la tabla dice que:\\(P(\\neg A) = 1 - P(A)\\)y lo que significa esto es que la probabilidad de “no \\(A\\)” es igual a 1 menos la probabilidad de \\(A\\). Con un poco de reflexión (y un ejemplo tedioso) se vuelve evidente por qué esto tiene que ser verdad. Si \\(A\\) corresponde al evento de que me pongo jeans (es decir, que ocurre alguno de los eventos \\(x_1\\), \\(x_2\\) o \\(x_3\\)), entonces la única definición razonable de “no \\(A\\)” (que se denota matemáticamente como \\(\\neg A\\)) es decir que\\(\\neg A\\)consiste en todos los eventos elementales que no pertenecen a\\(A\\). En el caso de los pantalones, eso significa que:\\[\\neg A = (x_4, x_5)\\]o, dicho en español: “no jeans” incluye todos los pantalones que no son jeans (es decir, el pantalón de traje negro y el pantalón deportivo azul). Por lo tanto, cada uno de los eventos elementales pertenece o bien a\\(A\\), o bien a\\(\\neg A\\), pero no a ambos. Bien, entonces reorganicemos lo anterior:\\[P(\\neg A) + P(A) = 1\\]que no es más que una forma matemática de decir: o uso jeans, o no uso jeans; la probabilidad de “no jeans” más la probabilidad de “jeans” es igual a 1.\nMatemáticamente:\n\\[\n\\begin{array}{rcl}\nP(\\neg A) &=& P(x_4) + P(x_5) \\\\\nP(A) &=& P(x_1) + P(x_2) + P(x_3)\n\\end{array}\n\\]\nasí que:\n\\[\n\\begin{array}{rcl}\nP(\\neg A) + P(A) &=& P(x_1) + P(x_2) + P(x_3) + P(x_4) + P(x_5) \\\\\n&=& \\sum_{x \\in X} P(x) \\\\\n&=& 1\n\\end{array}\n\\]Excelente. Todo parece funcionar.\n“Guau”, te escucho decir. “Todo esto para decirme lo increíblemente obvio”. Y tenés razón: esto es increíblemente obvio. El propósito de la teoría de la probabilidad es formalizar y matematizar unas pocas intuiciones de sentido común. Así que sigamos un poco más con esta línea de razonamiento. En la sección anterior definí un evento correspondiente a no A, que denoté como \\(\\neg A\\). Ahora vamos a definir dos nuevos eventos que corresponden a conceptos cotidianos importantes: \\(A\\)y\\(B\\), y **\\(A\\)o\\(B\\)**. Específicamente:\n\n\n\nAfirmación en español\nNotación matemática\n\n\n\n\n“\\(A\\)y\\(B\\)” ocurren ambos\n\\(A \\cap B\\)\n\n\nocurre al menos uno: “\\(A\\)o\\(B\\)”\n\\(A \\cup B\\)\n\n\n\nYa que \\(A\\)y\\(B\\) están definidos en términos de nuestros eventos elementales (los \\(x\\)s), necesitamos describir también \\(A \\cap B\\)y\\(A \\cup B\\) en términos de nuestros eventos elementales. ¿Podemos hacerlo? ¡Sí que podemos! La única forma en que ocurran ambos eventos,\\(A\\)y\\(B\\), es que el evento elemental observado pertenezca tanto a \\(A\\) como a \\(B\\). Así que “\\(A \\cap B\\)” incluye solamente aquellos eventos elementales que pertenecen a \\(A\\) y \\(B\\)…\n\\[\n\\begin{array}{rcl}\nA &=& (x_1, x_2, x_3) \\\\\nB &=& (x_3, x_4) \\\\\nA \\cap B &=& (x_3)\n\\end{array}\n\\] Es decir, la única forma en que pueda estar usando “jeans” (\\(x_1, x_2, x_3\\)) y “pantalón negro” (\\(x_3, x_4\\)) al mismo tiempo, es que esté usando los “jeans negros” (\\(x_3\\)). Otro triunfo para lo absolutamente obvio.\nA esta altura, no te va a sorprender para nada la definición de \\(A \\cup B\\), aunque probablemente te parezca increíblemente aburrida. La única forma en que puedo estar usando “jeans” o “pantalón negro” es si el pantalón elemental que realmente uso pertenece a\\(A\\), o a\\(B\\), o a ambos. Así que:\n\\[\n\\begin{array}{rcl}\nA &=& (x_1, x_2, x_3) \\\\\nB &=& (x_3, x_4) \\\\\nA \\cup B &=& (x_1, x_2, x_3, x_4)\n\\end{array}\n\\]\nVaaamooooooooo!!!! Matemáticas, sabelo!\nEntonces, ya definimos qué queremos decir con \\(A \\cap B\\)y\\(A \\cup B\\). Ahora asignemos probabilidades a esos eventos. Más específicamente, vamos a verificar la regla que dice que: \\[P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\]\nUsando las definiciones anteriores, sabemos que: \\(A \\cup B = (x_1, x_2, x_3, x_4)\\), entonces \\[\nP(A \\cup B) = P(x_1) + P(x_2) + P(x_3) + P(x_4)\n\\]\ny usando el hecho de que sabemos qué eventos elementales pertenecen a\\(A\\),\\(B\\), y\\(A \\cap B\\), tenemos: \\[\n\\begin{array}{rcl}\nP(A) &=& P(x_1) + P(x_2) + P(x_3) \\\\\nP(B) &=& P(x_3) + P(x_4) \\\\\nP(A \\cap B) &=& P(x_3)\n\\end{array}\n\\] y por lo tanto: $$\n\\[\\begin{array}{rcl}\nP(A) + P(B) - P(A \\cap B)\n&=& P(x_1) + P(x_2) + P(x_3) + P(x_3) + P(x_4) - P(x_3) \\\\\n&=& P(x_1) + P(x_2) + P(x_3) + P(x_4) \\\\\n&=& P(A \\cup B)\n\\end{array}\\]\n$$\nListo.\nEl próximo concepto que necesitamos definir es la notación “\\(B\\) dado \\(A\\)”, que se escribe típicamente como \\(B \\mid A\\). Esto es lo que quiero decir: supongamos que me levanto una mañana y me pongo un pantalón. Ocurre un evento elemental \\(x\\). Supongamos que le grito a mi esposa (que está en la otra habitación, y no puede ver qué pantalón me puse): “¡Hoy estoy usando jeans!”. Asumiendo que ella me cree, entonces sabe que \\(A\\) es verdadero. Dado que sabe que ocurrió \\(A\\), ¿cuál es la probabilidad condicional de que también sea cierto \\(B\\)? Bueno, pensemos qué información tiene. Estas son las cosas que sabe:\n\nLos eventos que no son jeans son imposibles. Si \\(A\\) es verdadero, entonces sabemos que los únicos eventos elementales que pueden haber ocurrido son\\(x_1\\), \\(x_2\\) y \\(x_3\\)(o sea, los jeans). Los eventos no jeans, \\(x_4\\) y \\(x_5\\), ahora son imposibles y deben tener probabilidad cero. En otras palabras, nuestro espacio muestral se ha restringido a los eventos jeans. Pero sigue siendo cierto que la suma de las probabilidades de estos eventos debe ser 1: sabemos con certeza que estoy usando jeans.\nElla no aprendió nada sobre qué jeans estoy usando. Antes de que yo anunciara que estoy usando jeans, ella ya sabía que era cinco veces más probable que estuviera usando jeans azules (\\(P(x_1) = 0.5\\)) que jeans negros (\\(P(x_3) = 0.1\\)). Mi anuncio no cambia eso… no dije nada sobre el color, así que la proporción \\(P(x_1) / P(x_3)\\) se mantiene igual, con un valor de 5.\n\nSolo hay una forma de satisfacer estas restricciones: asignar probabilidad cero a los eventos imposibles (es decir, \\(P(x | A) = 0\\) si \\(x\\) no está en \\(A\\)), y luego dividir las probabilidades de todos los demás por \\(P(A)\\). En este caso, como \\(P(A) = 0.9\\), dividimos entre 0.9. Esto da:\n\n\n\n\n\n\n\n\n\n¿Qué pantalón?\nEvento elemental\nProb. original \\(P(x)\\)\nNueva prob. $P(x A) $\n\n\n\n\nJean azul\n\\(x_1\\)\n0.5\n0.556\n\n\nJean gris\n\\(x_2\\)\n0.3\n0.333\n\n\nJean negro\n\\(x_3\\)\n0.1\n0.111\n\n\nTraje negro\n\\(x_4\\)\n0\n0\n\n\nPantalón deportivo\n\\(x_5\\)\n0.1\n0\n\n\n\nEn términos matemáticos, decimos que: \\[P(x \\mid A) = \\frac{P(x)}{P(A)} \\quad \\text{si } x \\in A, \\quad \\text{y } P(x \\mid A) = 0 \\text{ si } x \\notin A\\]\nPor lo tanto:\n\\[\n\\begin{array}{rcl}\nP(B \\mid A) &=& P(x_3 \\mid A) + P(x_4 \\mid A) \\\\\\\\\n&=& \\displaystyle \\frac{P(x_3)}{P(A)} + 0 \\\\\\\\\n&=& \\displaystyle \\frac{P(x_3)}{P(A)}\n\\end{array}\n\\]\nY recordando que \\(A \\cap B = (x_3)\\), podemos escribir esto como: \\(P(B \\mid A) = \\frac{P(A \\cap B)}{P(A)}\\)\nY si multiplicamos ambos lados por ( P(A) ), obtenemos:\n\\(P(A \\cap B) = P(B \\mid A) \\cdot P(A)\\) que es la tercera regla que habíamos listado en la tabla anterior."
  },
  {
    "objectID": "04-SamplesPopulations.html#la-distribución-binomial",
    "href": "04-SamplesPopulations.html#la-distribución-binomial",
    "title": "5  Probabilidad, muestreo y estimación",
    "section": "5.4 La distribución binomial",
    "text": "5.4 La distribución binomial\nComo te podés imaginar, las distribuciones de probabilidad varían muchísimo, y hay una enorme cantidad de distribuciones ahí afuera. Sin embargo, no todas son igual de importantes. De hecho, la gran mayoría del contenido de este libro se basa en una de cinco distribuciones: la distribución binomial, la distribución normal, la distribución\\(t\\), la distribución \\(\\chi^2\\) (“chi-cuadrado”) y la distribución \\(F\\). Dicho esto, lo que voy a hacer en las próximas secciones es darte una breve introducción a esas cinco, prestando especial atención a la binomial y a la normal. Voy a empezar con la distribución binomial, porque es la más sencilla de las cinco.\n\n5.4.1 Introducción a la binomial\nLa teoría de la probabilidad se originó como un intento de describir cómo funcionan los juegos de azar, así que parece apropiado que nuestra discusión sobre la distribución binomial empiece con tirar dados y lanzar monedas. Imaginá un experimento simple: en mi mano tengo 20 dados idénticos de seis caras. En una de las caras de cada dado hay una calavera; las otras cinco están en blanco. Si lanzo los 20 dados, ¿cuál es la probabilidad de que salgan exactamente 4 calaveras? Asumiendo que los dados son justos, sabemos que la chance de que cualquier dado salga calavera es 1 en 6; dicho de otra forma, la probabilidad de calavera para un solo dado es aproximadamente \\(0{,}167\\). Con eso ya tenemos suficiente información para responder la pregunta, así que veamos cómo se hace.\nComo siempre, vamos a introducir algunos nombres y notación. Vamos a usar \\(N\\) para referirnos al número de lanzamientos en nuestro experimento; esto es lo que se conoce como el parámetro de tamaño de la distribución binomial. También vamos a usar \\(\\theta\\) para referirnos a la probabilidad de éxito de un solo lanzamiento, es decir, la probabilidad de que salga calavera. Finalmente, usaremos \\(X\\) para referirnos al resultado de nuestro experimento, es decir, la cantidad de calaveras que obtengo al lanzar los dados. Como el valor concreto de \\(X\\) depende del azar, nos referimos a él como una variable aleatoria. Con toda esta terminología y notación, podemos expresar el problema de forma un poco más precisa: la cantidad que queremos calcular es la probabilidad de que \\(X = 4\\), dado que sabemos que \\(\\theta = 0{,}167\\) y \\(N = 20\\). La forma general de lo que queremos calcular se puede escribir como:\n\\(P(X \\mid \\theta, N)\\)\ny nos interesa el caso particular en el que \\(X = 4\\), \\(\\theta = 0{,}167\\) y \\(N = 20\\). Hay una última notación que quiero mencionar antes de pasar a cómo resolver el problema. Si quiero decir que \\(X\\) fue generado aleatoriamente a partir de una distribución binomial con parámetros \\(\\theta\\) y \\(N\\), lo escribo así:\nSí, sí, ya sé lo que estás pensando: notación, notación, notación. ¿A quién le importa? Muy pocos lectores están acá por la notación, así que mejor sigo y explico cómo usar la distribución binomial. Para eso, la Figura 5.3 muestra las probabilidades binomiales para todos los valores posibles de \\(X\\) en nuestro experimento con los dados, desde \\(X = 0\\) (ninguna calavera) hasta \\(X = 20\\) (todas calaveras). Notá que básicamente es un gráfico de barras, y no es distinto del gráfico de “probabilidad de pantalones” que hicimos en Figura 5.2. En el eje horizontal están todos los eventos posibles, y en el eje vertical podemos ver la probabilidad de cada uno de esos eventos. Entonces, la probabilidad de obtener 4 calaveras al lanzar 20 dados es de aproximadamente 0.20 (la respuesta exacta es 0.2022036, como veremos enseguida). En otras palabras, esperarías que eso ocurra alrededor del 20% de las veces que repitas este experimento.\n\n\n\n\n\nFigura 5.3: La distribución binomial con parámetro de tamaño\\(N = 20\\)y una probabilidad de éxito de 1/6. Cada barra vertical representa la probabilidad de un resultado específico (es decir, un valor posible de\\(X\\)). Como se trata de una distribución de probabilidad, cada probabilidad debe ser un número entre 0 y 1, y la suma de las alturas de las barras también debe ser 1.\n\n\n\n\n\n\n5.4.2 Trabajando con la distribución binomial en R\nR tiene una función llamada dbinom que calcula probabilidades binomiales por nosotros. Los argumentos principales de la función son:\n\nx: Un número (o vector de números) que especifica los valores para los que querés calcular la probabilidad.\nsize: Un número que le indica a R el tamaño del experimento.\nprob: La probabilidad de éxito en un solo intento del experimento.\n\nEntonces, para calcular la probabilidad de obtener 4 calaveras en un experimento de 20 lanzamientos, donde la probabilidad de calavera en cada lanzamiento es 1/6, el comando sería simplemente:\n\ndbinom(x = 4, size = 20, prob = 1/6)\n\n[1] 0.2022036\n\n\nPara que veas cómo cambia la distribución binomial cuando alteramos los valores de \\(\\theta\\) y \\(N\\), supongamos ahora que, en lugar de tirar dados, estoy lanzando monedas. En este caso, el experimento consiste en lanzar una moneda justa repetidamente, y el resultado que me interesa es cuántas veces sale cara. En este escenario, la probabilidad de éxito es ahora \\(\\theta = 1/2\\). Supongamos que lanzo la moneda \\(N = 20\\) veces. En este ejemplo, cambié la probabilidad de éxito, pero mantuve igual el tamaño del experimento. ¿Qué le pasa a la distribución binomial?\n\n\n\n\n\nFigura 5.4: Dos distribuciones binomiales, en un escenario donde lanzo una moneda justa, con probabilidad de éxito\\(1/2\\). En el panel (a), se supone que lanzo la moneda\\(N = 20\\)veces. En el panel (b), que la lanzo\\(N = 100\\)veces.\n\n\n\n\nBueno, como se ve en la Figura 5.4 (a), el efecto principal es que la distribución se desplaza, como cabría esperar. Ahora, ¿qué pasa si lanzo la moneda \\(N = 100\\) veces? En ese caso, obtenemos la Figura 5.4 (b). La distribución se mantiene más o menos centrada, pero hay más variabilidad en los posibles resultados.\nEn este punto, probablemente debería explicar el nombre de la función dbinom(). Obviamente, la parte “binom” proviene del hecho de que estamos trabajando con la distribución binomial, pero el prefijo “d” probablemente sea un pequeño misterio. En esta sección voy a dar una explicación parcial: específicamente, voy a explicar por qué existe un prefijo. En cuanto a por qué es precisamente una “d”, vas a tener que esperar hasta la próxima sección. R incluye cuatro funciones para trabajar con la distribución binomial. Estas funciones son dbinom, pbinom, rbinom y qbinom, y cada una calcula una cantidad distinta de interés. No solo eso, R hace lo mismo para todas las distribuciones de probabilidad que implementa. No importa de qué distribución se trate, siempre vas a encontrar una función d, una función p, una función r y una función q.\nVeamos qué hace cada una de estas funciones. Primero, las cuatro versiones de la función requieren que le indiques los argumentos size y prob: no importa qué querés que R calcule, siempre necesita saber cuáles son los parámetros de la distribución. Sin embargo, difieren en cuanto a cuál es el otro argumento y cuál es la salida. Así que veámoslas una por una:\n\nLa versión d ya la vimos. Se especifica un resultado particular x, y lo que devuelve es la probabilidad de obtener exactamente ese resultado (la “d” viene de density, aunque por ahora podés ignorarlo).\nLa versión p calcula la probabilidad acumulada. Se especifica un cuantil q, y te devuelve la probabilidad de obtener un resultado menor o igual a q.\nLa versión q calcula los cuantiles de la distribución. Se especifica un valor de probabilidad p, y te devuelve el percentil correspondiente. Es decir, el valor de la variable para el cual hay una probabilidad p de obtener un resultado inferior a ese valor.\nLa versión r es un generador de números aleatorios: genera n resultados aleatorios tomados de la distribución.\n\nEsto puede sonar algo abstracto, así que veamos algunos ejemplos concretos. Como ya vimos dbinom, vamos a concentrarnos en las otras tres. Empecemos con pbinom, y volvamos al ejemplo de los dados con calaveras. Recordá: lanzo 20 dados, cada uno tiene una chance de 1 en 6 de mostrar una calavera. Supongamos, en cambio, que quiero saber la probabilidad de obtener 4 o menos calaveras. Podría usar dbinom para calcular la probabilidad exacta de obtener 0, 1, 2, 3 y 4 calaveras, y luego sumar esos valores. Pero hay una forma más rápida: usar la función pbinom. El comando sería:\n\npbinom(q = 4, size = 20, prob = 1/6)\n\n[1] 0.7687492\n\n\nEn otras palabras, hay un 76.9% de probabilidad de que al lanzar los 20 dados obtenga 4 calaveras o menos. O, dicho de otra manera, R nos está diciendo que 4 es el percentil 76.9 de esta distribución binomial.\nAhora consideremos la función qbinom. Supongamos que quiero calcular el percentil 75 de la distribución binomial. Siguiendo con el ejemplo de las calaveras, el comando sería:\n\nqbinom(p = 0.75, size = 20, prob = 1/6)\n\n[1] 4\n\n\nMmm. Hay algo raro acá. Pensemos bien esto. Lo que parece decir la función qbinom es que el percentil 75 de la distribución binomial es 4, aunque vimos recién en la función que 4 es en realidad el percentil 76.9. Y la función pbinom es la correcta, te lo prometo. Lo raro acá surge del hecho de que nuestra distribución binomial en realidad no tiene un percentil 75. ¿Por qué no? Bueno, hay un 56.7% de chance de obtener 3 o menos calaveras (podés escribir pbinom(3, 20, 1/6) para comprobarlo), y un 76.9% de chance de obtener 4 o menos calaveras. Entonces, en cierto sentido, el percentil 75 debería estar “entre medio” de 3 y 4 calaveras. ¡Pero eso no tiene sentido! No podés tirar 20 dados y que salgan 3.9 calaveras. Este problema se puede manejar de distintas formas: podrías reportar un valor intermedio (o interpolado, como se dice técnicamente), como 3.9 podrías redondear hacia abajo (a 3) o podrías redondear hacia arriba (a 4).\nLa función qbinom redondea hacia arriba: si pedís un percentil que no existe literalmente (como el 75 en este ejemplo), R encuentra el valor más pequeño cuyo percentil acumulado sea al menos el que pediste. En este caso, como el “verdadero” percentil 75 (sea lo que sea que eso signifique) está entre 3 y 4 calaveras, R redondea hacia arriba y te da 4. Esta sutileza puede ser molesta, lo admito, pero por suerte solo es un problema en distribuciones discretas como la binomial. Las otras distribuciones de las que voy a hablar (normal, \\(t\\), \\(\\chi^2\\) y \\(F\\)) son todas continuas, así que R siempre puede devolver un cuantil exacto cuando se lo pedís.\nFinalmente, tenemos el generador de números aleatorios. Para usar la función rbinom, tenés que especificar cuántas veces querés que R “simule” el experimento usando el argumento n, y te va a generar resultados aleatorios tomados de la distribución binomial. Por ejemplo, si quisiera repetir el experimento de tirar dados 100 veces, puedo hacer que R simule esos resultados así:\n\nrbinom(n = 100, size = 20, prob = 1/6)\n\n  [1]  4  2  5  3  1  2  6  6  8  2  3  4  3  2  3  3 10  2  5  5  5  0  3  3  2\n [26]  5  1  3  5  1  7  2  5  4  6  1  3  1  5  5  4  2  2  3  3  2  3  2  1  5\n [51]  5  2  2  3  5  7  4  4  2  5  1  4  2  5  4  2  2  1  1  3  3  5  5  6  2\n [76]  2  4  2  5  2  2  3  5  3  6  2  4  7  4  3  5  4  2  6  1  2  4  5  5  1\n\n\nComo podés ver, los números que aparecen son lo que uno esperaría, dada la distribución que se muestra en la Figura 5.3. La mayoría de las veces saco entre 1 y 5 calaveras. Hay muchas sutilezas sobre cómo se generan números aleatorios en una computadora, pero para los fines de este libro no necesitamos preocuparnos demasiado."
  },
  {
    "objectID": "04-SamplesPopulations.html#la-distribución-normal",
    "href": "04-SamplesPopulations.html#la-distribución-normal",
    "title": "5  Probabilidad, muestreo y estimación",
    "section": "5.5 La distribución normal",
    "text": "5.5 La distribución normal\nAunque la distribución binomial es conceptualmente la más fácil de entender, no es la más importante. Ese honor particular le corresponde a la distribución normal, también conocida como “curva de campana” o “distribución gaussiana”.\n\nknitr::include_graphics(\"imgs/navarro_img/probability/standardNormal_es.png\")\n\n\n\n\nFigura 5.5: La distribución normal con media = 0 y desviación estándar = 1. El eje x representa los valores posibles de una variable, y el eje y nos dice qué tan probable es observar ese valor. Sin embargo, notá que el eje y está etiquetado como Densidad de Probabilidad y no como Probabilidad. Esto se debe a una característica sutil —y algo frustrante— de las distribuciones continuas, que hace que el eje y se comporte de manera un poco extraña: la altura de la curva no representa literalmente la probabilidad de observar un valor x en particular. Por otro lado, es cierto que las alturas de la curva te indican cuáles valores de x son más probables (¡los más altos!).\n\n\n\n\nUna distribución normal se describe usando dos parámetros: la media de la distribución (\\(\\mu\\)) y la desviación estándar (\\(\\sigma\\)). La notación que usamos a veces para indicar que una variable \\(X\\) tiene distribución normal es:\n\\(X \\sim \\text{Normal}(\\mu, \\sigma)\\)\nClaro que eso es solo notación. No nos dice nada especialmente interesante sobre la distribución normal en sí. La fórmula matemática de la distribución normal es:\n\n\n\n\n\nFórmula de la distribución normal\n\n\n\n\nLa fórmula es lo suficientemente importante como para que cualquiera que aprenda estadística la mire al menos una vez, pero como este es un texto introductorio, no quiero enfocarme demasiado en eso. En cambio, vamos a ver cómo se puede usar R para trabajar con distribuciones normales. Las funciones en R para la distribución normal son dnorm(), pnorm(), qnorm() y rnorm(). Sin embargo, se comportan exactamente igual que sus equivalentes para la distribución binomial, así que no hay mucho que necesites saber. Lo único que vale la pena señalar es que los argumentos para los parámetros se llaman mean y sd. En casi todo lo demás, no hay más que agregar.\nEn lugar de enfocarnos en las matemáticas, tratemos de entender qué significa que una variable tenga distribución normal. Para eso, mirá la Figura 5.5, que muestra una distribución normal con media \\(\\mu = 0\\) y desviación estándar \\(\\sigma = 1\\). Podés ver de dónde viene el nombre “curva de campana”: se parece un poco a una campana. Notá que, a diferencia de los gráficos que dibujé para ilustrar la distribución binomial, la Figura 5.5 muestra una curva suave, no un gráfico tipo histograma. Esto no es una elección arbitraria: la distribución normal es continua, mientras que la binomial es discreta. En el ejemplo anterior, era posible obtener 3 calaveras o 4 calaveras, pero era imposible obtener 3.9 calaveras.\nTeniendo eso en cuenta, veamos si podemos desarrollar una intuición de cómo funciona la distribución normal. Primero, vamos a ver qué pasa cuando jugamos con los parámetros de la distribución. Uno de los parámetros que podemos modificar es la media. Esto desplaza la distribución hacia la derecha o hacia la izquierda. La animación en la Figura 5.6 muestra una distribución normal con media = 0, que se va moviendo desde media = 0 hasta media = 5. Notá que, cuando cambiás la media, la forma de la distribución no cambia: simplemente se traslada de izquierda a derecha. En la animación, la curva normal sube y baja un poco, pero eso es solo una característica visual de la animación (además queda lindo así).\n\n\n\n\n\nFigura 5.6: Una distribución normal con media variable\n\n\n\n\nEn cambio, si aumentamos la desviación estándar manteniendo constante la media, el pico de la distribución se queda en el mismo lugar, pero la distribución se ensancha. La animación en Figura 5.7 muestra lo que pasa cuando empezamos con una desviación estándar pequeña (sd = 0.5), y la aumentamos gradualmente hasta sd = 5. Como podés ver, la distribución se expande y se vuelve más ancha a medida que aumenta la desviación estándar.\n\n\n\n\n\nFigura 5.7: Una distribución normal con desviación estándar variable.\n\n\n\n\nFijate que cuando ensanchamos la distribución, la altura del pico disminuye. Eso tiene que ocurrir: al igual que las alturas de las barras en la distribución binomial debían sumar 1, el área bajo la curva de la distribución normal también debe ser igual a 1. Antes de continuar, quiero señalar una característica importante de la distribución normal. Sin importar cuál sea la media o la desviación estándar específica, el 68.3% del área cae dentro de 1 desviación estándar de la media. De forma similar, el 95.4% cae dentro de 2 desviaciones estándar, y el 99.7% está dentro de 3 desviaciones estándar.\n\n5.5.1 Densidad de probabilidad\nHay algo que estuve intentando ocultar durante toda mi discusión sobre la distribución normal. Algo que algunos libros introductorios directamente omiten por completo. Tal vez tengan razón en hacerlo: esta “cosa” que estoy ocultando es rara y contraintuitiva, incluso según los estándares algo retorcidos que rigen en estadística. Afortunadamente, no es algo que necesites entender a fondo para hacer estadística básica: es algo que empieza a volverse importante más adelante, cuando salís de lo introductorio. Así que si no te queda completamente claro, no te preocupes: tratá de captar la idea general.\nA lo largo de esta discusión sobre la distribución normal, hay una o dos cosas que no cierran del todo. Quizás notaste que el eje \\(y\\) en esas figuras está etiquetado como “Densidad de probabilidad” y no como “Probabilidad”. Tal vez notaste que usé \\(p(X)\\) en lugar de \\(P(X)\\) al mostrar la fórmula de la normal. Quizás te preguntás por qué R usa el prefijo \"d\" para funciones como dnorm(). Y tal vez, solo tal vez, estuviste jugando con la función dnorm() y accidentamente escribiste un comando como este:\n\ndnorm(x = 1, mean = 1, sd = 0.1)\n\n[1] 3.989423\n\n\nY si hiciste eso, seguramente quedaste muy confundido. Le pedí a R que calcule la probabilidad de que \\(x = 1\\) para una variable con distribución normal, media = 1 y desviación estándar = 0.1; y R me responde que la “probabilidad” es 3.99. Pero, como discutimos antes, las probabilidades no pueden ser mayores que 1. Entonces, o me equivoqué, o eso que R me dio no es una probabilidad.\nResulta que la segunda opción es la correcta. Lo que calculamos ahí no es una probabilidad: es otra cosa. Para entender qué es esa otra cosa, tenemos que dedicar un momento a pensar qué significa realmente decir que \\(X\\) es una variable continua. Supongamos que estamos hablando de la temperatura exterior. El termómetro me dice que hace 23 grados, pero yo sé que eso no es del todo cierto. No son exactamente 23 grados. Quizás sean 23.1. Pero tampoco es del todo cierto, porque podrían ser 23.09. Pero, claro, también podrían ser 23.091. Bueno… ya entendés la idea. El problema con las cantidades verdaderamente continuas es que nunca sabés exactamente qué son.\nAhora pensá qué implica esto cuando hablamos de probabilidades. Supongamos que la temperatura máxima de mañana se extrae de una distribución normal con media 23 y desviación estándar 1. ¿Cuál es la probabilidad de que la temperatura sea exactamente 23 grados? La respuesta es “cero”, o si querés, “un número tan cercano a cero que es indistinguible de cero”. ¿Por qué?\nEs como intentar tirar un dardo a un blanco infinitamente pequeño: no importa qué tan buena sea tu puntería, nunca lo vas a acertar. En la vida real, nunca vas a obtener un valor exactamente igual a 23. Siempre va a ser algo como 23,1 o 22,99998 o algo por el estilo. En otras palabras, no tiene ningún sentido hablar de la probabilidad de que la temperatura sea exactamente 23 grados. Sin embargo, en el lenguaje cotidiano, si yo te dijera que afuera hace 23 grados y en realidad son 22,9998, probablemente no me dirías que estoy mintiendo. Porque en la vida diaria, decir “23 grados” suele significar algo así como “entre 22,5 y 23,5 grados”. Y aunque no tiene mucho sentido preguntar cuál es la probabilidad de que la temperatura sea exactamente 23 grados, sí parece razonable preguntar cuál es la probabilidad de que esté entre 22,5 y 23,5, o entre 20 y 30, o dentro de cualquier otro rango de temperaturas.\nEl punto de toda esta discusión es dejar en claro que, cuando hablamos de distribuciones continuas, no tiene sentido hablar de la probabilidad de un valor específico. Sin embargo, sí podemos hablar de la probabilidad de que el valor esté dentro de un cierto rango. Y para calcular esa probabilidad, lo que necesitamos hacer es calcular el “área bajo la curva”.\nOk, eso explica parte del asunto. Hablamos un poco sobre cómo deberían interpretarse las distribuciones continuas (es decir, el área bajo la curva es la clave), pero todavía no expliqué realmente qué calcula la función dnorm(). O, en forma equivalente, ¿qué significa la fórmula de \\(p(x)\\) que presentamos antes? Evidentemente, \\(p(x)\\) no describe una probabilidad. Entonces, ¿qué es? El nombre técnico de esa cantidad \\(p(x)\\) es densidad de probabilidad, y en los gráficos que venimos dibujando, corresponde a la altura de la curva. Las densidades no tienen sentido por sí solas; pero están “diseñadas” de manera que el área bajo la curva sea siempre interpretable como probabilidades genuinas. Para ser sincero, eso es más o menos todo lo que necesitás saber por ahora."
  },
  {
    "objectID": "04-SamplesPopulations.html#otras-distribuciones-útiles",
    "href": "04-SamplesPopulations.html#otras-distribuciones-útiles",
    "title": "5  Probabilidad, muestreo y estimación",
    "section": "5.6 Otras distribuciones útiles",
    "text": "5.6 Otras distribuciones útiles\nHay muchas otras distribuciones útiles, entre ellas la distribución t, la distribución F y la distribución chi cuadrado (χ²). Pronto vamos a aprender más sobre las distribuciones t y F cuando discutamos los t-tests y los ANOVA en capítulos posteriores."
  },
  {
    "objectID": "04-SamplesPopulations.html#resumen-sobre-la-probabilidad",
    "href": "04-SamplesPopulations.html#resumen-sobre-la-probabilidad",
    "title": "5  Probabilidad, muestreo y estimación",
    "section": "5.7 Resumen sobre la probabilidad",
    "text": "5.7 Resumen sobre la probabilidad\nHablamos sobre qué significa la probabilidad, y por qué los estadísticos no logran ponerse de acuerdo sobre su significado. Hablamos de las reglas que deben cumplir las probabilidades. Y presentamos la idea de una distribución de probabilidad, dedicando una buena parte del capítulo a algunas de las distribuciones más importantes con las que trabajan los estadísticos. Discutimos cosas como:\n\nTeoría de la probabilidad versus estadística\nVisiones frecuentista versus bayesiana de la probabilidad\nFundamentos básicos de la teoría de la probabilidad\nDistribuciones binomial y normal\n\nComo era de esperarse, esta cobertura está lejos de ser exhaustiva. La teoría de la probabilidad es una rama amplia de las matemáticas, con entidad propia, separada de su aplicación a la estadística y al análisis de datos. Hay miles de libros escritos sobre el tema, y en general las universidades ofrecen varias materias dedicadas exclusivamente a la teoría de la probabilidad. Incluso la tarea “más sencilla” de documentar las distribuciones de probabilidad estándar es un tema extenso. Por suerte para vos, muy poco de todo eso es necesario. Es poco probable que necesites conocer decenas de distribuciones estadísticas para hacer análisis de datos en el mundo real, y definitivamente no las vas a necesitar para este libro. Pero nunca está de más saber que existen otras posibilidades.\nRetomando ese último punto, en cierto sentido todo este capítulo es una especie de digresión. Muchas materias de estadística en psicología a nivel de grado pasan muy por arriba este contenido (sé que así fue en mi caso), e incluso las materias más avanzadas suelen “olvidarse” de volver sobre los fundamentos básicos del campo. La mayoría de los psicólogos académicos no sabría explicar la diferencia entre probabilidad y densidad, y hasta hace poco, muy pocos sabían que había una diferencia entre la probabilidad bayesiana y la frecuentista. Sin embargo, creo que es importante entender estas cosas antes de pasar a las aplicaciones. Por ejemplo, hay muchas reglas sobre lo que se puede o no se puede decir cuando hacés inferencia estadística, y muchas de esas reglas pueden parecer arbitrarias o extrañas. Pero empiezan a tener sentido cuando entendés que existe esta distinción entre lo bayesiano y lo frecuentista."
  },
  {
    "objectID": "04-SamplesPopulations.html#muestras-poblaciones-y-muestreo",
    "href": "04-SamplesPopulations.html#muestras-poblaciones-y-muestreo",
    "title": "5  Probabilidad, muestreo y estimación",
    "section": "5.8 Muestras, poblaciones y muestreo",
    "text": "5.8 Muestras, poblaciones y muestreo\nRecordá que el rol de la estadística descriptiva es resumir de forma concisa lo que sí sabemos. En cambio, el propósito de la estadística inferencial es “aprender lo que no sabemos a partir de lo que sí sabemos”. ¿Qué tipo de cosas nos gustaría aprender? ¿Y cómo las aprendemos? Estas son las preguntas que están en el corazón de la estadística inferencial, y que tradicionalmente se dividen en dos “grandes ideas”: estimación y pruebas de hipótesis. El objetivo de este capítulo es presentar la primera de estas ideas, la teoría de la estimación, pero antes vamos a hablar sobre teoría del muestreo, porque la estimación no tiene sentido si no entendés cómo funciona el muestreo. Así que este capítulo se divide en dos partes: teoría del muestreo, y cómo usar esa teoría para hablar sobre cómo piensan los estadísticos el problema de la estimación. Ya hicimos bastante muestreo, así que ya estás familiarizado con algunas de las ideas principales.\nLa teoría del muestreo cumple un rol enorme a la hora de definir los supuestos sobre los que se apoyan tus inferencias estadísticas. Y para poder hablar de hacer inferencias como lo hacen los estadísticos, tenemos que ser un poco más explícitos sobre de dónde extraemos inferencias (la muestra) y sobre qué extraemos inferencias (la población).\nEn casi cualquier situación de interés, lo que tenemos disponible como investigadores es una muestra de datos. Podés haber hecho un experimento con cierta cantidad de participantes; una encuestadora puede haber llamado a cierta cantidad de personas para preguntarles su intención de voto; etc. En todos los casos: el conjunto de datos que tenemos es finito e incompleto. No podemos hacer que cada persona del mundo participe en nuestro experimento; una encuestadora no tiene ni el tiempo ni el dinero para llamar a cada votante del país, etc. En nuestra discusión anterior sobre estadística descriptiva, la muestra era lo único que nos interesaba. Nuestro único objetivo era encontrar formas de describir, resumir y graficar esa muestra. Pero eso está por cambiar.\n\n5.8.1 Definir una población\nUna muestra es algo concreto. Podés abrir un archivo de datos y ver los valores de tu muestra ahí. Una población, en cambio, es un concepto más abstracto. Se refiere al conjunto de todas las personas, o de todas las observaciones, sobre las que querés sacar conclusiones, y generalmente es mucho más grande que la muestra. En un mundo ideal, los investigadores empezarían cada estudio con una idea clara de cuál es su población de interés, ya que el proceso de diseñar un estudio y poner a prueba hipótesis con los datos obtenidos depende de esa población sobre la que queremos hablar. Sin embargo, eso no siempre ocurre en la práctica: en general, los investigadores tienen solo una idea vaga de cuál es la población, y diseñan el estudio lo mejor que pueden con base en eso.\nA veces es fácil definir la población de interés. Por ejemplo, en el caso de la “encuestadora”, la población está compuesta por todas las personas habilitadas para votar en el momento de la encuesta —millones de personas. La muestra era un conjunto de 1000 personas y todas pertenecen a esa población. En la mayoría de los casos, la situación es mucho menos clara. En un experimento psicológico típico, determinar cuál es la población de interés puede ser algo más complicado. Supongamos que hago un experimento con 100 estudiantes universitarios como participantes. Mi objetivo, como científica cognitiva, es aprender algo sobre cómo funciona la mente. Entonces, ¿cuál de las siguientes opciones debería contar como “la población”?\n\n¿Todxs los estudiantes de psicología de grado de la Universidad de Adelaide?\n¿Estudiantes de psicología de grado en general, en cualquier parte del mundo?\n¿Personas que viven actualmente en Australia?\n¿Personas australianas de edades similares a mi muestra?\n¿Cualquier persona viva en la actualidad?\n¿Cualquier ser humano, pasado, presente o futuro?\n¿Cualquier organismo biológico con suficiente inteligencia y que opere en un entorno terrestre?\n¿Cualquier ser inteligente?\n\nCada una de estas opciones define un grupo real de entidades con mente, que podrían interesarme como científica cognitiva, y no está nada claro cuál de ellas debería considerarse la verdadera población de interés.\n\n\n5.8.2 Muestras aleatorias simples\nIndependientemente de cómo definamos la población, el punto clave es que la muestra es un subconjunto de la población, y nuestro objetivo es usar lo que sabemos de la muestra para hacer inferencias sobre las propiedades de la población. La relación entre la muestra y la población depende del procedimiento por el cual se seleccionó la muestra. Este procedimiento se conoce como método de muestreo, y es importante entender por qué eso importa.\nPara mantenerlo simple, imaginá que tenemos una bolsa con 10 fichas. Cada ficha tiene una letra única impresa, así que podemos distinguirlas. Las fichas vienen en dos colores: negro y blanco.\n\n\n\n\n\nFigura 5.8: Muestreo aleatorio simple sin reemplazo a partir de una población finita.\n\n\n\n\nEste conjunto de fichas es la población de interés, y está representada gráficamente en la parte izquierda de la Figura 5.8.\nComo podés ver en la imagen, hay 4 fichas negras y 6 fichas blancas, pero en la vida real no sabríamos eso a menos que miremos dentro de la bolsa. Ahora imaginá que hacés el siguiente “experimento”: agitás la bolsa, cerrás los ojos y sacás 4 fichas, sin devolverlas después de sacarlas. Primero sale la ficha\\(a\\)(negra), luego la\\(c\\)(blanca), después la\\(j\\)(blanca), y finalmente la\\(b\\)(negra). Si quisieras, podrías volver a meter todas las fichas en la bolsa y repetir el experimento, como se muestra en el lado derecho de la Figura 5.8. Cada vez vas a obtener resultados distintos, pero el procedimiento es idéntico en todos los casos. El hecho de que el mismo procedimiento pueda producir distintos resultados cada vez es lo que hace que lo llamemos un proceso aleatorio. Sin embargo, como agitamos la bolsa antes de sacar cualquier ficha, parece razonable pensar que cada ficha tenía la misma probabilidad de ser elegida. Un procedimiento en el que cada miembro de la población tiene la misma probabilidad de ser seleccionado se llama una muestra aleatoria simple. El hecho de que no devolvimos las fichas a la bolsa después de sacarlas significa que no podés observar la misma ficha dos veces, y en estos casos se dice que las observaciones fueron tomadas sin reemplazo.\nPara ayudarte a entender la importancia del procedimiento de muestreo, considerá una alternativa. Supongamos que mi hijo de 5 años abre la bolsa y decide sacar cuatro fichas negras, sin devolver ninguna. Este esquema de muestreo sesgado se muestra en la Figura 5.9.\n\n\n\n\n\nFigura 5.9: Muestreo sesgado sin reemplazo a partir de una población finita.\n\n\n\n\nAhora considerá el valor informativo de observar 4 fichas negras y 0 blancas. Claramente, eso depende del esquema de muestreo, ¿no? Si sabés que el procedimiento estaba sesgado a seleccionar solo fichas negras, entonces una muestra compuesta únicamente por fichas negras no te dice mucho sobre la población. Por esta razón, a los estadísticos les encanta cuando un conjunto de datos puede considerarse una muestra aleatoria simple, porque hace que el análisis de los datos sea mucho más fácil.\nHay un tercer procedimiento que vale la pena mencionar. Esta vez, cerramos los ojos, agitamos la bolsa y sacamos una ficha. Pero ahora, registramos la ficha y luego la devolvemos a la bolsa. De nuevo, cerramos los ojos, agitamos la bolsa y sacamos otra ficha. Repetimos este procedimiento hasta tener 4 fichas. Los conjuntos de datos generados de esta manera también se consideran muestras aleatorias simples, pero como devolvemos las fichas a la bolsa después de cada extracción, se dice que es un muestreo con reemplazo. La diferencia con el primer caso es que ahora es posible observar el mismo miembro de la población más de una vez, como se muestra en la Figura 5.10.\n\n\n\n\n\nFigura 5.10: Muestreo aleatorio simple con reemplazo a partir de una población finita.\n\n\n\n\nLa mayoría de los experimentos en psicología tienden a ser sin reemplazo, porque no se permite que una misma persona participe dos veces en el mismo experimento. Sin embargo, la mayoría de la teoría estadística asume que los datos provienen de una muestra aleatoria simple con reemplazo. En la vida real, esto rara vez importa. Si la población es grande (por ejemplo, más de 10 elementos), la diferencia entre muestreo con o sin reemplazo es demasiado pequeña como para preocuparse. En cambio, la diferencia entre una muestra aleatoria simple y una muestra sesgada no es algo que podamos ignorar tan fácilmente.\n\n\n5.8.3 La mayoría de las muestras no son aleatorias simples\nComo pudiste ver en la lista de posibles poblaciones que mostré antes, es casi imposible obtener una muestra aleatoria simple de la mayoría de las poblaciones de interés. Cuando hago experimentos, ya considero un pequeño milagro que mis participantes sean una muestra aleatoria de los estudiantes de psicología de grado en la Universidad de Adelaide, ¡y eso que esa es por lejos la población más acotada a la que podría querer generalizar! Una discusión detallada de otros esquemas de muestreo está más allá del alcance de este libro, pero para que tengas una idea de lo que existe, acá listo algunos de los más importantes:\n\nMuestreo estratificado. Supongamos que tu población se divide (o se puede dividir) en varias subpoblaciones diferentes, o estratos. Por ejemplo, tal vez estés haciendo un estudio en varios sitios distintos. En lugar de intentar muestrear al azar de la población total, podés intentar recolectar una muestra aleatoria separada de cada uno de los estratos. El muestreo estratificado a veces es más fácil de realizar que el muestreo aleatorio simple, especialmente cuando la población ya está dividida en estratos definidos. También puede ser más eficiente, sobre todo si algunas subpoblaciones son poco frecuentes. Por ejemplo, al estudiar esquizofrenia, sería mucho mejor dividir la población en dos estratos (con y sin esquizofrenia), y luego seleccionar la misma cantidad de personas de cada grupo. Si seleccionaras gente al azar, obtendrías tan pocas personas con esquizofrenia que el estudio sería inútil. Este tipo específico de muestreo estratificado se conoce como sobremuestreo, porque busca deliberadamente sobre-representar grupos poco frecuentes.\nMuestreo en bola de nieve es una técnica especialmente útil cuando la población de interés es “oculta” o difícil de acceder, y es bastante común en las ciencias sociales. Por ejemplo, supongamos que los investigadores quieren hacer una encuesta de opinión entre personas trans. El equipo de investigación tal vez tenga datos de contacto de unas pocas personas trans, así que la encuesta empieza por invitarlas a participar (etapa 1). Al final de la encuesta, se les pide a los participantes que proporcionen contactos de otras personas que podrían querer participar. En la etapa 2, se encuesta a esas nuevas personas. El proceso continúa hasta que los investigadores hayan reunido suficientes datos. La gran ventaja del muestreo en bola de nieve es que te permite obtener datos en situaciones donde de otro modo sería imposible. En el plano estadístico, la principal desventaja es que la muestra realmente es no aleatoria, y en formas difíciles de compensar. En el plano ético, la desventaja es que el procedimiento puede ser problemático si no se maneja bien, porque las poblaciones ocultas a menudo están ocultas por una razón. Elegí a las personas trans como ejemplo para destacar esto: si no sos cuidadoso, podrías terminar exponiendo a alguien que no quiere ser expuesto (algo realmente inaceptable). Y eso puede tener consecuencias personales o profesionales graves para esa persona. Incluso si no cometés ese error, sigue siendo una intromisión usar las redes sociales de las personas para estudiarlas. Es muy difícil obtener consentimiento informado antes de contactarlas, y en muchos casos el simple hecho de decirles “hola, queremos estudiar tus datos” puede ser molesto o invasivo. Las redes sociales son cosas complejas, y el hecho de que puedas usarlas para recolectar datos no siempre significa que debas hacerlo.\nMuestreo por conveniencia es más o menos lo que suena: las muestras se eligen de una forma que le resulte conveniente al investigador, y no son seleccionadas al azar de la población de interés. El muestreo en bola de nieve es una forma de muestreo por conveniencia, pero hay muchas otras. Un ejemplo común en psicología son los estudios que se hacen con estudiantes de grado en psicología. Estas muestras suelen no ser aleatorias en dos sentidos: primero, depender de estudiantes de psicología de grado implica automáticamente que los datos se limitan a una única subpoblación. Segundo, los propios estudiantes suelen elegir en qué estudios participar, por lo que la muestra termina siendo un subconjunto autoseleccionado de estudiantes de psicología, y no uno seleccionado al azar. En la práctica, la mayoría de los estudios se basan en muestras por conveniencia de una forma u otra. A veces esto representa una limitación importante, pero no siempre.\n\n\n\n5.8.4 ¿Qué tan grave es no tener una muestra aleatoria simple?\nOk, entonces recolectar datos en el mundo real casi nunca involucra una muestra aleatoria simple. ¿Y eso importa? Con un poco de reflexión, te debería quedar claro que puede importar si tus datos no son una muestra aleatoria simple: basta con pensar en la diferencia entre la Figura 5.8 y la Figura 5.9. Sin embargo, no es tan grave como suena. Algunos tipos de muestras sesgadas no son problemáticas en absoluto. Por ejemplo, cuando usás muestreo estratificado, en realidad sabés cuál es el sesgo porque vos mismo lo generaste a propósito, a menudo para incrementar la efectividad de tu estudio. Y existen técnicas estadísticas que permiten ajustar los análisis para tener en cuenta esos sesgos (¡aunque este libro no las cubre!).\nEn general, es importante recordar que el muestreo aleatorio es un medio para un fin, no un fin en sí mismo. Supongamos que usaste una muestra por conveniencia, y por eso podés asumir que tiene algún sesgo. Un sesgo en el método de muestreo solo es problemático si te lleva a sacar conclusiones equivocadas. Desde esa perspectiva, no necesitamos que la muestra sea aleatoria en absolutamente todos los aspectos: solo necesitamos que sea aleatoria en relación al fenómeno psicológico que estamos estudiando. Supongamos que estoy haciendo un estudio sobre la capacidad de memoria de trabajo. En el estudio 1, tengo la capacidad de muestrear al azar a toda la población humana viva, con una sola excepción: solo puedo muestrear personas nacidas un lunes. En el estudio 2, puedo muestrear al azar a la población australiana. Quiero generalizar mis resultados a toda la población humana actual. ¿Cuál de los dos estudios preferirías? La respuesta, obviamente, es el estudio 1. ¿Por qué? Porque no hay ninguna razón para pensar que “haber nacido un lunes” tenga alguna relación interesante con la capacidad de memoria de trabajo. En cambio, sí puedo imaginar varias razones por las que “ser australiano” podría importar. Australia es un país rico, industrializado, con un sistema educativo muy desarrollado. Las personas que crecieron en ese sistema habrán tenido experiencias de vida mucho más parecidas a las de quienes diseñaron los tests de capacidad de memoria de trabajo. Esa experiencia compartida podría traducirse en creencias similares sobre cómo “hacer un test”, en supuestos comunes sobre cómo funcionan los experimentos psicológicos, y así. Estas cosas podrían importar. Por ejemplo, el “estilo de rendir exámenes” puede haber enseñado a los participantes australianos a concentrarse exclusivamente en materiales bastante abstractos, comparado con personas que no crecieron en un entorno similar, lo que podría llevar a una imagen distorsionada de lo que realmente es la capacidad de memoria de trabajo.\nHay dos ideas importantes escondidas en esta discusión: Primero, cuando diseñás tus propios estudios, es importante pensar cuál es la población que te interesa, y esforzarte por muestrear de una forma apropiada para esa población. En la práctica, muchas veces no te queda otra que trabajar con una “muestra por conveniencia” (por ejemplo, docentes de psicología que muestrean estudiantes de psicología porque es la forma más barata de recolectar datos, y nuestros presupuestos no son exactamente desbordantes de oro). Pero si hacés eso, al menos tendrías que dedicar algo de tiempo a pensar cuáles son los posibles riesgos de esta práctica.\nSegundo, si vas a criticar el estudio de otra persona porque usó una muestra por conveniencia, en lugar de haber hecho el esfuerzo enorme de muestrear aleatoriamente de toda la población humana, al menos tené la cortesía de ofrecer una hipótesis concreta sobre cómo podría haberse distorsionado el resultado. Recordá: toda persona que trabaja en ciencia es consciente de este problema, y hace lo que puede por compensarlo. Decir simplemente “el estudio solo incluyó personas del grupo TAL” no ayuda en nada, y roza lo insultante para los investigadores, que probablemente sí estaban al tanto del problema, pero no disponían de la infinita cantidad de tiempo y dinero que haría falta para construir la muestra perfecta. En resumen: si querés ofrecer una crítica responsable sobre el muestreo, entonces sé útil. Repetir verdades obvias como las que acabo de presentar en esta sección es inútil.\n\n\n5.8.5 Parámetros poblacionales y estadísticas muestrales\nBien. Dejando de lado los espinosos problemas metodológicos asociados con obtener una muestra aleatoria, consideremos un tema algo distinto. Hasta este punto, venimos hablando de poblaciones como lo haría una persona científica. Para una psicóloga, una población podría ser un grupo de personas. Para una ecóloga, una población podría ser un grupo de osos. En la mayoría de los casos, las poblaciones que interesan a la ciencia son cosas concretas que realmente existen en el mundo.\nLos estadísticos, sin embargo, son un grupo curioso. Por un lado, sí se interesan en datos reales y en la ciencia, del mismo modo que los científicos. Pero por otro lado, también trabajan en el plano de la abstracción pura, como lo hacen los matemáticos. Como consecuencia, la teoría estadística tiende a definir las poblaciones de forma algo más abstracta. Del mismo modo que los psicólogos convierten ideas teóricas abstractas en mediciones concretas, los estadísticos operacionalizan el concepto de “población” en términos de objetos matemáticos con los que saben trabajar. Ya te cruzaste con estos objetos: se llaman distribuciones de probabilidad (¿te acordás? el lugar del que vienen los datos).\nLa idea es muy simple. Supongamos que hablamos de puntuaciones de CI (coeficiente intelectual). Para una psicóloga, la población de interés es un grupo de personas reales que tienen puntajes de CI. Un estadístico “simplifica” eso definiendo operativamente la población como la distribución de probabilidad mostrada en la Figura 5.11 (panel a).\n\n\n\n\n\nFigura 5.11: Distribución poblacional de puntuaciones de CI (panel a) y dos muestras extraídas al azar de esa población. En el panel b hay una muestra de 100 observaciones; en el panel c, una muestra de 10.000 observaciones.\n\n\n\n\nLos tests de CI están diseñados para que el promedio sea 100, la desviación estándar sea 15, y la distribución de puntajes sea normal. Estos valores se conocen como parámetros poblacionales, porque son características de toda la población. Es decir, decimos que la media poblacional \\(\\mu\\) es 100, y la desviación estándar poblacional \\(\\sigma\\) es 15.\nAhora supongamos que recolectamos algunos datos. Seleccionamos 100 personas al azar y les administramos un test de CI, lo que nos da una muestra aleatoria simple de la población. La muestra consistiría en una colección de números como esta:\n106 101 98 80 74 ... 107 72 100\nCada uno de estos puntajes de CI fue extraído de una distribución normal con media 100 y desviación estándar 15. Así que, si dibujo un histograma de la muestra, obtengo algo como lo que se muestra en ?fig-IQdist$b\\(. Como podés ver, el histograma tiene una forma **aproximadamente** correcta, pero es una aproximación bastante burda de la verdadera distribución poblacional que aparece en @fig-IQdist\\)a$. La media de la muestra está bastante cerca de la media poblacional (100), pero no es idéntica. En este caso, resulta que las personas de la muestra tienen un CI promedio de 98.5, y la desviación estándar de sus puntajes es 15.9. Estas estadísticas muestrales son propiedades del conjunto de datos, y aunque son bastante parecidas a los verdaderos valores poblacionales, no son iguales. En general, las estadísticas muestrales son las cosas que podés calcular a partir de tus datos, y los parámetros poblacionales son las cosas sobre las que querés aprender. Más adelante en este capítulo vamos a hablar sobre cómo podés estimar los parámetros poblacionales a partir de las estadísticas muestrales, y cómo calcular qué tan confiado podés estar en tus estimaciones. Pero antes de llegar a eso, todavía hay algunas ideas de teoría del muestreo que necesitás conocer."
  },
  {
    "objectID": "02-Describing_Data.html#así-se-ve-un-exceso-de-números",
    "href": "02-Describing_Data.html#así-se-ve-un-exceso-de-números",
    "title": "2  Describir los datos",
    "section": "2.1 Así se ve un exceso de números",
    "text": "2.1 Así se ve un exceso de números\nSupongamos que querés saber cuán feliz se siente la gente. Entonces salís a la calle y le preguntás a miles de personas cuán felices están. Les permitís elegir cualquier número, desde menos infinito hasta más infinito. Registrás todos esos números. ¿Y ahora?\nBueno, podrías mirar esos números y ver si eso te ayuda a entender algo sobre la felicidad de las personas. ¿Cómo podrían verse esos números? Quizás algo así:\n\n\n\n\n\n107\n378\n-86\n-67\n-237\n40\n536\n-93\n484\n-241\n\n\n-816\n580\n-636\n-276\n-152\n-243\n119\n1064\n614\n387\n\n\n-121\n239\n1018\n52\n597\n-307\n199\n476\n462\n567\n\n\n-474\n-28\n440\n-223\n306\n-460\n-52\n-206\n-294\n517\n\n\n37\n-211\n134\n755\n660\n1044\n588\n208\n603\n-919\n\n\n-126\n-193\n178\n559\n-111\n-1\n152\n-652\n-649\n-147\n\n\n-546\n825\n-44\n447\n309\n-585\n56\n611\n-276\n144\n\n\n-414\n-492\n-562\n-19\n9\n473\n263\n-503\n243\n843\n\n\n-155\n104\n3\n297\n-491\n488\n1021\n722\n-335\n597\n\n\n110\n347\n335\n180\n-20\n-609\n985\n514\n396\n571\n\n\n402\n512\n412\n1048\n-217\n625\n-36\n-112\n-1235\n-163\n\n\n899\n417\n333\n-56\n732\n6\n-275\n752\n212\n578\n\n\n298\n495\n-327\n-98\n-14\n-666\n-210\n-90\n-32\n268\n\n\n-819\n-243\n377\n275\n832\n472\n-420\n-662\n18\n660\n\n\n871\n592\n792\n265\n673\n538\n447\n593\n206\n-547\n\n\n973\n-382\n707\n653\n1430\n342\n-333\n-653\n421\n-110\n\n\n-80\n143\n-646\n617\n-556\n130\n888\n-675\n911\n166\n\n\n199\n-53\n290\n-379\n309\n-593\n869\n426\n-847\n-649\n\n\n658\n6\n-433\n349\n-549\n-266\n368\n677\n249\n-463\n\n\n538\n-200\n554\n216\n1603\n371\n633\n-752\n571\n480\n\n\n256\n-150\n-258\n788\n419\n-545\n-50\n-407\n659\n-813\n\n\n22\n432\n-16\n-89\n66\n-429\n-99\n510\n-376\n1066\n\n\n384\n-326\n980\n233\n711\n251\n-298\n247\n489\n198\n\n\n856\n589\n407\n441\n-205\n122\n-471\n570\n-165\n-363\n\n\n-565\n237\n-732\n-14\n889\n-663\n365\n193\n559\n-881\n\n\n-214\n197\n-312\n-897\n0\n595\n-331\n7\n220\n261\n\n\n555\n256\n-357\n-81\n-405\n-595\n609\n282\n401\n-49\n\n\n-171\n-378\n-2\n21\n619\n-196\n108\n19\n-260\n268\n\n\n504\n-591\n-148\n211\n241\n900\n-519\n-188\n239\n95\n\n\n-508\n1346\n-65\n-262\n-461\n-250\n556\n-684\n685\n691\n\n\n578\n-286\n-128\n491\n-22\n868\n841\n-121\n-385\n-238\n\n\n-150\n33\n-290\n848\n441\n-126\n-123\n86\n17\n221\n\n\n-245\n-80\n624\n-780\n122\n-189\n-188\n-84\n286\n299\n\n\n-269\n-172\n478\n700\n104\n-282\n67\n-93\n238\n-587\n\n\n-333\n1056\n-956\n349\n403\n72\n611\n-151\n-324\n-176\n\n\n221\n-169\n380\n1259\n401\n98\n62\n1339\n-688\n-54\n\n\n-429\n-48\n696\n-36\n-183\n-30\n814\n353\n-219\n618\n\n\n115\n507\n-40\n677\n1469\n639\n405\n176\n153\n583\n\n\n-7\n-124\n758\n240\n-428\n-361\n-674\n88\n1407\n-410\n\n\n1044\n196\n-498\n-327\n405\n-453\n289\n132\n-170\n-563\n\n\n294\n-665\n865\n-461\n-149\n658\n258\n86\n450\n779\n\n\n-779\n468\n785\n447\n817\n-89\n24\n778\n122\n284\n\n\n-697\n146\n1196\n609\n167\n986\n758\n431\n288\n284\n\n\n499\n-332\n-92\n-111\n51\n453\n699\n465\n484\n93\n\n\n764\n51\n-209\n386\n124\n71\n340\n-361\n620\n309\n\n\n377\n374\n-431\n779\n286\n193\n4\n213\n393\n-812\n\n\n539\n398\n-587\n526\n-81\n8\n319\n337\n-494\n-439\n\n\n-827\n134\n-268\n-973\n572\n74\n961\n831\n31\n-347\n\n\n485\n-35\n-80\n874\n-84\n-1104\n419\n234\n494\n329\n\n\n-73\n-863\n283\n1322\n-178\n-146\n253\n565\n842\n405\n\n\n\n\n\n¿Y qué vas a hacer con esa montaña de números? ¿Mirarlos todo el día? Cuando trabajás con datos, te vas a encontrar con tantos números que te van a abrumar. Por eso necesitamos formas de describirlos de una manera más manejable.\nLa descripción completa de los datos siempre son los datos mismos. Pero las estadísticas descriptivas y otras herramientas de resumen nos permiten ir un paso más allá: comprimir y simplificar la información para entenderla mejor. Resumir los datos es como comprimir las partes importantes de algo en una versión útil y manejable. Es como contarle a una amiga por qué debería ver una película: no le contás toda la película, sino que le marcás los momentos clave. Resumir los datos es como el adelanto de una película, pero aplicado a los datos."
  },
  {
    "objectID": "04-SamplesPopulations.html#la-ley-de-los-grandes-números",
    "href": "04-SamplesPopulations.html#la-ley-de-los-grandes-números",
    "title": "5  Probabilidad, muestreo y estimación",
    "section": "5.9 La ley de los grandes números",
    "text": "5.9 La ley de los grandes números\nEn el ejemplo anterior mostramos los resultados de un experimento ficticio sobre CI con un tamaño muestral de\\(N = 100\\). Los resultados fueron bastante alentadores: la verdadera media poblacional es 100, y la media muestral fue 98.5, una aproximación bastante razonable. En muchos estudios científicos, ese nivel de precisión es perfectamente aceptable, pero en otras situaciones quizás necesitemos ser mucho más precisos. Entonces, si queremos que nuestras estadísticas muestrales se acerquen más a los parámetros poblacionales, ¿qué podemos hacer?\nLa respuesta obvia es: recolectar más datos. Supongamos que hacemos un experimento mucho más grande, esta vez midiendo el CI de 10.000 personas. Podemos simular los resultados de este experimento usando R y la función rnorm(), que genera números aleatorios extraídos de una distribución normal. Para un experimento con tamaño muestral\\(n = 10000\\), y una población con media 100 y desviación estándar 15, R puede generar datos simulados de CI así:\n\nIQ &lt;- rnorm(n = 10000, mean = 100, sd = 15)  # generar los CI\nIQ &lt;- round(IQ)  # redondear a números enteros\n\nY listo, ya generamos 10.000 valores de CI simulados. ¿Dónde están esos datos? En la variable IQ de mi computadora. Podés hacer lo mismo en tu propia computadora copiando el código. Ver los 10.000 números sería demasiado, pero podés mirar los primeros 100 así:\n\nprint(IQ[1:100])\n\n  [1]  91  98  85  88  91  69 115 105 100 110 120  90  77  74  92  94 121  79\n [19] 108 104  95  76 106  94  88  99  88 128 104 100 114  95 118  91  92  85\n [37] 112  90 114 122  84 100  89 109  63  82 104 117  89 101 106  86 132 106\n [55] 112  93  66 103  86  93  91 105  99 113  68  95 114 106 128  94  97  93\n [73] 105 109 125 137 106  81  78 111 115  90 101 107 107  77 126 118 123  83\n [91] 114 120  88  90 115  88 106 115 109  92\n\n\nTambién podés calcular la media con mean(IQ) y la desviación estándar con sd(IQ), y hacer un histograma con hist(). El histograma de esta muestra mucho más grande se muestra en el panel c de la Figura 5.11. Con solo mirarlo, se nota que esta muestra es una mejor aproximación a la distribución poblacional real que la muestra más pequeña. Eso también se ve en las estadísticas muestrales: la media muestral es 99.9 y la desviación estándar es 15.1. Estos valores son ahora cercanos a los valores reales de la población.\nMe siento un poco tonta al decir esto, pero lo que quiero que te lleves de todo esto es que las muestras grandes, en general, te dan mejor información. Me siento tonta porque es tan tremendamente obvio que no debería hacer falta decirlo. De hecho, es tan obvio que cuando Jacob Bernoulli —uno de los fundadores de la teoría de la probabilidad— formalizó esta idea allá por 1713, fue medio arrogante al respecto. Así fue como describió el hecho de que todos compartimos esta intuición:\n\nPues incluso el más estúpido de los hombres, por algún instinto natural, por sí mismo y sin ninguna instrucción (lo cual es notable), está convencido de que cuantas más observaciones se han hecho, menor es el peligro de desviarse del objetivo (ver Stigler, 1986, p. 65).\n\nBueno, el pasaje suena un poco condescendiente (por no decir sexista), pero su punto principal es correcto: realmente parece obvio que más datos te van a dar mejores respuestas. La pregunta es: ¿por qué es así? No sorprende que esta intuición que todos compartimos resulta ser correcta, y los estadísticos se refieren a ella como la ley de los grandes números. La ley de los grandes números es una ley matemática que se aplica a muchas estadísticas muestrales, pero la forma más simple de entenderla es como una ley sobre promedios. La media muestral es el ejemplo más obvio de una estadística que se basa en promediar (porque eso es justamente lo que es la media… un promedio), así que vamos a mirar eso. Aplicada a la media muestral, lo que dice la ley de los grandes números es que a medida que la muestra se hace más grande, la media muestral tiende a acercarse a la media verdadera de la población. O, dicho de manera un poco más precisa, cuando el tamaño de la muestra “tiende” a infinito (se escribe\\(N \\rightarrow \\infty\\)), la media muestral se aproxima a la media poblacional (\\(\\bar{X} \\rightarrow \\mu\\)).\nNo tengo intención de torturarte con una demostración de que la ley de los grandes números es cierta, pero es una de las herramientas más importantes de la teoría estadística. Es lo que podemos usar para justificar nuestra creencia de que, si seguimos recolectando más y más datos, eventualmente vamos a llegar a la verdad. Para un conjunto de datos en particular, las estadísticas muestrales que calculemos estarán equivocadas, pero la ley de los grandes números nos dice que si seguimos recolectando más datos, esas estadísticas tenderán a acercarse cada vez más a los verdaderos parámetros de la población."
  },
  {
    "objectID": "04-SamplesPopulations.html#distribuciones-muestrales-y-el-teorema-central-del-límite",
    "href": "04-SamplesPopulations.html#distribuciones-muestrales-y-el-teorema-central-del-límite",
    "title": "5  Probabilidad, muestreo y estimación",
    "section": "5.10 Distribuciones muestrales y el teorema central del límite",
    "text": "5.10 Distribuciones muestrales y el teorema central del límite\nLa ley de los grandes números es una herramienta muy poderosa, pero no alcanza para responder todas nuestras preguntas. Entre otras cosas, lo único que nos ofrece es una “garantía en el largo plazo”. A largo plazo, si de alguna manera pudiéramos recolectar una cantidad infinita de datos, entonces la ley de los grandes números garantiza que nuestras estadísticas muestrales serán correctas. Pero como John Maynard Keynes argumentó con razón en economía, una garantía en el largo plazo tiene poca utilidad en la vida real:\n\n[**El] largo plazo es una guía engañosa para los asuntos actuales. A largo plazo, todos estaremos muertos. Los economistas se asignan una tarea demasiado fácil, y por lo tanto inútil, si en tiempos tormentosos sólo pueden decirnos que, cuando la tormenta haya pasado hace tiempo, el océano volverá a estar calmo.** — Keynes (1923, 80)\n\nLo mismo que en economía vale también para la psicología y la estadística. No basta con saber que eventualmente llegaremos a la respuesta correcta al calcular la media muestral. Saber que un conjunto de datos infinitamente grande nos dará el valor correcto no resuelve nuestros problemas prácticos en el presente. Saber que un conjunto de datos infinitamente grande me dará el valor exacto de la media poblacional no es precisamente reconfortante cuando mi conjunto de datos real tiene un tamaño de muestra de\\(N=100\\). En la vida real, entonces, necesitamos saber algo sobre el comportamiento de la media muestral cuando se calcula a partir de un conjunto de datos más modesto.\n\n5.10.1 Distribución muestral de las medias muestrales\n“Oh no, ¿qué es la distribución muestral de las medias muestrales? ¿Eso siquiera se puede decir en español?”. Sí, lamentablemente, se puede. La distribución muestral de las medias muestrales es la próxima cosa más importante que vas a tener que entender. ES TAN IMPORTANTE QUE ES NECESARIO ESCRIBIRLO TODO EN MAYÚSCULAS. Solo resulta confuso al principio porque es largo y usa muestreo y muestra en la misma frase.\nNo te preocupes, ya te venimos preparando para esto. Sabés lo que es una distribución, ¿no? Es de donde salen los números. Determina qué números ocurren más o menos seguido, o con la misma frecuencia. Y también sabés qué es una muestra, ¿no? Es un conjunto de números que tomamos de una distribución. Entonces, ¿qué sería la distribución muestral de las medias muestrales?\nPrimero: ¿a qué nos referimos con medias muestrales? Bueno, si tomás una muestra de números, tenés un conjunto de valores… y podés calcular la media de esos valores. Eso es la media muestral. Simple.\nAhora bien, ¿qué es esa distribución de la que hablamos? Imaginá que tomás muchas muestras distintas: una por acá, otra por allá, muchas más. Tenés un montón de muestras distintas de números. De cada muestra, podés calcular su media. Y entonces tenés muchas medias. ¿Cómo se ven esas medias? Si las ponés en un histograma, lo vas a saber. Y eso que estarías viendo —más o menos— es una distribución: la distribución muestral de las medias muestrales.\n“Más o menos te sigo… pero, ¿por qué haría esto en vez de mirar Netflix…?” Porque la distribución muestral de las medias te da una nueva manera de entender el azar. Una que podés manejar, igual que manejás el control remoto, si apretás los botones de diseño adecuados.\n\n\n5.10.2 Viendo las partes\nPara construir una distribución muestral de medias muestrales, solo necesitamos lo siguiente:\n\nUna distribución de donde tomar los números\nUn montón de muestras diferentes extraídas de esa distribución\nLa media de cada una de esas muestras\nJuntar todas esas medias muestrales y graficarlas en un histograma\n\n\nPregunta para vos: ¿Cómo pensás que se va a ver la distribución muestral de las medias? ¿Va a tener la misma forma que la distribución original de la que salieron las muestras? ¿O no? Buena pregunta, pensalo.\n\nHagamos esas cuatro cosas. Vamos a tomar números de una distribución uniforme. La Figura 5.12 muestra la distribución uniforme usada para muestrear números enteros del 1 al 10:\n\n\n\n\n\nFigura 5.12: Ilustración de las probabilidades de muestreo para los números del 1 al 10. En una distribución uniforme, todos los números tienen igual probabilidad, así que la línea es plana, indicando que todos tienen la misma chance.\n\n\n\n\nLa Figura 5.13 anima el proceso de tomar un montón de muestras de la distribución uniforme. Vamos a fijar el tamaño de muestra en 20. Es más fácil ver cómo se comporta la media muestral en una animación. Cada histograma muestra una nueva muestra. La línea roja indica dónde está la media de esa muestra. Las muestras son todas bastante distintas entre sí, pero la línea roja no se mueve mucho: siempre queda más o menos cerca del centro. Sin embargo, sí se mueve un poco, y esa variabilidad es lo que llamamos la distribución muestral de la media muestral.\n\n\n\n\n\nFigura 5.13: Animación que muestra histogramas para diferentes muestras de tamaño 20 tomadas de la distribución uniforme. La línea roja indica la media de cada muestra.\n\n\n\n\nOK, ¿qué tenemos acá? Tenemos una animación de 10 muestras diferentes. Cada muestra tiene 20 observaciones, que están resumidas en los histogramas que aparecen en la animación. Cada histograma tiene una línea roja. Esa línea roja muestra dónde está ubicada la media de esa muestra. Así que hemos calculado las medias muestrales de 10 muestras distintas tomadas de una distribución uniforme.\nPrimera pregunta: ¿Las medias muestrales son todas iguales? La respuesta es no. Aunque son todas más o menos parecidas, están todas alrededor de cinco, más o menos un par de números. Esto es interesante. Aunque nuestras muestras se ven bastante diferentes entre sí, las medias de esas muestras se ven más similares que distintas.\nSegunda pregunta: ¿Qué deberíamos hacer con las medias de nuestras muestras? Bueno, ¿qué tal si las juntamos todas y graficamos un histograma de ellas? Eso nos permitiría ver cómo se ve la distribución de las medias muestrales. El próximo histograma muestra exactamente eso. Solo que, en lugar de tomar 10 muestras, vamos a tomar 10.000. Y para cada una vamos a calcular su media. Así que vamos a tener 10.000 medias muestrales. La figura Figura 5.14 muestra ese histograma:\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigura 5.14: Histograma de las medias muestrales de 10.000 muestras, cada una de tamaño 20, tomadas de una distribución uniforme de números del 1 al 10. La media esperada es 5.5, y el histograma está centrado en 5.5. La media de cada muestra no es siempre 5.5, debido al error muestral o al azar.\n\n\n\n\n“Pará, ¿qué? Esto no tiene sentido. Pensé que estábamos tomando muestras de una distribución uniforme. Las distribuciones uniformes son planas. ESTO NO SE PARECE A UNA DISTRIBUCIÓN PLANA, ¿QUÉ ESTÁ PASANDO, AAAAAGGGHH.” Te entendemos. Te juro que sí.\nRecordá que estamos mirando la distribución de las medias muestrales. Y sí, es cierto que esta distribución no se parece a la distribución original de la que tomamos las muestras. Nuestra distribución de medias muestrales sube y baja. De hecho, esto es lo que ocurre casi siempre con las distribuciones de medias muestrales. Este hecho se conoce como el teorema central del límite, del que hablaremos más adelante.\nPor ahora, hablemos de lo que está pasando. Recordá que hemos estado muestreando números entre 1 y 10. Se supone que cada número debería aparecer con aproximadamente la misma frecuencia, porque estamos usando una distribución uniforme. Entonces, supongamos que tomamos una muestra de 10 números, y por casualidad obtuvimos uno de cada uno del 1 al 10:\n1 2 3 4 5 6 7 8 9 10\n¿Cuál es la media de esos números? Bueno, es 1+2+3+4+5+6+7+8+9+10 = 55, y 55 / 10 = 5.5. Imaginá que tomamos una muestra más grande, digamos de 20 números, y de nuevo obtenemos exactamente 2 de cada número. ¿Cuál sería la media? Sería (1+2+3+4+5+6+7+8+9+10)×2 = 110, y 110 / 20 = 5.5. Sigue siendo 5.5.\nPodés ver que el valor medio de nuestra distribución uniforme es 5.5. Ahora que sabemos esto, podríamos esperar que la mayoría de nuestras muestras tengan una media cercana a ese número. Ya sabemos que ninguna muestra va a ser perfecta, y no va a tener exactamente la misma cantidad de cada número. Así que esperamos que la media de nuestras muestras varíe un poco. El histograma que hicimos muestra justamente esa variación. Como era de esperar, los valores se agrupan en torno al 5.5.\n\n\n5.10.3 ¡Las distribuciones muestrales existen para cualquier estadístico muestral!\nAlgo importante a tener en cuenta cuando hablamos de distribuciones muestrales es que cualquier estadístico muestral que quieras calcular tiene su propia distribución muestral. Por ejemplo, supongamos que cada vez que tomás una muestra de números, anotás el valor más alto de esa muestra. Si repetís esto muchas veces, vas a obtener una distribución muestral completamente distinta: la distribución muestral del máximo. Podrías calcular el número más pequeño, o la moda, o la mediana, o la varianza, o el desvío estándar, o cualquier otra cosa a partir de tu muestra. Luego podrías repetir ese procedimiento muchas veces y obtener la distribución muestral de esos estadísticos. ¡Qué genial!\nSolo por diversión, acá van algunas distribuciones muestrales para distintos estadísticos. Vamos a tomar una distribución normal con media = 100 y desvío estándar = 20. Luego tomamos muchas muestras de tamaño n = 50 (50 observaciones por muestra). Guardamos todos los estadísticos muestrales y graficamos sus histogramas en la figura Figura 5.15. Hagámoslo:\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigura 5.15: Cada panel muestra un histograma de un estadístico muestral distinto\n\n\n\n\nAcabamos de calcular 4 distribuciones muestrales diferentes: para la media, el desvío estándar, el valor máximo y la mediana. Si mirás rápido los histogramas, podrías pensar que todos se ven más o menos iguales. Pero esperá un segundo. Es muy importante mirar los ejes x. Son diferentes. Por ejemplo, la media muestral va aproximadamente de 90 a 110, mientras que el desvío estándar va de 15 a 25.\nEstas distribuciones muestrales son súper importantes, y vale la pena pensarlas bien. ¿En qué deberías pensar? Bueno, acá va una pista: estas distribuciones te están diciendo qué podés esperar de tu muestra. Y lo más importante: te están diciendo qué deberías esperar cuando tomás una muestra de una distribución específica, en este caso, una distribución normal con media 100 y desvío estándar 20. ¿Qué aprendimos? Aprendimos muchísimo. Aprendimos que podemos esperar que nuestra muestra tenga una media más o menos entre 90 y 108. Notá que las medias muestrales nunca son más extremas que eso. Aprendimos que nuestras muestras normalmente van a tener cierta variabilidad, y que el desvío estándar va a estar entre 15 y 25 (nunca mucho más allá). También vemos que, a veces, obtenemos valores grandes —digamos entre 120 y 180— pero no mucho más que eso. Y que la mediana es bastante similar a la media. Entonces, si alguna vez tomás una muestra de 50 números y tus estadísticas descriptivas caen dentro de esos rangos, es razonable suponer que esa muestra podría venir de una distribución como esta. Pero si tus estadísticas son muy distintas, es probable que no vengan de esta distribución. Usando simulaciones, podemos ver cómo se ven las muestras cuando provienen de ciertas distribuciones, y usar esa información para hacer inferencias sobre si nuestra muestra proviene o no de una distribución en particular.\n\n\n5.10.4 El teorema central del límite\nBien, ya viste muchas distribuciones muestrales y sabés qué es la distribución muestral de la media. Acá nos vamos a enfocar en cómo cambia la distribución muestral de la media en función del tamaño de la muestra.\nIntuitivamente, ya conocés parte de la respuesta: si tenés pocas observaciones, es probable que la media muestral sea bastante imprecisa (ya viste que puede variar bastante); si repetís un experimento chico y volvés a calcular la media, vas a obtener un resultado bastante distinto. En otras palabras, la distribución muestral es bastante amplia. En cambio, si repetís un experimento grande y calculás de nuevo la media muestral, probablemente obtengas un valor muy parecido al anterior, así que la distribución muestral va a ser mucho más estrecha.\nVeamos ahora una linda animación para ver todo esto en acción. Vamos a tomar muestras de una distribución normal. La figura Figura 5.16 tiene cuatro paneles, cada uno representando un tamaño de muestra distinto (n): 10, 50, 100 y 1000. La línea roja muestra la forma de la distribución normal. Las barras grises muestran un histograma de cada una de las muestras que tomamos. La línea roja muestra la media de una muestra individual (es decir, el centro del histograma en gris). Como podés ver, la línea roja se mueve bastante, especialmente cuando el tamaño de la muestra es pequeño (10).\nLo nuevo acá son las barras azules y las líneas azules. Las barras azules representan la distribución muestral de la media muestral. Por ejemplo, en el panel con tamaño de muestra 10, vemos un montón de barras azules: es un histograma de 10 medias muestrales, cada una calculada a partir de una muestra de tamaño 10. En el panel de n = 50, vemos un histograma de 50 medias muestrales, tomadas de 50 muestras de tamaño 50, y así sucesivamente. La línea azul en cada panel es la media de esas medias muestrales (“¡aaaaagh, es una media de medias!”; sí, lo es).\n\n\n\n\n\nFigura 5.16: Animación de muestras (el histograma gris muestra la frecuencia de los datos en cada muestra) y de la distribución muestral de la media (histograma azul de medias muestrales de muchas muestras). Cada muestra proviene de la distribución normal mostrada en rojo. La línea roja móvil es la media de una muestra individual. La línea azul es la media del histograma azul, que representa la distribución muestral de la media para muchas muestras.\n\n\n\n\n¿Qué deberías notar? Fijate que el rango de las barras azules se reduce a medida que el tamaño de muestra aumenta. La distribución muestral de la media es bastante ancha cuando el tamaño de muestra es 10, se estrecha al aumentar a 50 y 100, y es prácticamente una sola barra en el centro cuando el tamaño llega a 1000. Lo que estamos viendo es que la media de la distribución muestral se aproxima a la media poblacional a medida que crece el tamaño de la muestra.\nEntonces, la distribución muestral de la media es otra distribución, y tiene cierta varianza. Varía más cuando el tamaño de muestra es pequeño, y varía menos cuando el tamaño es grande. Podemos cuantificar este efecto calculando el desvío estándar de la distribución muestral, que se conoce como el error estándar. El error estándar de un estadístico suele abreviarse como SE, y como habitualmente nos interesa el error estándar de la media muestral, usamos la sigla SEM (standard error of the mean). Como podés ver claramente en la animación, a medida que el tamaño de muestra\\(N\\)aumenta, el SEM disminuye.\nBueno, eso es una parte de la historia. Sin embargo, hay algo que hemos estado dejando un poco de lado. Ya lo vimos antes, pero vale la pena volver a observarlo. Acá va el punto clave: no importa qué forma tenga tu distribución poblacional, a medida que\\(N\\)aumenta, la distribución muestral de la media empieza a parecerse más y más a una distribución normal. Eso es el teorema central del límite.\nPara ver el teorema central del límite en acción, vamos a mirar algunos histogramas de medias muestrales provenientes de diferentes tipos de distribuciones. Es muy importante recordar que lo que estás viendo son distribuciones de medias muestrales, no distribuciones de valores individuales.\nEmpecemos. La figura ?fig-4sampledistmeannorm muestra un muestreo a partir de una distribución normal. La línea roja es la distribución normal de la que se extraen las muestras. Para cada muestra se calcula la media, y la distribución de esas medias se muestra con las barras azules. Notá que la forma de la línea roja y de las barras azules es similar: ambas se ven como una distribución normal.\nHagámoslo de nuevo. Esta vez vamos a tomar muestras de una distribución uniforme plana, representada por la línea roja. Sin embargo, la Figura 5.17 muestra que la distribución de las medias muestrales, representada por las barras azules, no es plana: se parece a una distribución normal.\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\nWarning: Removed 4 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\nFigura 5.17: Ilustración de que la forma de la distribución muestral de la media es normal, incluso cuando las muestras provienen de una distribución no normal (en este caso, uniforme)\n\n\n\n\nUna vez más, ahora con una distribución exponencial (mostrada en rojo), donde es más probable que se muestren valores pequeños que grandes. Aunque muchos más números en cada muestra serán pequeños en comparación con los grandes, según la Figura 5.18, la distribución muestral de la media no se parece a la línea roja. En cambio, la distribución muestral de la media se parece a una curva en forma de campana: una distribución normal. Esto es el teorema central del límite en acción.\n\n\n\n\n\nFigura 5.18: Ilustración de que la forma de la distribución muestral de la media es normal, incluso cuando las muestras provienen de una distribución exponencial\n\n\n\n\nA partir de estas figuras, parece que tenemos evidencia para las siguientes afirmaciones sobre la distribución muestral de la media:\n\nLa media de la distribución muestral es igual a la media de la población\nEl desvío estándar de la distribución muestral (es decir, el error estándar) se reduce a medida que aumenta el tamaño de la muestra\nLa forma de la distribución muestral se vuelve normal a medida que aumenta el tamaño de la muestra\n\nY resulta que no solo estas afirmaciones son verdaderas: hay un teorema muy famoso en estadística que las demuestra a las tres, conocido como el teorema central del límite. Entre otras cosas, este teorema nos dice que si la distribución poblacional tiene media\\(\\mu\\)y desvío estándar\\(\\sigma\\), entonces la distribución muestral de la media también tiene media\\(\\mu\\), y su error estándar es:\n\\[\n\\text{SEM} = \\frac{\\sigma}{\\sqrt{N}}\n\\]\nComo se divide el desvío estándar poblacional\\(\\sigma\\)por la raíz cuadrada del tamaño muestral\\(N\\), el SEM disminuye al aumentar el tamaño de la muestra. También nos dice que la forma de la distribución muestral tiende a ser normal.\nEste resultado es útil para todo tipo de cosas. Nos explica por qué los experimentos grandes son más confiables que los pequeños, y como nos da una fórmula explícita para el error estándar, también nos dice cuánto más confiables son. También nos explica por qué la distribución normal es, bueno… normal. En los experimentos reales, muchas de las cosas que queremos medir son en realidad promedios de un montón de cantidades diferentes (por ejemplo, uno podría decir que la inteligencia “general” medida por el CI es un promedio de muchas habilidades y aptitudes “específicas”), y cuando eso pasa, el valor promedio tiende a seguir una distribución normal. Por esta ley matemática, la distribución normal aparece una y otra vez en los datos reales."
  },
  {
    "objectID": "04-SamplesPopulations.html#puntajes-z",
    "href": "04-SamplesPopulations.html#puntajes-z",
    "title": "5  Probabilidad, muestreo y estimación",
    "section": "5.11 Puntajes z",
    "text": "5.11 Puntajes z\nAhora estamos en condiciones de combinar algunas de las cosas que vimos en este capítulo, e introducirte a una nueva herramienta: los puntajes z. Realmente que no vamos a usar mucho los puntajes z en este libro. Sin embargo, no podés hacer un curso de estadística sin aprender sobre los puntajes z.\nVamos a observar una distribución normal en la Figura 5.19, y vamos a trazar líneas en la distribución en 0, ±1, ±2 y ±3 desviaciones estándar desde la media:\n\n\n\n\n\nFigura 5.19: Una distribución normal. Cada línea representa una desviación estándar desde la media. Las etiquetas muestran la proporción de puntajes que caen entre cada barra.\n\n\n\n\nLa figura muestra una distribución normal con media = 0 y desvío estándar = 1. Trazamos líneas en cada desviación estándar: -3, -2, -1, 0, 1, 2 y 3. También mostramos algunos números en las etiquetas entre cada línea. Esos números son proporciones. Por ejemplo, vemos que la proporción entre 0 y 1 es 0.341: es decir, los puntajes entre 0 y 1 ocurren el 34.1% del tiempo. Los puntajes entre -1 y 1 ocurren el 68.2% del tiempo, lo cual es más de la mitad. Los puntajes entre 1 y 2 ocurren aproximadamente el 13.6% del tiempo, y los que están entre 2 y 3 ocurren aún menos: solo el 2.1%.\nLas distribuciones normales siempre tienen estas propiedades, incluso cuando tienen distintas medias y desviaciones estándar. Por ejemplo, mirá la distribución normal en la Figura 5.20, que tiene media = 100 y desvío estándar = 25.\n\n\n\n\n\nFigura 5.20: Una distribución normal. Cada línea representa una desviación estándar desde la media. Las etiquetas muestran la proporción de puntajes que caen entre cada barra.\n\n\n\n\nAhora estamos viendo una distribución normal con media = 100 y desvío estándar = 25. Fijate que la región entre 100 y 125 contiene el 34.1% de los puntajes. Esta región está a 1 desviación estándar de la media (la desviación estándar es 25, la media es 100, así que 125 está a una desviación estándar completa de 100). Como podés ver, las mismas proporciones aparecen entre cada una de las desviaciones estándar, tal como ocurría cuando la desviación estándar era 1 y la media era 0.\n\n5.11.1 La idea detrás de los puntajes z\nA veces puede ser útil transformar tus puntajes originales a otros que sean más fáciles de manejar. Por ejemplo, si tenés un montón de proporciones como .3, .5, .6, .7, podrías querer convertirlas en porcentajes: 30%, 50%, 60%, 70%. Para eso, multiplicás las proporciones por una constante, 100. Si querés volver de porcentajes a proporciones, simplemente dividís por 100. Este tipo de transformación simplemente cambia la escala de los números (de 0–1 a 0–100), pero no cambia el patrón de los datos.\nLa idea detrás de los puntajes z es un tipo de transformación similar. La idea es expresar cada puntaje original (o bruto) en función de su desviación estándar. Por ejemplo, si te digo que saqué un 75% en una prueba, no sabrías qué tan bien me fue comparado con el resto de la clase. Pero si te digo que obtuve una puntuación 2 desviaciones estándar por encima de la media, sí sabrías que me fue bastante bien, porque sabés que la mayoría de los puntajes (si están distribuidos normalmente) quedan por debajo de 2 desviaciones estándar de la media.\nAhora también sabemos, gracias al teorema central del límite, que muchas de nuestras medidas —como las medias muestrales— se distribuyen normalmente. Por eso, muchas veces conviene expresar los puntajes brutos en términos de sus desviaciones estándar.\nVeamos cómo se ve esto en una tabla, sin fórmulas. Vamos a considerar puntajes que provienen de una distribución normal con media = 100 y desviación estándar = 25. Vamos a listar algunos puntajes originales junto con sus correspondientes puntajes z:\n\noriginal &lt;- c(25, 50, 75, 100, 125, 150, 175)\nz   &lt;- c(-3, -2, -1, 0, 1, 2, 3)\ndf &lt;- data.frame(original, z)\nknitr::kable(df)\n\n\n\n\noriginal\nz\n\n\n\n\n25\n-3\n\n\n50\n-2\n\n\n75\n-1\n\n\n100\n0\n\n\n125\n1\n\n\n150\n2\n\n\n175\n3\n\n\n\n\n\nRecordá: la media es 100 y el desvío estándar es 25. ¿Cuántas desviaciones estándar está un puntaje de 100 por encima o por debajo de la media? Cero. Está justo en la media. Por eso, el puntaje z de 100 es 0. ¿Y el puntaje 125? Está exactamente 25 unidades por encima de 100, o sea, a una desviación estándar de distancia, por eso el puntaje z para 125 es es 1. El puntaje z para 150 es 2, 150 está “dos veces 25” por encima de 100. El puntaje z para 50 es -2, porque 50 está a “dos veces 25” de distancia de 100, pero en la dirección opuesta. Todo lo que estamos haciendo es reexpresar los puntajes brutos en función de cuántas desviaciones estándar están alejados de la media. Y como siempre, la media está justo en el centro: el centro de la distribución de puntajes z es siempre 0.\n\n\n5.11.2 Cómo calcular puntajes z\nPara calcular puntajes z, lo único que tenés que hacer es averiguar cuántas desviaciones estándar se encuentra cada número con respecto a la media. Supongamos que la media es 100 y la desviación estándar es 25. Tenés un puntaje de 97. ¿A cuántas desviaciones estándar está 97 de la media?\nPrimero, calculamos la diferencia entre el puntaje y la media:\n\\[\n97 - 100 = -3\n\\]\nBien, tenemos una diferencia total de -3. ¿Cuántas desviaciones estándar representa ese -3, si una desviación estándar es 25? Claramente -3 es mucho menor que 25, así que va a ser mucho menos de 1 desviación estándar. Para saber cuánto, simplemente dividimos -3 por la desviación estándar:\n\\[\n\\frac{-3}{25} = -0.12\n\\]\nNuestro puntaje z para 97 es -0.12.\nLa fórmula general es:\n\\[\nz = \\frac{\\text{puntaje bruto} - \\text{media}}{\\text{desvío estándar}}\n\\]\nPor ejemplo, si tenemos estos 10 puntajes tomados de una distribución normal con media = 100 y desvío estándar = 25:\nscores &lt;- round(rnorm(10, 100, 25), digits = 2)\nprint(scores)\nLos puntajes z correspondientes serían:\n(scores - 100) / 25\nUna vez que tenés los puntajes z, podés usarlos como una forma alternativa de describir tus datos. Por ejemplo, ahora con solo mirar un puntaje sabés si es más o menos probable que ocurra, porque ya conocés cómo funciona el área bajo la curva normal. Puntajes z entre -1 y 1 ocurren bastante seguido; mayores que ±1 todavía ocurren, pero con menos frecuencia; y los que superan ±2 son bastante poco comunes. Esta es una forma práctica de mirar tus números y tener una idea general de qué tan seguido ocurren.\nEn general, no conocés la media ni el desvío estándar de la población de la que provienen tus datos. Por eso, podés usar la media y el desvío estándar de tu muestra como estimaciones, y usarlos para calcular los puntajes z.\nFinalmente, los puntajes z también se llaman puntajes estandarizados, porque cada puntaje bruto se expresa en términos de cuántas desviaciones estándar se encuentra respecto de la media. Es muy probable que esta sea la última vez que hablemos de puntajes z en este libro. Y podrías preguntarte por qué nos molestamos en explicarlos. Primero, porque vale la pena saber que existen. Segundo, porque se vuelven importantes a medida que tu conocimiento estadístico se vuelve más avanzado. Tercero, porque algunos conceptos estadísticos —como la correlación— pueden expresarse en términos de puntajes z, y eso ilumina aspectos de esos estadísticos. Y por último, porque los puntajes z son muy útiles cuando estás trabajando con una distribución normal cuya media y desviación estándar son conocidas."
  },
  {
    "objectID": "04-SamplesPopulations.html#estimación-de-parámetros-poblacionales",
    "href": "04-SamplesPopulations.html#estimación-de-parámetros-poblacionales",
    "title": "5  Probabilidad, muestreo y estimación",
    "section": "5.12 Estimación de parámetros poblacionales",
    "text": "5.12 Estimación de parámetros poblacionales\nParemos un momento para ubicarnos. Estamos por meternos en el tema de la estimación. ¿Qué es eso, y por qué debería importarte? Primero: los parámetros poblacionales son cosas que describen una distribución. Por ejemplo, las distribuciones tienen medias. La media es un parámetro de la distribución. La desviación estándar también es un parámetro. Cualquier cosa que describa una distribución es un posible parámetro.\nOK, ¿y por qué importa? Buena pregunta. Hay razones concretas para que te importe. Y también hay muy buenas razones abstractas. Desafortunadamente, en investigación, casi siempre son las razones abstractas las que más importan —y también las más difíciles de entender al principio.\n\n5.12.1 Parámetros poblacionales concretos\nPrimero, algunas razones concretas. Existen poblaciones reales ahí afuera, y a veces queremos conocer sus parámetros. Por ejemplo, si sos una empresa que fabrica calzado, te interesaría conocer los parámetros poblacionales del tamaño de los pies. Como primera aproximación, querrías saber la media y la desviación estándar de la población. Si tu empresa conoce esto, y otras no, probablemente te vaya mejor (asumiendo que todas fabrican zapatos de calidad similar). ¿Por qué te iría mejor y cómo podrías usar esos parámetros? Acá va una buena razón: como empresa, querés que la oferta coincida con la demanda. Si fabricás demasiados zapatos de talles muy chicos o muy grandes, y no hay suficientes personas que los compren, estás haciendo un montón de calzado que no se va a vender. Y si no fabricás suficientes zapatos de los talles más comunes, vas a estar dejando plata sobre la mesa. ¿Verdad? Sí. Entonces, ¿qué sería lo óptimo? Tal vez fabricarías diferentes cantidades de zapatos en cada talle, de acuerdo a la demanda de cada uno. Y sabrías algo sobre la demanda si conocieras la frecuencia de cada talle en la población. Para eso, necesitarías conocer los parámetros poblacionales.\nPor suerte, no es tan difícil obtener los parámetros poblacionales sin medir a toda la población. ¿Quién tiene tiempo para medirle los pies a todo el mundo? Nadie. Lo que hacés, en cambio, es seleccionar al azar un grupo de personas, medirles los pies, y después calcular los parámetros de esa muestra. Si la muestra es suficientemente grande, ya aprendimos que la media muestral es una muy buena estimación de la media poblacional. Y pronto vamos a ver que también hay versiones del desvío estándar muestral que estiman bastante bien la desviación estándar poblacional. Quizás la distribución de talles de calzado no tenga una forma exactamente normal. Aun así, si recolectás una muestra lo bastante grande, la forma de la distribución muestral será una buena aproximación a la forma de la distribución poblacional. Todas estas son buenas razones para interesarse por la estimación de parámetros poblacionales. Pero… ¿acaso dirigís una empresa de calzado? Probablemente no.\n\n\n5.12.2 Parámetros poblacionales abstractos\nIncluso cuando pensamos que estamos hablando de algo concreto en psicología, a menudo enseguida se vuelve abstracto. En lugar de medir la población de talles de calzado, ¿qué tal si intentamos medir la población de la felicidad humana? Todos creemos saber lo que es la felicidad. Todos tienen más o menos cantidad de ella. Hay mucha gente, así que… debe haber una “población de felicidad”, ¿no? Tal vez. Pero no es algo muy concreto. El primer problema es cómo medir la felicidad. Supongamos que usamos un cuestionario. Considerá estas preguntas:\n\n¿Qué tan feliz te sentís ahora mismo en una escala del 1 al 7? ¿Qué tan feliz sos en general en una escala del 1 al 7? ¿Qué tan feliz te sentís a la mañana en una escala del 1 al 7? ¿Qué tan feliz te sentís a la tarde en una escala del 1 al 7?\n\n1 = muy infeliz 2 = infeliz 3 = algo infeliz 4 = en el medio 5 = algo feliz 6 = feliz 7 = muy feliz\nOlvidémonos de preguntarle esto a todo el mundo. Supongamos que se lo preguntamos a mucha gente (nuestra muestra). ¿Qué creés que va a pasar? Bueno, obviamente, la gente va a dar todo tipo de respuestas, ¿no? Podemos contar cuántas veces se eligió cada opción y graficarlas en un histograma. Eso nos mostraría la distribución de puntajes de felicidad en nuestra muestra. “¡Genial, fantástico!”, decís. Sí, está todo muy bien.\nPor un lado, podemos decir un montón de cosas sobre las personas en nuestra muestra. Podemos decir quién dijo que es feliz y quién no —¡al fin y al cabo, nos lo acaban de decir!\nPero… ¿qué podemos decir sobre la población más grande? ¿Podemos usar los parámetros de nuestra muestra (media, desvío, forma, etc.) para estimar algo sobre la población completa? ¿Podemos inferir qué tan feliz es “todo el mundo” a partir de lo que dijo nuestra muestra?\n\n5.12.2.1 Complicaciones de la inferencia\nAntes de listar un montón de complicaciones, dejame decirte qué creo que sí podemos hacer con nuestra muestra. Siempre que sea lo suficientemente grande, los parámetros de nuestra muestra serán una bastante buena estimación de cómo se vería otra muestra tomada de la misma población. Y por los motivos que vamos a discutir, muchas veces eso es todo lo que podemos decir. Pero está bien: como vas a ver a lo largo de este libro, podemos trabajar con eso.\nProblema 1: múltiples poblaciones Si observás una muestra grande de respuestas a un cuestionario, vas a encontrar evidencia de múltiples distribuciones dentro de tu muestra. Las personas responden de maneras distintas. Algunas personas son muy cautas y evitan los extremos. Sus respuestas tienden a concentrarse en el centro de la escala, con muchos 3, 4 y 5. Otras personas responden de manera muy bimodal: se sienten muy felices o muy infelices dependiendo del momento del día. Sus respuestas se agrupan mayormente en los extremos: 1, 2, 6 y 7. Esos números parecen provenir de una distribución completamente distinta. También hay personas que son completamente felices o completamente infelices. Una vez más, estos dos “grupos” de personas generan respuestas que parecen venir de dos distribuciones diferentes: una con casi todos 6 y 7, y otra con casi todos 1 y 2. Otras personas responden de forma bastante aleatoria, y sus respuestas se distribuyen de manera casi uniforme en toda la escala. Entonces, ¿existe una única población con parámetros que podamos estimar a partir de nuestra muestra? **Probablemente no. Podría tratarse de una mezcla de muchas poblaciones, cada una con su propia distribución.\nProblema 2: ¿Qué miden realmente estas preguntas? Si todo el propósito de hacer el cuestionario es estimar la felicidad de la población, entonces realmente tenemos que preguntarnos si nuestras mediciones nos dicen algo sobre la felicidad en primer lugar. Algunas preguntas que surgen: ¿Las personas son precisas al decir cuán felices son? ¿La medición de felicidad depende de la escala usada? Por ejemplo, ¿los resultados serían distintos si usáramos una escala de 0 a 100, o de -100 a +100, o si no usáramos números en absoluto? ¿La medición de felicidad depende de cómo está redactada la pregunta? ¿Una medida como esta nos dice todo lo que queremos saber sobre la felicidad? (probablemente no). ¿Qué cosas está dejando afuera? (¿quién sabe? probablemente muchas).\nEn resumen, nadie sabe con certeza si este tipo de preguntas miden lo que queremos que midan, solo esperamos que lo hagan. Lo que sí sabemos bastante bien es qué tipo de cosas miden realmente este tipo de preguntas. Es algo bastante evidente, que está justo frente a nosotros: las mediciones de cuestionarios miden cómo la gente responde cuestionarios. Dicho de otro modo: miden cómo se comportan las personas cuando se les da un cuestionario para responder. Esto puede también reflejar algo sobre la felicidad, si la pregunta trata sobre felicidad. Pero, resulta que las personas son notablemente consistentes en cómo responden, incluso cuando las preguntas son totalmente absurdas, o cuando ni siquiera hay preguntas (¡solo números para elegir!) (maul?_rethinking_2017).\nLa enseñanza que nos dejan estas complicaciones es que, en psicología, podemos recolectar muestras… pero muchas veces no tenemos una idea clara de qué población está asociada a esas muestras. Puede haber muchas poblaciones distintas. O la población puede variar dependiendo de a quién le preguntes. Y finalmente, la “población” quizás no sea la que vos querés que sea.\n\n\n\n5.12.3 Experimentos y parámetros poblacionales\nOK, entonces no tenemos una empresa de calzado, y tampoco podemos identificar con claridad la población que nos interesa en psicología… ¿podemos saltearnos esta sección sobre estimación? Después de todo, eso de “la población” parece demasiado raro, abstracto, inútil y polémico. ¡ESPERÁ!\nResulta que sí podemos aplicar todo lo que venimos aprendiendo para resolver un montón de problemas importantes en investigación. Estas herramientas nos permiten responder preguntas a partir de los datos que recolectamos. La estimación de parámetros es una de esas herramientas. Solo necesitamos ser un poco más creativos y un poco más abstractos para poder usarlas.\nYa sabemos algunas cosas. Los números que medimos vienen de algún lado. A eso le llamamos “distribuciones”. Las distribuciones controlan cómo aparecen los números. Algunos valores ocurren más que otros, según la distribución. Asumimos —incluso si no sabemos cuál es la distribución, ni qué significa— que los números provienen de una. Segundo: cuando tenemos algunos números, a eso le llamamos una muestra. Y todo este capítulo te enseñó una cosa fundamental: Cuando tu muestra es grande, se parece a la distribución de la que proviene. Y cuando tu muestra es grande, también se va a parecer mucho a otra muestra grande de la misma población. ¡Podemos usar este conocimiento!\nMuy a menudo, como psicólogos, lo que queremos saber es qué causa qué. Queremos saber si X causa un cambio en Y. ¿Comer chocolate te hace más feliz? ¿Estudiar mejora tus notas? Hay millones de preguntas como estas. Y queremos respuestas.\nHasta ahora traté de mantenerme en ejemplos concretos en este libro —por eso hablamos de chocolate y felicidad, al menos son cosas tangibles. Pero ahora intentemos ponernos abstractos. Podemos hacerlo.\nQueremos saber si X causa un cambio en Y. ¿Qué es X? ¿Qué es Y? X es algo que cambiás, es decir, la variable independiente. Y es algo que medís. Entonces, vamos a tomar muestras de Y. “¡Ah, ya entendí! Tomamos muestras de Y, y luego podemos usar los parámetros muestrales para estimar los parámetros poblacionales de Y.” NO, bueno, más o menos. Vamos a tomar muestras de Y, eso sí. De hecho, eso es todo lo que hacemos siempre, por eso hablar de “la población de Y” es un poco sin sentido. Lo que nos interesa de verdad son nuestras muestras de Y, y cómo se comportan.\nEntonces, ¿qué pasaría si elimináramos completamente X del universo, y luego tomáramos una gran muestra de Y? Vamos a suponer que Y mide algo en un experimento de psicología. Así que sabemos, desde ya, que Y es una variable. Cuando tomamos una muestra grande, va a tener una distribución (porque Y varía). Entonces podemos hacer cosas como calcular la media de Y, medir su desviación estándar, y cualquier otra cosa que nos interese saber sobre Y. Perfecto. ¿Y si repitiésemos esa medición? Es decir, tomamos otra muestra aleatoria de Y, del mismo tamaño que la anterior. ¿Qué debería pasar? Nuestra primera muestra debería parecerse mucho a la segunda. Después de todo, no hicimos nada a Y, simplemente tomamos dos muestras grandes, en dos momentos diferentes. Ambas muestras serán un poco distintas (por el error muestral), pero se parecerán bastante. Cuanto más grandes sean nuestras muestras, más similares serán entre sí, especialmente si no hicimos nada para que sean diferentes. En otras palabras: podemos usar los parámetros de una muestra para estimar los de otra, porque tienden a ser iguales, sobre todo cuando las muestras son grandes.\nAhora estamos listos para el segundo paso. Vos querés saber si X cambia Y. ¿Qué hacés? Hacés que X suba y tomás una muestra grande de Y, y después la mirás. Después hacés que X baje, tomás otra muestra grande de Y y la mirás también. Lo que sigue es comparar las dos muestras de Y. Si X no cambia nada, ¿qué tendrías que encontrar? Eso ya lo hablamos en el párrafo anterior. Si X no hace nada, entonces las dos muestras grandes de Y tendrían que ser bastante parecidas. Sin embargo, si X realmente tiene un efecto sobre Y, entonces una de tus muestras grandes de Y va a ser distinta de la otra. Habrás cambiado algo en Y. Tal vez X hace que cambie la media de Y. O tal vez X cambia la variabilidad de Y. O incluso, puede que X cambie toda la forma de la distribución. Si encontramos algún cambio importante que no pueda explicarse por error muestral, entonces podemos concluir que algo en X causó un cambio en Y. ¡Podemos usar este enfoque para aprender qué causa qué!\nLa idea muy importante sigue siendo la de estimación, solo que no se trata exactamente de estimar un parámetro poblacional. Sabemos que cuando tomamos muestras, hay variación natural. Entonces, cuando estimamos un parámetro de una muestra —como la media— sabemos que esa estimación va a estar equivocada en alguna medida. Cuando encontramos que dos muestras son distintas, lo que necesitamos averiguar es si la magnitud de esa diferencia es compatible con lo que esperaría del error muestral, o si la diferencia es más grande que eso. Si la diferencia es más grande, entonces podemos tener confianza en que no fue el error muestral lo que la produjo. Entonces, podemos inferir con confianza que otra cosa (como X) causó la diferencia. Este tipo de pensamiento más abstracto es el que ocupa la mayor parte del resto del libro. Se trata de determinar si hubo una diferencia causada por tu manipulación experimental. Hay mucho más para contar, siempre lo hay. Podemos ir más allá de simplemente preguntar si hay una diferencia, pero para fines introductorios, nos vamos a enfocar en encontrar diferencias como concepto fundamental.\n\n\n5.12.4 Resumen intermedio\nHemos hablado bastante sobre estimación, aunque hasta ahora no hemos hecho muchas estimaciones concretas. Así que en la próxima sección vamos a empezar a estimar la media y la desviación estándar. Formalmente, esto se conoce como el uso de una muestra para estimar un parámetro poblacional. Pero sentite libre de pensar en “la población” de distintas maneras. Podría ser una población concreta, como la distribución de tamaños de calzado. O podría ser algo más abstracto, como una estimación del parámetro que típicamente producen las muestras, si las repitieras muchas veces. Por ejemplo, si realizás muchas muestras grandes bajo condiciones similares, ¿qué promedio suelen darte? Eso es un parámetro. La mayoría de las veces, no podemos ver directamente ese parámetro. Lo que podemos ver es una muestra. Y lo que podemos hacer es usar esa muestra para estimar el parámetro que no podemos observar.\n\n\n5.12.5 Estimación de la media poblacional\nSupongamos que vamos a Brooklyn y 100 personas locales tienen la amabilidad de hacer una prueba de CI. El puntaje promedio de CI entre esas personas resulta ser\\(\\bar{X} = 98.5\\). Entonces, ¿cuál es la verdadera media del CI en toda la población de Brooklyn? Obviamente, no lo sabemos. Podría ser 97.2, o también podría ser 103.5. Como nuestra muestra no es exhaustiva, no podemos dar una respuesta definitiva. Sin embargo, si nos obligaran a dar nuestra “mejor aproximación”, tendría que decir 98.5. Esa es la esencia de la estimación estadística. Estamos usando la media muestral como nuestra mejor aproximación a la media poblacional.\nEn este ejemplo, estimar el parámetro poblacional desconocido es bastante directo. Calculamos la media muestral y la usamos como nuestra estimación de la media poblacional. Es bastante simple, y en la próxima sección vamos a ver la justificación estadística de por qué esta respuesta intuitiva tiene sentido. Pero por ahora, asegurémonos de distinguir bien que el estadístico muestral y la estimación del parámetro poblacional son conceptualmente distintos. El estadístico muestral describe tus datos; en cambio, una estimación es una conjetura sobre la población.\nCon eso en mente, los profesionales de la estadística suelen usar notaciones distintas para diferenciarlas. Por ejemplo, si la media poblacional verdadera se denota\\(\\mu\\), entonces usamos\\(\\hat{\\mu}\\)para referirnos a nuestra estimación de esa media poblacional. En contraste, la media muestral se denota\\(\\bar{X}\\), o a veces\\(m\\). Sin embargo, en muestras aleatorias simples, la estimación de la media poblacional es idéntica a la media muestral: si observo una media muestral de\\(\\bar{X} = 98.5\\), entonces mi estimación de la media poblacional es también\\(\\hat{\\mu} = 98.5\\).\nPara dejar clara la notación, acá va una tabla útil:\n\n\n\n\n\n\n\n\nSímbolo\nQué representa\n¿Conocido a partir de la muestra?\n\n\n\n\n\\(\\bar{X}\\)\nMedia muestral\nSí, se calcula a partir de los datos crudos\n\n\n\\(\\mu\\)\nVerdadera media poblacional\nCasi nunca se conoce con certeza\n\n\n\\(\\hat{\\mu}\\)\nEstimación de la media poblacional\nSí, es idéntica a la media muestral\n\n\n\n\n\n5.12.6 Estimación del desvío estándar poblacional\nHasta ahora, la estimación parece bastante sencilla, y tal vez te estés preguntando por qué te hice leer todo ese material sobre teoría del muestreo. En el caso de la media, nuestra estimación del parámetro poblacional (\\(\\hat{\\mu}\\)) resultó ser idéntica a la estadística muestral correspondiente (\\(\\bar{X}\\)). Sin embargo, eso no siempre es así. Para verlo, pensemos cómo construir una estimación del desvío estándar poblacional, que vamos a denotar como\\(\\hat{\\sigma}\\). ¿Qué deberíamos usar como estimación en este caso? Tu primera reacción podría ser hacer lo mismo que con la media, y simplemente usar el estadístico muestral como estimación. Eso está casi bien, pero no del todo.\nVeamos por qué. Supongamos que tengo una muestra que contiene una sola observación. Para este ejemplo, ayuda pensar en una situación donde no tengas ninguna intuición previa sobre cuál podría ser el valor real en la población. Así que usemos algo completamente ficticio: supongamos que la observación mide la cromulencia de mis zapatos. Resulta que mis zapatos tienen una cromulencia de 20. Así que mi muestra es:\n20\nEsta es una muestra perfectamente legítima, aunque tenga un tamaño muestral de\\(N = 1\\). Tiene una media muestral de 20, y como cada observación en esta muestra es igual a la media muestral (¡obviamente!), el desvío estándar muestral es 0. Como descripción de la muestra, esto está bien: la muestra contiene una sola observación, así que no hay variabilidad observada en la muestra. Un desvío estándar muestral de\\(s = 0\\)es la respuesta correcta acá. Pero como estimación del desvío estándar poblacional, eso suena completamente absurdo, ¿no? Seamos sinceros: vos y yo no tenemos ni idea de qué es la “cromulencia”, pero sabemos algo sobre datos. La única razón por la que no vemos ninguna variabilidad en la muestra es que la muestra es demasiado chica como para mostrar alguna variación. Entonces, si tenés un tamaño muestral de\\(N=1\\), lo que se siente como la respuesta correcta es simplemente decir: “ni idea, la verdad”.\nFijate que no tenemos la misma intuición cuando se trata de la media muestral y la media poblacional. Si alguien nos obliga a hacer una mejor aproximación sobre la media de la población, no suena completamente descabellado decir que la media poblacional es 20. Obvio, probablemente no te sientas muy seguro de esa aproximación, porque solo tenés una observación, pero sigue siendo la mejor aproximación que podés hacer.\nExtendamos un poco este ejemplo. Supongamos que ahora hago una segunda observación. Mi conjunto de datos ahora tiene\\(N=2\\)observaciones de la cromulencia de los zapatos, y la muestra completa se ve así:\n20, 22\nEsta vez, nuestra muestra es justo lo suficientemente grande como para que podamos ver algo de variabilidad: dos observaciones es el número mínimo necesario para que pueda observarse alguna variación. Para este nuevo conjunto de datos, el promedio muestral es\\(\\bar{X}=21\\)y la desviación estándar muestral es\\(s=1\\).\n¿Qué intuiciones tenemos ahora sobre la población? De nuevo, en lo que respecta a la media poblacional, la mejor aproximación que podemos hacer es la media muestral: si nos obligaran a adivinar, probablemente diríamos que la media de cromulencia poblacional es 21. ¿Y la desviación estándar? Acá la cosa se pone un poco más complicada. La desviación estándar muestral está basada solamente en dos observaciones, y si sos como yo, seguramente tenés la intuición de que, con solo dos datos, no le estamos dando a la población “suficiente oportunidad” de mostrarnos su verdadera variabilidad. No es solo que sospechamos que la estimación está equivocada —porque con solo dos observaciones ya esperamos que haya algún error—. La preocupación más grande es que el error sea sistemático.\nSi el error es sistemático, entonces está sesgado. Por ejemplo, imaginate que el promedio muestral siempre fuera menor que el promedio poblacional. Si eso fuera cierto (no lo es), entonces no podríamos usar el promedio muestral como estimador. Estaría sesgado, estaríamos usando el número equivocado.\nBueno, resulta que el desvío estándar muestral es un estimador sesgado del desvío estándar poblacional. Ya veníamos intuyéndolo con lo que estuvimos discutiendo. Cuando el tamaño muestral es 1, el desvío estándar es 0, lo cual claramente es demasiado bajo. Cuando el tamaño muestral es 2, el desvío estándar empieza a ser mayor que 0, pero como solo tenemos dos datos, sospechamos que sigue siendo demasiado bajo. Y resulta que esta intuición es correcta.\nEstaría bueno poder demostrar esto de alguna forma. Hay pruebas matemáticas que confirman esta intuición, pero salvo que tengas una formación matemática bastante sólida, no van a ayudarte mucho. Así que en vez de eso, lo que voy a hacer es usar R para simular los resultados de algunos experimentos. Con esto en mente, volvamos a nuestros estudios sobre CI. Supongamos que la verdadera media poblacional de CI es 100, y la desviación estándar poblacional es 15. Podemos usar la función rnorm() para generar los resultados de un experimento en el que medimos\\(N = 2\\)puntuaciones de CI, y luego calculamos el desvío estándar muestral. Si repetimos este procedimiento una y otra vez, y graficamos un histograma de esos desvíos estándar muestrales, obtenemos la distribución muestral del desvío estándar. Esa distribución está graficada en la Figura 5.21.\n\nknitr::include_graphics(\"imgs/navarro_img/estimation/sampdistsd_es.png\")\n\n\n\n\nFigura 5.21: La distribución muestral del desvío estándar de la muestra en un experimento con dos puntuaciones de CI. El verdadero desvío estándar poblacional es 15 (línea punteada), pero como puede verse en el histograma, la gran mayoría de los experimentos produce un desvío estándar muestral mucho menor. En promedio, este experimento arrojaría un desvío estándar muestral de apenas 8.5, muy por debajo del valor real. En otras palabras, el desvío estándar muestral es un estimador sesgado del desvío estándar poblacional.\n\n\n\n\nAunque la verdadera desviación estándar poblacional es 15, el promedio de los desvíos estándar muestrales es solo 8.5. Notá que esto es muy distinto de lo que pasaba cuando graficábamos distribuciones muestrales de la media: esas siempre estaban centradas en la media poblacional.\nAhora ampliemos la simulación. En lugar de restringirnos a un tamaño muestral de\\(N=2\\), repitamos el experimento para tamaños muestrales del 1 al 10. Si graficamos el promedio de la media muestral y el promedio del desvío estándar muestral como función del tamaño de la muestra, obtenemos los siguientes resultados.\nLa Figura 5.22 muestra cómo cambia la media muestral según el tamaño de la muestra. Notá que la línea es plana: la media muestral no sobrestima ni subestima la media poblacional. Es un estimador insesgado.\n\nknitr::include_graphics(\"imgs/navarro_img/estimation/biasMean_es.png\")\n\n\n\n\nFigura 5.22: Una ilustración del hecho de que la media muestral es un estimador insesgado de la media poblacional.\n\n\n\n\nLa Figura 5.23 muestra cómo cambia el desvío estándar muestral con el tamaño de muestra. Notá que no es una línea plana: el desvío estándar muestral subestima sistemáticamente la desviación estándar poblacional.\n\nknitr::include_graphics(\"imgs/navarro_img/estimation/biasSD_es.png\")\n\n\n\n\nFigura 5.23: Una ilustración del hecho de que el desvío estándar muestral es un estimador sesgado de la desviación estándar poblacional.\n\n\n\n\nEn otras palabras, si queremos hacer una “mejor aproximación” (\\(\\hat\\sigma\\), nuestra estimación de la desviación estándar poblacional) sobre el valor real de la desviación estándar de la población\\(\\sigma\\), deberíamos asegurarnos de que nuestra aproximación sea un poquito más grande que la desviación estándar muestral\\(s\\).\nLa solución a este sesgo sistemático resulta ser bastante simple. Así es como funciona. Antes de meternos con la desviación estándar, vamos a mirar la varianza. Si recordás el segundo capítulo, la varianza muestral se define como el promedio de los cuadrados de las desviaciones respecto de la media muestral. Es decir:\n\\[\ns^2 = \\frac{1}{N} \\sum_{i=1}^N (X_i - \\bar{X})^2\n\\]\nLa varianza muestral\\(s^2\\)es un estimador sesgado de la varianza poblacional\\(\\sigma^2\\). Pero resulta que solo necesitamos hacer un pequeño ajuste para transformarla en un estimador insesgado. Todo lo que hay que hacer es dividir por\\(N-1\\)en lugar de por\\(N\\). Si hacemos eso, obtenemos la siguiente fórmula:\n\\[\n\\hat\\sigma^2 = \\frac{1}{N-1} \\sum_{i=1}^N (X_i - \\bar{X})^2\n\\]\nEsto sí es un estimador insesgado de la varianza poblacional\\(\\sigma^2\\).\nUna historia similar se aplica al desvío estándar. Si dividimos por\\(N-1\\)en lugar de por\\(N\\), nuestra estimación del desvío estándar poblacional se convierte en:\n\\[\n\\hat\\sigma = \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^N (X_i - \\bar{X})^2}\n\\]\nVale la pena señalar que los programas de software toman decisiones por vos sobre qué tipo de varianza y desvío estándar estás calculando. Algunos programas dividen automáticamente por\\(N-1\\), otros no. Necesitás verificar qué están haciendo. No dejes que el software decida por vos. El software está para que vos le digas qué hacer.\nUn último punto: en la práctica, muchas personas se refieren a\\(\\hat{\\sigma}\\)(la fórmula con\\(N-1\\)) como el desvío estándar muestral. Técnicamente, esto es incorrecto: el desvío estándar muestral, como propiedad de la muestra, debería ser igual a\\(s\\)(la fórmula con\\(N\\)). No son la misma cosa, ni conceptualmente ni numéricamente. Una es una propiedad de la muestra; la otra, una estimación del parámetro poblacional. Sin embargo, en casi todas las aplicaciones reales, lo que realmente nos importa es la estimación del parámetro poblacional. Por eso, la gente casi siempre reporta\\(\\hat{\\sigma}\\)en lugar de\\(s\\).\n\n\n\n\n\n\nNota\n\n\n\nOjo, si hay que dividir por 𝑁 o por 𝑁−1 también depende de cómo pensás lo que estás haciendo. Por ejemplo, si no creés que lo que estás haciendo es estimar un parámetro poblacional, entonces, ¿por qué dividirías por N−1? Además, cuando 𝑁 es grande, no importa tanto. La diferencia entre un 𝑁 grande y un 𝑁−1 grande es… simplemente 1.\n\n\nEste es el número correcto que hay que reportar, claro. Pero la gente tiende a ser un poco imprecisa con la terminología cuando lo escribe, porque “desvío estándar muestral” es más corto que “estimación del desvío estándar poblacional”. No es gran cosa, y en la práctica yo hago lo mismo que todos los demás. Sin embargo, creo que es importante mantener los dos conceptos separados: nunca es buena idea confundir “propiedades conocidas de tu muestra” con “conjeturas sobre la población de la que proviene”. En el momento en que empezás a pensar que\\(s\\)y\\(\\hat{\\sigma}\\)son lo mismo, empezás a hacer justamente eso.\nPara cerrar esta sección, acá tenés un par de tablas más para ayudarte a mantener todo claro:\n\n\n\n\n\n\n\n\nSímbolo\n¿Qué es?\n¿Lo conocemos?\n\n\n\n\n\\(s^2\\)\nVarianza muestral\nSí, se calcula a partir de los datos crudos\n\n\n\\(\\sigma^2\\)\nVarianza poblacional\nCasi nunca se conoce con certeza\n\n\n\\(\\hat{\\sigma}^2\\)\nEstimación de la varianza poblacional\nSí, pero no es lo mismo que la varianza muestral"
  },
  {
    "objectID": "04-SamplesPopulations.html#estimación-de-un-intervalo-de-confianza",
    "href": "04-SamplesPopulations.html#estimación-de-un-intervalo-de-confianza",
    "title": "5  Probabilidad, muestreo y estimación",
    "section": "5.13 Estimación de un intervalo de confianza",
    "text": "5.13 Estimación de un intervalo de confianza\n\nLa estadística es no tener que decir nunca que estás seguro — Autor desconocido\n\nHasta este punto del capítulo, describimos los fundamentos de la teoría del muestreo en los que se apoyan los estadísticos para hacer conjeturas sobre los parámetros poblacionales a partir de una muestra de datos. Como muestra esta discusión, una de las razones por las que necesitamos toda esta teoría es que todo conjunto de datos deja un cierto grado de incertidumbre, así que nuestras estimaciones nunca van a ser perfectamente precisas. Lo que falta en esta discusión hasta ahora es un intento de cuantificar cuánto es esa incertidumbre. No alcanza con poder decir que el CI promedio de estudiantes de psicología es 115 (sí, me acabo de inventar ese número). También queremos poder decir qué tan seguros estamos de esa estimación. Por ejemplo, estaría bueno poder decir que hay un 95% de probabilidad de que el valor real esté entre 109 y 121. A eso se le llama un intervalo de confianza para la media.\nArmados con lo que ya sabemos sobre distribuciones muestrales, construir un intervalo de confianza para la media es bastante fácil. Así es como funciona: Supongamos que la verdadera media poblacional es\\(\\mu\\)y la desviación estándar poblacional es\\(\\sigma\\). Acabo de terminar mi estudio con\\(N\\)participantes, y la media muestral de CI fue\\(\\bar{X}\\). Sabemos, gracias al teorema central del límite, que la distribución muestral de la media es aproximadamente normal. También sabemos, por lo que aprendimos sobre la distribución normal, que hay un 95% de probabilidad de que un valor normalmente distribuido caiga dentro de dos desviaciones estándar de la media verdadera. Para ser más precisos, podemos usar la función qnorm() para calcular los percentiles 2.5 y 97.5 de la distribución normal:\n\nqnorm( p = c(.025, .975) )\n\n[1] -1.959964  1.959964\n\n\nOk, así que te mentí un poco antes: lo más correcto es decir que hay un 95% de probabilidad de que una cantidad normalmente distribuida caiga dentro de 1.96 desviaciones estándar de la media verdadera.\nAhora recordá que la desviación estándar de la distribución muestral se llama error estándar, y que el error estándar de la media se denota como SEM. Cuando juntamos todas estas piezas, aprendemos que hay un 95% de probabilidad de que la media muestral\\(\\bar{X}\\)que observamos esté a no más de 1.96 errores estándar de la media poblacional. Uf, eso fue mucha matemática junta. Tranquilo, ahora lo aclaramos.\nEn notación matemática, esto se escribe así:\n\\[\n\\mu - \\left(1.96 \\times \\text{SEM} \\right) \\ \\leq\\ \\bar{X} \\ \\leq\\ \\mu + \\left(1.96 \\times \\text{SEM} \\right)\n\\]donde el SEM es igual a\\(\\sigma / \\sqrt{N}\\), y podemos estar un 95% seguros de que esto es cierto.\nSin embargo, esto no responde la pregunta que realmente nos interesa. La ecuación anterior nos dice qué deberíamos esperar sobre la media muestral, dado que conocemos los parámetros poblacionales. Pero lo que queremos es que funcione al revés: queremos saber qué deberíamos creer sobre los parámetros poblacionales, dado que observamos una muestra particular. Sin embargo, no es muy difícil lograrlo. Con un poco de álgebra de secundaria, podemos reescribir nuestra ecuación así:\n\\[\n\\bar{X} - \\left( 1.96 \\times \\text{SEM} \\right) \\leq \\mu \\leq \\bar{X} + \\left( 1.96 \\times \\text{SEM} \\right)\n\\]\nEsto nos dice que el rango de valores tiene una probabilidad del 95% de contener la media poblacional\\(\\mu\\). A este rango lo llamamos intervalo de confianza del 95%, denotado como\\(\\text{CI}\\_{95}\\).\nEn resumen, siempre que\\(N\\)sea lo suficientemente grande —lo suficientemente grande como para que la distribución muestral de la media se parezca a una normal— podemos usar la siguiente fórmula para el intervalo de confianza del 95%:\n\\[\n\\text{CI}_{95} = \\bar{X} \\pm \\left( 1.96 \\times \\frac{\\sigma}{\\sqrt{N}} \\right)\n\\]\nPor supuesto, no hay nada especial en el número 1.96: simplemente es el multiplicador que necesitás usar si querés un intervalo de confianza del 95%. Si quisiera un intervalo de confianza del 70%, podría usar la función qnorm() para calcular los cuantiles 15 y 85:\n\nqnorm( p = c(.15, .85) )\n\n[1] -1.036433  1.036433\n\n\nAsí que la fórmula para el\\(\\text{CI}*{70}\\)sería igual a la del\\(\\text{CI}*{95}\\), excepto que usaríamos 1.04 como nuestro número mágico, en lugar de 1.96.\n\n5.13.1 Un pequeño error en la fórmula\nComo de costumbre, te mentí un poco. La fórmula que te di más arriba para el intervalo de confianza del 95% es aproximadamente correcta, pero me salté un detalle importante en la explicación. Fijate que mi fórmula requiere que uses el error estándar de la media (SEM), que a su vez necesita que conozcas la desviación estándar real de la población,\\(\\sigma\\).\nSin embargo, ya dijimos antes que en general no conocemos los parámetros poblacionales verdaderos. Y como no conocemos el valor real de\\(\\sigma\\), tenemos que usar una estimación del desvío estándar poblacional\\(\\hat{\\sigma}\\). Eso no es difícil de hacer, pero tiene una consecuencia: tenemos que usar los cuantiles de la distribución\\(t\\)en lugar de la distribución normal para calcular nuestro número mágico; y ese número depende del tamaño muestral. Además, todavía no hablamos de la distribución\\(t\\)en este capítulo.\nCuando usamos la distribución\\(t\\)en lugar de la distribución normal, obtenemos números más grandes, lo que indica que tenemos más incertidumbre. ¿Y por qué tenemos esa incertidumbre extra? Bueno, porque nuestra estimación de la desviación estándar poblacional,\\(\\hat\\sigma\\), puede estar equivocada. Y si está equivocada, eso significa que estamos menos seguros de cómo es realmente la distribución muestral de la media… y esa incertidumbre termina reflejándose en un intervalo de confianza más amplio."
  },
  {
    "objectID": "04-SamplesPopulations.html#resumen",
    "href": "04-SamplesPopulations.html#resumen",
    "title": "5  Probabilidad, muestreo y estimación",
    "section": "5.14 Resumen",
    "text": "5.14 Resumen\nEn este capítulo cubrimos dos grandes temas. La primera mitad trató sobre la teoría del muestreo, y la segunda mitad sobre cómo podemos usar esa teoría para construir estimaciones de parámetros poblacionales. La estructura fue más o menos así:\n\nIdeas básicas sobre muestras, muestreo y poblaciones\nTeoría estadística del muestreo: ley de los grandes números, distribuciones muestrales y el teorema central del límite\nEstimación de medias y desviaciones estándar\nIntervalos de confianza\n\nComo siempre, hay un montón de temas relacionados con el muestreo y la estimación que no se cubren en este capítulo, pero para una materia introductoria de psicología, creo que esto es bastante completo. Para la mayoría de quienes hacen investigación aplicada, no vas a necesitar mucha más teoría que esta. Una gran pregunta que no toqué en este capítulo es: ¿qué hacés cuando no tenés una muestra aleatoria simple? Hay muchísima teoría estadística que se puede usar para manejar esa situación, pero queda muy por fuera del alcance de este libro."
  },
  {
    "objectID": "01-Science_Data.html#hay-más-en-los-métodos-de-investigación-que-solo-la-estadística",
    "href": "01-Science_Data.html#hay-más-en-los-métodos-de-investigación-que-solo-la-estadística",
    "title": "1  ¿Por qué estadística?",
    "section": "1.5 Hay más en los métodos de investigación que solo la estadística",
    "text": "1.5 Hay más en los métodos de investigación que solo la estadística\nHasta ahora, la mayoría de lo que dije fue sobre estadística, así que no te culpo si pensás que es lo único que me importa en la vida. Para ser justos, no estarías tan equivocado, pero la metodología de investigación es un concepto más amplio. Por eso, los cursos de métodos suelen cubrir un montón de temas más ligados a los aspectos prácticos del diseño de investigaciones, especialmente cuando trabajás con personas. Sin embargo, el 99% de los miedos estudiantiles tienen que ver con la parte estadística del curso, así que me enfoqué en eso, y espero haberte convencido de que la estadística importa, y más importante aún, que no hay que tenerle miedo. Dicho eso, es bastante típico que los cursos introductorios de métodos estén muy cargados de estadística. Y no es (generalmente) porque los docentes sean gente malvada. Al contrario. Los cursos introductorios se enfocan tanto en estadística porque la vas a necesitar antes que el resto del entrenamiento metodológico. ¿Por qué? Porque en casi todas tus otras materias, los trabajos prácticos van a requerir análisis estadístico, mucho más que cualquier otra herramienta de metodología. No es común que te pidan diseñar un estudio desde cero (en cuyo caso sí necesitarías saber mucho sobre diseño de investigación), pero sí es común que te pidan analizar e interpretar datos recolectados por otra persona (en cuyo caso necesitás estadística). En ese sentido, si pensás en lo que más te va a servir para rendir bien en tus materias, la estadística es lo más urgente.\nPero tené en cuenta que “urgente” es diferente de “importante”; ambos son relevantes. De verdad quiero insistir en que el diseño de investigación es tan importante como el análisis de datos, y este libro le dedica bastante espacio. Aun así, mientras que la estadística tiene un carácter más universal y te da un conjunto de herramientas útiles para casi cualquier investigación en psicología, el diseño de investigación no es tan generalizable. Hay principios básicos que todos deberían tener en cuenta, pero muchos aspectos del diseño son muy específicos del área en la que trabajás. Y como lo que más importa está en los detalles, esos detalles casi nunca aparecen en un curso introductorio de estadística y métodos.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>¿Por qué estadística?</span>"
    ]
  },
  {
    "objectID": "05-Foundation_Inference.html",
    "href": "05-Foundation_Inference.html",
    "title": "6  Fundamentos de la inferencia",
    "section": "",
    "text": "6.1 Breve repaso de los experimentos\nHasta ahora hemos estado hablando sobre cómo describir datos y buscar posibles relaciones entre las cosas que medimos. Comenzamos con el problema de tener demasiados números y discutimos cómo podían resumirse con estadísticas descriptivas y comunicarse a través de gráficos. También analizamos la idea de relaciones entre cosas. Si una cosa causa un cambio en otra, entonces, si medimos cómo una varía, deberíamos ver que la otra también varía, o que cambia de forma sistemática como consecuencia de la primera. Al final del capítulo sobre correlación, mostramos cómo las correlaciones —que implican una relación entre dos cosas— son muy difíciles de interpretar. ¿Por qué? Porque una correlación observada puede ser causada por una tercera variable oculta, o puede ser un hallazgo espurio “causado” por el azar.\nAhora comenzamos nuestro recorrido por la estadística inferencial. Estas son herramientas que se utilizan para hacer inferencias sobre de dónde provienen nuestros datos, y para hacer inferencias sobre qué causa qué.\nEn este capítulo vamos a presentar algunas ideas fundamentales. Nos mantendremos mayormente en un nivel conceptual, y usaremos muchas simulaciones, como hicimos en los capítulos anteriores. En los capítulos siguientes formalizaremos las intuiciones desarrolladas acá para explicar cómo funcionan algunas estadísticas inferenciales comunes.\nEn el capítulo uno hablamos sobre métodos de investigación y experimentos. Los experimentos son una manera estructurada de recolectar datos que puede permitir inferencias sobre causalidad. Si quisiéramos saber si algo como ver videos de gatos en YouTube aumenta la felicidad, necesitaríamos un experimento. Ya vimos que simplemente buscar un montón de personas, medir cuántas horas ven videos de gatos y su nivel de felicidad, y correlacionar ambas cosas no permite inferencias causales. Por ejemplo, el flujo causal podría estar invertido: tal vez ser feliz causa que la gente quiera mirar más videos de gatos. Necesitamos un experimento.\nUn experimento tiene dos partes: una manipulación y una medición. La manipulación está bajo el control de quien realiza el experimento. Las manipulaciones también se llaman variables independientes. Por ejemplo, podríamos manipular el tiempo dedicado a mirar videos de gatos: 1 hora versus 2 horas. La medición es el dato que se recolecta. Podríamos medir cuán feliz se siente una persona después de ver videos de gatos, usando una escala del 1 al 100. Las mediciones también se llaman variables dependientes. Entonces, en un experimento básico como el que estamos considerando, recogemos medidas de felicidad de personas asignadas a una de dos condiciones experimentales definidas por la variable independiente. Supongamos que realizamos el experimento con 50 participantes. A 25 personas se las asigna al azar para ver 1 hora de videos de gatos, y a las otras 25 se las asigna al azar para ver 2 horas. Luego, medimos cuán felices se sienten al final del experimento.\n¿Qué deberíamos mirar en los datos? Si mirar videos de gatos causara un cambio en la felicidad, entonces esperaríamos que las medidas de felicidad de las personas que vieron 1 hora de videos de gatos fueran diferentes de las medidas de aquellas que vieron 2 horas. Si mirar videos de gatos no produjera ningún cambio en la felicidad, entonces no deberíamos encontrar diferencias en las medidas de felicidad entre las dos condiciones. Las fuerzas causales provocan cambios, y el experimento está diseñado para detectar ese cambio.\nAhora podemos plantear una gran pregunta general: ¿cómo sabemos si los datos cambiaron entre condiciones? Si podemos estar razonablemente seguros de que hubo un cambio entre condiciones, entonces podemos inferir que nuestra manipulación causó un cambio en la medición. Si no podemos estar seguros de que hubo un cambio, entonces no podemos inferir que la manipulación haya causado una diferencia. Necesitamos construir herramientas que nos permitan detectar cambios, para poder reconocer un cambio cuando ocurra.\n“Pará un poco, si solo estamos buscando un cambio, ¿no sería fácil ver eso mirando los números y viendo si son diferentes? ¿Qué tiene de difícil eso?” Buena pregunta. Ahora necesitamos hacer un desvío. La respuesta breve es que siempre va a haber cambio en los datos (recordá la varianza).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Fundamentos de la inferencia</span>"
    ]
  },
  {
    "objectID": "05-Foundation_Inference.html#los-datos-provienen-de-una-distribución",
    "href": "05-Foundation_Inference.html#los-datos-provienen-de-una-distribución",
    "title": "6  Fundamentos de la inferencia",
    "section": "6.2 Los datos provienen de una distribución",
    "text": "6.2 Los datos provienen de una distribución\nEn el capítulo anterior hablamos sobre muestras y distribuciones, y sobre la idea de que podés tomar muestras desde una distribución. Así que, de ahora en más, cada vez que veas un conjunto de números deberías preguntarte: “¿De dónde vienen estos números?”. ¿Qué causó que algunos tipos de números aparecieran más que otros? La respuesta a esta pregunta nos lleva de nuevo al mundo abstracto de las distribuciones.\nUna distribución es un lugar de donde pueden salir los números. La distribución establece las restricciones: determina qué números es probable que ocurran y cuáles no. Las distribuciones son ideas abstractas, pero pueden hacerse concretas, y podemos representarlas con dibujos —como ya has visto antes— llamados histogramas.\nLo que viene a continuación puede parecer un poco repetitivo con respecto al capítulo anterior. Volvemos a mirar muestreo de números desde una distribución uniforme. Mostramos que las muestras individuales pueden lucir bastante diferentes entre sí. Gran parte del comienzo de este capítulo ya te va a resultar familiar, pero llevamos los conceptos en una dirección un poco distinta: la dirección es cómo hacer inferencias sobre el papel del azar en tu experimento.\n\n6.2.1 Distribución uniforme\nComo recordatorio del capítulo anterior, la Figura 6.1 muestra que la forma de una distribución uniforme es completamente plana.\n\n\n\n\n\nFigura 6.1: Distribución uniforme que muestra que los números del 1 al 10 tienen la misma probabilidad de ser muestreados\n\n\n\n\nOK, eso no parece gran cosa. ¿Qué está pasando acá? El eje y está etiquetado como Probabilidad, y va de 0 a 1. El eje x está etiquetado como Número, y va del 1 al 10. Hay una línea horizontal atravesando el gráfico. Esa línea indica la probabilidad de cada número del 1 al 10. Notá que la línea es plana. Esto significa que todos los números tienen la misma probabilidad de ocurrir. Más específicamente, hay 10 números del 1 al 10 (1,2,3,4,5,6,7,8,9,10), y todos tienen la misma chance de aparecer. 1 dividido 10 es 0,1, que es la probabilidad indicada por la línea horizontal.\n“¿Y qué con eso?” Imaginá que esta distribución uniforme es una máquina generadora de números. Escupe números, pero escupe cada número con la probabilidad indicada por la línea. Si esta distribución comenzara a generar números, escupiría un 10% de unos, un 10% de doses, un 10% de treses, y así sucesivamente, hasta un 10% de dieces. ¿Querés ver cómo se vería eso? Hagamos que escupa 100 números y pongámoslos en la Tabla 6.1.\n\n\n\n\nTabla 6.1: 100 números muestreados aleatoriamente de una distribución uniforme.\n\n\n3\n10\n1\n7\n9\n3\n1\n7\n7\n9\n\n\n7\n1\n2\n4\n3\n9\n7\n4\n1\n6\n\n\n4\n10\n3\n5\n1\n7\n1\n5\n9\n1\n\n\n8\n8\n1\n2\n5\n3\n2\n1\n2\n4\n\n\n10\n8\n9\n4\n6\n3\n10\n6\n7\n5\n\n\n3\n5\n1\n2\n8\n2\n9\n10\n5\n5\n\n\n3\n7\n9\n1\n3\n7\n10\n4\n7\n2\n\n\n4\n8\n4\n6\n9\n8\n2\n5\n2\n7\n\n\n1\n7\n6\n4\n9\n5\n8\n6\n8\n3\n\n\n2\n2\n10\n9\n3\n8\n6\n9\n5\n2\n\n\n\n\n\n\nUsamos la distribución uniforme para generar estos números. Formalmente, a esto lo llamamos muestreo desde una distribución. Muestrear es lo que hacés en el supermercado cuando hay comida para degustar. Podés seguir tomando más. Sin embargo, si tomás todas las muestras, lo que tenés se llama población. Vamos a hablar más sobre muestras y poblaciones más adelante. Como usamos la distribución uniforme para generar los números, ya sabemos de dónde vienen. Pero por ahora, podemos fingir que alguien aparece en tu puerta, te muestra estos números, y vos te preguntás de dónde salieron. ¿Podés decir, solo mirando los números, que provienen de una distribución uniforme?\n¿Qué deberías observar? Quizás quieras saber si todos los números ocurren con aproximadamente la misma frecuencia —después de todo, eso es lo que debería pasar, ¿no? Es decir, si cada número tiene la misma chance de ocurrir, deberíamos ver que cada número ocurre más o menos la misma cantidad de veces. Ya sabemos qué es un histograma, así que podemos poner nuestra muestra de 100 números en un histograma y ver cómo se distribuyen los conteos. Si todos los números del 1 al 10 aparecen con igual frecuencia, entonces cada número individual debería aparecer unas 10 veces. La Figura 6.2 muestra el histograma:\n\n\n\n\n\nFigura 6.2: Histograma de 100 números muestreados aleatoriamente de una distribución uniforme que contiene los enteros del 1 al 10\n\n\n\n\nUy… como podés ver, no todos los números ocurrieron exactamente 10 veces. Las barras no tienen toda la misma altura. Esto muestra que el muestreo aleatorio de números desde esta distribución no garantiza que los números muestrados se vean exactamente como la distribución original. A esto lo llamamos error muestral, o variabilidad del muestreo.\n\n\n6.2.2 No todas las muestras son iguales — de hecho, suelen ser bastante distintas\nVeamos más de cerca el error muestral. Vamos a tomar una muestra de 20 números de la distribución uniforme. Deberíamos esperar que cada número entre 1 y 10 ocurra unas dos veces. Como antes, esta expectativa puede visualizarse con un histograma.\nPara tener una mejor idea de la variabilidad muestral, repitamos ese proceso diez veces. La Figura 6.3 muestra 10 histogramas, cada uno representando una muestra distinta de 20 números:\n\n\n\n\n\nFigura 6.3: Histogramas de 10 muestras distintas de la distribución uniforme. Cada muestra contiene 20 números. Todos los histogramas se ven bastante distintos. Las diferencias entre las muestras se deben al error muestral o al azar.\n\n\n\n\nProbablemente notes enseguida que ninguno de los histogramas es igual. Aunque estamos tomando aleatoriamente 20 números de la misma distribución uniforme, cada muestra de 20 números resulta diferente. Esto es variabilidad muestral, o error de muestreo.\nLa Figura 6.4 muestra una versión animada del proceso de elegir repetidamente 20 números aleatorios nuevos y graficar un histograma. La línea horizontal muestra la forma plana de la distribución uniforme. La línea cruza el eje y en el valor 2; y esperamos que cada número (del 1 al 10) ocurra unas 2 veces en una muestra de 20. Sin embargo, cada muestra varía bastante, debido al azar.\n\n\n\n\n\nFigura 6.4: Animación de histogramas para diferentes muestras de 20 valores de una distribución uniforme (números del 1 al 10). Las líneas negras muestran la cantidad esperada de veces que debería aparecer cada número. El hecho de que no todos los números aparezcan exactamente 2 veces ilustra el error asociado al muestreo.\n\n\n\n\n:::\nObservar los histogramas anteriores nos muestra que puede ser difícil averiguar de dónde vinieron nuestros números. En el mundo real, nuestras mediciones son muestras. Por lo general, solo tenemos el lujo de obtener una única muestra de mediciones, en lugar de repetirlas 10 veces o más. Si mirás los histogramas, verás que algunos de ellos parecen haber salido de una distribución uniforme: la mayoría de las barras están cerca de 2, y todas más o menos alineadas. Pero si te tocara otra muestra distinta, podrías ver un histograma muy irregular, con algunos números ocurriendo mucho más que otros. Eso podría hacerte pensar que esos números no provienen de una distribución uniforme (parecen demasiado irregulares). Pero recordá: todas estas muestras vinieron de una distribución uniforme. Así es como se ven las muestras tomadas de esa distribución. Esto es lo que el azar le hace a las muestras: introduce ruido en los datos individuales.\n\n\n6.2.3 Las muestras grandes se parecen más a la distribución de la que provienen\nVolvamos a la pregunta: ¿cuál de las dos muestras en Figura 6.5 te parece que vino de una distribución uniforme?\n\n\n\n\n\nFigura 6.5: ¿Cuál de estas dos muestras proviene de una distribución uniforme?\n\n\n\n\nLa respuesta es que ambas vinieron de una distribución uniforme. Pero ninguna parece que lo haya hecho.\n¿Podemos mejorar las cosas y facilitar ver si una muestra proviene de una distribución uniforme? Sí, podemos. Lo único que necesitamos hacer es aumentar el tamaño de la muestra. A menudo usamos la letra n para referirnos al tamaño muestral. n es el número de observaciones en la muestra.\nAsí que aumentemos el número de observaciones en cada muestra, de 20 a 100. Vamos a crear 10 muestras (cada una con 100 observaciones), y a graficar un histograma para cada una. Todas estas muestras se extraen de la misma distribución uniforme. Esto significa que deberíamos esperar que cada número del 1 al 10 ocurra unas 10 veces en cada muestra. Los histogramas están en la Figura 6.6.\n\n\n\n\n\nFigura 6.6: Histogramas de diferentes muestras de una distribución uniforme. N = 100 para cada muestra.\n\n\n\n\nLa Figura 6.7 muestra que los histogramas empiezan a aplanarse. Las barras todavía no están perfectamente en 100, porque sigue habiendo error muestral (siempre lo habrá). Pero si vieras un histograma que se ve plano y supieras que la muestra contiene muchas observaciones, podrías tener más confianza en que esos números provienen de una distribución uniforme.\n\n\n\n\n\nFigura 6.7: Histogramas de diferentes muestras de una distribución uniforme. N = 100 para cada muestra.\n\n\n\n\nSolo por diversión, hagamos muestras realmente grandes. Digamos, 100.000 observaciones por muestra. En ese caso, deberíamos esperar que cada número ocurra unas 10.000 veces. ¿Qué pasa?\n\n\n\n\n\nFigura 6.8: Histogramas de distintas muestras de una distribución uniforme. N = 100.000 para cada muestra.\n\n\n\n\nLa Figura 6.8 muestra que los histogramas de cada muestra empiezan a parecerse entre sí. Todas tienen 100.000 observaciones, y eso le da suficiente oportunidad al azar para distribuir los números de manera más equitativa, asegurándose aproximadamente de que todos ocurran más o menos la misma cantidad de veces. Como podés ver, las barras están todas muy cerca de 10.000, que es donde deberían estar si la muestra realmente proviene de una distribución uniforme. De nuevo, no todas las barras tienen exactamente el mismo alto —todavía hay error muestral—, pero el patrón general es ahora mucho más claro. Cada número ocurre más o menos la misma cantidad de veces, y las diferencias entre muestras son mucho menores.\n\n\n\n\n\n\nConsejo pro\n\n\n\nEl patrón que muestra una muestra tiende a estabilizarse a medida que aumenta el tamaño muestral. Las muestras pequeñas pueden mostrar todo tipo de patrones, debido al error muestral (el azar).\n\n\nAntes de volver al tema de los experimentos con el que comenzamos, hagamos dos preguntas más. Primero: ¿cuál de las dos muestras en Figura 6.9 creés que proviene de una distribución uniforme? Dato: cada una de estas muestras tiene 20 observaciones.\n\n\n\n\n\nFigura 6.9: ¿Cuál de estas muestras proviene de una distribución uniforme?\n\n\n\n\nSi no estás muy segurx de la respuesta, es porque el error muestral (el azar) está “difuminando” los histogramas.\nAcá va exactamente la misma pregunta, solo que esta vez tomamos 1.000 observaciones por muestra. ¿Cuál histograma en Figura 6.10 creés que proviene de una distribución uniforme, y cuál no?\n\n\n\n\n\nFigura 6.10: ¿Cuál de estas muestras proviene de una distribución uniforme?\n\n\n\n\nAhora que aumentamos N, podemos ver que el patrón en cada muestra se vuelve más evidente. El histograma de la muestra 1 tiene barras cercanas a 100; no son perfectamente planas, pero se parece a una distribución uniforme. El histograma de la muestra 2 no es plano en absoluto.\n¡Felicitaciones! Acabamos de hacer inferencias estadísticas sin usar fórmulas.\n“¿Lo hicimos?” Sí: al mirar nuestras dos muestras, inferimos que la muestra 2 no provino de una distribución uniforme. También inferimos que la muestra 1 podría haber venido de una distribución uniforme. Fantástico. Este tipo de inferencias son las mismas que vamos a hacer durante el resto del curso.\nVamos a mirar algunos números, preguntarnos de dónde salieron, y luego organizarlos de forma tal que podamos hacer inferencias sobre el tipo de distribución de la que provinieron. Eso es todo."
  },
  {
    "objectID": "05-Foundation_Inference.html#hay-una-diferencia",
    "href": "05-Foundation_Inference.html#hay-una-diferencia",
    "title": "6  Fundamentos de la inferencia",
    "section": "6.3 ¿Hay una diferencia?",
    "text": "6.3 ¿Hay una diferencia?\nVolvamos a los experimentos. En un experimento, queremos saber si una variable independiente (nuestra manipulación) causa un cambio en una variable dependiente (la medición). Si esto ocurre, entonces esperamos ver alguna diferencia en nuestra medición como función de la manipulación.\nConsiderá el ejemplo del interruptor de luz:\nExperimento del interruptor de luz: manipulás el interruptor hacia arriba (condición 1 de la variable independiente), la luz se enciende (medición). Manipulás el interruptor hacia abajo (condición 2 de la variable independiente), la luz se apaga (otra medición). La medición (la luz) cambia (se enciende o se apaga) como función de la manipulación (mover el interruptor hacia arriba o hacia abajo).\nPodés ver el cambio en la medición entre condiciones, es tan obvio como el día y la noche. Entonces, cuando realizás una manipulación y ves la diferencia (el cambio) en tu medición, podés estar bastante seguro de que tu manipulación está causando el cambio.\n\nNota: para ser cautos, podemos decir que “algo” relacionado con tu manipulación está causando el cambio; tal vez no sea exactamente lo que pensás que es, si tu manipulación es muy complicada e involucra muchos elementos en juego.\n\n\n6.3.1 El azar puede producir diferencias\n¿Creés que el azar puede producir la apariencia de diferencias, incluso cuando en realidad no hay ninguna? Espero que sí. Ya mostramos que el proceso de muestreo de números desde una distribución es algo azaroso que genera muestras distintas. Las muestras diferentes son diferentes entre sí, así que sí: el azar puede producir diferencias. Y eso puede complicar la interpretación de los experimentos.\nHagamos un experimento ficticio en el que no esperamos encontrar diferencias, porque vamos a manipular algo que no debería generar ningún efecto. Este es el planteo:\nVos sos el experimentador y estás parado frente a una máquina expendedora de bolitas. Es muy grande, tiene miles de bolitas. El 50% son verdes y el 50% son rojas. Querés averiguar si sacar bolitas con la mano derecha o con la izquierda te hace sacar más bolitas verdes. Además, vas a estar con los ojos vendados todo el tiempo. La variable independiente es mano: derecha vs. izquierda. La variable dependiente es el color de cada bolita que sacás.\nRealizás el experimento así:\n\nte colocás una venda en los ojos.\nsacás 10 bolitas al azar con la mano izquierda y las dejás a un lado.\nsacás 10 bolitas al azar con la mano derecha y las dejás a otro lado.\ncontás cuántas bolitas verdes y cuántas rojas sacaste con cada mano.\n\nEsperamos que estés de acuerdo en que tus manos no pueden distinguir entre bolitas verdes y rojas. Si no estás de acuerdo, vamos a estipular además que las bolitas son completamente idénticas en todo, salvo en el color, así que sería imposible diferenciarlas usando el tacto. Entonces, ¿qué debería pasar en este experimento?\n—“Eh… quizás saques 5 bolitas rojas y 5 verdes con la mano izquierda, y lo mismo con la derecha?” Más o menos, eso es lo que usualmente pasaría. Pero no es lo único que podría pasar. Acá tenés algunos datos de un experimento ficticio:\n\n\n\n\n\nmano\nbolita\n\n\n\n\nizquierda\n0\n\n\nizquierda\n1\n\n\nizquierda\n0\n\n\nizquierda\n0\n\n\nizquierda\n0\n\n\nizquierda\n0\n\n\nizquierda\n0\n\n\nizquierda\n1\n\n\nizquierda\n1\n\n\nizquierda\n1\n\n\nderecha\n0\n\n\nderecha\n0\n\n\nderecha\n0\n\n\nderecha\n1\n\n\nderecha\n1\n\n\nderecha\n0\n\n\nderecha\n1\n\n\nderecha\n1\n\n\nderecha\n1\n\n\nderecha\n1\n\n\n\n\n\n—“¿Qué estoy viendo acá?” Esta es una tabla en formato largo. Cada fila es una bolita. La primera columna indica con qué mano se sacó. La segunda indica qué tipo de bolita es. Vamos a decir que los valores 1 representan bolitas verdes y los 0 representan bolitas rojas.\nEntonces… ¿la mano izquierda te hizo sacar más bolitas verdes que la mano derecha? Sería más fácil mirar los datos usando un gráfico de barras (Figura 6.11). Para simplificar, vamos a contar solo las bolitas verdes (las otras necesariamente son rojas). Así que solo necesitamos sumar los unos; los ceros no suman nada.\n\n\n\n\n\nFigura 6.11: Conteo de bolitas verdes seleccionadas al azar por cada mano.\n\n\n\n\n“¡Ah mirá! Las barras no son iguales. Una mano sacó más bolitas verdes que la otra. ¿Eso significa que una de tus manos secretamente sabe cómo encontrar bolitas verdes?” No. Es solo otro caso de error muestral, eso que también llamamos suerte o azar. La diferencia acá fue causada por el azar, no por la manipulación (es decir, por qué mano usaste).\n\n¡Alerta de un problema importante para la inferencia! Hacemos experimentos para buscar diferencias, de modo que podamos inferir si nuestras manipulaciones causan cambios en nuestras mediciones. Sin embargo, este ejemplo demuestra que podemos encontrar diferencias solo por azar. Entonces, ¿cómo sabemos si una diferencia es real o simplemente fue causada por el azar?\n\n\n\n6.3.2 Las diferencias causadas por el azar se pueden simular\nRecordá cuando mostramos que el azar puede producir correlaciones. También mostramos que el azar tiene limitaciones en su capacidad para producir correlaciones. Por ejemplo, el azar genera con más frecuencia correlaciones débiles que fuertes. ¿Te acordás de la “ventana del azar”? Antes descubrimos que las correlaciones que caen fuera de esa ventana son muy poco probables.\nPodemos hacer lo mismo con las diferencias. Vamos a investigar exactamente qué puede hacer el azar en nuestro experimento. Una vez que sepamos de lo que el azar es capaz, vamos a estar en una mejor posición para juzgar si nuestra manipulación causó una diferencia, o si simplemente podría haber sido el azar.\nLo primero que vamos a hacer es simular que realizás el experimento de las bolitas 10 veces seguidas. Eso va a producir 10 conjuntos distintos de resultados. La Figura 6.12 muestra gráficos de barras para cada repetición del experimento. Ahora podemos ver si la mano izquierda sacó más bolitas verdes que la derecha.\n\n\n\n\n\nFigura 6.12: 10 repeticiones simuladas del experimento de sacar bolitas. Cada repetición da una respuesta ligeramente distinta. Cualquier diferencia entre las barras se debe al azar, o al error muestral. Esto muestra que el azar por sí solo puede producir diferencias, simplemente por el acto de muestrear.\n\n\n\n\nEstos 10 experimentos nos permiten observar mejor lo que el azar puede hacer. Y también debería coincidir con tus expectativas: si todo está determinado por el azar (como lo diseñamos acá), entonces a veces tu mano izquierda va a sacar más bolitas verdes, otras veces será la derecha, y otras veces ambas sacarán la misma cantidad. ¿Correcto? Correcto."
  },
  {
    "objectID": "05-Foundation_Inference.html#el-azar-hace-que-algunas-diferencias-sean-más-probables-que-otras",
    "href": "05-Foundation_Inference.html#el-azar-hace-que-algunas-diferencias-sean-más-probables-que-otras",
    "title": "6  Fundamentos de la inferencia",
    "section": "6.4 El azar hace que algunas diferencias sean más probables que otras",
    "text": "6.4 El azar hace que algunas diferencias sean más probables que otras\nBien, ya vimos que el azar puede producir diferencias. Pero todavía no tenemos una buena idea de qué es lo que el azar usualmente puede o no puede hacer. Por ejemplo, si pudiéramos encontrar la “ventana de oportunidad” acá, podríamos descubrir que el azar normalmente no produce diferencias demasiado grandes. Y si supiéramos cuál es ese umbral, entonces, si realizamos un experimento y nuestra diferencia es más grande que lo que el azar puede producir, podríamos tener confianza en que no fue el azar lo que generó nuestra diferencia.\nPensemos en nuestra medición de bolitas verdes en términos de una diferencia. Por ejemplo, en cada experimento contamos las bolitas verdes para la mano izquierda y la derecha. Lo que realmente queremos saber es si hay una diferencia entre ellas. Así que podemos calcular el puntaje de diferencia. Vamos a definir ese puntaje de diferencia como: # de bolitas verdes con la izquierda menos # de bolitas verdes con la derecha. Ahora vamos a simular este experimento mil veces. Cada vez, vamos a contar la cantidad de bolitas verdes para cada mano y calcular la diferencia. Luego, vamos a graficar los puntajes de diferencia para ver qué tipo de valores produce el azar.\n\n\n\n\n\nFigura 6.13: Un vistazo a las diferencias en el número de bolitas verdes entre manos en distintas repeticiones. La diferencia esperada es cero, pero el error muestral produce diferencias distintas de cero.\n\n\n\n\nLas barras faltantes significan que se extrajo la misma cantidad de bolitas verdes con ambas manos (puntaje de diferencia igual a 0). Un valor positivo indica que la mano izquierda sacó más bolitas verdes que la derecha. Un valor negativo significa que fue la derecha la que sacó más verdes. Notá que si hubiéramos decidido (y lo podemos hacer) calcular la diferencia al revés (mano derecha menos mano izquierda), los signos de los puntajes se invertirían.\nEmpezamos a ver mejor qué tipo de diferencias puede generar el azar. Los puntajes de diferencia están mayormente entre -2 y +2. Podríamos hacernos una idea más clara si corremos este experimento ficticio 100 veces en lugar de solo 10. Los resultados se muestran en Figura 6.14.\n\n\n\n\n\nFigura 6.14: Repetimos el experimento 100 veces y observamos las diferencias en cada repetición. Hay muchas diferencias que pueden ser causadas solo por el azar.\n\n\n\n\nUpss, acabamos de realizar tantos experimentos simulados que el eje x es ilegible, pero va de 1 a 100. Cada barra representa la diferencia de número de bolas verdes elegidas al azar por la mano izquierda o derecha. ¿Empiezas a notar algo? Mira el eje y, éste muestra el tamaño de la diferencia. Sí, hay muchas barras de diferentes tamaños, esto nos muestra que muchos tipos de diferencias ocurren por azar. Sin embargo, el eje y también está restringido. No va de -10 a +10. Las grandes diferencias superiores a 5 o -5 no ocurren muy a menudo.\nAhora que tenemos un método para simular las diferencias debidas al azar, vamos a realizar 10.000 experimentos simulados. Pero, en lugar de trazar las diferencias en un gráfico de barras para cada experimento, ¿qué tal si observamos el histograma de puntuaciones de diferencia? El histograma en la Figura 6.15 proporciona una imagen más clara sobre qué diferencias ocurren con más frecuencia y cuáles no. Esta será otra ventana para observar qué tipo de diferencias es capaz de producir el azar.\n\n\n\n\n\nFigura 6.15: Histograma de las diferencias obtenidas por azar en 10.000 repeticiones. La diferencia más frecuente es 0, que es lo que esperamos por azar. Pero las diferencias pueden llegar a ser tan grandes como -10 o +10. Las diferencias más grandes ocurren con menor frecuencia. El azar no puede hacer cualquier cosa.\n\n\n\n\nNuestra simulación por computadora nos permite forzar al azar a actuar miles de veces; cada vez produce una diferencia. Registramos la diferencia, y al final de la simulación graficamos el histograma de esas diferencias. Ese histograma empieza a mostrarnos de dónde vinieron esas diferencias. Recordá la idea de que los números provienen de una distribución, y que la distribución nos dice con qué frecuencia aparece cada número. Estamos viendo una de esas distribuciones. Nos está mostrando que el azar produce algunas diferencias más a menudo que otras.\nPrimero, el azar usualmente produce diferencias de 0 —esa es la barra más alta en el centro. También puede producir diferencias más grandes, pero a medida que las diferencias se alejan de 0 (positivas o negativas), ocurren con menos frecuencia. La forma de este histograma es tu ventana del azar. Te dice lo que el azar puede hacer, lo que el azar usualmente hace, y lo que usualmente no hace.\nPodés usar esta ventana del azar para ayudarte a hacer inferencias. Si vos mismo hicieras el experimento de las bolitas y encontraras que tu mano izquierda eligió 2 bolitas verdes más que la derecha, ¿concluirías que tu mano izquierda es especial, y que te hizo elegir más bolitas verdes? Esperemos que no. Podés mirar la ventana del azar y ver que diferencias de +2 ocurren bastante seguido por puro azar. No deberías sorprenderte si obtuviste una diferencia de +2. Ahora bien, ¿qué pasa si tu mano izquierda eligió 5 bolitas verdes más que la derecha? Bueno, el azar no suele hacer eso. Tal vez empieces a pensar que pasa algo con tu mano izquierda. Y si sacaras 9 bolitas verdes más con la izquierda que con la derecha, realmente podrías empezar a preguntarte. Eso es algo que puede pasar (es posible), pero prácticamente nunca pasa por azar.\nCuando obtenés resultados que casi nunca ocurren por azar, podés tener más confianza en que la diferencia refleja una fuerza causal que no es el azar."
  },
  {
    "objectID": "05-Foundation_Inference.html#la-prueba-de-crump",
    "href": "05-Foundation_Inference.html#la-prueba-de-crump",
    "title": "6  Fundamentos de la inferencia",
    "section": "6.5 La prueba de Crump",
    "text": "6.5 La prueba de Crump\nVamos a hacer muchas inferencias a lo largo del resto del curso. Y casi todas se reducen a una sola pregunta:\n\n¿Fue el azar el que produjo las diferencias en mis datos?\n\nVamos a hablar sobre experimentos, principalmente. Y en los experimentos queremos saber si nuestra manipulación causó una diferencia en nuestra medición. Pero, medimos cosas que tienen variabilidad natural. Así que cada vez que medimos algo, siempre vamos a encontrar alguna diferencia. Lo que queremos saber es si la diferencia que encontramos entre nuestras condiciones experimentales podría haber sido producida por azar. Si el azar es una explicación muy poco probable para la diferencia observada, vamos a hacer la inferencia de que el azar no produjo la diferencia, y que algo en nuestra manipulación experimental sí lo hizo. Eso es todo (por ahora).\n\n\n\n\n\n\nNota\n\n\n\nLa estadística no consiste únicamente en determinar si el azar ha podido producir un patrón en los datos observados. Las mismas herramientas de las que hablamos aquí pueden generalizarse para preguntarse si cualquier tipo de distribución podría haber producido las diferencias. Esto permite hacer comparaciones entre distintos modelos de los datos, para ver cuál era el más probable, en lugar de limitarse a rechazar los improbables (por ejemplo, el azar). Pero, dejaremos esos temas avanzados para otro libro de texto.\n\n\nEste capítulo trata de construir intuiciones para hacer este tipo de inferencias sobre el papel del azar en tus datos. No me queda claro cuáles son las mejores cosas para decir, para ayudarte a desarrollar intuiciones sobre cómo hacer inferencia estadística. Así que este capítulo prueba diferentes caminos, algunos más estándar, y otros completamente inventados. Lo que vas a leer ahora es una forma inventada de hacer inferencia estadística, sin usar la jerga que normalmente usamos para hablar del tema. El objetivo es trabajar sin fórmulas ni probabilidades, y simplemente usar algunas ideas básicas con simulaciones para ver qué pasa. Vamos a mirar qué puede hacer el azar, y luego hablaremos de qué tiene que pasar en tus datos para que puedas tener confianza en que no fue el azar lo que produjo el patrón que observás.\n\n6.5.1 Métodos intuitivos\nAtención: esto es una prueba estadística no oficial, inventada por Matt Crump. Tiene sentido para él (para mí), y si resulta que alguien ya inventó esto antes, entonces Crump no hizo bien la tarea, y le cambiaremos el nombre al autor original más adelante. El punto de esta prueba es mostrar cómo operaciones aritméticas simples que ya conocés pueden usarse para construir una herramienta estadística para la inferencia. Esta prueba utiliza:\n\nMuestreo aleatorio de números desde una distribución\nSumar y restar\nDividir para calcular la media\nContar\nGraficar y trazar líneas\nNINGUNA FÓRMULA\n\n\n\n6.5.2 Parte 1: Intuición basada en frecuencia sobre ocurrencias\nPregunta: ¿Cuántas veces tiene que pasar algo para que consideres que pasa mucho? ¿Y cuántas veces tiene que pasar para que consideres que pasa muy poco, o casi nunca? ¿Tan pocas veces como para que no te preocupe que te pase? ¿Saldrías a la calle todos los días si pensás que te puede caer un rayo 1 de cada 10 veces? Yo no. Probablemente te caería un rayo más de una vez por mes, y morirías bastante rápido. 1 de cada 10 es mucho (para mí, tal vez no para vos —acá no hay una única respuesta correcta).\n¿Saldrías todos los días si pensaras que te puede caer un rayo 1 de cada 100 veces? Uf, esa es difícil. ¿Qué haría yo? Si saliera todos los días, probablemente estaría muerto en un año. Tal vez saldría 2 o 3 veces por año —soy algo arriesgado—, pero probablemente viviría más si me quedara en casa para siempre. Sería un bajón.\n¿Saldrías todos los días si pensás que te podría caer un rayo 1 de cada 1000 días? Bueno, probablemente te morirías en 3 a 6 años si hicieras eso. ¿Sos timbero? Tal vez saldrías una vez por mes. Igual, un garrón.\n¿Saldrías todos los días si pensás que te puede caer un rayo 1 de cada 10.000 días? 10.000 es un número grande, difícil de imaginar. Equivale a que te caiga un rayo más o menos una vez cada 27 años. Sí, probablemente saldría unas 150 veces por año, y cruzaría los dedos.\n¿Saldrías todos los días si pensás que te puede caer un rayo 1 de cada 100.000 días? ¿Cuántos años es eso? Son unos 273 años. Con esas probabilidades, probablemente saldría todo el tiempo y me olvidaría del tema. No pasa muy seguido.\nEl punto de considerar estas preguntas es que desarrolles tu propia intuición sobre qué cosas pasan mucho, qué cosas no pasan mucho, y cómo tomar decisiones importantes en función de qué tan frecuente es algo.\n\n\n6.5.3 Parte 2: Simulando el azar\nEsta parte podría hacerse de muchas maneras. Voy a hacer un montón de suposiciones que no voy a justificar, y no voy a afirmar que la prueba de Crump esté libre de problemas. Lo que sí voy a afirmar es que ayuda a hacer una inferencia sobre si el azar podría haber producido ciertas diferencias en los datos. Ya nos hemos familiarizado con las simulaciones, así que vamos a hacer otra.\nEsto es lo que vamos a hacer. Soy un psicólogo cognitivo que está midiendo cierta variable X. Gracias a investigaciones previas, sé que cuando mido X, mis muestras tienden a tener una media y una desviación estándar particulares. Supongamos que la media suele ser 100, y la desviación estándar 15. En este caso, no me interesa usar esos números como estimaciones de parámetros poblacionales, simplemente estoy pensando en cómo suelen ser mis muestras. Lo que quiero saber es cómo se comportan cuando las muestreo. Quiero ver qué tipo de muestras ocurren con frecuencia, y cuáles no. Además, vivo en el mundo real, y en el mundo real, cuando hago experimentos para ver qué modifica X, usualmente tengo acceso a una cantidad limitada de participantes (a quienes agradezco mucho por colaborar). Supongamos que puedo incluir a 20 sujetos por condición. Mantenemos el experimento simple: dos condiciones, así que necesito 40 sujetos en total.\nMe gustaría aprender algo que me ayude a inferir. Una posibilidad sería ver cómo se ve la distribución muestral de la media muestral. Esa distribución me diría qué valores de la media aparecen frecuentemente, y cuáles son raros. Pero en realidad voy a saltar esa parte.\nPorque lo que realmente me interesa es cómo se ve la distribución muestral de la diferencia entre medias muestrales. Después de todo, voy a ejecutar un experimento con 20 personas en una condición y 20 en otra. Voy a calcular la media del grupo A, la media del grupo B, y mirar la diferencia. Probablemente encuentre una diferencia, pero mi pregunta es: ¿esa diferencia fue causada por mi manipulación, o es del tipo que suele ocurrir por azar? Si supiera qué puede hacer el azar y con qué frecuencia produce diferencias de determinado tamaño, entonces podría comparar la diferencia que observé con lo que el azar puede hacer, y entonces tomar una decisión. Si mi diferencia no ocurre muy seguido, (ya vamos a definir qué es “muy seguido”), entonces tal vez esté dispuesto a creer que mi manipulación causó la diferencia. Si mi diferencia ocurre todo el tiempo solo por azar, entonces no voy a pensar que fue causada por mi manipulación, porque podría haber sido simplemente el azar.\nEntonces, esto es lo que vamos a hacer, incluso antes de ejecutar el experimento. Vamos a hacer una simulación. Vamos a muestrear números para el grupo A y para el grupo B, luego vamos a calcular las medias de cada grupo, y después vamos a calcular la diferencia entre esas medias.\nPero vamos a hacer algo muy importante: vamos a fingir que no hicimos ninguna manipulación. Si hacemos esto —no hacemos nada, ninguna intervención que pudiera causar una diferencia—, entonces sabemos que cualquier diferencia entre las medias de los grupos A y B solo puede deberse al error muestral. Eliminamos todas las demás causas: solo queda el azar. Haciendo esto, vamos a poder ver exactamente qué puede hacer el azar. Más importante aún, vamos a ver qué tipo de diferencias ocurren frecuentemente, y cuáles no.\nAntes de hacer la simulación, necesitamos responder una pregunta: ¿cuánto es “mucho”? Podríamos elegir cualquier número. Yo voy a elegir 10.000. Eso es mucho. Si algo ocurre solo 1 vez en 10.000, estoy dispuesto a decir que eso no es mucho.\nBien, ya tenemos nuestro número. Vamos a simular las diferencias de medias entre los grupos A y B que podrían surgir solo por azar. Lo vamos a hacer 10.000 veces. Eso le da al azar muchas oportunidades para mostrarnos qué puede hacer y qué no.\nEsto es lo que hice: muestreé 20 números para el grupo A y 20 para el grupo B. Ambos grupos provienen de la misma distribución normal, con media = 100 y desviación estándar = 15. Como las muestras provienen de la misma distribución, esperamos que, en promedio, sean similares (aunque ya sabemos que las muestras siempre difieren entre sí). Después calculo la media de cada grupo, y luego la diferencia entre esas medias. Guardo ese puntaje de diferencia de medias, y repito esto hasta tener 10.000 de ellos. Después, dibujo un histograma como el de la Figura 6.16.\n\n\n\n\n\nFigura 6.16: Histograma de diferencias entre medias surgidas por azar.\n\n\n\n\n\n\n\n\n\n\nNota\n\n\n\nPor supuesto, podríamos reconocer que el azar podría producir una diferencia mayor que 15. Simplemente no le dimos la oportunidad. Solo corrimos la simulación 10.000 veces. Si la corriéramos un millón de veces, tal vez una diferencia mayor que 15 o incluso 20 ocurriría un par de veces. Si la corriéramos un trillón de veces, tal vez una diferencia mayor que 30 ocurriría muy de vez en cuando. Si fuéramos hasta el infinito, entonces el azar podría producir todo tipo de diferencias grandes cada tanto. Pero ya decidimos que 1 en 10.000 no es mucho. Así que cosas que ocurren 0 veces en 10.000 simulaciones —como diferencias mayores a 15— se consideran extremadamente improbables.\n\n\nAhora podemos ver lo que el azar puede hacer con respecto al tamaño de nuestras diferencias de medias. El eje x muestra el tamaño de la diferencia de medias. Como tomamos ambas muestras de la misma distribución poblacional, la diferencia entre ellas debería tender a ser 0, y eso es lo que vemos en el histograma.\nPausa un segundo. ¿Por qué las diferencias de medias deberían tender a cero? ¿No era que la media poblacional era 100? ¿No deberían las diferencias rondar 100? No. La media del grupo A tenderá a estar alrededor de 100, y la del grupo B también. Entonces, la diferencia tenderá a ser 100 − 100 = 0.Por eso esperamos una diferencia de medias igual a cero cuando las muestras provienen de la misma población.\nEntonces, las diferencias cercanas a cero ocurren con más frecuencia —eso es bueno, es lo que esperábamos. Las diferencias más grandes (positivas o negativas) ocurren cada vez menos. Diferencias mayores que 15 o menores que -15 no ocurrieron nunca. Para nuestros fines, parece que el azar solo produce diferencias entre -15 y 15.\nOK, hagamos un par de preguntas simples.¿Cuál fue el número negativo más grande (es decir, el más negativo) que ocurrió en la simulación? Vamos a usar R para esto. Todos los 10.000 puntajes de diferencia están guardados en una variable que llamé diferencia. Si queremos encontrar el valor mínimo, usamos la función min. Este es el resultado:\n\nmin(diferencia)\n\n[1] -16.31298\n\n\nBien, ¿y cuál fue el número positivo más grande que ocurrió? Usemos la función max para averiguarlo. Esa función encuentra el valor más grande (máximo) de una variable. Por cierto, acabamos de calcular el rango: el mínimo y el máximo de los datos. Recordá que ya aprendimos eso antes. De todos modos, acá va el máximo:\n\nmax(diferencia)\n\n[1] 19.00466\n\n\nAmbos valores extremos ocurrieron solo una vez. Fueron tan poco frecuentes que ni siquiera los pudimos ver en el histograma —la barra era tan baja que no se notaba. Además, el mayor valor negativo y el mayor positivo tienen prácticamente el mismo tamaño (ignorando el signo), lo cual tiene sentido porque la distribución parece bastante simétrica.\nEntonces, ¿qué podemos decir sobre estos dos números —el mínimo y el máximo? Podemos decir que el mínimo ocurrió 1 vez de cada 10.000. Y que el máximo ocurrió 1 vez de cada 10.000. ¿Eso es mucho? Para mí, no. No es mucho.\n¿Y con qué frecuencia ocurre una diferencia de 30 (mucho más grande que el máximo)? No podemos decirlo exactamente, porque 30 no ocurrió nunca en la simulación. Con lo que tenemos, podemos decir: 0 veces de cada 10.000. Eso significa: nunca.\nEstamos por pasar a la parte tres, que consiste en trazar líneas de decisión y hablar sobre ellas. La parte realmente importante de la parte 3 es esta: ¿Qué dirías si hicieras este experimento una vez y encontraras una diferencia de medias de 30? Yo diría: eso ocurre 0 veces de cada 10.000 por azar. Yo diría: el azar no produjo mi diferencia de 30. Eso es lo que diría. Vamos a desarrollar esta idea ahora mismo.\n\n\n6.5.4 Parte 3: Juicio y toma de decisiones\nRecordá: ni siquiera hemos hecho un experimento real. Simplemente simulamos lo que podría pasar si lo hiciéramos. Hicimos un histograma. Vimos que el azar produce algunas diferencias más que otras, y que nunca produce diferencias realmente grandes. ¿Qué deberíamos hacer con esa información?\nLo que vamos a hacer es hablar sobre juicio y toma de decisiones. ¿De qué tipo? Bueno, cuando finalmente hagas un experimento real, vas a obtener dos medias —una para el grupo A y otra para el grupo B—, y entonces vas a tener que hacer algunos juicios, y quizás incluso tomar una decisión, si querés.\nVas a tener que juzgar si el azar (es decir, el error muestral) podría haber producido la diferencia que observaste. Si juzgás que no fue el azar, quizás decidas contarle al mundo que tu manipulación experimental realmente funciona. Si juzgás que sí podría haber sido el azar, quizás tomes una decisión distinta. Estas decisiones son importantes para los investigadores. Sus carreras pueden depender de ellas. Y además, sus decisiones importan para el público. A nadie le gusta escuchar noticias falsas en los medios sobre hallazgos científicos.\nAsí que lo que estamos haciendo es prepararnos para tomar esos juicios. Vamos a trazar un plan, antes incluso de ver los datos, sobre cómo vamos a juzgar y decidir lo que encontremos. Este tipo de planificación también puede usarse para interpretar los resultados de otras personas, como una forma de comprobar dos veces si creés que esos resultados son plausibles. El tema con el juicio y la toma de decisiones es que las personas razonables no se ponen de acuerdo sobre cómo hacerlo, las personas poco razonables realmente no se ponen de acuerdo, y los estadísticos e investigadores también discrepan sobre cómo hacerlo. Voy a proponer algunas ideas con las que la gente va a estar en desacuerdo. Está bien. Estas ideas igual tienen sentido. Y esas ideas que generan desacuerdo apuntan a problemas importantes que son muy reales en cualquier test de inferencia estadística “real”.\nHablemos ahora de algunos hechos objetivos que sabemos con certeza a partir de nuestra simulación de 10.000 repeticiones. Por ejemplo, podemos trazar algunas líneas en el gráfico y marcar algunas regiones distintas. Vamos a hablar de dos tipos de regiones:\n\nRegión del azar. El azar lo hizo. El azar podría haberlo hecho.\nRegión del no azar. El azar no lo hizo. El azar no podría haberlo hecho.\n\nLas regiones están definidas por el valor mínimo y el valor máximo observados en la simulación. El azar nunca produjo un número menor ni mayor que esos extremos. La región dentro del rango es lo que el azar sí hizo, y la región fuera del rango, en ambos extremos, es lo que el azar nunca hizo.\nSe ve así en la Figura 6.17:\n\n\n\n\n\nFigura 6.17: Aplicación de límites de decisión al histograma de diferencias de medias. Los límites identifican qué diferencias produjo (o no produjo) el azar en la simulación.\n\n\n\n\nAcabamos de trazar unas líneas, sombrear algunas regiones y construir un plan que podríamos usar para tomar decisiones. ¿Cómo funcionarían esas decisiones? Supongamos que hiciste el experimento y encontraste una diferencia de medias entre los grupos A y B de 25. ¿Dónde está el 25 en la figura? Está en la parte verde. ¿Qué dice la parte verde? “NO AZAR”. ¿Qué significa esto? Significa que el azar nunca produjo una diferencia de 25. Eso ocurrió 0 veces de cada 10.000. Si encontramos una diferencia de 25, quizás podamos concluir con confianza que el azar no causó esa diferencia. Si yo encontrara una diferencia de 25 con este tipo de datos, estaría bastante seguro de que el azar no la causó, y me daría permiso a mí mismo para considerar que mi manipulación experimental podría estar causando la diferencia.\n¿Y qué pasa con una diferencia de +10? Eso está en la parte roja, donde vive el azar. El azar podría haber producido una diferencia de +10, porque podemos ver que a veces lo hizo. La parte roja es la ventana de lo que el azar hizo en nuestra simulación. Cualquier cosa dentro de esa ventana podría haber sido una diferencia causada por el azar. Si encontrara una diferencia de +10, diría: “sí, podría haber sido el azar”. Y también estaría menos seguro de que la diferencia fue causada solamente por mi manipulación experimental.\nLa inferencia estadística podría ser tan simple como esto: El número que obtenés de tu experimento podría estar dentro de la ventana del azar (entonces no podés descartar al azar como causa), o podría estar fuera de la ventana del azar (entonces sí podés descartarlo). Caso cerrado. Vámonos todos a casa.\n\n\n6.5.5 Zonas grises\nEntonces, ¿cuál es el problema? Dependiendo de quién seas, y qué tipo de riesgos estés dispuesto a tomar, puede que no haya ningún problema. Pero si sos aunque sea un poco arriesgado, entonces sí hay un problema que hace difícil emitir juicios claros sobre el papel del azar.\nNos gustaría decir que el azar causó o no causó la diferencia que observamos. Pero en realidad, siempre estamos en la posición de tener que admitir que el azar podría haberla causado a veces, o que no la habría causado la mayoría de las veces.\nEstas son afirmaciones ambiguas, imprecisas, entre el “sí” y el “no”. Y está bien. El gris también es un color. Vamos a darle un poco de respeto al gris.\n—“¿De qué zonas grises estás hablando? ¡Solo veo rojo y verde! ¿Tengo daltonismo para el gris?”—\nVeamos dónde podrían estar algunas zonas grises. Digo “podrían”, porque la gente no se pone de acuerdo sobre dónde empieza el gris. Las personas tienen distintos niveles de tolerancia al gris.\nLa Figura 6.18 muestra mi opinión sobre dónde se ubican las zonas grises:\n\n\n\n\n\nFigura 6.18: Los signos de interrogación indican una zona donde hay cierta incertidumbre. Las diferencias dentro de esta zona no ocurren muy seguido por azar. Cuando encontrás diferencias de ese tamaño, ¿deberías rechazar la idea de que fueron causadas por azar? Siempre vas a tener algo de incertidumbre asociada a esa decisión, porque está claro que el azar podría haber causado la diferencia. Pero, el azar usualmente no produce diferencias de ese tamaño.\n\n\n\n\nHice dos zonas grises, y son grisáceo-rojizas, porque todavía estamos dentro de la ventana del azar. Hay signos de interrogación (?) en las zonas grises. ¿Por qué? Los signos de interrogación reflejan cierta incertidumbre que tenemos respecto a esas diferencias en particular. Por ejemplo, si encontrás una diferencia dentro de una zona gris, digamos una de 15. 15 está por debajo del máximo, lo que significa que el azar sí produjo diferencias cercanas a 15. Pero, las diferencias de 15 no ocurren muy seguido.\n¿Qué podés concluir o decir sobre ese 15 que encontraste? ¿Podés decir, sin lugar a dudas, que el azar no produjo esa diferencia? Por supuesto que no. Sabés que el azar podría haberla producido. Aun así, es una de esas cosas que no ocurren mucho. Eso hace que el azar sea una explicación poco probable. En lugar de pensar que fue el azar, quizás estés dispuesto a tomar el riesgo y decir que tu manipulación experimental causó la diferencia. Estás haciendo una apuesta de que no fue el azar… pero podría ser una apuesta razonable, ya que sabés que las probabilidades están de tu lado.\nTal vez estés pensando que tus zonas grises no son iguales a las que dibujé yo. Quizás quieras ser más conservador y hacerlas más chicas. O, tal vez seas más arriesgado y prefieras hacerlas más grandes. O incluso, quizás quieras extender la zona gris un poco dentro de la zona verde (después de todo, el azar podría producir diferencias más grandes alguna que otra vez, y para evitarlas tendrías que hacer que la zona gris invada un poco el área verde).\nOtra cosa en la que pensar es en tu política de decisión. ¿Qué vas a hacer cuando tu diferencia observada caiga dentro de la zona gris? ¿Siempre vas a tomar la misma decisión sobre el papel del azar? ¿O a veces vas a cambiar de parecer, según cómo te sientas? Tal vez pienses que no debería haber una política estricta, y que deberías aceptar cierto nivel de incertidumbre. La diferencia que encontraste podría ser real… o tal vez no lo sea. Hay incertidumbre, es difícil evitarla.\nAsí que vamos a ilustrar otro tipo de estrategia para tomar decisiones. Recién hablamos de una que incluía líneas y regiones. Eso hace que parezca una elección binaria: podemos descartar o no descartar el papel del azar. Otra perspectiva es que todo es un matiz de gris, como en la Figura 6.19.\n\n\n\n\n\nFigura 6.19: El sombreado de las barras azules indica los niveles de confianza en si una diferencia podría haber sido producida por azar. Las barras más oscuras representan mayor confianza en que la diferencia no fue causada por azar. Las barras se oscurecen a medida que la diferencia de medias aumenta en valor absoluto.\n\n\n\n\nOK, así que lo hice con tonos de azul (porque era más fácil en R). Ahora podemos ver dos planes de decisión al mismo tiempo. Notá que a medida que las barras se vuelven más bajas, también se vuelven de un azul más oscuro e intenso. El color puede usarse como guía para tu confianza. Es decir, tu confianza en la creencia de que tu manipulación causó la diferencia, y no el azar. Si encontrás una diferencia cerca de una barra azul bien oscura, esas diferencias no ocurren muy seguido por azar, así que podrías estar muy confiado en que el azar no la causó. Si encontrás una diferencia cerca de una barra azul un poco más clara, podrías estar un poco menos confiado. Eso es todo. Ejecutás tu experimento, obtenés tus datos, y después tenés cierto nivel de confianza en que la diferencia no fue producida por azar.\nEsta forma de pensar se desarrolla de manera mucho más interesante en el mundo bayesiano de la estadística. No vamos a meternos demasiado en eso, pero lo mencionaremos un poco acá y allá. Vale la pena saber que existe.\n\n6.5.5.1 Tomar decisiones y equivocarse\nNo importa cómo planees tomar decisiones sobre tus datos, siempre vas a estar propenso a cometer errores. Podrías declarar que un resultado es real, cuando en realidad fue causado por azar. Eso se llama un error tipo I, o un falso positivo. Podrías ignorar un resultado, decir que fue azar, cuando en realidad no lo fue (aunque estaba dentro de la ventana). Eso se llama un error tipo II, o un falso negativo. La forma en que tomás decisiones puede influir en con qué frecuencia cometés estos errores a lo largo del tiempo.\nLa forma en que tomás decisiones puede influir en con qué frecuencia cometés errores a lo largo del tiempo. Si sos investigador, vas a correr muchos experimentos, y vas a cometer cierta cantidad de errores con el tiempo.\nSi usás algo como el método muy estricto de solo aceptar resultados como reales cuando están en la zona de “no azar”, entonces no cometerás muchos errores tipo I. Prácticamente todos tus resultados serán reales. Pero también vas a cometer errores tipo II, porque vas a perderte cosas reales que tu criterio de decisión dice que son producto del azar. Y también vale lo contrario. Si estás dispuesto a ser más liberal, y aceptar como reales los resultados que están en la zona gris, entonces vas a cometer más errores tipo I, pero no vas a cometer tantos errores tipo II. Bajo la estrategia de decisión que usa estas regiones de corte para decidir, hay un compromiso necesario: no podés minimizar ambos tipos de error a la vez.\nLa perspectiva bayesiana evita un poco este problema. Los bayesianos hablan de actualizar sus creencias y niveles de confianza con el tiempo. En esa visión, lo único que tenés es un cierto nivel de confianza sobre si algo es real o no, y al ejecutar más experimentos podés aumentar o disminuir ese nivel de confianza. Esto, de alguna forma, evita el compromiso entre errores tipo I y tipo II.\nDe todos modos, hay otra forma de reducir los errores tipo I y tipo II, y de aumentar tu confianza en los resultados incluso antes de hacer el experimento. Se llama: Saber cómo diseñar un buen experimento.\n\n\n\n6.5.6 Parte 4: Diseño experimental\nYa vimos lo que puede hacer el azar. Ahora ejecutamos un experimento. Manipulamos algo entre los grupos A y B, obtenemos los datos, calculamos las medias de grupo, y luego observamos la diferencia. Después cruzamos todos los dedos de las manos y los pies, y rezamos con todas nuestras fuerzas que la diferencia sea lo suficientemente grande como para no haber sido causada por azar. Eso es mucho rezar.\nEl asunto es el siguiente: a menudo no sabemos qué tan fuerte es nuestra manipulación desde el principio. Entonces, incluso si puede causar un cambio, no necesariamente sabemos cuánto puede cambiar las cosas. Por eso estamos ejecutando el experimento.\nMuchas manipulaciones en Psicología no son lo suficientemente fuertes como para causar grandes cambios. Y eso es un problema cuando queremos detectar esas fuerzas causales más pequeñas. En nuestro ejemplo ficticio, podrías manipular algo que tenga una influencia muy leve, que nunca empuje la diferencia de medias más allá de, digamos, 5 o 10. En nuestra simulación, necesitábamos algo más cercano a 15, 17 o 21 —o mejor aún, ¡30!— porque el azar nunca hace eso. Digamos que tu manipulación es escuchar música vs. no escuchar música. Escuchar música podría cambiar algo sobre X, pero si solo cambia X en +5, nunca vas a poder afirmar con confianza que no fue el azar. Y no es tan fácil cambiar completamente la condición de música para que sea súper intensa y realmente cause un cambio en X comparado con la condición sin música.\n\n¡DISEÑO EXPERIMENTAL AL RESCATE! Último momento: a menudo es posible cambiar cómo ejecutás tu experimento para que sea más sensible a efectos pequeños.\n\n¿Tenés alguna idea de cómo se puede hacer esto? Acá va una pista: es todo eso que aprendiste sobre la distribución muestral de la media muestral, y el papel del tamaño muestral. ¿Qué le pasa a la distribución muestral de la media cuando N (el tamaño de muestra) aumenta? La distribución se vuelve más y más angosta, y empieza a parecerse a un único número (la media hipotética de la población hipotética). Eso es genial.\nSi cambiás el enfoque y pensás en las diferencias de medias, como la distribución que creamos en esta prueba, ¿qué creés que va a pasar con esa distribución a medida que aumentamos N? También se va a achicar. A medida que aumentamos N hacia el infinito, esa distribución se reduce a 0. Lo que significa que, cuando N es infinito, el azar nunca produce diferencias. Podemos usar esto a nuestro favor.\nPor ejemplo, podríamos correr nuestro experimento con 20 sujetos por grupo. O podríamos decidir invertir más tiempo y correrlo con 40 sujetos por grupo, o 80, o 150. Cuando vos sos quien ejecuta el experimento, vos decidís el diseño. Y esas decisiones importan muchísimo. Básicamente, cuantos más sujetos tenés, más sensible es tu experimento. Con un N más grande, vas a poder detectar de manera confiable diferencias de medias más pequeñas, y podés concluir con confianza que esas diferencias no fueron producidas por azar.\nFijate en los histogramas de ?fig-5sampleDistNormal . Es la misma simulación de antes, pero con cuatro tamaños muestrales distintos: 20, 40, 80, 160. Vamos duplicando el tamaño muestral en cada simulación, solo para ver qué pasa con el ancho de la ventana del azar.\nAhí lo tenés. La distribución muestral de las diferencias de medias se contrae hacia 0 a medida que aumenta el tamaño muestral. Esto significa que, si ejecutás un experimento con un tamaño muestral mayor, vas a poder detectar diferencias de medias más pequeñas, y estar seguro de que no se deben al azar.\nLa Tabla 6.2 muestra los valores mínimos y máximos que el azar produjo para cada uno de los cuatro tamaños muestrales:\n\n\n\n\nTabla 6.2: La diferencia de medias más pequeña y más grande producida por azar en función del tamaño muestral.\n\n\nsample_size\nsmallest\nlargest\n\n\n\n\n20\n-22.434891\n25.716813\n\n\n40\n-16.933165\n16.179850\n\n\n80\n-12.813746\n11.380099\n\n\n160\n-9.081009\n8.279297\n\n\n\n\n\n\nLa tabla muestra que el rango de comportamiento del azar es mucho más amplio para tamaños muestrales pequeños, y más estrecho para tamaños muestrales grandes. Pensá en lo que implica ese estrechamiento para el diseño de tu experimento. Por ejemplo, un aspecto del diseño es la elección del tamaño muestral, N —o, en un experimento de psicología, la cantidad de participantes. Si resulta que tu manipulación va a causar una diferencia de +11, ¿qué deberías hacer? ¿Ejecutar un experimento con N = 20 personas? Espero que no. Si lo hicieras, podrías obtener una diferencia de medias de +11 bastante seguido solo por azar.\nSin embargo, si ejecutarás el experimento con 160 personas, entonces sí podrías decir con seguridad que +11 no fue causado por azar, estaría fuera del rango de lo que el azar puede producir. Incluso podrías considerar correr el experimento con 80 participantes. Una diferencia de +11 ahí no ocurriría muy seguido por azar, y además serías más eficiente en términos de tiempo y recursos.\nEl punto es: el diseño del experimento determina el tamaño de los efectos que puede detectar. Si querés detectar un efecto pequeño, aumentá el tamaño muestral.\nEs importante aclarar que esto no es lo único que podés hacer. También podés aumentar el tamaño de cada celda. Por ejemplo, muchas veces tomamos varias mediciones de un solo sujeto. Cuantas más mediciones tomás (es decir, mayor tamaño de celda), más estable es tu estimación de la media del sujeto. Vamos a hablar más de estas cuestiones más adelante. También podés intentar aplicar una manipulación más fuerte, cuando sea posible.\n\n\n6.5.7 Parte 5: Yo tengo el poder\n\nPor el poder de Grayskull… ¡YO TENGO EL PODER! – He-Man\n\nEl último tema de esta sección se llama poder. Más adelante vamos a definirlo con ideas estadísticas más específicas. Acá solo vamos a hablar de la idea general. Y vamos a mostrar cómo asegurarte de que tu diseño tenga un poder del 100%. Porque, ¿por qué ejecutar un diseño que no tenga poder?\nLa gran idea detrás del poder es el concepto de sensibilidad. El concepto de sensibilidad parte de la suposición de que hay algo a lo que ser sensible.\nEs decir, existe una diferencia real puede ser medida. Entonces, la pregunta es: ¿cuán sensible es tu experimento? Ya vimos que el número de sujetos (tamaño muestral) cambia la sensibilidad del diseño. Más sujetos = más sensibilidad para detectar efectos más pequeños.\nVeamos un gráfico más. Lo que vamos a hacer es simular una medida de sensibilidad para un conjunto de tamaños muestrales, desde 10 hasta 300. Lo haremos en incrementos de 10. Para cada simulación, vamos a calcular las diferencias de medias como lo hicimos antes. Pero, en lugar de mostrar el histograma, simplemente vamos a calcular el valor mínimo y el máximo. Esto es una buena medida del alcance extremo del azar. Luego vamos a graficar esos valores en función del tamaño muestral y ver qué obtenemos.\n\n\n\n\n\nFigura 6.20: Gráfico del máximo y mínimo de las diferencias de medias producidas por azar en función del tamaño muestral. El rango se reduce a medida que el tamaño muestral aumenta, mostrando que el azar por sí solo produce un rango más pequeño de diferencias cuando el tamaño muestral es mayor.\n\n\n\n\nLa Figura 6.20 muestra una ventana de sensibilidad razonablemente precisa en función del tamaño muestral. Para cada tamaño muestral, podemos ver la diferencia máxima que el azar produjo y la mínima. En esas simulaciones, el azar nunca produjo diferencias mayores ni menores que esos valores extremos. Así que cada diseño es sensible a cualquier diferencia que esté por debajo de la línea inferior o por encima de la línea superior.\nOtra forma de decirlo es la siguiente: ¿Cuáles de los tamaños muestrales son sensibles a una diferencia de +10 o -10? Es decir, si se observa una diferencia de +10 o -10, entonces podríamos decir con mucha confianza que esa diferencia no fue causada por azar, porque según estas simulaciones, el azar nunca produjo diferencias tan grandes. Para ayudarnos a ver cuáles son sensibles, la Figura 6.21 traza líneas horizontales en -10 y +10:\n\n\n\n\n\nFigura 6.21: La línea roja representa el tamaño de una diferencia de medias que un investigador podría estar interesado en detectar. Todos los puntos que están por fuera (por encima o por debajo) de la línea roja representan diseños con tamaños muestrales pequeños. Cuando ocurre una diferencia de 10 en esos diseños, podemos descartar con confianza que haya sido causada por azar. Los puntos entre las líneas rojas representan diseños con tamaños muestrales grandes. Estos diseños nunca producen diferencias tan grandes como 10, así que cuando ocurren, podemos confiar en que no fueron producto del azar.\n\n\n\n\nSegún una estimación visual aproximada, los diseños con tamaño muestral mayor o igual a 100 son todos sensibles a diferencias reales de 10. Los diseños con tamaño muestral mayor a 100 nunca produjeron diferencias extremas fuera de las líneas rojas solo por azar. Si se usaran esos diseños, y se observara un efecto de 10 o más, entonces podríamos tener confianza en que el azar no produjo ese efecto. Diseñar tu experimento de modo que sepas que es sensible a lo que estás buscando detectar es la gran idea detrás del poder estadístico.\n\n\n6.5.8 Resumen de la prueba de Crump\n¿Qué aprendimos con esta supuesta “prueba de Crump falsa” que nadie usa? Bueno, aprendimos lo básico de lo que estaremos haciendo de ahora en más. Y lo hicimos sin matemática difícil ni fórmulas.\nMuestreamos números, calculamos medias, restamos las medias entre condiciones experimentales, luego repetimos ese proceso muchas veces, contamos las diferencias de medias y las pusimos en un histograma. Eso nos mostró lo que puede hacer el azar en un experimento. Después, discutimos cómo tomar decisiones en torno a esos hechos. Y mostramos cómo podemos controlar el papel del azar simplemente cambiando cosas como el tamaño muestral."
  },
  {
    "objectID": "05-Foundation_Inference.html#la-prueba-de-aleatorización-o-prueba-de-permutación",
    "href": "05-Foundation_Inference.html#la-prueba-de-aleatorización-o-prueba-de-permutación",
    "title": "6  Fundamentos de la inferencia",
    "section": "6.6 La prueba de aleatorización (o prueba de permutación)",
    "text": "6.6 La prueba de aleatorización (o prueba de permutación)\nBienvenidos a la primera estadística inferencial oficial de este libro. Hasta ahora estuvimos construyendo algunas intuiciones. A continuación, vamos a ser un poco más formales y mostrar cómo podemos usar el azar para decirnos si nuestro resultado experimental probablemente fue causado por el azar o no. Hacemos esto con algo llamado una prueba de aleatorización. Las ideas detrás de la prueba de aleatorización son las mismas ideas que están detrás del resto de las estadísticas inferenciales que vamos a discutir en los capítulos siguientes. Y —sorpresa— ya hemos hablado de todas las ideas principales. Ahora, simplemente las vamos a reunir, y darles el nombre de prueba de aleatorización.\nAcá va la idea principal. Cuando ejecutás un experimento y recolectás algunos datos, podés descubrir qué pasó esa única vez. Pero, como ejecutaste el experimento solo una vez, no podés saber qué podría haber pasado. La prueba de aleatorización es una forma de averiguar qué podría haber pasado. Y, una vez que sabés eso, podés comparar lo que pasó en tu experimento con lo que podría haber pasado.\n\n6.6.1 Ejemplo ficticio: ¿masticar chicle mejora tus calificaciones?\nSupongamos que ejecutás un experimento para saber si masticar chicle hace que los estudiantes obtengan mejores notas en un examen de estadística. Asignás al azar a 20 estudiantes a la condición “con chicle”, y a otros 20 estudiantes a la condición “sin chicle”. Después, les tomás un examen de estadística a todos y medís sus notas. Si masticar chicle causa mejores calificaciones, entonces el grupo con chicle debería tener, en promedio, notas más altas que el grupo sin chicle.\nDigamos que los datos lucen así:\n\n\n\n\n\nestudiante\nchicle\nno_chicle\n\n\n\n\n1\n94\n40\n\n\n2\n70\n71\n\n\n3\n84\n67\n\n\n4\n99\n74\n\n\n5\n79\n44\n\n\n6\n92\n85\n\n\n7\n89\n83\n\n\n8\n99\n61\n\n\n9\n75\n55\n\n\n10\n76\n67\n\n\n11\n79\n55\n\n\n12\n82\n52\n\n\n13\n88\n43\n\n\n14\n77\n49\n\n\n15\n77\n82\n\n\n16\n91\n82\n\n\n17\n71\n63\n\n\n18\n91\n48\n\n\n19\n79\n79\n\n\n20\n94\n77\n\n\nSumas\n1686\n1277\n\n\nMedias\n84.3\n63.85\n\n\n\n\n\nEntonces, ¿los estudiantes que masticaron chicle tuvieron mejor desempeño que los que no lo hicieron? Mirá la fila de medias al final de la tabla. La media para los estudiantes que masticaron chicle fue 84.3, y la media para los estudiantes sin chicle fue 63.85. A simple vista, parece que masticar chicle funcionó.\n“¡PAREN LAS ROTATIVAS, esto es ridículo!”. Ya sabemos que es ridículo porque estamos usando datos inventados. Pero, incluso si estos fueran datos reales, podrías pensar: “Eso del chicle no tiene sentido, esta diferencia podría haber sido causada por azar. Quizás los estudiantes más inteligentes cayeron por azar en el grupo con chicle, y por eso sus notas fueron más altas… el chicle en realidad no hizo nada…”. Estamos de acuerdo. Pero echemos un vistazo más de cerca. Ya sabemos cómo salieron los datos. Lo que queremos saber es cómo podrían haber salido. ¿Cuáles son todas las posibilidades?\nPor ejemplo, los datos habrían salido un poco distintos si hubiéramos asignado a algunos estudiantes del grupo con chicle al grupo sin chicle, y viceversa. Pensá en todas las formas posibles de asignar a los 40 estudiantes en dos grupos; hay muchísimas maneras. Y las medias de cada grupo serían diferentes dependiendo de cómo se asignen los estudiantes.\nEn la práctica, no es posible ejecutar el experimento de todas esas formas posibles —eso llevaría demasiado tiempo. Pero sí podemos estimar cómo habrían salido todos esos experimentos usando simulación. Esta es la idea: Tomamos las 40 mediciones (las notas del examen) que obtuvimos para todos los estudiantes. Luego, seleccionamos aleatoriamente a 20 de ellas y fingimos que fueron del grupo con chicle, y las 20 restantes las asignamos al grupo sin chicle. Entonces, volvemos a calcular las medias para ver qué habría pasado. Y seguimos haciendo esto una y otra vez, cada vez calculando lo que habría pasado en esa versión alternativa del experimento.\n\n6.6.1.1 Haciendo la aleatorización\nAntes de hacer eso, vamos a mostrar cómo funciona la parte de la aleatorización. Usaremos menos números para que el proceso sea más fácil de visualizar. Acá están los primeros 5 puntajes de examen para los estudiantes en ambos grupos:\n\n\n\n\n\nestudiante\nchicle\nno_chicle\n\n\n\n\n1\n94\n40\n\n\n2\n70\n71\n\n\n3\n84\n67\n\n\n4\n99\n74\n\n\n5\n79\n44\n\n\nSums\n426\n296\n\n\nMeans\n85.2\n59.2\n\n\n\n\n\nLas cosas podrían haber salido diferente si algunos de los sujetos en el grupo con chicle hubieran sido intercambiados con sujetos del grupo sin chicle. Así es cómo podemos hacer un poco de aleatorización.\n\n\n[1] 71 84 44 40 74\n\n\n[1] 67 94 99 79 70\n\n\nTomamos los primeros 5 números de los datos originales, y los pusimos todos en una variable llamada all_scores. Luego usamos la función sample en R para mezclar los puntajes. Finalmente, tomamos los primeros 5 puntajes mezclados y los guardamos en una nueva variable llamada new_chicle. Después, colocamos los últimos 5 puntajes en la variable new_no_chicle. Luego los imprimimos, así podemos verlos.\nSi hacemos esto un par de veces y lo ponemos en una tabla, podemos ver que las medias para “con chicle” y “sin chicle” serían distintas si los sujetos se asignaran al azar de otra forma. Fijate:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nestudiante\nchicle\nno_chicle\nchicle2\nno_chicle2\nchicle3\nno_chicle3\n\n\n\n\n1\n94\n40\n67\n71\n79\n71\n\n\n2\n70\n71\n70\n40\n67\n94\n\n\n3\n84\n67\n94\n84\n84\n44\n\n\n4\n99\n74\n44\n99\n70\n99\n\n\n5\n79\n44\n74\n79\n40\n74\n\n\nSumas\n426\n296\n349\n373\n340\n382\n\n\nMedias\n85.2\n59.2\n69.8\n74.6\n68\n76.4\n\n\n\n\n\n\n\n6.6.1.2 Simulando las diferencias de medias en distintas aleatorizaciones\nEn nuestro experimento ficticio encontramos que la media para los estudiantes que masticaron chicle fue 84.3, y la media para los que no lo hicieron fue 63.85. La diferencia de medias (chicle – sin chicle) fue 20.45. Esa es una diferencia bastante grande. Eso es lo que pasó. Pero, ¿qué podría haber pasado? Si probáramos todos los experimentos posibles en los que los sujetos se asignan de forma diferente, ¿cómo luciría la distribución de las posibles diferencias de medias?\nVamos a averiguarlo. De eso se trata exactamente la prueba de aleatorización. Cuando hacemos nuestra prueba de aleatorización, vamos a medir la diferencia de medias en los puntajes del examen entre el grupo con chicle y el grupo sin chicle. Cada vez que hagamos una aleatorización, guardaremos la diferencia de medias.\nVeamos una pequeña animación de lo que ocurre durante la prueba de aleatorización. La Figura 6.22 muestra datos de otro experimento ficticio, pero los principios son los mismos. Después volveremos al experimento del chicle.\nLa animación muestra tres cosas importantes: Primero, los puntos púrpuras muestran las medias de los puntajes en dos grupos (no estudió vs. estudió). Parece que hay una diferencia, ya que un punto está más bajo que el otro. Queremos saber si el azar podría producir una diferencia tan grande.\nAl comienzo de la animación, los puntos verde claro y rojo muestran los puntajes individuales de cada uno de los 10 sujetos en el diseño (los puntos púrpura son las medias de estos puntajes originales). Luego, durante las aleatorizaciones, mezclamos aleatoriamente los puntajes originales entre los grupos. Podés ver esto ocurriendo a lo largo de la animación, mientras los puntos verde y rojo aparecen en diferentes combinaciones aleatorias. Los puntos amarillos en movimiento te muestran las nuevas medias para cada grupo después de la aleatorización. Las diferencias entre los puntos amarillos te muestran el rango de diferencias que el azar podría producir.\n\n\n\n\n\nFigura 6.22: Animación de una prueba de aleatorización. Los puntos púrpura representan la ubicación de las medias originales en cada condición. Los puntos amarillos representan las medias de cada muestra aleatorizada. Los puntos azules y rojos muestran cómo los puntajes originales son redistribuidos al azar en cada aleatorización.\n\n\n\n\nEstamos haciendo una inferencia estadística visual. Al observar el rango de movimiento de los puntos amarillos, estamos viendo qué tipo de diferencias puede producir el azar. En esta animación, los puntos púrpura —que representan la diferencia original— están generalmente fuera del rango del azar. Los puntos amarillos no se mueven más allá de los puntos púrpura, así que el azar parece ser una explicación poco probable de la diferencia.\nSi los puntos púrpura estuvieran dentro del rango de los puntos amarillos, entonces sabríamos que el azar es capaz de producir la diferencia que observamos, y que lo hace con bastante frecuencia. En ese caso, no deberíamos concluir que la manipulación causó la diferencia, porque podría haber ocurrido fácilmente por azar.\nVolvamos al ejemplo del chicle. Después de aleatorizar nuestras notas muchas veces, y de calcular las nuevas medias y las diferencias de medias, vamos a tener un montón de diferencias para observar, que podemos representar en un histograma. Ese histograma da una imagen de lo que podría haber pasado. Luego, podemos comparar lo que pasó realmente con lo que podría haber pasado.\nEste es el histograma de las diferencias de medias obtenidas con la prueba de aleatorización. Para esta simulación, aleatorizamos los resultados del experimento original 1000 veces. Esto es lo que podría haber pasado. La línea azul en la Figura 6.23 muestra dónde se ubica la diferencia observada sobre el eje x.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigura 6.23: Histograma de diferencias de medias simuladas en una prueba de aleatorización\n\n\n\n\n¿Qué pensás? ¿Podría la diferencia representada por la línea azul haber sido causada por azar? Mi respuesta es: probablemente no. El histograma nos muestra la ventana del azar. La línea azul no está dentro de esa ventana. Esto significa que podemos estar bastante seguros de que la diferencia que observamos no se debe al azar.\nEstamos mirando otra ventana del azar. Estamos viendo un histograma de los tipos de diferencias de medias que podrían haber ocurrido en nuestro experimento, si hubiéramos asignado a los sujetos al grupo con chicle y al grupo sin chicle de otra forma.\nComo podés ver, las diferencias de medias varían de negativas a positivas. La diferencia más frecuente es 0. Además, la distribución parece simétrica en torno al cero, lo que muestra que teníamos aproximadamente las mismas probabilidades de obtener una diferencia positiva o negativa. También notá que a medida que las diferencias se hacen más grandes (ya sea en dirección positiva o negativa), se vuelven menos frecuentes. La línea azul nos muestra la diferencia observada, la que encontramos en nuestro experimento inventado. ¿Dónde está? Muy a la derecha. Está bien fuera del histograma. En otras palabras, cuando miramos lo que podría haber pasado, vemos que lo que pasó realmente no ocurre muy seguido.\nIMPORTANTE: En este caso, cuando hablamos de lo que podría haber pasado, nos referimos a lo que podría haber pasado por azar. Cuando comparamos lo que pasó con lo que el azar podría haber producido, podemos tener una mejor idea de si nuestro resultado fue o no causado por azar.\nOK, ahora vamos a fingir que obtuvimos una diferencia de medias mucho más pequeña cuando ejecutamos el experimento. Podemos trazar nuevas líneas (azul y roja) para representar una media más pequeña que podríamos haber encontrado.\n\n\n\n\n\nFigura 6.24: ¿Esperarías que una diferencia de medias representada por la línea azul ocurra más o menos seguido por azar, en comparación con la diferencia representada por la línea roja?\n\n\n\n\nFijate en la línea azul de la Figura 6.24. Si encontraras una diferencia de medias de 10, ¿te convencería de que esa diferencia no fue causada por azar? Como podés ver, la línea azul está dentro de la ventana del azar. Aunque las diferencias de +10 no ocurren muy seguido, podrías inferir que probablemente no fueron causadas por azar (pero tal vez te mantendrías algo escéptico, porque sí podrían haberlo sido).\n¿Y qué pasa con la línea roja? La línea roja representa una diferencia de +5. Si encontraras una diferencia de +5 acá, ¿estarías seguro de que no fue causada por azar? Yo no lo estaría. La línea roja está completamente dentro de la ventana del azar —este tipo de diferencia ocurre con bastante frecuencia. Necesitaría más evidencia para considerar la afirmación de que alguna variable independiente realmente causó la diferencia. Me sentiría mucho más cómodo asumiendo que la diferencia fue probablemente causada por error muestral.\n\n\n\n6.6.2 Lo que nos queda hasta ahora\n¿Notaste que hasta ahora no usamos ninguna fórmula, pero aun así pudimos hacer estadística inferencial? Veremos algunas fórmulas más adelante, pero las ideas detrás de esas fórmulas son más importantes que las fórmulas mismas.\nLa estadística inferencial es un intento de resolver el problema: ¿De dónde provienen mis datos? En el ejemplo de la prueba de aleatorización, nuestra pregunta fue: ¿De dónde provienen las diferencias entre las medias de mis datos?\nSabemos que esas diferencias podrían haber sido producidas solo por azar. Simulamos lo que puede hacer el azar usando aleatorización. Luego trazamos lo que puede hacer el azar usando un histograma. Y después usamos esa imagen para ayudarnos a hacer una inferencia: ¿La diferencia observada proviene de la distribución del azar, o no?\nCuando la diferencia observada está claramente dentro de la distribución del azar, entonces podemos inferir que nuestra diferencia podría haber sido producida por azar. Cuando la diferencia observada no está claramente dentro de la distribución del azar, entonces podemos inferir que nuestra diferencia probablemente no fue producida por azar.\nEn mi opinión, estas imágenes son muy, muy útiles. Si uno de nuestros objetivos es ayudarnos a resumir un montón de números complicados para llegar a una inferencia, entonces las imágenes hacen un excelente trabajo. Ni siquiera necesitamos un número resumen: solo necesitamos mirar la imagen y ver si la diferencia observada está dentro o fuera de la ventana del azar. De eso se trata todo esto. Crear formas intuitivas y significativas de hacer inferencias a partir de nuestros datos. A medida que avancemos, lo principal que haremos será formalizar nuestro proceso y hablar más sobre las estadísticas inferenciales “estándar”.\nPor ejemplo, en lugar de mirar una imagen (aunque eso es algo útil), vamos a construir algunos números útiles. Por ejemplo: ¿y si quisieras saber cuál es la probabilidad de que tu diferencia haya sido producida por azar? Esa podría ser un número único, como el 95%. Si hubiera una probabilidad del 95% de que el azar pueda producir la diferencia que observaste, tal vez no estés muy confiado de que tu manipulación experimental fue la causa. Si hubiera solo un 1% de probabilidad de que el azar pueda producir esa diferencia, entonces podrías tener más confianza en que el azar no produjo la diferencia; y podrías, en cambio, estar cómodo con la posibilidad de que tu manipulación experimental fue realmente la causa.\nEntonces, ¿cómo podemos llegar a esos números? Para llegar a eso, vamos a presentarte algunas herramientas más fundamentales para la inferencia estadística."
  },
  {
    "objectID": "06-ttests.html",
    "href": "06-ttests.html",
    "title": "7  Pruebas t",
    "section": "",
    "text": "7.1 Revisá tu confianza en el promedio\nVolvamos atrás en el tiempo. William Sealy Gosset consiguió trabajo en la cervecería Guinness. Sí, la que hace la famosa stout irlandesa. Lo que pasó después fue más o menos así (lo estoy inventando, pero la idea general es cierta).\nGuinness quería que todas sus cervezas fueran las mejores. Nada de errores, nada de birra mala. Querían mejorar el control de calidad para que, sin importar en qué parte del mundo sirvieras una Guinness, siempre saliera perfecta: 5 estrellas sobre 5, siempre.\nTenían catadores expertos. Cada vez que probaban una Guinness que no era 5 de 5, lo notaban al instante.\nPero había un gran problema. Hacían un barril de cerveza, y querían saber si cada pinta que saliera de ahí iba a ser perfecta. Así que los catadores se tomaban pinta tras pinta del barril… hasta que se acababa. Algunos barriles salían bien, otros no. Guinness tenía que arreglar eso. Pero el mayor problema era que, después del testeo, no quedaba cerveza para vender —se la habían tomado toda (recordá que esta parte me la estoy inventando para ilustrar el punto; seguro algo les quedaba para vender).\nGuinness tenía un problema de población y muestreo. Querían saber si toda la población de cervezas producidas era de 5 estrellas. Pero si probaban toda la población, no les quedaba nada para vender.\nAhí entra William Sealy Gosset. Gosset resolvió el problema. Se hizo preguntas como:\nGosset resolvió esas preguntas e inventó algo llamado la prueba t de Student. Como trabajaba para Guinness, podía ser despedido si divulgaba secretos comerciales (como la prueba t). Pero igual publicó su trabajo, bajo un seudónimo (Student 1908). Se hizo llamar “Student”, y por eso el test se llama prueba t de Student. Ahora ya sabés el resto de la historia.\nY resultó ser algo muy útil. La prueba t se usa todo el tiempo. Y se usa porque sirve. En este capítulo vamos a ver cómo funciona.\nTe va a sorprender saber que lo que ya vimos antes (el test de Crump, el test de aleatorización) es muy parecido a la prueba t. Así que ya venís pensando en lo necesario para entender las pruebas t. Seguramente te estás preguntando qué es eso de \\(t\\), qué significa. Ya vamos a contarte. Pero antes, una idea más.\nYa hablamos de tomar una muestra de datos. Sabemos cómo calcular la media, la desviación estándar, y cómo ver los datos en un histograma. Todo eso nos ayuda a entender mejor nuestras muestras.\nCapaz pensás que la media y la desviación estándar son cosas muy distintas, que no deberían combinarse. La media te dice dónde están la mayoría de los datos, y la desviación estándar cuánto varían. Sí, son distintas. Pero combinarlas puede darnos información útil.\nSi te digo que mi media muestral es 50, y no te digo nada más, ¿cuán confiado estás de que los números están cerca de 50? ¿Te preguntás si hay mucha variabilidad, si algunos valores están lejos? Deberías preguntártelo. La media sola no dice cuánto representa al resto de los datos.\nPuede ser representativa, si la desviación estándar es chica. O no, si la desviación es grande. Para confiar en la media, necesitás saber cuánta variabilidad hay.\n¿Cómo combinamos media y desviación para obtener una medida de confianza?\nPodemos usar un cociente:\n\\[\\frac{\\text{media}}{\\text{desviación estándar}}\\]\nVeamos qué pasa:\n\\[\\frac{\\text{número}}{\\text{mismo número}} = 1\\]\n\\[\\frac{\\text{número}}{\\text{número chico}} = \\text{número grande}\\]\n\\[\\frac{\\text{número}}{\\text{número grande}} = \\text{número chico}\\]\nSi la media es 50 y la desviación es 1:\n\\[\\frac{50}{1} = 50\\]\nSi la desviación es 100:\n\\[\\frac{50}{100} = 0.5\\]\nCuando la desviación es baja, el cociente es grande: más confianza. Si la desviación es alta, el cociente es chico: menos confianza. Así que esta proporción nos da una idea de cuán confiable es la media.\nEn resumen: creamos una estadística descriptiva dividiendo la media por la desviación estándar. Y tenemos una forma de interpretarla. Si el resultado es grande, la media representa bien a los datos. Si es chico, no tanto. Este tipo de razón es clave en estadística: compara lo que sabemos con lo que no sabemos.\nCasi todo lo que viene en este libro se basa en esta lógica. Las fórmulas siguen este esquema:\n\\[\\text{estadístico} = \\frac{\\text{medida de lo que sabemos}}{\\text{medida de lo que no sabemos}}\\]\no también:\n\\[\\text{estadístico} = \\frac{\\text{efecto}}{\\text{error}}\\]\nY esta es, justamente, la fórmula general dla prueba t. ¡Sorpresa!",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Pruebas t</span>"
    ]
  },
  {
    "objectID": "06-ttests.html#prueba-t-de-una-muestra-una-nueva-prueba-t",
    "href": "06-ttests.html#prueba-t-de-una-muestra-una-nueva-prueba-t",
    "title": "7  Pruebas t",
    "section": "7.2 Prueba t de una muestra: una nueva prueba t",
    "text": "7.2 Prueba t de una muestra: una nueva prueba t\nAhora sí, hablemos de las pruebas t. Vamos a ver tres tipos. Empezamos con la prueba t de una muestra.\nEste test se usa para estimar la probabilidad de que tu muestra venga de una población específica. En particular, querés saber si la media de tu muestra podría venir de una población con cierta media.\nAcá empieza lo confuso (y todavía no te dije la fórmula). En teoría, la prueba t usa parámetros conocidos de la población, como su media y su desviación. Pero en la práctica, casi nunca conocés esos valores. Así que tenés que estimarlos a partir de la muestra.\nRecordá lo que vimos sobre estadística descriptiva y muestreo: la media muestral es un estimador insesgado de la media poblacional. Y la desviación muestral (la que divide por n–1) también lo es. Gosset se dio cuenta de que podía usar estas estimaciones para construir la prueba t.\nAcá va la fórmula, primero en palabras:\n\n7.2.1 Fórmulas de la prueba t de una muestra\n\\[\\text{estadístico} = \\frac{\\text{efecto}}{\\text{error}}\\]\n\\[t = \\frac{\\text{efecto}}{\\text{error}}\\]\n\\[t = \\frac{\\text{diferencia de medias}}{\\text{error estándar}}\\]\n\\[t = \\frac{\\bar{X}-\\mu}{S_{\\bar{X}}}\\]\n\\[t = \\frac{\\text{Media muestral - Media poblacional}}{\\text{Error estándar muestral}}\\]\n\\[\\text{Error estándar estimado} = \\text{Error estándar de la muestra} = \\frac{s}{\\sqrt{N}}\\]\ndonde s es la desviación estándar de la muestra.\nCapaz que todo esto te hizo bizquear un poco. Tranquilo, ya lo vimos antes, cuando dividimos la media por la desviación estándar. La prueba t no es más que la media muestral dividida por el error estándar de esa media. Eso es todo.\n\n\n7.2.2 ¿Qué representa t?\n\\(t\\) nos da una medida de confianza, igual que la razón anterior en la que dividíamos la media por la desviación estándar. La única diferencia es que ahora dividimos por el error estándar de la media (que, recordá, también es una desviación estándar: la de la distribución muestral de la media).\n\n\n\n\n\n\nNote\n\n\n\n¿La t en prueba t significa algo? Aparentemente no. Gosset originalmente usó z. Y después Fisher lo llamó t, tal vez porque viene después de s, que suele usarse para la desviación estándar muestral.\n\n\n\\(t\\) es una propiedad de los datos que recolectás. Se calcula con la media de la muestra y su error estándar (y también con la media poblacional, a la que llegamos enseguida). Por eso se dice que t es una estadística muestral: se calcula a partir de la muestra. ¿Qué valores deberíamos esperar obtener para esos t? ¿Cómo podríamos saberlo?\nEmpecemos con un ejemplo simple. Imaginá que tu media muestral es 5, y querés saber si viene de una población con media 5. ¿Cuál sería el valor de t? Cero: restamos 5 - 5 = 0. Como el numerador es 0, t también lo es. Así que t = 0 ocurre cuando no hay diferencia. Ahora tomás otra muestra. ¿La media será siempre 5? Probablemente no. Supongamos que da 6. ¿Qué valor tendrá t? El numerador es positivo: 6 - 5 = +1. Pero t no necesariamente será 1, eso depende del error estándar. Si es 1, entonces t será 1, porque 1/1 = 1. Si el error estándar es menor que 1, ¿qué pasa con t? Aumenta. Por ejemplo, 1 dividido por 0.5 da 2. Si el error estándar fuera 0.5, entonces t sería 2. Y con ese valor podríamos tener más confianza en que la diferencia que observamos no es casual.\n¿Puede t ser menor que 1? Claro. Si el error estándar es grande, digamos 2, entonces t será más chico (1/2 = 0.5). La dirección de la diferencia entre la media muestral y la poblacional también puede hacer que t sea negativo. Si la media muestral fuera 4, entonces la diferencia es negativa, y t también lo será, porque el numerador es negativo. El denominador (el error estándar) siempre será positivo (acordate por qué: se calcula a partir de la desviación estándar de la muestra, que siempre es positiva porque usamos cuadrados al calcularla). Así que estas son algunas intuiciones sobre los valores posibles de t. Puede ser positivo o negativo, grande o chico.\nHagamos una cosa más para afinar la intuición. ¿Qué tal si generamos varias muestras, calculamos la media muestral y el error estándar de la media, y graficamos esas dos cosas entre sí? Así vamos a ver cómo varía típicamente la media con respecto al error estándar.\nEn la Figure 7.1, tomé 1.000 muestras de tamaño $N = 10$ de una distribución normal (media = 0, desviación estándar = 1). En cada caso, calculé la media y el error estándar. Eso nos da dos estadísticas descriptivas por muestra, y podemos graficarlas como puntos en un scatterplot.\n\nmedia_muestral &lt;- length(1000)\ndesvío_muestral &lt;- length(1000)\n\nfor (i in 1:1000) {\n  s &lt;- rnorm(10, 0, 1)\n  media_muestral[i] &lt;- mean(s)\n  desvío_muestral[i] &lt;- sd(s) / sqrt(length(s))\n}\n\nplot(media_muestral, desvío_muestral)\n\n\n\n\n\n\n\nFigure 7.1: Diagrama de dispersión con la media muestral en el eje x y el error estándar en el eje y\n\n\n\n\n\nLo que obtenemos es una nube de puntos. Podés notar que tiene una forma más o menos circular. Hay más puntos en el centro, y menos a medida que se alejan. La nube muestra el rango típico de las medias muestrales: la mayoría está entre -1 y 1. El error estándar también varía entre aproximadamente 0.2 y 0.5. Recordá que cada punto representa una muestra.\nPodemos ver estos datos de otra forma. En lugar de graficar directamente las medias y errores, podemos dividir la media de cada muestra por su error estándar. La Figure 7.2 muestra un histograma de esos valores.\n\n\n\n\n\n\n\n\nFigure 7.2: Histograma de las medias muestrales divididas por sus errores estándar: una distribución t.\n\n\n\n\n\nInteresante, el histograma tiene forma de campana normal. Está centrado en 0, que es el valor más común. A medida que los valores se alejan de 0, son menos frecuentes. Si recordás, nuestra fórmula para t era la media dividida por el error estándar. Eso es exactamente lo que hicimos acá. Este histograma te está mostrando una distribución t.\n\n\n7.2.3 Calcular t a partir de datos\nProbemos calcular un valor de t con una muestra chica. Supongamos que 10 estudiantes hicieron un test de verdadero/falso con 5 preguntas. Hay un 50% de chance de acertar cada una al azar.\nTodos completan el test, corregimos, y calculamos el desempeño (porcentaje de respuestas correctas). Queremos saber si estuvieron adivinando. Si fue así, el promedio debería rondar el 50%, que sería igual a azar. Veamos la tabla @tbl-tsmall.\n\n\n\n\nTable 7.1: Cálculo del valor t para un test de una muestra.\n\n\n\n\n\n\n\n\n\n\n\n\n\nestudiantes\npuntajes\npromedio\nDiferencia_de_la_media\nDesvíos_al_cuadrado\n\n\n\n\n1\n50\n61\n-11\n121\n\n\n2\n70\n61\n9\n81\n\n\n3\n60\n61\n-1\n1\n\n\n4\n40\n61\n-21\n441\n\n\n5\n80\n61\n19\n361\n\n\n6\n30\n61\n-31\n961\n\n\n7\n90\n61\n29\n841\n\n\n8\n60\n61\n-1\n1\n\n\n9\n70\n61\n9\n81\n\n\n10\n60\n61\n-1\n1\n\n\nSumas\n610\n610\n0\n2890\n\n\nMedias\n61\n61\n0\n289\n\n\n\n\n\nDE\n17.92\n\n\n\n\n\nEEM\n5.67\n\n\n\n\n\nt\n1.94003527336861\n\n\n\n\n\n\n\n\nPodés ver que la columna puntajes tiene los puntajes de cada uno de los 10 estudiantes. Hicimos todo lo necesario para calcular la desviación estándar. Recordá que la desviación estándar muestral es la raíz cuadrada de la varianza muestral, o sea:\n\\(\\text{desviación estándar} = \\sqrt{\\frac{\\sum_{i}^{n}({x_{i}-\\bar{x})^2}}{N-1}}\\)\n\\(DE = \\sqrt{\\frac{2890}{10-1}} = 17.92\\)\nEl error estándar de la media se obtiene dividiendo la desviación estándar por la raíz de N:\n\\(EEM = \\frac{s}{\\sqrt{N}} = \\frac{17.92}{10} = 5.67\\)\nt es la diferencia entre nuestra media muestral (61) y la media poblacional (50, si asumimos azar), dividida por el error estándar:\n\\(t = \\frac{\\bar{X}-\\mu}{S_{\\bar{X}}} = \\frac{61-50}{5.67} = 1.94\\)\nY así se calcula t a mano. Es un embole. A mí ya me molestó hacerlo así. En el laboratorio vas a aprender a calcular t con software, que te lo da de una. Por ejemplo, en R solo tenés que escribir:\n\nt.test(puntajes, mu=50)\n\n\n    One Sample t-test\n\ndata:  puntajes\nt = 1.9412, df = 9, p-value = 0.08415\nalternative hypothesis: true mean is not equal to 50\n95 percent confidence interval:\n 48.18111 73.81889\nsample estimates:\nmean of x \n       61 \n\n\n\n\n7.2.4 ¿Cómo se comporta t?\nSi t es solo un número que podemos calcular a partir de una muestra (y lo es), ¿qué hacemos con él? ¿Cómo usamos t para inferencia estadística?\nVolvamos al capítulo sobre muestreo y distribuciones. Ahí hablamos de la distribución muestral de la media. Generamos muchas muestras, calculamos la media de cada una, y armamos un histograma de esas medias. Más adelante dijimos que podíamos hacer eso con cualquier estadístico. Para cada muestra podíamos calcular la media, la desviación estándar, el error estándar, e incluso t. Podríamos generar 10.000 muestras y hacer cuatro histogramas, uno por cada estadístico. Eso fue exactamente lo que hice, y los resultados están en los cuatro paneles de la Figure 7.3. Usé un tamaño de muestra de 20, y tomé observaciones aleatorias de una distribución normal, con media = 0, y la desviación estándar = 1. Veamos las distribuciones muestrales de cada estadístico. t se calculó asumiendo que la media poblacional es 0.\n\nall_df &lt;- data.frame()\nfor (i in 1:10000) {\n  muestra &lt;- rnorm(20, 0, 1)\n  media_muestral&lt;- mean(muestra)\n  desvío_muestral &lt;- sd(muestra)\n  muestra_ee &lt;- sd(muestra) / sqrt(length(muestra))\n  muestra_t &lt;- as.numeric(t.test(muestra, mu = 0)$statistic)\n  t_df &lt;- data.frame(i, media_muestral, desvío_muestral, muestra_ee, muestra_t)\n  all_df &lt;- rbind(all_df, t_df)\n}\n\nlibrary(ggpubr)\na &lt;- ggplot(all_df, aes(x = media_muestral)) +\n  geom_histogram(color = \"white\") +\n  theme_classic(base_size=20)\nb &lt;- ggplot(all_df, aes(x = desvío_muestral)) +\n  geom_histogram(color = \"white\") +\n  theme_classic(base_size=20)\nc &lt;- ggplot(all_df, aes(x = muestra_ee)) +\n  geom_histogram(color = \"white\") +\n  theme_classic(base_size=20)\nd &lt;- ggplot(all_df, aes(x = muestra_t)) +\n  geom_histogram(color = \"white\") +\n  theme_classic(base_size=20)\n\nggarrange(a, b, c, d,\n          ncol = 2, nrow = 2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nFigure 7.3: Distribuciones muestrales de la media, la desviación estándar, el error estándar de la media, y \\(t\\).\n\n\n\n\n\nVemos cuatro distribuciones muestrales. Así es como se comportan los resúmenes estadísticos de muestras. Antes hablamos de ventanas de azar —estas son cuatro ventanas de azar, que miden distintos aspectos de una muestra. En este caso, todas las muestras provienen de la misma distribución normal. Pero por el error muestral, cada muestra es diferente. Las medias no son idénticas, las desviaciones tampoco, los errores estándar tampoco, y los valores de t tampoco. Todas varían, como muestran los histogramas. Así se comportan las muestras de tamaño 20.\nPodemos ver de entrada que es poco probable obtener una media muestral de 2. Está muy fuera del rango típico. La distribución de las medias va más o menos de -0.5 a +0.5, y está centrada en 0 (¡justo la media poblacional, mirá vos!).\nTambién es poco probable obtener desviaciones estándar entre 0.6 y 1.5; ese es el rango típico específico para la desviación estándar muestral.\nLo mismo pasa con el error estándar de la media: el rango es más chico, entre 0.1 y 0.3. Rara vez vas a encontrar una muestra con error estándar mayor a 0.3. Prácticamente nunca uno de 1 (en esta situación).\nY ahora mirá los valores de t. El rango va más o menos de -3 a +3. Los valores 3 apenas aparecen. Casi nunca vas a ver un 5 o un -5 acá.\nTodas estas ventanas de azar se pueden usar como usamos las distribuciones muestrales antes (por ejemplo, en el test de Crump o el test de aleatorización), para hacer inferencias estadísticas. El proceso es el mismo para todas:\n\nGenerar estas distribuciones\nMirar tus estadísticos muestrales (media, SD, SEM, y t)\nCalcular la probabilidad de obtener ese valor de t o uno más extremo\nObtener esa probabilidad\nEvaluar si tus estadísticas muestrales parecen probables o improbables\n\nEnseguida vamos a formalizar todo esto. Por ahora, quiero que veas que esto que vas a hacer es algo que ya hiciste antes. Por ejemplo, en el test de Crump y el test de aleatorización, nos enfocamos en la distribución de diferencias de medias. Podríamos hacer lo mismo acá, pero esta vez nos vamos a centrar en la distribución de valores t. Después aplicamos las mismas reglas de decisión que usamos con las otras distribuciones.\nAbajo vas a ver un gráfico que ya viste antes, pero ahora muestra una distribución de t, no de diferencias de medias. Recordá: si obtenemos un único valor t a partir de una muestra que recolectamos, podemos consultar la ventana de azar en Figure 7.4 para ver si ese valor era esperable por azar o no.\n\nmuestra_t &lt;- all_df$muestra_t\n\nggplot(all_df, aes(x = muestra_t)) +\n  annotate(\n    \"rect\",\n    xmin = min(muestra_t),\n    xmax = max(muestra_t),\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.5,\n    fill = \"red\"\n  ) +\n  annotate(\n    \"rect\",\n    xmin = min(muestra_t),\n    xmax = -1.94,\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.7,\n    fill = \"light grey\"\n  ) +\n  annotate(\n    \"rect\",\n    xmin = 1.94,\n    xmax = max(muestra_t),\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.7,\n    fill = \"light grey\"\n  ) +\n  geom_rect(aes(\n    xmin = -Inf,\n    xmax = min(muestra_t),\n    ymin = 0,\n    ymax = Inf\n  ),\n  alpha = .5,\n  fill = \"lightgreen\") +\n  geom_rect(aes(\n    xmin = max(muestra_t),\n    xmax = Inf,\n    ymin = 0,\n    ymax = Inf\n  ),\n  alpha = .5,\n  fill = \"lightgreen\") +\n  geom_histogram(bins = 50, color = \"white\") +\n  theme_classic() +\n  geom_vline(xintercept = min(muestra_t)) +\n  geom_vline(xintercept = max(muestra_t)) +\n  geom_vline(xintercept = -1.94) +\n  geom_vline(xintercept = 1.94) +\n  xlim(-8, 8) +\n  geom_label(data = data.frame(x = 0, y = 250, label = \"AZAR\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = -7, y = 250, label = \"NO\\nAZAR\"),\n             aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = 7, y = 250, label = \"NO\\nAZAR\"),\n             aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = -4, y = 250,\n                               label = \"?\"),\n             aes(x = x, y = y, label = label)) +\n  xlab(\"media de muestra_t\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\nFigure 7.4: Aplicación de criterios de decisión a la distribución t. Histograma de valores t de muestras (n = 20) tomadas de una misma distribución normal (μ = 0, sd = 1)\n\n\n\n\n\n\n\n7.2.5 Tomando una decisión\nVolvamos a nuestro ejemplo del test de verdadero/falso. Ahora podemos tomar una decisión sobre qué pasó ahí. Encontramos una diferencia de medias de 11. El valor de t fue 1.9411765. La probabilidad de obtener ese valor de t o uno más extremo es \\(p\\) = 0.0841503.\nEstábamos probando si nuestra media muestral de 61 podría venir de una distribución normal con media = 50. El test t nos dice que obtener ese t, o uno más grande, pasaría con probabilidad \\(p\\) = 0.0841503. En otras palabras, el azar puede lograrlo, pero no muy seguido. En criollo: todos los estudiantes podrían haber estado adivinando… pero no es muy probable que haya sido solo eso.\nLa próxima prueba-t se llama prueba t de muestras pareadas. Y, spoiler: vamos a descubrir que en realidad es un prueba t de una muestra disfrazado (¡¿QUÉ?!). Sí. Si no entendiste el de una muestra, seguí leyendo.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Pruebas t</span>"
    ]
  },
  {
    "objectID": "06-ttests.html#prueba-t-de-muestras-pareadas",
    "href": "06-ttests.html#prueba-t-de-muestras-pareadas",
    "title": "7  Pruebas t",
    "section": "7.3 Prueba t de muestras pareadas",
    "text": "7.3 Prueba t de muestras pareadas\nPara mí (Crump), muchos análisis terminan siendo una prueba t de muestras pareadas. Simplemente porque muchas de las cosas que hago se reducen a eso.\nSoy psicólogo cognitivo. Hago investigación sobre cómo la gente recuerda, presta atención y aprende habilidades. Hay muchos psicólogos como yo que hacen cosas parecidas. Hacemos experimentos muy similares. Son lo que se llama diseños de medidas repetidas. Se llaman así porque medimos cómo una persona hace algo más de una vez. Repetimos la medición. Por ejemplo, puedo medir cómo alguien realiza una tarea en la condición A, y luego ver cómo esa misma persona la hace en la condición B. Y veo que hace cosas distintas. Estoy midiendo repetidamente a la misma persona en ambas condiciones. Lo que me interesa es si la manipulación experimental cambia algo en cómo la persona realiza la tarea.\n\n7.3.1 Mehr, Song y Spelke (2016)\nVamos a introducir la prueba t de muestras pareadas con un ejemplo real, de un estudio real. Mehr, Song, and Spelke (2016) investigaron si cantarles canciones a los bebés los ayuda a ser más sensibles a las señales sociales. Por ejemplo, los bebés tienen que aprender a dirigir su atención hacia las personas como parte de aprender a interactuar socialmente. Tal vez cantarle a un bebé ayuda con ese proceso. Cuando un bebé escucha una canción familiar, puede empezar a prestar más atención a la persona que la canta, incluso después de que termina. Esa persona puede volverse más importante para el bebé en términos sociales. Vas a aprender más sobre este estudio en el laboratorio de esta semana. Este ejemplo te prepara para las actividades de ese laboratorio. Acá va un resumen breve de lo que hicieron.\nPrimero, los padres fueron entrenados para cantarle una canción a sus bebés. Luego de varios días de cantarles esa canción, los padres fueron al laboratorio con sus bebés. En la primera sesión, se sentaron con sus hijos en las rodillas, de forma que los bebés pudieran mirar dos videos. En cada video aparecían dos personas nuevas que el bebé nunca había visto. Cada una de esas personas (los cantantes) les cantaba una canción al bebé. Uno de los cantantes cantaba la canción familiar, que el bebé ya conocía por sus padres. El otro cantaba una canción no familiar, que el bebé nunca había oído.\nHubo dos fases de medición importantes: la fase de línea base (baseline) y la fase de prueba (test).\nLa fase de línea base ocurrió antes de que los bebés vieran y oyeran a los cantantes. Durante esta fase, los bebés miraban un video con ambos cantantes en simultáneo. Los investigadores registraban qué proporción del tiempo el bebé miraba a cada uno. Esta fase servía para ver si los bebés ya tenían una preferencia por alguno de los dos (antes de escucharlos cantar).\nLa fase de prueba fue después de que los bebés vieron y oyeron a cada cantante. En esta fase, los bebés veían videos mudos de los mismos cantantes. Se medía la proporción de tiempo que pasaban mirando a cada uno. La pregunta era: ¿los bebés mirarían más tiempo al cantante que había cantado la canción familiar, en comparación con el otro?\nHay más de una forma de describir el diseño del estudio, pero lo vamos a presentar así: fue un diseño de medidas repetidas, con una variable independiente (la manipulación), llamada fase de visualización: línea base vs. prueba. Hubo una variable dependiente (la medición), que fue la proporción de tiempo mirando al cantante que había cantado la canción familiar. Fue un diseño de medidas repetidas porque esa proporción se midió dos veces: una en la línea base (antes de que los bebés oyeran a los cantantes), y otra en la prueba (después de que los oyeran).\nLa pregunta clave era si los bebés cambiarían su tiempo de mirada, mirando más tiempo al cantante de la canción familiar durante la fase de prueba que en la línea base. Es una pregunta sobre un cambio dentro de cada bebé. En general, los posibles resultados eran:\n\nSin cambio: la diferencia entre el tiempo de mirada al cantante de la canción familiar durante línea base y prueba es cero.\nCambio positivo: los bebés miran más tiempo al cantante de la canción familiar durante la fase de prueba (después de haberlo visto y escuchado), comparado con la fase de línea base (antes de oírlos). Esto da una diferencia positiva si usamos la fórmula: Tiempo de mirada en prueba − Tiempo de mirada en línea base (al cantante de la canción familiar).\nCambio negativo: los bebés miran más tiempo al cantante de la canción no familiar durante la fase de prueba (después de haberlos escuchado), en comparación con la línea base (antes de oírlos). Esto da una diferencia negativa con la misma fórmula: Tiempo de mirada en prueba − Tiempo de mirada en línea base (al cantante de la canción familiar).\n\n\n\n7.3.2 Los datos\nVeamos los datos de los primeros 5 bebés del estudio. Esto nos va a ayudar a entender mejor algunas propiedades de los datos antes de analizarlos. Vamos a ver que los datos están estructurados de forma que podemos aprovechar con una prueba t de muestras pareadas. (Ojo: miramos solo los primeros 5 bebés para mostrar cómo funcionan los cálculos. El resultado final dla prueba t cambia si usamos todos los datos del estudio).\nAcá está la tabla con los datos:\n\n\n\n\n\ninfant\nLínea_de_base\nTest\n\n\n\n\n1\n0.44\n0.60\n\n\n2\n0.41\n0.68\n\n\n3\n0.75\n0.72\n\n\n4\n0.44\n0.28\n\n\n5\n0.47\n0.50\n\n\n\n\n\nLa tabla muestra la proporción de tiempo mirando al cantante de la canción familiar durante las fases de línea base y prueba. Fijate que hay cinco bebés (del 1 al 5). Cada uno fue medido dos veces: una en la fase de línea base y otra en la de prueba. Como dijimos antes, esto es un diseño de medidas repetidas, porque se mide a cada bebé más de una vez (en este caso, dos). También se le llama diseño de muestras pareadas. ¿Por qué? Porque cada participante aporta un par de mediciones (una por cada condición del diseño).\nBuenísimo, entonces… ¿qué nos interesa en realidad? Queremos saber si el tiempo medio de mirada al cantante de la canción familiar en la fase de prueba es mayor que en la fase de línea base. Estamos comparando las dos medias muestrales entre sí, buscando una diferencia. Ya sabemos que esas diferencias pueden surgir por azar, simplemente por el hecho de que tomamos dos muestras, y que las muestras naturalmente pueden diferir. Así que lo que queremos saber es si es probable o improbable que la diferencia observada se deba al azar.\nEn otras palabras, nos interesa mirar los valores de diferencia entre línea base y prueba para cada bebé. La pregunta es: ¿para cada bebé, aumentó su proporción de mirada al cantante de la canción familiar en la fase de prueba comparado con la fase de línea base?\n\n\n7.3.3 Los puntajes de diferencia\nAgreguemos los puntajes de diferencia a la tabla para que sea más fácil ver de qué hablamos. El primer paso es decidir cómo vamos a calcular la diferencia. Hay dos opciones:\n\nPuntaje en fase de prueba − Puntaje en fase de línea base\nPuntaje en fase de línea base − Puntaje en fase de prueba\n\nVamos a usar la primera fórmula. ¿Por qué? Porque da diferencias positivas cuando el puntaje de la prueba es mayor que el de línea base. Eso hace que los puntajes positivos tengan sentido en este estudio: indican que el bebé miró más al cantante de la canción familiar después de escucharlo que antes.\n\npaired_sample_df &lt;- cbind(paired_sample_df, \n                          diferencias = (paired_sample_df$Test - \n                                           paired_sample_df$Línea_de_base))\nknitr::kable(paired_sample_df)\n\n\n\n\ninfant\nLínea_de_base\nTest\ndiferencias\n\n\n\n\n1\n0.44\n0.60\n0.16\n\n\n2\n0.41\n0.68\n0.27\n\n\n3\n0.75\n0.72\n-0.03\n\n\n4\n0.44\n0.28\n-0.16\n\n\n5\n0.47\n0.50\n0.03\n\n\n\n\n\nListo, ahora tenemos los puntajes de diferencia. Lo primero que podemos hacer es mirar esos valores y preguntar: ¿cuántos bebés mostraron el efecto que nos interesa? Específicamente: ¿cuántos tuvieron una diferencia positiva?\nPodemos ver que tres de los cinco bebés mostraron un puntaje positivo (miraron más al cantante de la canción familiar en la fase de prueba que en la de línea base), y dos bebés mostraron lo contrario (miraron más al cantante de la canción familiar durante la línea base que durante la prueba).\n\n\n7.3.4 La diferencia media\nComo venimos diciendo, el efecto de interés en este estudio es la diferencia media entre la proporción de tiempo de mirada en línea base y en prueba. Podemos calcularla como la media de los puntajes de diferencia. Vamos a hacerlo. De paso, para divertirnos, también vamos a calcular la media de los puntajes en línea base, los de prueba y los de diferencia.\n\npaired_sample_df &lt;- paired_sample_df %&gt;%\n   rbind(c(\"Sums\",colSums(paired_sample_df[1:5,2:4]))) %&gt;%\n   rbind(c(\"Means\",colMeans(paired_sample_df[1:5,2:4])))\n\nknitr::kable(paired_sample_df)\n\n\n\n\ninfant\nLínea_de_base\nTest\ndiferencias\n\n\n\n\n1\n0.44\n0.6\n0.16\n\n\n2\n0.41\n0.68\n0.27\n\n\n3\n0.75\n0.72\n-0.03\n\n\n4\n0.44\n0.28\n-0.16\n\n\n5\n0.47\n0.5\n0.03\n\n\nSums\n2.51\n2.78\n0.27\n\n\nMeans\n0.502\n0.556\n0.054\n\n\n\n\n\nPodemos ver que hubo una diferencia media positiva de 0.054 entre las fases de prueba y línea base. ¿Podemos apurarnos a concluir que los bebés se sienten más atraídos socialmente hacia personas que les cantaron una canción familiar? Espero que no, al menos no con una muestra tan pequeña. Primero, la diferencia en proporción de tiempo mirando no es muy grande, y por supuesto reconocemos que esa diferencia podría haberse producido por azar.\nVamos a evaluar más formalmente si esta diferencia pudo haber sido causada por el azar usando una prueba t de muestras pareadas. Pero antes, calculemos de nuevo t y pensemos qué nos dice t además de lo que ya nos dice la media de los puntajes de diferencia.\n\n\n7.3.5 Calcular t\nBueno, ¿cómo se calcula t para una prueba t de muestras pareadas? ¡Sorpresa! Usamos la misma fórmula dla prueba t de una muestra que ya aprendiste. Específicamente, usamos la fórmula dla prueba t de una muestra aplicada a los puntajes de diferencia. Tenemos una sola muestra de puntajes de diferencia (están en una sola columna), así que podemos usar la prueba t de una muestra. Lo que nos interesa es saber si la media de esos puntajes de diferencia podría provenir de una distribución con media 0. Esa distribución especial se llama la distribución nula (null distribution): es la distribución de la ausencia de diferencias. Claro que esta distribución puede generar diferencias pequeñas por error muestral, pero esas diferencias no están causadas por ninguna manipulación experimental, sino por el simple azar del muestreo.\nVamos a calcular t en un momento. Pero antes, pensemos de nuevo: ¿por qué queremos calcular t? ¿Por qué no quedarnos solo con la diferencia media?\nRecordá: toda la idea detrás de t es que nos da una indicación de cuánta confianza deberíamos tener en nuestra media. El valor de t se obtiene al dividir una medida de la media (el numerador) por una medida de variación (el error estándar, en el denominador). t será chico cuando la diferencia media es chica, o cuando la variación es grande. O sea, valores bajos de t nos dicen que no deberíamos estar muy seguros sobre esa estimación de diferencia. En cambio, cuando la diferencia media es grande y/o la variación es pequeña, obtenemos un valor alto de t. Eso nos da más confianza en la estimación.\nVamos a calcular t para los puntajes de diferencia. Usamos las mismas fórmulas de antes:\n\n\n\n\n\n\n\n\n\n\n\n\n\ninfant\nLinea_de_base\nTest\ndiferencias\ndiferencias_a_la_media\ndiferencias_al_cuadrado\n\n\n\n\n1\n0.44\n0.6\n0.16\n0.106\n0.011236\n\n\n2\n0.41\n0.68\n0.27\n0.216\n0.046656\n\n\n3\n0.75\n0.72\n-0.03\n-0.084\n0.00705600000000001\n\n\n4\n0.44\n0.28\n-0.16\n-0.214\n0.045796\n\n\n5\n0.47\n0.5\n0.03\n-0.024\n0.000575999999999999\n\n\nSumas\n2.51\n2.78\n0.27\n0\n0.11132\n\n\nMedias\n0.502\n0.556\n0.054\n0\n0.022264\n\n\n\n\n\n\nsd\n0.167\n\n\n\n\n\n\nSEM\n0.075\n\n\n\n\n\n\nt\n0.72\n\n\n\n\n\nSi hiciéramos este test usando R, obtendríamos casi los mismos números (hay un poco de redondeo en la tabla).\n\nt.test(diferencias, mu = 0)\n\n\n    One Sample t-test\n\ndata:  diferencias\nt = 0.72381, df = 4, p-value = 0.5092\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.1531384  0.2611384\nsample estimates:\nmean of x \n    0.054 \n\n\nAcá va una forma rápida de reportar los resultados dla prueba t: t(4) = .72, p = .509.\n¿Qué nos dice todo esto? Hay algunos detalles que todavía no abordamos mucho. Por ejemplo, ese 4 representa los grados de libertad, de eso hablaremos más adelante. Lo importante por ahora es que el valor de t empiece a tener un poco más de sentido. Obtuvimos un valor t bastante chico: 0.72. ¿Qué podemos decir a partir de eso? Primero, es positivo, así que sabemos que la diferencia media también lo fue. El signo del valor t siempre es el mismo que el de la diferencia media (en nuestro caso fue +0.054).\nTambién vemos que el valor p fue .509. Ya vimos valores p antes. Esto nos dice que obtener un valor de t igual o mayor al observado ocurre en aproximadamente un 50.9% de los casos… Bueno, en realidad significa algo más que eso. Y para entenderlo, tenemos que hablar de los conceptos de test bilateral (two-tailed) y unilateral (one-tailed).\n\n\n7.3.6 Interpretar valores t\nRecordá lo que estamos haciendo: estamos evaluando si nuestros datos muestrales podrían haber salido de una distribución específica. La distribución nula, de ausencia de diferencias. Esta es la distribución de valores t que ocurren con muestras de tamaño 5, con diferencia media igual a 0, y error estándar de la media de .075 (ese es el SEM que calculamos a partir de nuestra muestra).\nPodemos ver cómo luce esta distribución nula en Figure 7.5:\n\nrange &lt;- seq(-3, 3, .1)\nnull_distribution &lt;- dt(range, 4, log = FALSE)\nplot_df &lt;- data.frame(range, null_distribution)\n\nggplot(plot_df, aes(x = range, y = null_distribution)) +\n  geom_line() +\n  xlab(\"valores t\") +\n  ylab(\"Probabilidad\") +\n  theme(axis.text.y = element_blank(), axis.ticks = element_blank()) +\n  scale_x_continuous(breaks = seq(-3, 3, .5)) +\n  geom_label(data = data.frame(x = -.7, y = .1, label = \"50% \\n (-)\"),\n             aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = .7, y = .1, label = '50% \\n (+)'), aes(x = x, y = y, label = label)) +\n  geom_vline(xintercept = 0) +\n  theme_classic(base_size = 20)\n\n\n\n\n\n\n\nFigure 7.5: Una distribución de valores t que pueden ocurrir solo por azar, cuando no hay diferencia entre la muestra y la población\n\n\n\n\n\nLa distribución t de arriba muestra los valores que puede tomar t solo por azar, cuando medimos diferencias de medias en pares de muestras de tamaño 5 (como en nuestro caso). El valor más probable de t es cero, lo cual tiene sentido, porque estamos viendo la distribución de la ausencia de diferencias, que debería tener su mayor densidad en 0. Pero, a veces, por error muestral, pueden aparecer valores de t mayores o menores que cero. Notá que la distribución es simétrica: los valores t de la distribución nula son positivos la mitad de las veces, y negativos la otra mitad. Eso es justo lo que esperaríamos por azar.\nEntonces, ¿qué queremos saber cuando obtenemos un valor t específico de nuestra muestra? Queremos saber qué tan probable es que ocurra ese valor t solo por azar. Esta es una pregunta con sutilezas. Por ejemplo, un valor específico de t no tiene una única probabilidad de ocurrir. Cuando hablamos de probabilidades, nos referimos a rangos de valores.\nUsamos la letra p para referirnos a la probabilidad de que ocurran ciertos valores de t. Veamos algunos ejemplos:\n\n¿Cuál es la probabilidad de que t sea igual a cero, o distinto de cero? La respuesta es p = 1, o 100%. Siempre vamos a obtener un valor de t que es cero o no cero… Bueno, a menos que no podamos calcularlo (por ejemplo, si la desviación estándar no está definida), pero si lo podemos calcular, será positivo, negativo o cero.\n¿Cuál es la probabilidad de que t sea mayor o igual a 0? La respuesta es p = .5, o 50%. El 50% de los valores t son mayores o iguales a 0.\n¿Y de que t sea menor o igual a 0? También p = .5. La mitad de los valores están en esa región.\n\nPodemos responder estas preguntas solo mirando la distribución t y dividiéndola en dos regiones iguales: la mitad izquierda (50% de los valores t) y la mitad derecha (los otros 50%).\n¿Y si queremos un enfoque más fino? Por ejemplo, dividir en regiones del 10%. ¿Qué valores de t ocurren solo el 10% del tiempo? Podemos marcar líneas que dividan la distribución en regiones así. Notá que los valores extremos (más grandes o más chicos) son menos probables, entonces los intervalos más alejados del centro tienen que ser más anchos para contener el mismo 10%. Así se ve la figura Figure 7.6:\n\nrange &lt;- seq(-3, 3, .1)\nnull_distribution &lt;- dt(range, 4, log = FALSE)\nplot_df &lt;- data.frame(range, null_distribution)\nt_ps &lt;- qt(seq(.1, .9, .1), 4)\n\nggplot(plot_df, aes(x = range, y = null_distribution)) +\n  geom_line() +\n  xlab(\"valores t\") +\n  ylab(\"Probabilidad\") +\n  geom_vline(xintercept = t_ps) +\n  theme_classic(base_size = 15) +\n  theme(axis.text.y = element_blank(), axis.ticks = element_blank()) +\n  scale_x_continuous(breaks = round(t_ps, digits = 1)) +\n  geom_label(data = data.frame(x = -2.5, y = .2, label = \"10%\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = -1.3, y = .2, label = \"10%\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = 2.5, y = .2, label = \"10%\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = 1.3, y = .2, label = \"10%\"), aes(x = x, y = y, label = label))\n\n\n\n\n\n\n\nFigure 7.6: Dividiendo la distribución t en regiones que contienen cada una el 10% de los valores t. Los intervalos son más angostos cerca del centro, donde hay más valores t.\n\n\n\n\n\nConsiderá las probabilidades (p) de t para los distintos rangos:\n\nt ≤ -1.5 → p = 10%\n-1.5 ≥ t ≤ -0.9 → p = 10%\n-0.9 ≥ t ≤ -0.6 → p = 10%\nt ≥ 1.5 → p = 10%\n\nFijate que en todos los casos la probabilidad es del 10%. Los valores t ocurren en esos rangos con un 10% de probabilidad.\n\n\n7.3.7 Obtener valores p para valores t\nCapaz te estás preguntando de dónde saqué algunos de estos valores. Por ejemplo, ¿cómo sé que el 10% de los valores t en esta distribución nula son aproximadamente 1.5 o mayores? La respuesta es: se lo pedí a R. En la mayoría de los libros de estadística, te dirían: hay una tabla al final del libro donde podés buscar estos valores… Este libro no tiene esa tabla. Podríamos hacer una para vos. Y tal vez lo hagamos. Pero todavía no…\nEntonces, ¿de dónde salen esos valores y cómo podés saberlos? La respuesta complicada es que no vamos a explicar las matemáticas detrás de cómo se obtienen estos valores, porque:\n\nAlgunos autores de este libro admiten que no entienden bien esas matemáticas;\nNos desviaría mucho del foco del libro;\nVas a aprender cómo obtener estos valores en el laboratorio usando software;\nTambién vas a aprender cómo obtenerlos sin fórmulas, solo haciendo simulaciones;\nPodés hacerlo en R, en Excel, o usando esta calculadora online.\n\nEn resumen: podés encontrar los valores t y sus p asociados usando software. Pero el software no te explica qué significan esos valores. Eso es lo que estamos haciendo acá. Además, vas a ver que el software te pide algunos datos más: los grados de libertad del test, y si el test es unilateral o bilateral. Todavía no explicamos eso. Vamos a hacerlo ahora.\nOjo: vamos a explicar los grados de libertad al final. Primero, vamos con el test unilateral.\n\n\n7.3.8 Test unilateral (one-tailed)\nUn test unilateral (one-tailed test) también se llama a veces test direccional. Se le dice así porque el investigador puede tener una hipótesis en mente que sugiere que la diferencia entre medias va a tener una dirección específica: o bien una diferencia positiva, o bien una negativa.\nTípicamente, el investigador establece un criterio alfa (alpha criterion). Este criterio es como una línea en la arena para tomar decisiones. A menudo se fija en $p = .05$. ¿Qué significa eso? La figura Figure 7.7 muestra la distribución t con ese criterio alfa:\n\nrange &lt;- seq(-3, 3, .1)\nnull_distribution &lt;- dt(range, 4, log = FALSE)\nplot_df &lt;- data.frame(range, null_distribution)\n\nt_ps &lt;- qt(seq(.1, .9, .1), 4)\n\nggplot(plot_df, aes(x = range, y = null_distribution)) +\n  geom_line() +\n  xlab(\"valores t\") +\n  ylab(\"Probabilidad\") +\n  geom_vline(xintercept = qt(.95, 4, lower.tail = TRUE)) +\n  ggtitle(\"t crítico para test unilateral\") +\n  theme_classic(base_size = 10) +\n  theme(axis.text.y = element_blank(), axis.ticks = element_blank()) +\n  annotate(\n    \"rect\",\n    xmin = qt(.95, 4, lower.tail = TRUE),\n    xmax = 3,\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.5,\n    fill = \"green\"\n  ) +\n  geom_label(data = data.frame(x = 2.5, y = .2, label = \"5%\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.95, 4, lower.tail = TRUE),\n    y = .3,\n    label = \"t crítico\"\n  ),\n  aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.95, 4, lower.tail = TRUE),\n    y = .25,\n    label = round(qt(.95, 4, lower.tail = TRUE), digits = 2)\n  ),\n  aes(x = x, y = y, label = label))\n\n\n\n\n\n\n\nFigure 7.7: El valor crítico de t para un criterio alfa de 0.05. El 5% de todos los valores t son iguales o mayores a este valor.\n\n\n\n\n\nLa figura muestra que los valores t de +2.13 o más ocurren el 5% de las veces. Como la distribución t es simétrica, también sabemos que valores de –2.13 o menos ocurren el 5% de las veces. Ambas afirmaciones son ciertas bajo la distribución nula (sin diferencias). Esto significa que, cuando realmente no hay diferencias, un investigador puede esperar encontrar un valor t de 2.13 o mayor en el 5% de los casos.\nRecapitulemos y conectemos algunos conceptos:\n\nCriterio alfa: el criterio que define el investigador para decidir si cree que la diferencia se debe o no al azar. En este caso, es \\(p = .05\\).\nt crítico: el valor t asociado al criterio alfa. Para un test unilateral, es el valor tal que el 5% de todos los t están en ese valor o más. En nuestro ejemplo, t crítico = 2.13. El 5% de los valores t (con 4 grados de libertad) son iguales o mayores a 2.13.\nt observado: el valor t que calculaste a partir de tu muestra. En el ejemplo de los bebés, fue t(4) = 0.72.\nValor p: el valor p es la probabilidad de obtener el valor t observado o uno mayor. En nuestro ejemplo anterior, el valor p para t(4) = 0.72 fue p = .509. SIN EMBARGO, ese valor p no fue calculado para un test direccional (unilateral)… (hablamos más sobre qué significa ese .509 en la siguiente sección).\n\nLa Figure 7.8 muestra cómo se vería el valor p para t(4) = 0.72 en un test unilateral:\n\nrange &lt;- seq(-3, 3, .1)\nnull_distribution &lt;- dt(range, 4, log = FALSE)\nplot_df &lt;- data.frame(range, null_distribution)\n\nt_ps &lt;- qt(seq(.1, .9, .1), 4)\n\nggplot(plot_df, aes(x = range, y = null_distribution)) +\n  geom_line() +\n  xlab(\"valores t\") +\n  ylab(\"Probabilidad\") +\n  geom_vline(xintercept = .72) +\n  geom_vline(xintercept = qt(.95, 4, lower.tail = TRUE)) +\n  ggtitle(\"Valor t y región p para test unilateral\") +\n  theme_classic(base_size = 10) +\n  theme(axis.text.y = element_blank(), axis.ticks = element_blank()) +\n  annotate(\n    \"rect\",\n    xmin = .72,\n    xmax = 3,\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.5,\n    fill = \"grey\"\n  ) +\n  annotate(\n    \"rect\",\n    xmin = qt(.95, 4, lower.tail = TRUE),\n    xmax = 3,\n    ymin = 0,\n    ymax = .25,\n    alpha = 0.5,\n    fill = \"green\"\n  ) +\n  geom_label(data = data.frame(x = 2.5, y = .2, label = \"5%\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.95, 4, lower.tail = TRUE),\n    y = .3,\n    label = \"t crítico\"\n  ), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.95, 4, lower.tail = TRUE),\n    y = .25,\n    label = round(qt(.95, 4, lower.tail = TRUE), digits = 2)\n  ), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = .72, y = .1, label = \"t observado\"),\n             aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(x = .72, y = .05, label = \".72, p=\"),\n             aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = 1.5,\n    y = .05,\n    label = round(pt(.72, 4, lower.tail = FALSE), digits = 3)\n  ), aes(x = x, y = y, label = label))\n\n\n\n\n\n\n\nFigure 7.8: Un caso en el que el valor t observado es mucho menor que el valor crítico para un test t unilateral.\n\n\n\n\n\nVamos paso a paso. Marcamos en el gráfico el valor t observado (0.72). Sombreamos de gris toda la región a su derecha. Esa región gris representa un 25.6% de todos los valores t. En otras palabras, el 25.6% de los valores t son mayores o iguales a 0.72. Es decir, por azar, se puede obtener un valor de t igual o mayor a 0.72 en más de una de cada cuatro veces.\nY de hecho, eso fue lo que encontramos: un t de 0.72. Sabiendo que un valor así o mayor ocurre el 25.6% del tiempo por azar, ¿estarías confiado en que la diferencia media no se debe al azar? Probablemente no, ya que el azar puede producir ese valor bastante seguido.\n\nrange &lt;- seq(-4, 4, .1)\nnull_distribution &lt;- dt(range, 4, log = FALSE)\nplot_df &lt;- data.frame(range, null_distribution)\n\nt_ps &lt;- qt(seq(.1, .9, .1), 4)\n\nggplot(plot_df, aes(x = range, y = null_distribution)) +\n  geom_line() +\n  xlab(\"valores t\") +\n  ylab(\"Probabilidad\") +\n  geom_vline(xintercept = qt(.975, 4, lower.tail = TRUE)) +\n  geom_vline(xintercept = qt(.025, 4, lower.tail = TRUE)) +\n  ggtitle(\"Valores t críticos para test bilateral\") +\n  theme_classic(base_size = 10) +\n  theme(axis.text.y = element_blank(), axis.ticks = element_blank()) +\n  annotate(\n    \"rect\",\n    xmin = qt(.975, 4, lower.tail = TRUE),\n    xmax = 4,\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.5,\n    fill = \"green\"\n  ) +\n  geom_label(data = data.frame(x = 3.5, y = .2, label = \"2.5%\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.975, 4, lower.tail = TRUE),\n    y = .3,\n    label = \"t crítico\"\n  ), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.975, 4, lower.tail = TRUE),\n    y = .25,\n    label = round(qt(.975, 4, lower.tail = TRUE), digits = 2)\n  ), aes(x = x, y = y, label = label)) +\n  annotate(\n    \"rect\",\n    xmin = -4,\n    xmax = qt(.025, 4, lower.tail = TRUE),\n    ymin = 0,\n    ymax = Inf,\n    alpha = 0.5,\n    fill = \"green\"\n  ) +\n  geom_label(data = data.frame(x = -3.5, y = .2, label = \"2.5%\"), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.025, 4, lower.tail = TRUE),\n    y = .3,\n    label = \"t crítico\"\n  ), aes(x = x, y = y, label = label)) +\n  geom_label(data = data.frame(\n    x = qt(.025, 4, lower.tail = TRUE),\n    y = .25,\n    label = round(qt(.025, 4, lower.tail = TRUE), digits = 2)\n  ), aes(x = x, y = y, label = label))\n\n\n\n\nEl valor crítico de t para un test bilateral. El 5% total de los valores t son mayores o menores que los valores críticos.\n\n\n\n\n¿Qué estamos viendo acá? Una distribución de ausencia de diferencias (la nula, que es la que estamos considerando) puede producir valores de t mayores o iguales a 2.78 el 2.5% del tiempo, y menores o iguales a -2.78 también el 2.5% del tiempo. En total, eso da un 5%. Podemos decir que ts mayores que ±2.78 ocurren el 5% de las veces.\nPor eso, el valor crítico de t para un test bilateral es ±2.78. Como podés ver, el test bilateral no toma en cuenta la dirección del efecto. Justamente por eso el valor crítico de t es más alto en un test bilateral que en uno unilateral. Esperamos que ahora veas por qué se llama test de dos colas: hay dos colas de la distribución, a la izquierda y a la derecha, ambas sombreadas en verde.\n\n\n7.3.9 ¿Test unilateral o bilateral?\nAhora que sabés que existen los dos tipos —test unilateral y test bilateral—, ¿cuál deberías usar? Hay cierta sabiduría convencional sobre esto… pero también hay debate. Al final, lo importante es que puedas justificar tu elección y por qué es adecuada para tus datos. Esa es la respuesta real.\nLa respuesta convencional es que usás un test unilateral cuando tenés una teoría o hipótesis que hace una predicción direccional (la teoría predice que la diferencia será positiva o negativa). Del mismo modo, usás un test bilateral cuando estás buscando cualquier diferencia, sin una predicción direccional (la teoría solo predice que habrá una diferencia, sin decir si será positiva o negativa).\nTambién parece que las personas eligen entre test unilaterales o bilaterales según cuán arriesgados son como investigadores. Si siempre usaras tests unilaterales, los valores t críticos para tu criterio alfa serían más bajos que los de un test bilateral. A largo plazo, cometerías más errores tipo I, porque el umbral para detectar un efecto es más bajo en un test unilateral que en uno bilateral.\n\nRecordá: los errores tipo I ocurren cuando rechazás la idea de que la diferencia se deba al azar. Muchas veces no sabés que cometiste este error. Ocurre cuando el error muestral fue en realidad la causa de la diferencia, pero el investigador descarta esa posibilidad y concluye que su manipulación fue la causa.\n\nDe forma similar, si siempre usás tests bilaterales, incluso cuando tenés una predicción direccional, vas a cometer menos errores tipo I a largo plazo, porque el t crítico es más alto. Es bastante común que los investigadores usen un test bilateral más conservador, incluso cuando hacen predicciones direccionales basadas en teoría.\nEn la práctica, los investigadores tienden a adoptar un estándar de reporte común en su disciplina. Si esa práctica está justificada o no, puede ser una cuestión abierta. Lo importante para cualquier investigador (o estudiante que aprende estadística) es poder justificar su elección del tipo de test.\n\n\n7.3.10 Grados de libertad\nAntes de cerrar con las pruebas t de muestras pareadas, tenemos que hablar de los grados de libertad.\nTenemos la impresión de que los estudiantes no entienden muy bien qué son los grados de libertad. Si estás leyendo este libro, capaz todavía te estás preguntando qué significa eso, ya que no lo explicamos hasta ahora.\nPara la prueba t, hay una fórmula para los grados de libertad. Para las pruebas t de una muestra y de muestras pareadas, la fórmula es:\n\\(\\text{Grados de libertad} = \\text{df} = n - 1\\)\nDonde n es el número de muestras del test.\nEn nuestro ejemplo con la prueba t apareado, había 5 bebés. Entonces:\n\\(df = 5 - 1 = 4\\)\nBueno, eso es una fórmula. Pero, ¿a quién le importan los grados de libertad? ¿Qué significa ese número? ¿Y por qué lo reportamos cuando presentamos una prueba t?\nSeguro notaste el número entre paréntesis, por ejemplo: t(4) = .72. Ese 4 es el df, o grados de libertad.\nLos grados de libertad son tanto un concepto como una corrección. El concepto es que, si estás estimando alguna propiedad de los datos y usás esa estimación, eso impone ciertas restricciones sobre tus datos.\nPor ejemplo, considerá los números: 1, 2, 3. La media de estos números es 2. Ahora imaginá que te digo que la media de tres números es 2. Entonces, ¿cuántos de esos tres números tienen libertad? Suena como una pregunta rara, ¿no? Lo que queremos decir es: ¿cuántos de esos tres números pueden ser cualquier número libremente? Los primeros dos números pueden ser cualquier cosa. Pero una vez que fijás esos dos, el tercer número ya no tiene libertad: tiene que ser un número específico para que el promedio dé 2. Es decir, los dos primeros tienen libertad, el tercero no.\nPara ilustrarlo: elijamos dos números cualquiera —por ejemplo, 51 y -3. Usé mi libertad para elegir esos dos. Ahora, si nuestros tres números son 51, -3 y x, y queremos que la media sea 2, solo hay una solución: x tiene que ser -42. De otro modo, el promedio no sería 2.\nEsta es una forma de entender los grados de libertad. En este ejemplo, hay 3 números y n–1 = 2 grados de libertad, porque 2 pueden ser libres, pero el tercero queda determinado.\nEn estadística, los grados de libertad se usan mucho, especialmente cuando una estimación depende de otra estimación. Por ejemplo: cuando calculamos la desviación estándar muestral, primero calculamos la media muestral, ¿no? Al estimar la media, estamos fijando un aspecto de la muestra. Entonces, cuando calculamos la desviación estándar, usamos n–1 en lugar de n. Ahí aparece otra vez ese famoso n–1.\n\n7.3.10.1 Simulación del efecto de los grados de libertad sobre la distribución t\nHay al menos dos formas de pensar los grados de libertad para una prueba t. Por un lado, si querés usar fórmulas matemáticas para calcular aspectos de la distribución t, necesitás los grados de libertad como parámetro de entrada. (Si querés ver las fórmulas, bajá en la página de Wikipedia dla prueba t hasta la parte de funciones de densidad de probabilidad o de distribución acumulada. A nosotros eso nos parece bastante intimidante, y quizás por eso los grados de libertad no se entienden bien.)\nPero si queremos simular la distribución t, ahí se ve más fácil cómo influyen los grados de libertad en su forma. Recordá: t es una estadística muestral, se calcula a partir de la muestra. Así que podríamos simular el proceso de calcular t en muchas muestras distintas, y luego graficar el histograma de esos valores para ver cómo se ve la distribución t.\n\nts &lt;- c(rt(10000, 4), rt(10000, 100))\ndfs &lt;- as.factor(rep(c(4, 100), each = 10000))\n\nt_df &lt;- data.frame(dfs, ts)\nt_df &lt;- t_df[abs(t_df$ts) &lt; 5, ]\n\nggplot(t_df, aes(x = ts, group = dfs, color = dfs)) +\n  geom_histogram() +\n  theme_classic() +\n  facet_wrap(~ dfs) +\n  theme_classic(base_size = 15)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nFigure 7.9: La distribución t se vuelve más angosta a medida que aumentan el tamaño muestral y los grados de libertad (de 4 a 100).\n\n\n\n\n\nEn la Figure 7.9, fijate que la distribución roja para $df = 4$ es un poco más baja y más ancha que la distribución azul verdosa para $df = 100$. A medida que aumentan los grados de libertad, la distribución t se vuelve más alta (en el centro) y más angosta en el rango. Se vuelve más picuda. ¿Adivinás por qué?\nRecordá: estamos estimando una estadística muestral, y los grados de libertad son solo un número que representa cuántos sujetos hay (menos uno). Y ya sabemos que, al aumentar n, nuestras estadísticas muestrales se vuelven mejores estimaciones (con menos varianza) de los parámetros poblacionales. Entonces, t se vuelve una mejor estimación de su “valor verdadero” a medida que crece el tamaño muestral, lo que produce una distribución de t más angosta.\nExiste una distribución t ligeramente distinta para cada grado de libertad, y las regiones críticas asociadas al 5% de los valores extremos cambian cada vez. Por eso reportamos los grados de libertad en cada prueba t: definen la distribución de valores t para el tamaño muestral que usamos.\n¿Por qué usamos n–1 y no n? Bueno, calculamos t usando la desviación estándar muestral para estimar el error estándar de la media, y esa estimación usa n–1 en el denominador. Por eso, la distribución t se construye asumiendo n–1.\nCon eso, ya tenés suficiente sobre grados de libertad…",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Pruebas t</span>"
    ]
  },
  {
    "objectID": "06-ttests.html#la-prueba-t-de-muestras-pareadas-contraataca",
    "href": "06-ttests.html#la-prueba-t-de-muestras-pareadas-contraataca",
    "title": "7  Pruebas t",
    "section": "7.4 La prueba t de muestras pareadas contraataca",
    "text": "7.4 La prueba t de muestras pareadas contraataca\nCapaz te estás preguntando si alguna vez vamos a terminar de hablar sobre la prueba t de muestras pareadas… ¿¡por qué hay una segunda vuelta!? Tranquilo, esto es breve. Solo vamos a:\n\nRecordarte qué estábamos haciendo con el estudio con bebés.\nHacer una prueba t de muestras pareadas usando todos los datos del estudio y discutir los resultados.\n\nRecordá que nos preguntábamos si los bebés mirarían más tiempo al cantante de la canción familiar durante la fase de prueba, comparado con la línea base. Ya te mostramos los datos de 5 bebés, y caminamos paso a paso por los cálculos dla prueba t. A modo de repaso, se veía así:\n\n\n\n\n\n\n\n\n\n\n\n\n\ninfant\nBaseline\nTest\ndiferencias\ndiferencias_a_la_media\nSquared_diferencias\n\n\n\n\n1\n0.44\n0.6\n0.16\n0.106\n0.011236\n\n\n2\n0.41\n0.68\n0.27\n0.216\n0.046656\n\n\n3\n0.75\n0.72\n-0.03\n-0.084\n0.00705600000000001\n\n\n4\n0.44\n0.28\n-0.16\n-0.214\n0.045796\n\n\n5\n0.47\n0.5\n0.03\n-0.024\n0.000575999999999999\n\n\nSumas\n2.51\n2.78\n0.27\n0\n0.11132\n\n\nMedias\n0.502\n0.556\n0.054\n0\n0.022264\n\n\n\n\n\n\nde\n0.167\n\n\n\n\n\n\nEEM\n0.075\n\n\n\n\n\n\nt\n0.72\n\n\n\n\n\n\n    One Sample t-test\n\ndata:  round(diferencias, digits = 2)\nt = 0.72381, df = 4, p-value = 0.5092\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.1531384  0.2611384\nsample estimates:\nmean of x \n    0.054 \n\n\nVolvamos a escribir el resultado una vez más: la diferencia media fue 0.054, t(4) = .72, p = .509. Ahora también podemos confirmar que el valor p se obtuvo de un test bilateral. Entonces, ¿qué significa todo esto?\nPodemos decir que un valor t con valor absoluto de .72 o más ocurre el 50.9% del tiempo. Más precisamente: la distribución de ausencia de diferencias (la nula) produce un valor t de ese tamaño o mayor el 50.9% de las veces. En otras palabras: el azar por sí solo podría haber producido perfectamente el valor t de nuestra muestra, y la diferencia media de 0.054 que observamos podría deberse simplemente al azar.\nAhora metamos todos los datos dla prueba t y corramos el análisis con los 32 bebés del estudio:\n\npaired_sample_df &lt;-  data.frame(infant = 1:32, \n                                Baseline = round(experiment_one$Baseline_Proportion_Gaze_to_Singer[1:32], digits = 2), \n                                Test = round(experiment_one$Test_Proportion_Gaze_to_Singer[1:32], digits = 2))\n\ndiferencias &lt;- paired_sample_df$Test - paired_sample_df$Baseline\nt.test(diferencias, mu = 0)\n\n\n    One Sample t-test\n\ndata:  diferencias\nt = 2.4388, df = 31, p-value = 0.02066\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.01192088 0.13370412\nsample estimates:\nmean of x \n0.0728125 \n\n\nAhora obtenemos una respuesta muy distinta. Podríamos resumir los resultados diciendo que la diferencia media fue 0.073, t(31) = 2.44, p = 0.020. ¿Cuántos bebés hubo en total? Bueno, los grados de libertad fueron 31, así que debió haber 32 bebés en el estudio.\nAhora vemos un valor p mucho más bajo. Este también fue un test bilateral, así que observar un valor t de 2.4 o más (en valor absoluto) ocurre solo el 2% del tiempo. En otras palabras, la distribución de ausencia de diferencias rara vez produce un valor t como el que observamos. Por lo tanto, es poco probable que la diferencia media observada de 0.073 se deba al azar (podría deberse al azar, sí, pero es muy poco probable).\nComo resultado, podemos tener cierta confianza en concluir que ver y escuchar a una persona desconocida cantar una canción familiar lleva al bebé a prestar más atención a esa persona, y eso podría beneficiar su aprendizaje social.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Pruebas t</span>"
    ]
  },
  {
    "objectID": "06-ttests.html#prueba-t-de-muestras-independientes-el-regreso-dla-prueba-t",
    "href": "06-ttests.html#prueba-t-de-muestras-independientes-el-regreso-dla-prueba-t",
    "title": "7  Pruebas t",
    "section": "7.5 Prueba t de muestras independientes: ¿el regreso dla prueba t?",
    "text": "7.5 Prueba t de muestras independientes: ¿el regreso dla prueba t?\nSi venís siguiendo las referencias a Star Wars, esta sería la última película de la trilogía original… la prueba t de muestras independientes. Es básicamente la misma historia de antes, pero con un giro. Recordá que hay distintos tipos de t-tests para distintos diseños experimentales. Cuando tu diseño es de grupos independientes (between-subjects), usás una prueba t de muestras independientes.\nLos diseños entre sujetos implican personas diferentes en cada condición experimental. Si hay dos condiciones y 10 personas por condición, entonces hay 20 personas en total. Y no hay puntajes apareados, porque cada persona se mide una sola vez. No hay medidas repetidas. Como no hay medidas repetidas, no tiene sentido calcular diferencias entre condiciones para cada sujeto. Los puntajes no están emparejados de forma significativa, así que no tiene sentido restarlos. Entonces, ¿qué hacemos?\nLa lógica dla prueba t de muestras independientes es la misma que en las otras pruebas t. Calculamos la media de cada grupo, luego la diferencia. Esa diferencia va al numerador de la fórmula del t. Después estimamos la variación para el denominador. Dividimos la diferencia de medias por esa estimación, y obtenemos t. Es como antes. La única complicación es: ¿qué va en el denominador? ¿Cómo estimamos la varianza? Estaría bueno si pudiéramos hacer algo directo, como esto (en un experimento con dos grupos, A y B):\n\\(t = \\frac{\\bar{A} - \\bar{B}}{\\left(\\frac{SEM_A + SEM_B}{2}\\right)}\\)\nEn lenguaje simple, esto sería:\n\nCalculás la diferencia de medias y la ponés arriba;\nCalculás el error estándar de la media (SEM) para cada grupo;\nPromediás esos SEMs para obtener una sola estimación, combinando ambas muestras.\n\nEso estaría bueno, pero lamentablemente no es la mejor forma de hacerlo. Tomar el promedio de los errores estándar genera un estimador sesgado de la variación en la distribución nula (la de ausencia de diferencias).\nTomar el promedio de los errores estándar generaría un estimador sesgado de la variación bajo la hipótesis nula (ausencia de diferencias). No vamos a entrar en los detalles matemáticos acá, pero en vez de esa fórmula, podemos usar otra que nos da una estimación insesgada del error estándar combinado de la media muestral. Nuestra nueva y mejorada fórmula del t se ve así:\n\\(t = \\frac{\\bar{X}_A - \\bar{X}_B}{s_p \\cdot \\sqrt{\\frac{1}{n_A} + \\frac{1}{n_B}}}\\)\nY $s_p$, que es la desviación estándar combinada de las muestras, se define así (notá que las s en la fórmula son varianzas):\n\\(s_p = \\sqrt{\\frac{(n_A - 1)s_A^2 + (n_B - 1)s_B^2}{n_A + n_B - 2}}\\)\nCréeme: es mucha más fórmula de la que quería tipear. ¿Hacemos un ejemplo de prueba t de muestras independientes a mano, para ver cómo se hacen los cálculos? Dale… pero lo vamos a hacer de una forma ligeramente distinta a la que esperás. Voy a mostrarte los pasos usando R.\nInventé unos puntajes ficticios para los grupos A y B. Luego seguí todos los pasos de la fórmula, pero hice que R haga cada uno de los cálculos. Esto te muestra los pasos necesarios siguiendo el código. Al final imprimo los valores dla prueba t que calculé “a mano”, y luego el valor dla prueba t que da la función t.test() de R. Deberías obtener el mismo valor de t si te animás a calcularlo a mano.\n\n## A mano, usando código en R\na &lt;- c(1,2,3,4,5)\nb &lt;- c(3,5,4,7,9)\n\nmean_difference &lt;- mean(a) - mean(b) # diferencia de medias\n\nvariance_a &lt;- var(a) # varianza para A\nvariance_b &lt;- var(b) # varianza para B\n\n# Parte superior e inferior de la fórmula de sp\nsp_numerator &lt;- (4 * variance_a + 4 * variance_b) \nsp_denominator &lt;- 5 + 5 - 2\nsp &lt;- sqrt(sp_numerator / sp_denominator) # calcular sp\n\n# calcular t según la fórmula\nt &lt;- mean_difference / (sp * sqrt((1/5) + (1/5)))\n\nt # mostrar resultado\n\n[1] -2.017991\n\n# usando la función de R\nt.test(a, b, paired = FALSE, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  a and b\nt = -2.018, df = 8, p-value = 0.0783\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -5.5710785  0.3710785\nsample estimates:\nmean of x mean of y \n      3.0       5.6 \n\n\n\n7.5.1 Simular una prueba t de una muestra\nAcá están los pasos que podrías seguir para simular datos para una prueba t de una muestra:\n\nHacés algunas suposiciones sobre cómo podría ser tu muestra (que estás planeando recolectar). Por ejemplo, podrías planear recolectar datos de 30 personas. Los puntajes podrían venir de una distribución normal (media = 50, sd = 10).\nSimulás números desde esa distribución y luego hacés una prueba t sobre esos datos simulados. Guardás las estadísticas que te interesan (como los t y p) y después observás cómo se comportan.\n\nHagámoslo un par de veces. Primero, vamos a simular muestras con N = 30, tomadas de una distribución normal (media = 50, sd = 25). Vamos a hacer una simulación con 1000 repeticiones. En cada repetición, comparamos la media muestral con una media poblacional de 50. En promedio, no debería haber diferencia. La Figure 7.10 es la distribución nula que estamos simulando.\n\n# pasos para crear datos ficticios desde una distribución\n# y hacer pruebas t sobre los datos simulados\n\nguarda_ps &lt;- length(1000)\nguarda_ts &lt;- length(1000)\n\nfor (i in 1:1000) {\n  mi_muestra &lt;- rnorm(n = 30, mean = 50, sd = 25)\n  t_test &lt;- t.test(mi_muestra, mu = 50)\n  guarda_ps[i] &lt;- t_test$p.value\n  guarda_ts[i] &lt;- t_test$statistic\n}\n\n\n# histogramas de t y p para las 1000 simulaciones\nhist(guarda_ts)\n\n\n\n\n\n\n\nFigure 7.10: Distribución de valores t bajo la nula. Estos son los t que surgen solo por azar.\n\n\n\n\n\n\nhist(guarda_ps)\n\n\n\n\n\n\n\nFigure 7.11: La distribución de valores p observados es plana bajo la nula.\n\n\n\n\n\nBuenísimo. Vemos una distribución t, que se parece a una t como debe ser. Y también vemos la distribución de valores p. Esto nos muestra qué tan seguido obtenemos valores t de distintos tamaños.\nCapaz te llame la atención que la distribución de p es plana bajo la nula (que es lo que estamos simulando). Esto significa que tenés la misma probabilidad de obtener un valor p entre 0 y 0.05 que de obtener uno entre 0.90 y 0.95. Esos rangos tienen un 5% de ancho, así que contienen la misma cantidad de valores t por definición.\nAcá va otra forma de hacer la misma simulación en R, usando la función replicate en vez de un for:\n\nt_simulados &lt;- replicate(1000,\n                          t.test(rnorm(30, 50, 25), mu = 50)$statistic)\nhist(t_simulados)\n\n\n\n\n\n\n\nFigure 7.12: Simulando valores t en R.\n\n\n\n\n\n\np_simulados &lt;- replicate(1000,\n                          t.test(rnorm(30, 50, 25), mu = 50)$p.value)\nhist(p_simulados)\n\n\n\n\n\n\n\nFigure 7.13: Simulando valores p en R.\n\n\n\n\n\n\n\n7.5.2 Simular una prueba t de muestras pareadas\nEl código de abajo está preparado para tomar 10 puntajes de la condición A y 10 de la condición B, ambos desde la misma distribución normal. La simulación se corre 1000 veces, y se guardan los t y p para graficarlos luego.\n\nguarda_ps &lt;- length(1000)\nguarda_ts &lt;- length(1000)\n\nfor (i in 1:1000) {\n  condicion_A &lt;- rnorm(10,10,5)\n  condicion_B &lt;- rnorm(10,10,5)\n  diferencias &lt;- condicion_A - condicion_B\n  t_test &lt;- t.test(diferencias, mu = 0)\n  guarda_ps[i] &lt;- t_test$p.value\n  guarda_ts[i] &lt;- t_test$statistic\n}\n\n\nhist(guarda_ts)\n\n\n\n\n\n\n\nFigure 7.14: 1000 valores t simulados de la distribución nula\n\n\n\n\n\n\nhist(guarda_ps)\n\n\n\n\n\n\n\nFigure 7.15: 1000 valores p simulados de la distribución nula\n\n\n\n\n\nSegún la simulación, cuando no hay diferencias entre las condiciones y las muestras provienen de la misma distribución, se obtienen estas dos distribuciones para t y p. Una vez más, muestran cómo se comporta la distribución nula de ausencia de diferencias.\nEn cualquiera de estas simulaciones, si rechazaras la hipótesis nula (que la diferencia se debe al azar), estarías cometiendo un error tipo I. Si fijás tu criterio alfa en $= .05$, podemos preguntar cuántos errores tipo I se cometieron en estas 1000 simulaciones. La respuesta es:\n\nlength(guarda_ps[guarda_ps &lt; .05])\n\n[1] 39\n\nlength(guarda_ps[guarda_ps &lt; .05]) / 1000\n\n[1] 0.039\n\n\nResulta que cometimos 39. La expectativa a largo plazo es una tasa de error tipo I del 5% (si tu alfa es .05).\n¿Qué pasa si en realidad sí hay una diferencia en los datos simulados? Probemos con una condición que tenga una media mayor que la otra:\n\nguarda_ps &lt;- length(1000)\nguarda_ts &lt;- length(1000)\n\nfor (i in 1:1000) {\n  condicion_A &lt;- rnorm(10, 10, 5)\n  condicion_B &lt;- rnorm(10, 13, 5)\n  diferencias &lt;- condicion_A - condicion_B\n  t_test &lt;- t.test(diferencias, mu = 0)\n  guarda_ps[i] &lt;- t_test$p.value\n  guarda_ts[i] &lt;- t_test$statistic\n}\n\n\nhist(guarda_ts)\n\n\n\n\n\n\n\nFigure 7.16: 1000 valores t cuando hay una diferencia real\n\n\n\n\n\n\nhist(guarda_ps)\n\n\n\n\n\n\n\nFigure 7.17: 1000 valores p cuando hay una diferencia real\n\n\n\n\n\nAhora podés ver que la distribución de valores p está sesgada hacia la izquierda. Esto pasa porque cuando hay un efecto real, obtenés valores p menores a .05 más seguido. O sea, obtenés valores t mayores de lo que obtendrías si no hubiera diferencias.\nEn este caso, no estarías cometiendo un error tipo I si rechazás la nula cuando p es menor a .05. ¿Cuántas veces lo harías de las 1000 simulaciones?\n\nlength(guarda_ps[guarda_ps &lt; .05])\n\n[1] 204\n\nlength(guarda_ps[guarda_ps &lt; .05]) / 1000\n\n[1] 0.204\n\n\nObtuvimos 204 simulaciones donde p fue menor a .05, lo cual representa 0.204 experimentos. Si fueras el investigador, ¿te gustaría correr un experimento que solo tiene éxito 0.204 del tiempo? Yo no. Yo correría un experimento mejor.\n¿Cómo harías una mejor simulación? Bueno, podés aumentar n, el número de sujetos. Aumentemos n de 10 a 100 y veamos qué pasa con la cantidad de experimentos “significativos”.\n\nguarda_ps &lt;- length(1000)\nguarda_ts &lt;- length(1000)\nfor (i in 1:1000) {\n  condicion_A &lt;- rnorm(100, 10, 5)\n  condicion_B &lt;- rnorm(100, 13, 5)\n  diferencias &lt;- condicion_A - condicion_B\n  t_test &lt;- t.test(diferencias, mu = 0)\n  guarda_ps[i] &lt;- t_test$p.value\n  guarda_ts[i] &lt;- t_test$statistic\n}\n\n\nhist(guarda_ts)\n\n\n\n\n\n\n\nFigure 7.18: 1000 valores t para n = 100, cuando hay un efecto real\n\n\n\n\n\n\nhist(guarda_ps)\n\nlength(guarda_ps[guarda_ps &lt; .05])\n\n[1] 987\n\nlength(guarda_ps[guarda_ps &lt; .05]) / 1000\n\n[1] 0.987\n\n\n\n\n\n\n\n\nFigure 7.19: 1000 valores p para n = 100, cuando hay un efecto real\n\n\n\n\n\nGenial, ahora casi todos los experimentos muestran un valor p menor a .05 (usando un test bilateral, que es la opción por defecto en R). ¿Ves? Podés usar este proceso de simulación para determinar cuántos sujetos necesitás para detectar confiablemente tu efecto.\n\n\n7.5.3 Simular una prueba t de muestras independientes\nSolo cambiá la función t.test de esta forma… esto es para simular bajo la nula, asumiendo que no hay diferencia entre los grupos.\n\nguarda_ps &lt;- length(1000)\nguarda_ts &lt;- length(1000)\nfor (i in 1:1000) {\n  group_A &lt;- rnorm(10, 10, 5)\n  group_B &lt;- rnorm(10, 10, 5)\n  t_test &lt;- t.test(group_A, group_B, paired = FALSE, var.equal = TRUE)\n  guarda_ps[i] &lt;- t_test$p.value\n  guarda_ts[i] &lt;- t_test$statistic\n}\n\n\nhist(guarda_ts)\n\n\n\n\n\n\n\nFigure 7.20: 1000 valores t para n = 100, cuando hay un efecto real\n\n\n\n\n\n\nhist(guarda_ps)\n\nlength(guarda_ps[guarda_ps &lt; .05])\n\n[1] 59\n\nlength(guarda_ps[guarda_ps &lt; .05]) / 1000\n\n[1] 0.059\n\n\n\n\n\n\n\n\nFigure 7.21: 1000 valores p para n = 100, cuando hay un efecto real",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Pruebas t</span>"
    ]
  },
  {
    "objectID": "02b-tablas_final.html",
    "href": "02b-tablas_final.html",
    "title": "3  Tablas",
    "section": "",
    "text": "3.1 Contar: la primera estadística\nAntes de hablar de porcentajes, probabilidades o inferencias, hay algo que hacemos todo el tiempo sin pensarlo mucho: contar. ¿Cuántas personas dijeron que sí? ¿Cuántos eligieron la opción A? ¿Cuántos pacientes presentaron síntomas?\nContar es tan básico que a veces no lo tratamos como una operación estadística. Pero lo es. De hecho, en datos categóricos, contar es hacer estadística descriptiva.\nCuando contamos, estamos calculando lo que se llama una frecuencia absoluta. Es simplemente eso: el número de veces que aparece una categoría.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tablas</span>"
    ]
  },
  {
    "objectID": "02b-tablas_final.html#contar-la-primera-estadística",
    "href": "02b-tablas_final.html#contar-la-primera-estadística",
    "title": "3  Tablas",
    "section": "3.1 Contar: la primera estadística",
    "text": "3.1 Contar: la primera estadística\nAntes de hablar de porcentajes, probabilidades o inferencias, hay algo que hacemos todo el tiempo sin pensarlo mucho: contar. ¿Cuántas personas dijeron que sí? ¿Cuántos eligieron la opción A? ¿Cuántos pacientes presentaron síntomas?\nContar es tan básico que a veces no lo tratamos como una operación estadística. Pero lo es. De hecho, en datos categóricos, contar es hacer estadística descriptiva.\nCuando contamos, estamos calculando lo que se llama una frecuencia absoluta. Es simplemente eso: el número de veces que aparece una categoría.\n\n3.1.1 Ejemplo: animales favoritos\nSupongamos que en una clase de 40 estudiantes les pedimos que elijan su animal favorito entre tres opciones: gatos, perros y aves. Obtenemos las siguientes respuestas:\n\nrespuestas &lt;- c(\"Gato\", \"Perro\", \"Perro\", \"Gato\", \"Ave\", \"Perro\", \"Perro\", \"Gato\",\n                \"Perro\", \"Gato\", \"Gato\", \"Perro\", \"Ave\", \"Perro\", \"Perro\", \"Gato\",\n                \"Perro\", \"Ave\", \"Gato\", \"Gato\", \"Perro\", \"Perro\", \"Perro\", \"Gato\",\n                \"Ave\", \"Perro\", \"Perro\", \"Gato\", \"Perro\", \"Ave\", \"Gato\", \"Perro\",\n                \"Perro\", \"Perro\", \"Gato\", \"Gato\", \"Ave\", \"Perro\", \"Gato\", \"Perro\")\n\ntabla_absoluta &lt;- table(respuestas)\nknitr::kable(tabla_absoluta,col.names = c(\"respuestas\",\"frecuencia\"),align=\"c\")\n\n\n\n\nrespuestas\nfrecuencia\n\n\n\n\nAve\n6\n\n\nGato\n14\n\n\nPerro\n20\n\n\n\n\n\nEsto nos da la tabla de frecuencias absolutas. Es lo más básico, pero ya nos permite ver cuál fue la opción más popular (en este caso, claramente Perro). Pero esa es solo una parte de la historia."
  },
  {
    "objectID": "02b-tablas_final.html#ver-lo-que-contamos",
    "href": "02b-tablas_final.html#ver-lo-que-contamos",
    "title": "3  Tablas",
    "section": "3.2 Ver lo que contamos",
    "text": "3.2 Ver lo que contamos\nLos humanos somos visuales. Podemos leer la tabla de frecuencias, pero la vemos mucho mejor si la graficamos. Acá va el gráfico de barras correspondiente:\n\nbarplot(tabla_absoluta,\n        main = \"Animales favoritos\",\n        col = \"skyblue\",\n        ylab = \"Cantidad de estudiantes\")\n\n\n\n\nFrecuencias absolutas de animales favoritos\n\n\n\n\nEsta es una tabla sencilla. Pero más adelante vamos a ver que a veces, incluso con una tabla simple, se pueden cometer errores de interpretación si no se tiene en cuenta el tipo de frecuencia o el contexto.\nPor ahora, la gran idea es esta:\n\nContar categorías es el primer paso para entender los datos.\n\nTodo lo que viene después —proporciones, porcentajes, pruebas estadísticas— se construye sobre ese primer conteo.\nPerfecto. Seguimos avanzando en el capítulo, manteniendo el estilo narrativo y el uso de código en R. Pasamos ahora a frecuencias relativas y porcentuales."
  },
  {
    "objectID": "02b-tablas_final.html#qué-tan-grande-es-mucho",
    "href": "02b-tablas_final.html#qué-tan-grande-es-mucho",
    "title": "3  Tablas",
    "section": "3.3 ¿Qué tan grande es “mucho”?",
    "text": "3.3 ¿Qué tan grande es “mucho”?\nContar está bien. Pero a veces no alcanza. Saber que 22 personas eligieron “Perro” está bueno, pero… ¿22 sobre cuántas? ¿Es la mayoría? ¿Es más de la mitad? ¿Es mucho?\nAhí es donde entra la frecuencia relativa. Lo que antes era “cuántas personas eligieron esta opción” ahora se convierte en:\n\n¿Qué proporción del total eligió esta opción?\n\nEs una pregunta diferente. Y muchas veces, más útil.\n\n3.3.1 Del número al total\nVolvamos a nuestro ejemplo. Ya teníamos la tabla de frecuencias absolutas. Ahora vamos a calcular la proporción que representa cada categoría respecto al total de respuestas:\n\ntabla_relativa &lt;- prop.table(tabla_absoluta)\nknitr::kable(round(tabla_relativa, 3),col.names = c(\"respuestas\",\"frecuencia\"),align=\"c\")\n\n\nFrecuencias relativas (proporciones) de animales favoritos \n\n\nrespuestas\nfrecuencia\n\n\n\n\nAve\n0.15\n\n\nGato\n0.35\n\n\nPerro\n0.50\n\n\n\n\n\nAhora vemos que:\n\n“Perro” fue elegido por el 50% de los estudiantes.\n“Gato”, por el 35%.\n“Ave”, por el 15%.\n\nMás allá del conteo, ahora sabemos qué fracción del grupo representa cada preferencia. Y eso nos permite comparar categorías con tamaños distintos.\n\n\n\n3.3.2 Lo mismo, pero con el 100%\nA veces no queremos ver proporciones con coma. Queremos porcentajes. Porque a veces es más fácil entender: “El 15% eligió Ave” que “0.15 eligió Ave”\nEso es una frecuencia porcentual: la frecuencia relativa multiplicada por 100.\n\nround(100 * tabla_relativa, 1)\n\nrespuestas\n  Ave  Gato Perro \n   15    35    50 \n\n\n\n\n3.3.3 Ver lo relativo\nAsí como hicimos con los conteos, podemos graficar las proporciones. Ahora, en vez de cantidad de personas, mostramos porcentaje sobre el total.\n\nbarplot(100 * tabla_relativa,\n        main = \"Animales favoritos (porcentaje)\",\n        col = \"lightgreen\",\n        ylab = \"Porcentaje\")\n\n\n\n\nFrecuencias porcentuales de animales favoritos\n\n\n\n\nLa tabla y el gráfico muestran lo mismo, pero de otra forma. A veces te conviene contar. A veces, comparar proporciones. Y lo importante es saber cuál estás usando.\nPorque, como veremos más adelante, muchos errores de interpretación vienen de confundir una tabla con números crudos con otra que muestra proporciones."
  },
  {
    "objectID": "02b-tablas_final.html#sumar-hacia-arriba-la-frecuencia-acumulada",
    "href": "02b-tablas_final.html#sumar-hacia-arriba-la-frecuencia-acumulada",
    "title": "3  Tablas",
    "section": "3.4 Sumar hacia arriba: la frecuencia acumulada",
    "text": "3.4 Sumar hacia arriba: la frecuencia acumulada\nHasta ahora hablamos de categorías como si fueran simplemente distintas entre sí. Perros, gatos, aves. Ninguna tiene un “más” o un “menos”, ninguna está antes o después que otra. Solo son distintas. Pero no todas las categorías son así.\nA veces, las categorías tienen orden.\nPensá en escalas de respuesta como estas:\n\n“Nada”, “Poco”, “Algo”, “Mucho”\n“Nunca”, “A veces”, “Frecuentemente”, “Siempre”\n“Totalmente en desacuerdo” a “Totalmente de acuerdo”\n\nEn estos casos, no estamos eligiendo simplemente una etiqueta. Estamos ubicándonos en una serie ordenada de niveles. Esa estructura permite hacer algo que antes no podíamos: acumular.\n\n3.4.1 ¿Para qué sirve acumular?\nImaginá que estás estudiando el nivel de ansiedad en una población, con una escala ordinal del 1 al 5, donde 1 es “muy baja ansiedad” y 5 es “muy alta ansiedad”.\nTenés los siguientes resultados de una encuesta a 30 personas:\n\nset.seed(123)\nansiedad &lt;- sample(1:5, size = 30, replace = TRUE, prob = c(0.2, 0.3, 0.3, 0.1, 0.1))\nansiedad_tabla &lt;- table(ansiedad)\nansiedad_tabla\n\nansiedad\n1 2 3 4 5 \n6 7 9 4 4 \n\n\nPodés ver cuántas personas hay en cada nivel. Eso es útil. Pero capaz lo que te interesa no es tanto “¿cuántas dijeron 3?”, sino:\n\n¿Cuántas personas tienen ansiedad de 3 o menos?\n¿Qué porcentaje del grupo está en los niveles más bajos?\n\nAhí es donde entra la frecuencia acumulada: sumás progresivamente las frecuencias hasta cada punto del orden.\n\n\n3.4.2 Cómo se calcula\n\nacumulada &lt;- cumsum(ansiedad_tabla)\nacumulada\n\n 1  2  3  4  5 \n 6 13 22 26 30 \n\n\nTambién podés convertirla en proporciones:\n\nacumulada_porcentual &lt;- round(100 * cumsum(prop.table(ansiedad_tabla)), 1)\nacumulada_porcentual\n\n    1     2     3     4     5 \n 20.0  43.3  73.3  86.7 100.0 \n\n\n\n\n3.4.3 ¿Qué aprendemos con esto?\nPor ejemplo, podemos ver que el 63% de las personas tiene una puntuación de ansiedad 3 o menos. Esa información no la da ninguna frecuencia aislada, y es clave si querés responder preguntas como:\n\n¿Qué proporción de la población está en el rango “bajo” o “moderado”?\n\nLas frecuencias acumuladas nos ayudan a identificar umbrales, cortes y agrupamientos en escalas ordinales. Son especialmente útiles en psicología, donde se usan constantemente escalas de Likert, niveles de severidad, o clasificaciones de síntomas.\n\n\n\n3.4.4 Ver la acumulación\nPodemos representar la acumulación como una línea creciente, que muestra cómo se van sumando los porcentajes.\n\nplot(acumulada_porcentual, type = \"b\", pch = 16,\n     xlab = \"Nivel de ansiedad\", ylab = \"Porcentaje acumulado\",\n     main = \"Frecuencia acumulada de ansiedad\",\n     ylim = c(0, 100))\nabline(h = 50, col = \"gray\", lty = 2)\n\n\n\n\nFrecuencia acumulada de ansiedad (escala 1–5)\n\n\n\n\nLa línea gris horizontal marca el 50%. Donde la curva lo cruza, está la mediana categórica: el punto donde la mitad de las personas están por debajo.\nContar es solo el principio. Pero cuando hay orden, sumar nos cuenta una historia diferente. Una historia de acumulación, de corte, de “hasta aquí” y “a partir de acá”. Y eso, en psicología, es a veces la diferencia entre un caso leve y uno severo.\nPerfecto. Voy a continuar con la sección sobre tablas de contingencia, cuidando que el tono se mantenga narrativo, reflexivo y accesible, con ejemplos claros y progresivos. Acá vamos."
  },
  {
    "objectID": "02b-tablas_final.html#dos-variables-una-tabla-cuando-las-categorías-se-cruzan",
    "href": "02b-tablas_final.html#dos-variables-una-tabla-cuando-las-categorías-se-cruzan",
    "title": "3  Tablas",
    "section": "3.5 Dos variables, una tabla: cuando las categorías se cruzan",
    "text": "3.5 Dos variables, una tabla: cuando las categorías se cruzan\nHasta ahora veníamos mirando una sola variable a la vez. Contábamos cuántas personas eligieron esta opción o aquella. Calculábamos qué porcentaje respondió de cierta manera. Veíamos cómo se acumulaban los niveles de una escala.\nPero la mayoría de las preguntas interesantes en psicología —y en ciencias sociales en general— no se responden mirando una sola variable. Nos interesa saber cómo se relacionan entre sí.\n\n¿Tienen más actitudes negativas hacia la terapia los varones que las mujeres?\n¿Se diagnostica más ansiedad en quienes tienen trabajos precarios?\n¿Hay más rechazo a la eutanasia entre personas con menor nivel educativo?\n\nTodas estas preguntas implican dos variables categóricas. Y cuando eso pasa, aparece un recurso fundamental: la tabla de contingencia.\n\n3.5.1 ¿Qué es una tabla de contingencia?\nEs simplemente una forma ordenada de mostrar cuántas personas (o casos) caen simultáneamente en cada combinación de categorías de dos variables.\nPor ejemplo, si tenemos información sobre el género de una persona (hombre, mujer, otro) y su opinión sobre la eficacia de la psicoterapia (positiva o negativa), podemos construir una tabla que muestre cómo se distribuyen esas respuestas:\n\ngenero &lt;- c(rep(\"Mujer\", 50), rep(\"Hombre\", 40), rep(\"Otro\", 10))\nopinion &lt;- c(rep(\"Positiva\", 35), rep(\"Negativa\", 15),\n             rep(\"Positiva\", 20), rep(\"Negativa\", 20),\n             rep(\"Positiva\", 8), rep(\"Negativa\", 2))\n\ntabla_conjunta &lt;- table(genero, opinion)\ntabla_conjunta\n\n        opinion\ngenero   Negativa Positiva\n  Hombre       20       20\n  Mujer        15       35\n  Otro          2        8\n\n\nEsta tabla nos muestra frecuencias absolutas conjuntas. Es decir, cuántas personas hay en cada cruce de categorías: cuántas mujeres con opinión positiva, cuántos hombres con opinión negativa, etc.\n\n\n3.5.2 ¿Qué se puede leer de esta tabla?\nA simple vista, parece que:\n\nHay más mujeres con opinión positiva que negativa.\nEntre los hombres, la distribución es más pareja.\nEl grupo “Otro” también tiende a opinar más positivamente.\n\nPero, como veremos más adelante, mirar sólo los números absolutos puede ser engañoso.\nPor ahora, quedémonos con la idea de que la tabla de contingencia resume cómo se distribuyen dos variables categóricas, y que es la puerta de entrada a casi todo lo que vamos a querer analizar después: proporciones condicionales, relaciones, independencias, efectos.\nY, por supuesto, la vamos a querer ver también como gráfico.\n\n\n3.5.3 Visualizar la tabla\nPodemos convertir esa tabla en un gráfico de barras apiladas o agrupadas. Por ejemplo:\n\nlibrary(ggplot2)\n\ndatos &lt;- data.frame(genero, opinion)\n\nggplot(datos, aes(x = genero, fill = opinion)) +\n  geom_bar(position = \"dodge\") +\n  labs(title = \"Opinión sobre la terapia según género\",\n       x = \"Género\", y = \"Cantidad de personas\") +\n  theme_minimal(base_size = 14)\n\n\n\n\nDistribución conjunta de opinión sobre terapia según género\n\n\n\n\nEste gráfico nos permite ver en paralelo las frecuencias de cada categoría, lo que a veces es más fácil que comparar números en una cuadrícula. Pero —y esto es clave— este gráfico está mostrando los valores absolutos, no proporciones. Y ahí es donde pueden empezar los malentendidos.\nPerfecto. Esta sección es clave porque muchos errores de interpretación se originan aquí. Vamos a desarrollar con cuidado y ejemplos cómo los totales estructuran la lectura, y cómo se pueden malinterpretar relaciones si no se considera esto. Aquí va."
  },
  {
    "objectID": "02b-tablas_final.html#los-totales-mandan-leer-bien-una-tabla-y-no-cometer-errores",
    "href": "02b-tablas_final.html#los-totales-mandan-leer-bien-una-tabla-y-no-cometer-errores",
    "title": "3  Tablas",
    "section": "3.6 Los totales mandan: leer bien una tabla (y no cometer errores)",
    "text": "3.6 Los totales mandan: leer bien una tabla (y no cometer errores)\nCuando uno mira una tabla de contingencia, lo primero que suele hacer es comparar los números. Cuántos hay en una celda, cuántos en otra. Y a veces esa comparación es útil. Pero otras veces puede ser totalmente engañosa.\nEl problema está en no tener en cuenta de qué total estamos hablando.\n\n\n3.6.1 Tres tipos de totales, tres formas de leer\nUna tabla de contingencia puede organizarse (y leerse) de distintas maneras según qué total se está usando como referencia.\nVeamos los tres casos principales:\n\n\n3.6.2 🧱 1. Totales por fila\nEsto pasa cuando preguntamos:\n\n“Dado que la persona es mujer, ¿qué opinó sobre la terapia?”\n\nEs decir: ya sabemos a qué grupo pertenece (la fila), y queremos ver cómo se distribuyen sus respuestas (las columnas). En este caso, las proporciones se calculan por fila.\n\nprop_filas &lt;- prop.table(tabla_conjunta, margin = 1)\nround(prop_filas, 2)\n\n        opinion\ngenero   Negativa Positiva\n  Hombre      0.5      0.5\n  Mujer       0.3      0.7\n  Otro        0.2      0.8\n\n\nEsto responde preguntas del tipo:\n\n¿Qué porcentaje de mujeres tuvo opinión positiva?\n¿Qué porcentaje de hombres fue negativo?\n\nEstas proporciones son condicionales a la fila. Y se usan todo el tiempo en psicología, especialmente cuando hay grupos naturales (género, diagnóstico, condición experimental).\n\n\n\n3.6.3 🧱 2. Totales por columna\nAhora cambiamos la pregunta:\n\n“Dado que la persona tuvo opinión positiva, ¿de qué grupo fue?”\n\nEs decir: ya sabemos la respuesta (la columna), y queremos ver su distribución por grupo (la fila). Calculamos proporciones por columna.\n\nprop_columnas &lt;- prop.table(tabla_conjunta, margin = 2)\nround(prop_columnas, 2)\n\n        opinion\ngenero   Negativa Positiva\n  Hombre     0.54     0.32\n  Mujer      0.41     0.56\n  Otro       0.05     0.13\n\n\nEsto responde preguntas como:\n\n¿De qué género son la mayoría de los que opinaron negativamente?\n¿Qué grupo está sobrerrepresentado entre los que opinan positivamente?\n\nEstas proporciones pueden ser útiles, pero se prestan a confusión si uno cree que está viendo lo mismo que en la tabla anterior. No lo está.\n\n\n3.6.4 🌐 3. Total general\nA veces lo que queremos es mirar las proporciones respecto al total de la muestra, sin condicionar por fila ni por columna.\n\nprop_total &lt;- prop.table(tabla_conjunta)\nround(prop_total, 2)\n\n        opinion\ngenero   Negativa Positiva\n  Hombre     0.20     0.20\n  Mujer      0.15     0.35\n  Otro       0.02     0.08\n\n\nEsto responde:\n\n¿Qué fracción de la muestra son mujeres con opinión positiva?\n¿Qué porcentaje total del estudio representa cada celda?\n\nEs útil para estimar prevalencias conjuntas. Pero no sirve para comparar respuestas dentro de grupos."
  },
  {
    "objectID": "02b-tablas_final.html#por-qué-importa-esto",
    "href": "02b-tablas_final.html#por-qué-importa-esto",
    "title": "3  Tablas",
    "section": "3.7 ⚠️ ¿Por qué importa esto?",
    "text": "3.7 ⚠️ ¿Por qué importa esto?\nPorque si no sabemos qué total estamos usando, podemos decir cosas como:\n\n“La mayoría de los que están en contra de la terapia son hombres”.\n\nCuando en realidad hay más hombres en contra simplemente porque hay más hombres en total.\nO peor:\n\n“Los hombres son más negativos hacia la terapia”.\n\nCuando al calcular los porcentajes dentro de cada grupo, tal vez descubrimos que en realidad hay más negatividad proporcional entre las mujeres.\nEstos errores son muy comunes en artículos, medios, e incluso investigaciones. Y se derivan de no distinguir claramente la estructura del total.\n\n🧠 Regla de oro Nunca interpretes una tabla de contingencia sin saber primero de qué total estás hablando.\n\nPerfecto. Ahora seguimos con una sección dedicada a las tablas de prevalencia, un tipo muy común y muy útil de tabla en psicología y epidemiología, pero que también puede inducir a errores si no se interpreta bien. La idea es seguir desarrollando la intuición del lector y mostrarle cómo identificar este tipo especial de tabla."
  },
  {
    "objectID": "02b-tablas_final.html#tablas-de-prevalencia-ver-cuánto-hay-de-algo-dentro-de-algo",
    "href": "02b-tablas_final.html#tablas-de-prevalencia-ver-cuánto-hay-de-algo-dentro-de-algo",
    "title": "3  Tablas",
    "section": "3.8 Tablas de prevalencia: ver cuánto hay de algo, dentro de algo",
    "text": "3.8 Tablas de prevalencia: ver cuánto hay de algo, dentro de algo\nEn psicología clínica, epidemiología y salud mental, una de las preguntas más frecuentes que nos hacemos es:\n\n“¿Cuánta gente presenta esta condición en este grupo?”\n\nPor ejemplo:\n\n¿Qué porcentaje de adolescentes varones tiene síntomas de depresión?\n¿Cuál es la proporción de pacientes con ansiedad en personas desempleadas?\n¿Cuántas personas con bajo nivel educativo han intentado suicidarse?\n\nEste tipo de preguntas no apunta a una comparación entre grupos, ni a una inferencia estadística (todavía). Lo que busca es saber cuánto hay de una característica (un diagnóstico, un síntoma, una conducta) dentro de una población determinada.\nEsa pregunta tiene nombre: se llama prevalencia.\n\n\n3.8.1 ¿Qué es la prevalencia?\nEs la proporción de personas que presentan una característica determinada dentro de un grupo dado, en un momento específico.\n\nPrevalencia = casos positivos / total del grupo\n\nEs una proporción condicional. Es decir, está condicionada al grupo de referencia.\n\n\n\n3.8.2 Ejemplo: prevalencia de ansiedad por nivel educativo\nSupongamos que se encuestó a 120 personas, divididas en tres niveles educativos: básico, medio y superior. Se les preguntó si habían experimentado ansiedad clínica el último mes. Obtenemos la siguiente tabla:\n\neducacion &lt;- c(rep(\"Básico\", 40), rep(\"Medio\", 40), rep(\"Superior\", 40))\nansiedad &lt;- c(rep(\"Sí\", 15), rep(\"No\", 25),\n              rep(\"Sí\", 10), rep(\"No\", 30),\n              rep(\"Sí\", 5), rep(\"No\", 35))\n\ntabla_ansiedad &lt;- table(educacion, ansiedad)\ntabla_ansiedad\n\n          ansiedad\neducacion  No Sí\n  Básico   25 15\n  Medio    30 10\n  Superior 35  5\n\n\n\n\n3.8.3 ¿Cómo se calcula la prevalencia?\nQueremos saber qué proporción de personas de cada nivel educativo reportó ansiedad. Para eso, normalizamos por fila: calculamos proporciones dentro de cada grupo.\n\nprevalencias &lt;- prop.table(tabla_ansiedad, margin = 1)\nround(prevalencias, 2)\n\n          ansiedad\neducacion    No   Sí\n  Básico   0.62 0.38\n  Medio    0.75 0.25\n  Superior 0.88 0.12\n\n\nAhora vemos que:\n\nEn el grupo de educación básica, el 37.5% reportó ansiedad.\nEn el grupo medio, el 25%.\nEn el grupo superior, solo el 12.5%.\n\nEstas proporciones son las prevalencias de ansiedad en cada grupo.\n\n\n3.8.4 Visualizar una tabla de prevalencia\nUn gráfico de barras permite ver esto claramente:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndatos_prev &lt;- as.data.frame(tabla_ansiedad) |&gt;\n  group_by(educacion) |&gt;\n  mutate(Prevalencia = Freq / sum(Freq)) |&gt;\n  filter(ansiedad == \"Sí\")\n\nggplot(datos_prev, aes(x = educacion, y = Prevalencia)) +\n  geom_col(fill = \"tomato\") +\n  scale_y_continuous(labels = scales::percent) +\n  labs(title = \"Prevalencia de ansiedad por nivel educativo\",\n       x = \"Nivel educativo\",\n       y = \"Prevalencia (porcentaje)\") +\n  theme_minimal(base_size = 14)\n\n\n\n\nPrevalencia de ansiedad por nivel educativo\n\n\n\n\n\n\n3.8.5 ¿Cómo sé si estoy viendo una tabla de prevalencia?\nMuy fácil. Preguntate:\n\n¿Estoy viendo qué porcentaje de cada grupo presenta cierta condición?\n¿Las proporciones están calculadas por fila?\n\nSi la respuesta es sí, estás frente a una tabla de prevalencia.\nY ahora viene lo más importante:\n\nNunca confundas una tabla de prevalencia con una tabla de distribución general.\n\nPorque si alguien te dice:\n\n“La mayoría de las personas con ansiedad tienen educación básica”, eso podría ser cierto simplemente porque hay más personas con ese nivel educativo.\n\nPero eso no significa que ese grupo tenga mayor prevalencia. Por eso, es clave identificar el tipo de tabla que estás mirando. Y saber desde qué total están calculadas las proporciones.\nExcelente. Esta sección va a mostrar con claridad cómo dos tablas con los mismos números absolutos pueden generar interpretaciones muy distintas, dependiendo de cómo se estructuren las proporciones. Es un momento ideal para profundizar en la lectura crítica y el concepto de condicionalidad."
  },
  {
    "objectID": "02b-tablas_final.html#dos-historias-una-tabla-cómo-el-mismo-dato-puede-decir-cosas-distintas",
    "href": "02b-tablas_final.html#dos-historias-una-tabla-cómo-el-mismo-dato-puede-decir-cosas-distintas",
    "title": "3  Tablas",
    "section": "3.9 Dos historias, una tabla: cómo el mismo dato puede decir cosas distintas",
    "text": "3.9 Dos historias, una tabla: cómo el mismo dato puede decir cosas distintas\nSupongamos que tenés la siguiente tabla. Es simple, clara, y 100% realista. En algún momento de tu formación (o carrera), vas a encontrarte con algo parecido:\n\ntabla &lt;- matrix(c(15, 25,\n                  10, 30,\n                  5, 35),\n                nrow = 3, byrow = TRUE)\n\ndimnames(tabla) &lt;- list(\n  Nivel_educativo = c(\"Básico\", \"Medio\", \"Superior\"),\n  Ansiedad = c(\"Sí\", \"No\")\n)\n\ntabla &lt;- as.table(tabla)\ntabla\n\n               Ansiedad\nNivel_educativo Sí No\n       Básico   15 25\n       Medio    10 30\n       Superior  5 35\n\n\nEsta tabla muestra cuántas personas de cada nivel educativo reportaron ansiedad clínica (“Sí”) o no (“No”). A simple vista, podrías pensar que el grupo de educación básica tiene más casos de ansiedad que el de nivel medio o superior. Pero, ¿eso significa que la ansiedad es más prevalente en ese grupo?\nDepende. De cómo leas la tabla.\n\n3.9.1 🧭 Historia 1: la tabla de prevalencia\nVamos a calcular proporciones por fila, es decir, respecto al total de personas dentro de cada grupo educativo:\n\nprop_fila &lt;- prop.table(tabla, margin = 1)\nround(prop_fila, 2)\n\n               Ansiedad\nNivel_educativo   Sí   No\n       Básico   0.38 0.62\n       Medio    0.25 0.75\n       Superior 0.12 0.88\n\n\nY ahí vemos:\n\n37.5% de ansiedad en educación básica\n25% en educación media\n12.5% en educación superior\n\nHistoria 1: la ansiedad es más prevalente entre personas con menor nivel educativo. Cuanto mayor el nivel, menor la prevalencia.\n\nEsta es una tabla de prevalencia.\n\n\n\n3.9.2 🧭 Historia 2: proporciones por columna\nAhora miremos la misma tabla, pero calculando proporciones por columna, es decir, respecto al total de personas que respondieron “Sí” o “No”:\n\nprop_col &lt;- prop.table(tabla, margin = 2)\nround(prop_col, 2)\n\n               Ansiedad\nNivel_educativo   Sí   No\n       Básico   0.50 0.28\n       Medio    0.33 0.33\n       Superior 0.17 0.39\n\n\nY ahora vemos otra historia:\n\nDe todas las personas con ansiedad (“Sí”), el 50% tiene educación básica\nEl 33% tiene educación media\nEl 17% tiene educación superior\n\nHistoria 2: la mayoría de las personas con ansiedad tienen bajo nivel educativo.\n\nEsta es una tabla de distribución condicional inversa: qué tipo de personas hay entre los que dijeron “sí”.\n\n\n\n3.9.3 🧠 ¿Cuál está “bien”?\nLas dos son válidas. Y las dos cuentan cosas distintas.\n\nLa tabla de prevalencia (por fila) te dice cómo afecta una condición a un grupo.\nLa tabla por columna te dice quiénes componen una categoría de respuesta.\n\nEl problema no está en cuál tabla usás, sino en creer que una te está diciendo lo que en realidad dice la otra.\n\n\n3.9.4 Visualizar ambas historias\n\n3.9.4.1 Historia 1: prevalencia\n\ndf_fila &lt;- as.data.frame(prop_fila)\n\nggplot(df_fila[df_fila$Ansiedad == \"Sí\", ], aes(x = Nivel_educativo, y = Freq)) +\n  geom_col(fill = \"steelblue\") +\n  scale_y_continuous(labels = scales::percent) +\n  labs(title = \"Prevalencia de ansiedad por nivel educativo\",\n       x = \"Nivel educativo\", y = \"Prevalencia\") +\n  theme_minimal(base_size = 14)\n\n\n\n\nPrevalencia de ansiedad por nivel educativo (proporciones por fila)\n\n\n\n\n\n\n3.9.4.2 Historia 2: composición del grupo ansioso\n\ndf_col &lt;- as.data.frame(prop_col)\n\nggplot(df_col[df_col$Ansiedad == \"Sí\", ], aes(x = Nivel_educativo, y = Freq)) +\n  geom_col(fill = \"darkred\") +\n  scale_y_continuous(labels = scales::percent) +\n  labs(title = \"¿Quiénes componen el grupo con ansiedad?\",\n       x = \"Nivel educativo\", y = \"Porcentaje dentro de la columna 'Sí'\") +\n  theme_minimal(base_size = 14)\n\n\n\n\nDistribución del nivel educativo entre quienes reportaron ansiedad (proporciones por columna)"
  },
  {
    "objectID": "02b-tablas_final.html#lo-que-ves-depende-de-cómo-mirás",
    "href": "02b-tablas_final.html#lo-que-ves-depende-de-cómo-mirás",
    "title": "3  Tablas",
    "section": "3.10 Lo que ves depende de cómo mirás",
    "text": "3.10 Lo que ves depende de cómo mirás\nEste es uno de los momentos clave del capítulo. La misma tabla puede sostener dos narrativas distintas, ambas basadas en números reales, pero con énfasis diferente. Y si no sabés cuál estás leyendo, podés llegar a conclusiones opuestas sin darte cuenta."
  },
  {
    "objectID": "02b-tablas_final.html#qué-porcentaje-de-qué-pensar-en-condicionales",
    "href": "02b-tablas_final.html#qué-porcentaje-de-qué-pensar-en-condicionales",
    "title": "3  Tablas",
    "section": "3.11 ¿Qué porcentaje de qué? Pensar en condicionales",
    "text": "3.11 ¿Qué porcentaje de qué? Pensar en condicionales\nLas proporciones no son neutrales. Siempre están condicionadas a algo. Decir que “el 30% de las personas tienen ansiedad” no es lo mismo que decir “el 30% de las personas con ansiedad son estudiantes”.\nA simple vista, ambas frases parecen similares. Pero están diciendo cosas completamente distintas.\nEsta distinción no es un detalle técnico. Es una herramienta fundamental para pensar correctamente en datos categóricos. Y es una de las fuentes más comunes de confusión en psicología, salud y ciencias sociales.\n\n3.11.1 Dos preguntas, dos tablas\nVolvamos al ejemplo anterior con ansiedad y nivel educativo. Recordá que teníamos la siguiente tabla:\n\ntabla\n\n               Ansiedad\nNivel_educativo Sí No\n       Básico   15 25\n       Medio    10 30\n       Superior  5 35\n\n\nA partir de esta tabla, podés hacer dos preguntas:\n\nP1. Entre las personas de nivel educativo básico, ¿qué proporción tiene ansiedad?\nP2. Entre las personas con ansiedad, ¿qué proporción tiene nivel educativo básico?\n\nLas dos preguntas son válidas. Pero están invirtiendo la condicionalidad.\n\n\n3.11.2 Proporciones por fila (P1)\n\nprop_fila\n\n               Ansiedad\nNivel_educativo    Sí    No\n       Básico   0.375 0.625\n       Medio    0.250 0.750\n       Superior 0.125 0.875\n\n\nEsta tabla responde a la primera pregunta: ¿qué porcentaje de cada grupo educativo tiene ansiedad?\n\nEs decir: “Dado el grupo, ¿cuál es la respuesta?”\n\n\n\n3.11.3 Proporciones por columna (P2)\n\nprop_col\n\n               Ansiedad\nNivel_educativo        Sí        No\n       Básico   0.5000000 0.2777778\n       Medio    0.3333333 0.3333333\n       Superior 0.1666667 0.3888889\n\n\nEsta tabla responde a la segunda pregunta: ¿qué porcentaje de las personas con ansiedad pertenece a cada grupo educativo?\n\nEs decir: “Dada la respuesta, ¿de dónde viene?”\n\n\n\n3.11.4 No confundas estas dos\nEsto puede parecer obvio en teoría, pero en la práctica se confunde todo el tiempo. Y las consecuencias son reales. Por ejemplo:\n\nPodés tener una condición que afecta proporcionalmente más a un grupo (mayor prevalencia)… …pero que está compuesta en su mayoría por otro grupo (mayor peso absoluto).\nPodés pensar que hay una “relación fuerte” entre dos variables, solo porque no estás viendo desde qué total se calculó cada proporción.\n\n\n\n3.11.5 ¿Cómo saber cuál estás viendo?\nBuena práctica: mirá cómo están normalizadas las proporciones. Si cada fila suma 1 (o 100%), entonces estás viendo proporciones condicionales al grupo. Si cada columna suma 1, entonces estás viendo proporciones condicionales a la respuesta.\nVisualmente, podés graficarlo así:\n\n3.11.5.1 ¿Qué porcentaje tiene ansiedad dentro de cada grupo?\n\ndf_f &lt;- as.data.frame(prop_fila)\n\nggplot(df_f, aes(x = Nivel_educativo, y = Freq, fill = Ansiedad)) +\n  geom_col(position = \"fill\") +\n  scale_y_continuous(labels = scales::percent) +\n  labs(title = \"¿Qué porcentaje tiene ansiedad dentro de cada grupo educativo?\",\n       y = \"Proporción dentro del grupo\", x = \"Nivel educativo\") +\n  theme_minimal(base_size = 14)\n\n\n\n\nProporciones condicionales por grupo educativo (fila)\n\n\n\n\n\n\n3.11.5.2 ¿De qué grupo vienen quienes tienen ansiedad?\n\ndf_c &lt;- as.data.frame(prop_col)\n\nggplot(df_c, aes(x = Ansiedad, y = Freq, fill = Nivel_educativo)) +\n  geom_col(position = \"fill\") +\n  scale_y_continuous(labels = scales::percent) +\n  labs(title = \"¿De qué grupo educativo vienen quienes tienen ansiedad?\",\n       y = \"Proporción dentro de cada respuesta\", x = \"Respuesta\") +\n  theme_minimal(base_size = 14)\n\n\n\n\nProporciones condicionales dentro del grupo que reportó ansiedad (columna)\n\n\n\n\n\n\n\n3.11.6 Entender condicionales ≠ solo leer bien\nEntender bien las proporciones condicionales no solo evita errores. También te entrena para algo más grande: pensar en términos de relaciones causales, probabilísticas y contextuales. Algo que vas a necesitar en casi todo lo que viene después.\nPerfecto, vamos con la sección sobre incidencia, diferenciándola cuidadosamente de la prevalencia, usando un tono narrativo y orientado a ejemplos en psicología, como siempre."
  },
  {
    "objectID": "02b-tablas_final.html#incidencia-cuando-contar-también-implica-tiempo",
    "href": "02b-tablas_final.html#incidencia-cuando-contar-también-implica-tiempo",
    "title": "3  Tablas",
    "section": "3.12 Incidencia: cuando contar también implica tiempo",
    "text": "3.12 Incidencia: cuando contar también implica tiempo\nHasta ahora veníamos preguntándonos cuántas personas tienen algo. Pero en muchos casos, la pregunta más importante no es quién tiene, sino quién empezó a tener.\nEsa diferencia —sutil pero crucial— nos lleva a un nuevo concepto: la incidencia.\n\n3.12.1 ¿Qué es la incidencia?\nLa incidencia es la proporción de nuevos casos de una condición que aparecen en una población durante un período determinado de tiempo.\nSi la prevalencia te dice cuántas personas tienen ansiedad ahora, la incidencia te dice cuántas personas desarrollaron ansiedad desde la última vez que mediste.\n\n\n3.12.2 ¿Por qué importa?\nPorque no es lo mismo que muchas personas tengan algo, a que muchas personas empiecen a tenerlo.\nUn trastorno puede tener:\n\nAlta prevalencia y baja incidencia: por ejemplo, una condición crónica que pocas personas desarrollan, pero que dura mucho tiempo (como la esquizofrenia).\nBaja prevalencia y alta incidencia: por ejemplo, una reacción aguda y breve que mucha gente experimenta pero que se resuelve rápido (como ataques de pánico en situaciones específicas).\n\nY eso cambia radicalmente lo que entendemos del fenómeno.\n\n\n3.12.3 Ejemplo: ansiedad en estudiantes al empezar el semestre\nSupongamos que medimos ansiedad clínica en una muestra de estudiantes al comienzo y al final del primer mes del semestre. De los 100 estudiantes:\n\n10 ya tenían ansiedad cuando comenzó el semestre → prevalencia al inicio.\n15 desarrollaron ansiedad durante el primer mes → incidencia en el primer mes.\n\nEsto nos dice que hubo 15 nuevos casos, y que la incidencia mensual fue del 15%.\n\n\n3.12.4 Diferencias en visualización\nPara mostrar estas diferencias, necesitamos saber cuándo se midió cada cosa.\n\nlibrary(tibble)\n\nestudiantes &lt;- tibble(\n  ID = 1:100,\n  Ansiedad_inicio = c(rep(\"Sí\", 10), rep(\"No\", 90)),\n  Ansiedad_mes1 = c(rep(\"Sí\", 10), rep(\"Sí\", 15), rep(\"No\", 75))\n)\n\ntabla_cambios &lt;- table(estudiantes$Ansiedad_inicio, estudiantes$Ansiedad_mes1)\ndimnames(tabla_cambios) &lt;- list(\"Inicio\" = c(\"No\", \"Sí\"), \"Mes1\" = c(\"No\", \"Sí\"))\ntabla_cambios\n\n      Mes1\nInicio No Sí\n    No 75 15\n    Sí  0 10\n\n\nEsto muestra cuántas personas desarrollaron ansiedad (de “No” a “Sí”) durante el mes.\n\n\n3.12.5 Calcular la incidencia\n\n# Casos que pasaron de No a Sí\ncasos_nuevos &lt;- tabla_cambios[\"No\", \"Sí\"]\n\n# Total de personas que no tenían ansiedad al inicio\npoblacion_riesgo &lt;- sum(tabla_cambios[\"No\", ])\n\nincidencia &lt;- casos_nuevos / poblacion_riesgo\nround(incidencia, 2)\n\n[1] 0.17\n\n\n\nResultado: 15 de 90 personas sin ansiedad al inicio desarrollaron ansiedad en el primer mes. La incidencia fue del 17%.\n\n\n\n3.12.6 Comparar no es mezclar\nUna tabla puede mostrarte:\n\nla prevalencia actual, si te dice “cuántos tienen ansiedad ahora”;\no la incidencia, si te muestra “cuántos nuevos casos hubo entre los que no tenían”.\n\nSi no sabés cuál es cuál, podés malinterpretar el fenómeno. Por ejemplo, podés pensar que un tratamiento fue efectivo porque bajó la prevalencia, sin darte cuenta de que la incidencia sigue siendo alta."
  },
  {
    "objectID": "02b-tablas_final.html#lo-que-cuentan-las-tablas-y-lo-que-no",
    "href": "02b-tablas_final.html#lo-que-cuentan-las-tablas-y-lo-que-no",
    "title": "3  Tablas",
    "section": "3.13 Lo que cuentan las tablas… y lo que no",
    "text": "3.13 Lo que cuentan las tablas… y lo que no\nA lo largo de este capítulo recorrimos un camino que parece sencillo: contar cosas. Pero con cada paso, ese conteo se volvió más rico, más preciso… y también más propenso a malinterpretaciones.\nAprendimos que no todas las frecuencias son iguales. Que contar puede significar:\n\nsimplemente saber cuántos casos hay (frecuencia absoluta),\no qué proporción representan (frecuencia relativa y porcentual),\no incluso, en casos con orden, cuántos están por debajo de cierto umbral (frecuencia acumulada).\n\nVimos que cuando dos variables se cruzan, todo se vuelve más interesante —y más confuso. Que no basta con ver los números: hay que saber desde qué total se están leyendo.\nNos enfrentamos a la diferencia entre prevalencia e incidencia, entre contar lo que hay y contar lo que empieza a haber. Entre saber cuántos y saber cuándo.\nY, sobre todo, aprendimos algo fundamental:\n\nLos números en una tabla no hablan por sí solos. Necesitan contexto. Dirección. Una lectura cuidadosa."
  },
  {
    "objectID": "02b-tablas_final.html#y-si-lo-que-vemos-es-solo-azar",
    "href": "02b-tablas_final.html#y-si-lo-que-vemos-es-solo-azar",
    "title": "3  Tablas",
    "section": "3.14 ¿Y si lo que vemos es solo azar?",
    "text": "3.14 ¿Y si lo que vemos es solo azar?\nAl final de este recorrido, aparece una nueva inquietud. Una que viene acechando desde el principio, aunque no la hayamos dicho en voz alta:\n\n¿Y si las diferencias que veo en esta tabla se deben simplemente al azar?\n\nCuando ves que un grupo tiene más casos que otro… Cuando una categoría aparece con más frecuencia… Cuando una proporción parece desbalanceada…\n… ¿cómo sabés que eso no es simplemente un efecto de la muestra? ¿De cómo justo se dieron los datos?\nAhí es donde empieza la inferencia estadística.\nNo porque los datos no alcancen, sino porque queremos ir más allá. No para complicarnos, sino para poder decir con mayor confianza:\n\n“esto que vemos no es casualidad”.\n\nY eso, precisamente, es lo que vamos a empezar a explorar en el próximo capítulo.\nPero antes de probar nada, hay que saber cómo se mide una diferencia. Y para eso, necesitamos dar otro paso."
  },
  {
    "objectID": "06b-chi_cuadrado.html#de-promedios-a-categorías",
    "href": "06b-chi_cuadrado.html#de-promedios-a-categorías",
    "title": "8  Pruebas chi cuadrado",
    "section": "8.1 De promedios a categorías",
    "text": "8.1 De promedios a categorías\nHasta ahora, cuando queríamos saber si había una diferencia entre grupos, lo hacíamos comparando promedios. Por ejemplo: ¿tienen más ansiedad las personas que hacen ejercicio que las que no? ¿Tienen mayor rendimiento los estudiantes que duermen más horas? Para responder eso, usábamos la prueba t.\nPero no todas las preguntas se contestan con medias. Muchas veces, especialmente en psicología y ciencias sociales, lo que tenemos no son números continuos, sino categorías. Personas que dijeron “sí” o “no”. Participantes que eligieron una opción. Pacientes con o sin diagnóstico. Grupos definidos por género, edad, nivel educativo. Respuestas a ítems cerrados. Datos que no se miden en una escala, sino que se cuentan.\nY entonces, lo que cambia no es solo el tipo de dato. Cambia también la pregunta estadística. Ya no preguntamos si el promedio de un grupo es mayor que el del otro. Preguntamos algo más básico, pero igual de importante: ¿Lo que estamos viendo en estas categorías podría haber ocurrido solo por azar?"
  },
  {
    "objectID": "06b-chi_cuadrado.html#una-tabla-que-da-que-pensar",
    "href": "06b-chi_cuadrado.html#una-tabla-que-da-que-pensar",
    "title": "8  Pruebas chi cuadrado",
    "section": "8.2 Una tabla que da que pensar",
    "text": "8.2 Una tabla que da que pensar\nImaginemos que queremos saber si hay una relación entre el nivel educativo y la postura frente a la eutanasia. Entrevistamos a un grupo de personas y les preguntamos dos cosas:\n\n¿Cuál es su nivel educativo?\n¿Está usted a favor o en contra de la legalización de la eutanasia?\n\nLos datos nos dan esta tabla:\n\n\n\nNivel educativo\nA favor\nEn contra\n\n\n\n\nBajo\n25\n15\n\n\nMedio\n30\n10\n\n\nAlto\n35\n5\n\n\n\nA simple vista, parece haber una tendencia. Cuanto mayor el nivel educativo, mayor el apoyo a la eutanasia. Pero… ¿esa diferencia es real? ¿O podría deberse simplemente al azar?"
  },
  {
    "objectID": "06b-chi_cuadrado.html#la-lógica-inferencial-otra-vez",
    "href": "06b-chi_cuadrado.html#la-lógica-inferencial-otra-vez",
    "title": "8  Pruebas chi cuadrado",
    "section": "8.3 La lógica inferencial, otra vez",
    "text": "8.3 La lógica inferencial, otra vez\nEsta pregunta —que parece nueva— en realidad es la misma que nos hacíamos con las medias. Lo único que cambió es el tipo de dato.\nCon la prueba t, comparamos dos promedios. Con chi cuadrado, comparamos lo que observamos en una tabla de frecuencias con lo que esperaríamos ver si no hubiera ninguna diferencia.\nLa lógica es la misma:\n\nTenemos una hipótesis nula: no hay relación entre las variables.\nTenemos una observación: en la muestra, las frecuencias no son iguales.\nY queremos saber: ¿esas diferencias son demasiado grandes como para explicarlas por azar?"
  },
  {
    "objectID": "06b-chi_cuadrado.html#el-plan-para-este-capítulo",
    "href": "06b-chi_cuadrado.html#el-plan-para-este-capítulo",
    "title": "8  Pruebas chi cuadrado",
    "section": "8.4 El plan para este capítulo",
    "text": "8.4 El plan para este capítulo\nA lo largo de este capítulo vamos a aprender a responder esa pregunta. Pero, como siempre, lo haremos paso a paso:\n\nPrimero vamos a aprender a mirar una tabla y preguntarnos qué esperaríamos ver si no hubiera relación.\nDespués vamos a aprender a medir cuán distintas son las frecuencias observadas de las esperadas.\nVamos a ver dos tipos de prueba chi cuadrado:\n\nuna para ver si una variable sigue cierta distribución (bondad de ajuste),\notra para ver si dos variables están relacionadas (independencia).\n\nVamos a usar gráficos y simulaciones para entender mejor qué está en juego.\nY vamos a terminar con un repaso de lo que podemos y no podemos concluir con esta herramienta.\n\nPero antes de hacer cuentas, fórmulas o tests, hay algo que siempre conviene hacer primero: mirar bien los datos, y preguntarse si lo que estamos viendo coincide con lo que esperaríamos ver si no pasara nada especial."
  },
  {
    "objectID": "06b-chi_cuadrado.html#observar-no-es-lo-mismo-que-esperar",
    "href": "06b-chi_cuadrado.html#observar-no-es-lo-mismo-que-esperar",
    "title": "8  Pruebas chi cuadrado",
    "section": "8.5 Observar no es lo mismo que esperar",
    "text": "8.5 Observar no es lo mismo que esperar\nCuando miramos una tabla de frecuencias, solemos enfocarnos en los números que están ahí. Cuántas personas dijeron sí. Cuántas dijeron no. Cuántas eligieron cada opción. Pero si queremos hacer inferencia, no alcanza con ver qué hay. Tenemos que preguntarnos: ¿Esto que hay… es lo que uno esperaría ver si no hubiera ningún efecto?\nEsa es la clave. Porque las diferencias entre grupos, por sí solas, no dicen nada. En cualquier muestra real vamos a tener variaciones. Lo importante es saber si esas variaciones son demasiado grandes como para atribuirlas al azar. Y para eso, primero tenemos que imaginar cómo se verían los datos si no hubiera ninguna relación entre las variables."
  },
  {
    "objectID": "06b-chi_cuadrado.html#un-ejemplo-simple",
    "href": "06b-chi_cuadrado.html#un-ejemplo-simple",
    "title": "8  Pruebas chi cuadrado",
    "section": "8.6 Un ejemplo simple",
    "text": "8.6 Un ejemplo simple\nSupongamos que queremos saber si hay una preferencia de colores en los caramelos que vende una empresa. Según la etiqueta, cada paquete debería tener la misma cantidad de cinco colores: rojo, azul, verde, amarillo y violeta.\nCompramos un paquete, lo abrimos, y contamos los caramelos. Obtenemos:\n\n#&gt;     Rojo     Azul    Verde Amarillo  Violeta \n#&gt;        6        2        5        3        4\n\nLo que vemos es que hay más rojos, menos azules, y los otros colores más o menos cerca de lo prometido. Pero la pregunta no es si hay diferencias. Las hay. La pregunta es: ¿Estas diferencias son lo suficientemente grandes como para pensar que los caramelos no están bien distribuidos?\nSi la empresa cumpliera con su promesa, y la distribución fuera perfectamente equitativa, deberíamos haber visto 4 caramelos de cada color:\n\n#&gt;     Rojo     Azul    Verde Amarillo  Violeta \n#&gt;        4        4        4        4        4"
  },
  {
    "objectID": "06b-chi_cuadrado.html#pongámoslos-uno-al-lado-del-otro",
    "href": "06b-chi_cuadrado.html#pongámoslos-uno-al-lado-del-otro",
    "title": "8  Pruebas chi cuadrado",
    "section": "8.7 Pongámoslos uno al lado del otro",
    "text": "8.7 Pongámoslos uno al lado del otro\nPara verlo más claro, vamos a graficar las frecuencias observadas y esperadas:\n\n\n\n\n\nFrecuencias observadas y esperadas por color de caramelo\n\n\n\n\nAhora sí podemos ver la diferencia. El color rojo tiene 6 caramelos, cuando deberían ser 4. El azul, apenas 2. El resto está más cerca de lo esperado, pero también hay pequeñas desviaciones.\nA simple vista, podríamos decir: parece haber un pequeño desequilibrio. Pero, otra vez, lo que importa no es solo que haya diferencia, sino cuánto importa esa diferencia.\nPor eso, vamos a dar un paso más: vamos a mirar celda por celda, y tratar de identificar dónde están las discrepancias más grandes, y si pueden ser explicadas por el azar.\nPerfecto, seguimos con la sección que pone el foco en las discrepancias observadas–esperadas, no solo para ver “cuánto se desviaron” los datos, sino para visualizar qué tan fuerte es esa discrepancia en cada categoría. Esta sección culmina con un mapa de calor interpretativo, que ilustra el nivel de evidencia a favor o en contra de la hipótesis nula."
  },
  {
    "objectID": "06b-chi_cuadrado.html#dónde-están-las-diferencias-más-fuertes",
    "href": "06b-chi_cuadrado.html#dónde-están-las-diferencias-más-fuertes",
    "title": "8  Pruebas chi cuadrado",
    "section": "8.8 ¿Dónde están las diferencias más fuertes?",
    "text": "8.8 ¿Dónde están las diferencias más fuertes?\nYa vimos los datos observados. Ya sabemos lo que esperaríamos ver si los caramelos estuvieran perfectamente distribuidos. Pero una pregunta sigue abierta:\n\n¿Qué tanto se desvió cada color de lo que debería haber sido?\n\nY más aún:\n\n¿Cuáles de esas diferencias son suficientemente grandes como para dudar de la hipótesis nula?\n\nPorque en la práctica, incluso si la empresa es honesta y mezcla bien los colores, es normal que en un paquete aparezcan 3 verdes o 5 amarillos. Lo que necesitamos es una forma de medir cada discrepancia en su contexto. Que nos diga: esto está dentro de lo esperable, o esto sí parece demasiado."
  },
  {
    "objectID": "06b-chi_cuadrado.html#residuales-estandarizados-cuánto-se-desvía-cada-celda",
    "href": "06b-chi_cuadrado.html#residuales-estandarizados-cuánto-se-desvía-cada-celda",
    "title": "8  Pruebas chi cuadrado",
    "section": "8.9 Residuales estandarizados: cuánto se desvía cada celda",
    "text": "8.9 Residuales estandarizados: cuánto se desvía cada celda\nPara cada color, podemos calcular una medida que nos diga cuánto se aleja lo observado de lo esperado, pero ajustado al valor esperado. Se llama residual estandarizado y se calcula así:\n\\[\n\\text{Residual estandarizado} = \\frac{O - E}{\\sqrt{E}}\n\\]\n\nSi el valor está cerca de 0, lo observado es parecido a lo esperado → no hay evidencia de problema.\nSi el valor es mayor a +2 o menor a -2, la discrepancia es grande en relación a lo esperado → evidencia en contra de la hipótesis nula.\nSi es menor a ±1, podemos decir que la diferencia es irrelevante → evidencia a favor de la nula.\n\n\n\n\n\n\n¿Dónde están las mayores discrepancias? Evidencia por celda\n\n\n\n\nEsta visualización no reemplaza el test estadístico. Pero lo complementa de forma poderosa. Nos dice, sin fórmulas, sin cálculo manual:\n\nDónde están las discrepancias más marcadas.\nEn qué dirección van (más o menos de lo esperado).\nY si esas discrepancias son lo bastante grandes como para generar sospechas.\n\nEn este ejemplo:\n\nEl color rojo aparece bastante más de lo esperado → residual positivo grande → evidencia en contra de la hipótesis nula.\nEl azul aparece bastante menos → residual negativo → también evidencia en contra.\nOtros colores están cerca de lo esperado → residual bajo → evidencia a favor de la nula.\n\nY ahora que sabemos qué tanto se desvían los datos celda por celda, podemos avanzar al siguiente paso: resumir todas esas discrepancias en un único número. Ese número es el estadístico chi cuadrado.\nPerfecto, vamos a seguir con la sección donde presentamos de forma intuitiva el estadístico chi cuadrado y lo conectamos con su distribución de referencia, ilustrando cómo cambia con los grados de libertad y cómo se interpreta el valor p. Incluiré visualizaciones y dejaré preparado el terreno para una animación opcional de simulación más adelante."
  },
  {
    "objectID": "06b-chi_cuadrado.html#un-número-para-resumir-toda-la-discrepancia",
    "href": "06b-chi_cuadrado.html#un-número-para-resumir-toda-la-discrepancia",
    "title": "8  Pruebas chi cuadrado",
    "section": "8.10 Un número para resumir toda la discrepancia",
    "text": "8.10 Un número para resumir toda la discrepancia\nYa calculamos las diferencias entre lo observado y lo esperado. Las miramos una por una, y hasta las coloreamos según si apoyaban o desafiaban la hipótesis nula. Ahora vamos a dar el siguiente paso: resumir todas esas diferencias en un solo número.\nEse número es el estadístico chi cuadrado, y su lógica es simple:\n\nCuanto más lejos estén los valores observados de los esperados, más grande será el valor de \\(\\chi^2\\).\n\nY si el valor de \\(\\chi^2\\) es muy grande, tenemos motivos para pensar que las diferencias no se deben solo al azar.\n\n\n8.10.1 Cómo se calcula\nEl estadístico chi cuadrado se define así:\n\\[\n\\chi^2 = \\sum \\frac{(O - E)^2}{E}\n\\]\n\n\\(O\\) = frecuencia observada\n\\(E\\) = frecuencia esperada\n\nEsto se hace celda por celda, y después se suman todos los valores. El resultado es un único número, que representa el total de discrepancia relativa en la tabla.\n\n\n\n8.10.2 Calculemos el \\(\\chi^2\\) de los caramelos\n\n#&gt; \n#&gt;  Chi-squared test for given probabilities\n#&gt; \n#&gt; data:  observado\n#&gt; X-squared = 2.5, df = 4, p-value = 0.6446\n\nEste resultado nos da:\n\nEl valor de \\(\\chi^2\\)\nLos grados de libertad, que en este caso son: número de categorías − 1 = 4\nEl valor p\n\nPero… ¿qué significa ese número de \\(\\chi^2\\)? ¿Es grande? ¿Es chico? ¿Cómo lo interpretamos?\nPara eso, necesitamos ver la distribución chi cuadrado."
  },
  {
    "objectID": "06b-chi_cuadrado.html#la-distribución-chi-cuadrado",
    "href": "06b-chi_cuadrado.html#la-distribución-chi-cuadrado",
    "title": "8  Pruebas chi cuadrado",
    "section": "8.11 La distribución chi cuadrado",
    "text": "8.11 La distribución chi cuadrado\nIgual que la t, el estadístico \\(\\chi^2\\) tiene una distribución de referencia. Esa distribución nos dice qué tan comunes o raros son distintos valores de \\(\\chi^2\\) si la hipótesis nula fuera cierta.\nLa distribución chi cuadrado:\n\nSolo tiene valores positivos (porque las discrepancias al cuadrado nunca dan negativo).\nTiene una forma asimétrica: es más alta cerca de 0, y cae rápidamente.\nCambia de forma según los grados de libertad (df).\n\nVeamos cómo se ve:\n\n\n\n\n\nDistribuciones chi cuadrado para diferentes grados de libertad\n\n\n\n\n\nCon df = 1, la curva es muy sesgada: muchas observaciones cerca de 0, y pocas muy grandes.\nA medida que aumentan los grados de libertad, la curva se “desplaza” hacia la derecha y se va haciendo más simétrica.\nSiempre se mantiene positiva: no hay valores negativos de \\(\\chi^2\\).\n\nEl valor p es el área bajo la curva, a la derecha del valor observado. Eso representa: ¿qué tan probable es obtener un valor así (o mayor) si la hipótesis nula fuera cierta?"
  },
  {
    "objectID": "06b-chi_cuadrado.html#cómo-cambia-la-distribución-chi-cuadrado",
    "href": "06b-chi_cuadrado.html#cómo-cambia-la-distribución-chi-cuadrado",
    "title": "8  Pruebas chi cuadrado",
    "section": "8.12 Cómo cambia la distribución chi cuadrado",
    "text": "8.12 Cómo cambia la distribución chi cuadrado\nYa vimos que la distribución chi cuadrado es la que usamos para decidir si un valor observado de \\(\\chi^2\\) es raro o no, bajo la hipótesis nula. Pero algo que aún no exploramos es cómo cambia esa distribución dependiendo de los grados de libertad.\nEsto es importante por dos motivos:\n\nEl número de grados de libertad (df) depende de la cantidad de categorías que estamos comparando.\nLa forma de la distribución determina qué valores de \\(\\chi^2\\) son considerados grandes o raros.\n\nA menor df, la curva es muy sesgada hacia la izquierda, y valores grandes son extremadamente improbables. A mayor df, la curva se desplaza y se vuelve más simétrica.\n\n8.12.1 Una animación para verlo mejor\nEn lugar de mostrar una curva estática para cada valor de df, vamos a ver cómo evoluciona la forma de la distribución chi cuadrado cuando los grados de libertad aumentan.\n\nknitr::include_graphics(\"imgs/gifs/distribucion_chi2_animada.gif\")\n\n\nNota: Este bloque debe ejecutarse en un entorno Python con matplotlib y pillow instalados. El resultado será una animación .gif que puede incrustarse en el documento.\n\n\n\n8.12.2 ¿Qué muestra la animación?\n\nCon df = 1, la curva es muy concentrada cerca de 0 y cae abruptamente.\nA medida que df aumenta, la curva se desplaza hacia la derecha y se vuelve más suave.\nSiempre es una distribución de cola positiva (nunca hay valores negativos).\n\nEsto es fundamental para interpretar el valor p. Lo que consideramos “inusual” depende del df. Un valor de \\(\\chi^2 = 10\\) puede ser muy raro si df = 2, pero bastante común si df = 10. Perfecto. A continuación integro la sección completa al estilo del capítulo, donde mostramos mediante simulación cómo varía el valor de \\(\\chi^2\\) cuando los datos no contienen un efecto real, es decir, cuando la hipótesis nula es verdadera. Esto complementa la intuición sobre el valor p, mostrando cómo se genera su distribución bajo la nula."
  },
  {
    "objectID": "06b-chi_cuadrado.html#qué-tan-grande-es-grande-simular-para-entender",
    "href": "06b-chi_cuadrado.html#qué-tan-grande-es-grande-simular-para-entender",
    "title": "8  Pruebas chi cuadrado",
    "section": "8.13 ¿Qué tan grande es grande? Simular para entender",
    "text": "8.13 ¿Qué tan grande es grande? Simular para entender\nYa sabemos que el estadístico chi cuadrado mide cuán distintas son las frecuencias observadas de las esperadas, y que su valor aumenta cuando las diferencias son grandes. Y también sabemos que para interpretar ese valor, lo comparamos con una distribución teórica: la distribución chi cuadrado.\nPero ahora vamos a hacer algo distinto. No vamos a mirar la fórmula. Vamos a simular el azar. Vamos a generar miles de situaciones donde la hipótesis nula sea cierta, y vamos a ver qué valores de \\(\\chi^2\\) produce el azar por sí solo.\nLa idea es simple:\n\nSi el azar genera a veces valores como el que observamos, entonces lo que vimos podría no ser tan raro. Pero si el azar casi nunca produce valores así… entonces lo que vimos probablemente no sea azar.\n\n\n8.13.1 Simulemos caramelos honestos\nSupongamos que la empresa de caramelos efectivamente cumple con la proporción uniforme (20% para cada color). Simulamos muchos paquetes de 80 caramelos, cada uno con colores elegidos al azar con esa distribución, y para cada uno calculamos su \\(\\chi^2\\).\n\n\n\n\n\nDistribución simulada del estadístico chi cuadrado bajo la hipótesis nula\n\n\n\n\nLa línea roja muestra el valor de \\(\\chi^2\\) que obtuvimos con nuestro paquete de caramelos. Las barras muestran qué tan comunes son distintos valores de \\(\\chi^2\\) si la distribución es justa. Si la línea roja está muy a la derecha —en la cola de la distribución— entonces el valor observado es raro bajo la nula. Y eso es exactamente lo que nos dice el valor p.\n\n\n8.13.2 El valor p, una vez más\nPodemos calcular el valor p directamente desde esta simulación:\n\n#&gt; [1] 0.673\n\nEste número es la proporción de simulaciones que dieron un valor de \\(\\chi^2\\) igual o mayor al observado. Es decir: una versión empírica del valor p. Y debería coincidir bastante bien con el valor p que calculamos antes con chisq.test().\n¿Por qué esto importa? Porque a veces, mirar un número no alcanza. Pero ver cuán frecuente —o infrecuente— es ese número en un mundo donde no hay efecto real, puede ayudar a construir una mejor intuición. Y eso es lo que hace el valor p: te dice qué tan probable es tu resultado si todo fuera azar.\nPerfecto, vamos ahora con la segunda gran aplicación de la prueba chi cuadrado: la prueba de independencia entre dos variables categóricas. Retomamos el estilo narrativo y visual, incorporando código en R y guiando al lector paso a paso para entender qué significa “independencia”, cómo se construyen las esperadas, cómo se ve la discrepancia y cómo se interpreta el test."
  },
  {
    "objectID": "06b-chi_cuadrado.html#dos-variables-una-pregunta-están-relacionadas",
    "href": "06b-chi_cuadrado.html#dos-variables-una-pregunta-están-relacionadas",
    "title": "8  Pruebas chi cuadrado",
    "section": "8.14 Dos variables, una pregunta: ¿están relacionadas?",
    "text": "8.14 Dos variables, una pregunta: ¿están relacionadas?\nHasta ahora usamos chi cuadrado para ver si una sola variable se distribuía como esperábamos. Pero la pregunta más común en psicología y ciencias sociales es otra:\n\n¿Están relacionadas estas dos variables?\n\nPor ejemplo:\n\n¿Están asociadas la orientación política y la actitud frente a la legalización del cannabis?\n¿Varía el apoyo a la eutanasia según el nivel educativo?\n¿Hay diferencias de género en la elección de carrera?\n\nLo que queremos saber no es si los porcentajes son iguales. Lo que queremos saber es si las diferencias que vemos en la tabla podrían haberse generado por azar."
  },
  {
    "objectID": "06b-chi_cuadrado.html#un-ejemplo-realista-eutanasia-y-nivel-educativo",
    "href": "06b-chi_cuadrado.html#un-ejemplo-realista-eutanasia-y-nivel-educativo",
    "title": "8  Pruebas chi cuadrado",
    "section": "8.15 Un ejemplo realista: eutanasia y nivel educativo",
    "text": "8.15 Un ejemplo realista: eutanasia y nivel educativo\nSupongamos que en una encuesta preguntamos dos cosas:\n\n¿Está usted a favor o en contra de la legalización de la eutanasia?\n¿Cuál es su nivel educativo (bajo, medio, alto)?\n\nLas respuestas nos dan esta tabla:\n\n#&gt;        \n#&gt;         A favor En contra\n#&gt;   Alto       35         5\n#&gt;   Bajo       25        15\n#&gt;   Medio      30        10"
  },
  {
    "objectID": "06b-chi_cuadrado.html#visualizar-la-tabla-proporciones-por-fila",
    "href": "06b-chi_cuadrado.html#visualizar-la-tabla-proporciones-por-fila",
    "title": "8  Pruebas chi cuadrado",
    "section": "8.16 Visualizar la tabla: proporciones por fila",
    "text": "8.16 Visualizar la tabla: proporciones por fila\nPara ver mejor la tendencia, graficamos la proporción de personas a favor y en contra dentro de cada nivel educativo.\n\n\n\n\n\nDistribución de opiniones por nivel educativo\n\n\n\n\nParece haber un patrón: cuanto mayor el nivel educativo, mayor el apoyo a la eutanasia. Pero… ¿es esa diferencia suficientemente grande como para pensar que las variables están asociadas?"
  },
  {
    "objectID": "06b-chi_cuadrado.html#qué-esperaríamos-ver-si-no-hubiera-relación",
    "href": "06b-chi_cuadrado.html#qué-esperaríamos-ver-si-no-hubiera-relación",
    "title": "8  Pruebas chi cuadrado",
    "section": "8.17 ¿Qué esperaríamos ver si no hubiera relación?",
    "text": "8.17 ¿Qué esperaríamos ver si no hubiera relación?\nSi nivel educativo y opinión fueran independientes, esperaríamos ver una distribución en la que la proporción de personas a favor o en contra fuera la misma en todos los niveles.\nEso significa que las frecuencias esperadas se calculan como producto de los totales marginales. R lo hace por nosotros:\n\n#&gt;        \n#&gt;         A favor En contra\n#&gt;   Alto       30        10\n#&gt;   Bajo       30        10\n#&gt;   Medio      30        10"
  },
  {
    "objectID": "06b-chi_cuadrado.html#dónde-están-las-mayores-discrepancias",
    "href": "06b-chi_cuadrado.html#dónde-están-las-mayores-discrepancias",
    "title": "8  Pruebas chi cuadrado",
    "section": "8.18 ¿Dónde están las mayores discrepancias?",
    "text": "8.18 ¿Dónde están las mayores discrepancias?\nPodemos usar el mismo enfoque visual que antes: calcular residuales estandarizados y clasificarlos según el nivel de evidencia frente a la hipótesis nula.\n\n\n\n\n\nEvidencia por celda: diferencias entre observadas y esperadas"
  },
  {
    "objectID": "06b-chi_cuadrado.html#aplicar-la-prueba-chi-cuadrado",
    "href": "06b-chi_cuadrado.html#aplicar-la-prueba-chi-cuadrado",
    "title": "8  Pruebas chi cuadrado",
    "section": "8.19 Aplicar la prueba chi cuadrado",
    "text": "8.19 Aplicar la prueba chi cuadrado\n\n#&gt; \n#&gt;  Pearson's Chi-squared test\n#&gt; \n#&gt; data:  tabla\n#&gt; X-squared = 6.6667, df = 2, p-value = 0.03567\n\nLa prueba nos da el valor de \\(\\chi^2\\), los grados de libertad, y el valor p.\n\nSi p &lt; 0.05 → rechazamos la hipótesis de independencia → hay evidencia de asociación.\nSi p &gt; 0.05 → no tenemos evidencia suficiente para decir que las variables están relacionadas.\n\nExcelente, tiene todo el sentido cerrar con ambos: primero una sección que advierta sobre errores comunes, para prevenir malos usos, y luego un resumen conceptual que refuerce los aprendizajes clave del capítulo."
  },
  {
    "objectID": "06b-chi_cuadrado.html#errores-comunes-y-buenas-prácticas",
    "href": "06b-chi_cuadrado.html#errores-comunes-y-buenas-prácticas",
    "title": "8  Pruebas chi cuadrado",
    "section": "8.20 Errores comunes y buenas prácticas",
    "text": "8.20 Errores comunes y buenas prácticas\nAunque la prueba chi cuadrado es simple y poderosa, también es fácil de malinterpretar. Acá te dejamos una lista de errores típicos, con sus respectivas recomendaciones.\n\n8.20.1 ❌ Error 1: Usar chi cuadrado con frecuencias esperadas muy bajas\nLa prueba chi cuadrado asume que las frecuencias esperadas no son demasiado pequeñas. Si hay celdas con valores esperados menores a 5, especialmente en tablas pequeñas, el test puede volverse poco fiable.\n✅ Buena práctica: Si tenés celdas con esperadas &lt; 5, intentá agrupar categorías o considerá una prueba exacta (como la de Fisher).\n\n\n8.20.2 ❌ Error 2: Interpretar el valor p como tamaño del efecto\nUn valor p bajo te dice que la diferencia es improbable bajo la nula, pero no te dice cuán grande o importante es la diferencia.\n✅ Buena práctica: Complementá la prueba con una visualización de las discrepancias y, si es posible, medidas de efecto (como el V de Cramer para tablas grandes).\n\n\n8.20.3 ❌ Error 3: Leer mal las proporciones condicionales\nA veces se malinterpreta una tabla normalizada por filas como si fuera por columnas, o viceversa. Eso puede llevar a conclusiones completamente erróneas.\n✅ Buena práctica: Siempre chequeá qué representa cada celda: ¿proporción dentro de qué grupo? Acompañar las tablas con gráficos ayuda muchísimo.\n\n\n8.20.4 ❌ Error 4: Pensar que una relación estadística implica causalidad\nLa prueba chi cuadrado detecta asociación, no dirección ni causalidad. Dos variables pueden estar relacionadas por muchos motivos.\n✅ Buena práctica: Interpretá los resultados con cautela. Preguntate siempre qué otra variable podría estar influyendo.\n\n\n8.20.5 ❌ Error 5: Usar chi cuadrado para variables que no son categóricas\nNo tiene sentido aplicar esta prueba a datos continuos discretizados sin justificación, o a variables numéricas que deberían analizarse con una prueba de medias.\n✅ Buena práctica: Reservá chi cuadrado para variables nominales u ordinales con pocos niveles. Para otras situaciones, hay mejores herramientas."
  },
  {
    "objectID": "06b-chi_cuadrado.html#lo-que-aprendimos",
    "href": "06b-chi_cuadrado.html#lo-que-aprendimos",
    "title": "8  Pruebas chi cuadrado",
    "section": "8.21 Lo que aprendimos",
    "text": "8.21 Lo que aprendimos\nEste capítulo fue una expansión natural del anterior. Aprendimos a aplicar la lógica de la inferencia —comparar lo que observamos con lo que esperaríamos por azar— al mundo de los datos categóricos.\nVimos que hay dos grandes usos de la prueba chi cuadrado:\n\nPrueba de bondad de ajuste ¿Una sola variable se reparte como debería?\nPrueba de independencia ¿Dos variables categóricas están relacionadas?\n\nEn ambos casos:\n\nCalculamos las frecuencias esperadas bajo la hipótesis nula.\nComparamos esas esperadas con las observadas.\nSumamos las diferencias, ponderadas por el tamaño esperado.\nObtenemos un valor \\(\\chi^2\\).\nY con ese valor, consultamos la distribución chi cuadrado para saber si ese resultado es raro o no bajo la nula.\n\nVimos que el valor p representa la probabilidad de ver un resultado así (o más extremo) si no hubiera relación. Y aprendimos a visualizar las diferencias celda por celda, para entender dónde están las mayores discrepancias.\nUna tabla puede contener una historia. La prueba chi cuadrado te ayuda a saber si esa historia es una coincidencia pasajera… o algo más profundo que vale la pena investigar."
  },
  {
    "objectID": "01-Science_Data.html",
    "href": "01-Science_Data.html",
    "title": "1  ¿Por qué estadística?",
    "section": "",
    "text": "1.1 Sobre la psicología de la estadística\nPara sorpresa de muchos estudiantes, la estadística es una parte bastante importante de la formación en psicología. Para sorpresa de nadie, la estadística rara vez es la parte favorita de esa formación. Después de todo, si realmente te encantara la idea de hacer estadística, probablemente estarías en una clase de estadística ahora mismo, no en una clase de psicología. Así que, como era de esperar, hay una buena parte del estudiantado que no está muy feliz con el hecho de que la psicología incluya tanta estadística. Por eso, me pareció un buen punto de partida responder algunas de las preguntas más comunes que suelen tener sobre este tema…\nUna parte importante del problema tiene que ver con la idea misma de estadística. ¿Qué es? ¿Para qué sirve? ¿Y por qué los científicos están tan obsesionados con ella? Son todas buenas preguntas. Empecemos por la última. Como grupo, los científicos parecen estar extrañamente empeñados en hacer pruebas estadísticas de todo. De hecho, usamos estadísticas tan seguido que a veces se nos olvida explicar por qué lo hacemos. Es casi un dogma entre los científicos —y especialmente entre los de las ciencias sociales— que los resultados no son confiables hasta que hacemos estadística. A los estudiantes de grado se les podría perdonar por pensar que estamos completamente locos, porque nadie se toma el tiempo de responder una pregunta muy simple:\nEs una pregunta ingenua en cierto sentido, pero muchas de las buenas preguntas lo son. Hay muchas buenas respuestas, pero para mí, la mejor es muy simple: no confiamos lo suficiente en nosotros mismos. Nos preocupa ser humanos, y estar sujetes a todos los sesgos, tentaciones y debilidades que eso implica. Gran parte de la estadística funciona como una especie de red de seguridad. Usar el “sentido común” para evaluar evidencia implica confiar en las corazonadas, en los argumentos verbales y en el puro poder del razonamiento humano para llegar a la respuesta correcta. La mayoría de los científicos no cree que ese enfoque funcione muy bien.\nDe hecho, pensándolo bien, esto suena bastante a una pregunta psicológica. Y como trabajo en un departamento de psicología, parece buena idea profundizar un poco más. ¿Es realmente razonable pensar que el “sentido común” es confiable? Los argumentos verbales tienen que construirse con lenguaje, y todo lenguaje tiene sesgos: algunas cosas son más difíciles de decir que otras, y no necesariamente porque sean falsas (por ejemplo, la electrodinámica cuántica es una buena teoría, pero difícil de explicar con palabras). Las corazonadas no están diseñadas para resolver problemas científicos, sino para manejar inferencias cotidianas —y considerando que la evolución biológica es más lenta que el cambio cultural, deberíamos decir que están diseñadas para resolver problemas de otro mundo, distinto al que vivimos ahora. Más fundamentalmente, razonar con sentido común requiere hacer “inducción”, es decir, conjeturas razonables y generalizaciones más allá de la evidencia inmediata. Si creés que podés hacer eso sin verte influenciado por distracciones o sesgos, bueno… te puedo vender el obelisco. De hecho, como veremos en la siguiente sección, ni siquiera podemos resolver problemas deductivos (que no requieren conjeturas) sin que nuestros sesgos previos se metan en el medio.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>¿Por qué estadística?</span>"
    ]
  },
  {
    "objectID": "05-Foundation_Inference.html#breve-repaso-de-los-experimentos",
    "href": "05-Foundation_Inference.html#breve-repaso-de-los-experimentos",
    "title": "6  Fundamentos de la inferencia",
    "section": "6.1 Breve repaso de los experimentos",
    "text": "6.1 Breve repaso de los experimentos\nEn el capítulo uno hablamos sobre métodos de investigación y experimentos. Los experimentos son una manera estructurada de recolectar datos que puede permitir inferencias sobre causalidad. Si quisiéramos saber si algo como ver videos de gatos en YouTube aumenta la felicidad, necesitaríamos un experimento. Ya vimos que simplemente buscar un montón de personas, medir cuántas horas ven videos de gatos y su nivel de felicidad, y correlacionar ambas cosas no permite inferencias causales. Por ejemplo, el flujo causal podría estar invertido: tal vez ser feliz causa que la gente quiera mirar más videos de gatos. Necesitamos un experimento.\nUn experimento tiene dos partes: una manipulación y una medición. La manipulación está bajo el control de quien realiza el experimento. Las manipulaciones también se llaman variables independientes. Por ejemplo, podríamos manipular el tiempo dedicado a mirar videos de gatos: 1 hora versus 2 horas. La medición es el dato que se recolecta. Podríamos medir cuán feliz se siente una persona después de ver videos de gatos, usando una escala del 1 al 100. Las mediciones también se llaman variables dependientes. Entonces, en un experimento básico como el que estamos considerando, recogemos medidas de felicidad de personas asignadas a una de dos condiciones experimentales definidas por la variable independiente. Supongamos que realizamos el experimento con 50 participantes. A 25 personas se las asigna al azar para ver 1 hora de videos de gatos, y a las otras 25 se las asigna al azar para ver 2 horas. Luego, medimos cuán felices se sienten al final del experimento.\n¿Qué deberíamos mirar en los datos? Si mirar videos de gatos causara un cambio en la felicidad, entonces esperaríamos que las medidas de felicidad de las personas que vieron 1 hora de videos de gatos fueran diferentes de las medidas de aquellas que vieron 2 horas. Si mirar videos de gatos no produjera ningún cambio en la felicidad, entonces no deberíamos encontrar diferencias en las medidas de felicidad entre las dos condiciones. Las fuerzas causales provocan cambios, y el experimento está diseñado para detectar ese cambio.\nAhora podemos plantear una gran pregunta general: ¿cómo sabemos si los datos cambiaron entre condiciones? Si podemos estar razonablemente seguros de que hubo un cambio entre condiciones, entonces podemos inferir que nuestra manipulación causó un cambio en la medición. Si no podemos estar seguros de que hubo un cambio, entonces no podemos inferir que la manipulación haya causado una diferencia. Necesitamos construir herramientas que nos permitan detectar cambios, para poder reconocer un cambio cuando ocurra.\n“Pará un poco, si solo estamos buscando un cambio, ¿no sería fácil ver eso mirando los números y viendo si son diferentes? ¿Qué tiene de difícil eso?” Buena pregunta. Ahora necesitamos hacer un desvío. La respuesta breve es que siempre va a haber cambio en los datos (recordá la varianza)."
  }
]